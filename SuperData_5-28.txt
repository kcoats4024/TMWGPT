Video ID: TwPHTdfRtO8
Do you need to integrate industry standard communication protocols in your project? If so, our SCADA Data Gateway product can help. The SCADA Data Gateway is a Windows application. It's compatible with embedded and industrial PCs, as well as desktop PCs and servers. It can serve as an OPC driver, communicating with external OPC clients and or servers. It can also act as a protocol translator or as a data concentrator. The SCADA Data Gateway supports bidirectional data translation. For example, it can pass data from an outstation to a master, as well as send commands from that master back to the outstation. It also supports expressions and equations to perform data transformation. For example, if a device reports integer values, the SCADA Data Gateway can scale those values and report them as floating point numbers in engineering units. The built-in protocol analyzer display simplifies debugging, and the event logger makes it easy to see your data changes. An ODBC client component is available for the SCADA Data Gateway, providing access to any database management system, such as SQL or Microsoft Access. The Insight component adds data visualization in a scripting tool through the SCADA Data Gateway's OPCDA interface. The SCADA Data Gateway supports a variety of OPC options, including both OPC UA and OPC Classic. OPC UA allows platform-independent interfacing with SCADA systems. Support in the gateway includes client, server, and alarming conditions. OPC Classic support includes OPC Data Access, OPC Alarm and Events, and OPC XML Data Access. The SCADA Data Gateway also supports secure authentication for DNP3 and IEC 60870-5, as well as TLS security for DNP3, IEC 60870-5, and Modbus. The SCADA Data Gateway also supports IEC 62351 security for IEC 61850 and IEC 60870-6, TASI-2 ICCP. It also supports security for OPC UA, as spelled out in the OPC UA specification. For more information, please visit the SCADA Data Gateway page of our website at www.trianglemicroworks.com slash products slash SCADA-Data-Gateway.


Video ID: SdEKlS9erBQ
This video demonstrates the flexibility of the Triangle Microworks IEC 61850 source code libraries. These libraries are designed from the ground up to be very efficient, and to easily scale from low-cost embedded systems to much larger processor-intensive systems. In this demo, our IEC 61850 source code library is running on a low-cost ARM processor, in this case, the Texas Instruments BeagleBone. We designed a CAPE, or I-O card, to plug into the BeagleBone. The CAPE has switches to simulate operating a breaker, and LEDs to show its status. We'll be using two BeagleBone boards, each with a CAPE. On one board, the lower switch puts the breaker into fail test mode. A second switch signals the board that an overcurrent condition has occurred. The board responds by simulating a breaker failure and sending goose messages. The second board simulates the next breaker up the line. This board receives the goose messages, recognizes the downstream breaker has failed, and opens its breaker. The open condition is shown via the LED lights. Here I have Hammer, the 61850 client and goose subscriber component of the 61850 test suite. In this demo, Hammer has subscribed to goose messages from the BeagleBone boards. I will clear the goose message display, and then we'll run the demo. First, I'll push the button, which will tell the device to open the breaker. However, it will simulate a breaker failure. It will turn on the fail light, and then on this board, we'll see the upstream breaker open as it receives and processes the goose messages. So here we go. Notice the fail, and the breaker opened. And if we look at Hammer, we can see the goose message being sent from the board that caused the upstream device to open the breaker. So this demo showed a simulation of a breaker fail condition, sending goose messages, receiving those goose messages and processing them, and responding to a downstream breaker fail condition, all running on our IEC 61850 source code library on a low-cost ARM processor.


Video ID: Un2gu3xR4m0
The system-specific description files, or SSD files, are typically created by a system specification tool, and it�s used to describe the overall substation specification to the system configuration tool. Basically, this is the description of the single line diagram of the substation and the required logical nodes to implement the functionality needed.


Video ID: oD9J9D_6NIA
In this video, we'll look at an Insight display. To create this display, we copied the display used in the Distributech 2015 demo, which is shown in other videos. We modified the display to remove the actual devices, that is, the two BeagleBone boards and anvil, and we also modified the text. Then we mapped the breaker failure fail indicators and breaker status indicators from the two binary inputs in the DTM DNP3 Masters instead of the IEC 61A50 clients. We also mapped the phase A current display from the analog point in the DNP3 Master. To change the mappings of these points, we can show a points list. By right-clicking on the DNP3 Masters folder and selecting Show Point List, we can open a tab that has all of the points from all of the Masters. Next, we select an insight element, such as a bar graph, right-click, and Show Properties. We can then drag a point from the Master to the points list to the property in Insight. Now the element is updated to display the value from the DNP3 Master. We can repeat this process for each of the breaker fail, breaker state, and phase A current elements. We'll use this display to monitor the points read by the DNP3 Masters. In the next video, we'll simulate data in the DNP3 Outstations.


Video ID: y89OI6ITjuk
The substation configuration description, or SCD file, is basically a combination of those two. It puts together the substation specification and the build-out of all the IEDs, and within a system configuration tool, this generates a full substation configuration description. This basically contains all IEDs, communications, configuration, and the substation description. It ties the IEDs into both the communications infrastructure and the power system description. It's used then to inform a number of tools, IED configuration tools, typically database tools and things in the substation and up in the control center eventually. This basically is the description of the entire substation.


Video ID: Q7JdL8uXiQw
To add an OPC client, right-click on Gateway and select Add OPC Client. Choose the node name or IP address. If you'll be talking to a server on the same machine, you can use the internal loopback address of 127.0.0.1. Next, click Find the OPC Server and find the server you want to communicate with. You can double-click this server or click it and say OK. Next, enter an alias name and say OK.


Video ID: ztcFYubHjPY
In this final section, let's look at some of the benefits of industry standard protocols. Utilities benefit by selecting an industry standard protocol in many ways. One way is by breaking their dependence on a single vendor. The utility is now free to select products based on the performance, quality, and price, not just on which vendor has the best protocol. There are other benefits too. For example, the field technicians only need to learn one protocol, which can reduce training costs. Industry standard protocols typically have a wide number of third-party support services and test tools available as well. The utilities can participate directly in the evolution of the protocol by participating in the users group. Some utilities have even chosen to have representation on the technical committee. In this way, they can ensure that the protocol continues to evolve to meet their market needs. Vendors also benefit from using industry standard protocols. They can avoid non-recurring engineering costs to add new or update existing protocols for each project. They also benefit from using well-documented, proven protocols. Vendors can also leverage others' experience by participating in the development of a common protocol instead of investing in a proprietary protocol. Many vendors have representatives on the DNP3 technical committee for this reason. By supporting industry standard protocols, vendors can be assured of a large utility client base. Vendors can also benefit from having readily available support services and test tools, which lower their development and maintenance costs. One of the key benefits of industry standard protocols is interoperability. Interoperability allows utilities and vendors to focus on performance, quality, and price rather than on the protocol. As with any protocol, equipment vendors develop products with the goal of selling them to utilities and other customers. With DNP3, the technical committee clarifies the specification and develops conformance test procedures. The end-user utility may specify in its request for quotes or proposals that the supplied equipment must have successfully completed the conformance test procedures. Vendors can use the conformance test procedures to test their products and claim DNP3 compliance. The DNP user group website lists products that have completed self-testing and or independent testing for compliance. The independent test lab may issue a certificate of compliance. Interoperability requirements are defined in Volume 8 of the DNP3 specification. Each device should have an associated DNP3 device profile document. This document has typically been provided in a printed format. However, going forward, devices should supply the document using the defined XML schema. An XSLT document is available to provide an HTML version of the device profile that is more human readable and can be printed. The device profile document provides configuration information such as protocol capabilities supported and device settings. The XML-based device profile document facilitates mapping the device configuration to IEC 61850 object models, too. The device profile document provides a point list, which includes scaling, units, and a text description. It also includes an implementation table, which indicates the supported data types, function codes, and qualifier codes. Certification testing is designed to promote interoperability. This allows utilities to use equipment from different manufacturers with the high degree of confidence that the equipment will work together. The DNP3 conformance test procedures are currently approved for subset level 1 and subset level 2 outstations. Conformance tests for master stations are currently under development. Vendors may self-test their devices, or they may have an independent test lab perform the conformance tests. The DNP user group website lists devices and their certification status. An end user may opt to specify that self or independent testing is required of the equipment to be used in their networks. Any vendor may use the conformance test procedure to self-certify their devices. The Triangle Microworks Communication Protocol test harness supports an optional module that automates the conformance test procedure. This screenshot shows an example of this module. Individual tests are shown down the left, and the right-hand column shows the current status of the testing. The test results are color-coded in both columns to highlight tests that have passed or failed. This slide provides a summary of normal DNP operations. On startup, the master clears the device restart IIN indication in the outstation and issues a class 1230 poll. This poll will retrieve all buffered events and the current static data in order to initialize the master's database. Periodically after that, the master issues class 1, 2, and 3 event polls, either together or individually at different rates. If the outstation sets its need time IIN bit, then the master should issue a write-time request. And the outstation should request confirmation for any messages that contain events. DNP 3 provides the features required of a SCADA communications protocol. It provides standardized rules for data transfer and promotes interoperability among vendors. It ensures reliable data transfer using CRCs. It provides useful features such as timestamps and freeze operations, provides data quality indicators, and provides features to detect or prevent unauthorized use or monitoring of data. It also minimizes protocol overhead. DNP 3 is well established in the electrical utility industry and is becoming established in other industries as well. It has an active user group that is eager to enhance the protocol to meet new requirements while maintaining backwards compatibility. The DNP users group maintains an active website. This site contains all the protocol documentation and meeting minutes. It also has a list of equipment that supports DNP 3 and indicates certification status of that equipment. Users may join forums to have discussions with other users as well. The DNP user group holds a general meeting each year at the Distributech conference. This presentation provided a high-level overview of DNP 3. For more information, consider joining the DNP user group and downloading the complete set of documentation. Once again, the DNP user group can be found at www.dnp.org. You may also wish to study the IEEE P1379 document, which recommends DNP 3. You may also consider joining the SCADA mailing list. While not specifically focused on DNP 3, the SCADA mailing list does provide worldwide collaboration on SCADA topics. Finally, feel free to contact us at Triangle Microworks at www.trianglemicroworks.com.


Video ID: qT4l75dADsY
Three protocols are also defined for use on the process bus. The Sample Values Protocol is a specialized link layer protocol for use in distributing the digital samples across the land. The Goose GSSE is again in the same link layer protocol used on the station bus for substation events, and TimeSync can also be used on the process bus as well. These definitions all occur in a document 61850 part 9-2 that defines the use of these.


Video ID: TJgB-Qf9700
DNP3 is a layered protocol. However, rather than adhering to the OSI or Open System Interconnection seven-layer protocol, DNP3 adheres to a simplified three-layer standard proposed by the IEC, the International Electrotechnical Commission, for more basic implementations. IEC calls this the Enhanced Performance Architecture or EPA. However, DNP3 enhances the EPA by adding a transport function that allows for message segmentation. Each layer in the stack communicates with the same layer in another device, relying on lower layers to provide more primitive functions. When sending, each layer receives data from the layer above it. Each layer then adds more information in order to allow the equivalent layer in the receiving device to properly process the message. When receiving, each layer examines the data for the specific information for that layer and processes the message appropriately. That layer's information is then removed from the message and the message is passed up the stack to the next higher level for further processing. The physical layer is primarily concerned with the physical media over which the protocol is being communicated. For example, it handles the state of the media, clear or busy, and synchronization across the media, starting and stopping. Most commonly, DNP is specified over a simple serial physical layer, such as RS-232 or RS-485, using physical media such as copper, fiber, radio, or satellite. More recent applications have implemented DNP3 over an Ethernet connection.


Video ID: lSiwvgYJxjM
Now let's look at how to map data between master and slave components. This example will show mapping points from an OPC client to an IEC 60870-5 slave. However, the process is the same for mapping between any master and any slave component. To map a master tag to a slave component, expand the slave component to show the data types. Then click on the master tag and drag it to the desired data type. The map slave point dialog will open. Set the desired information object address, or IOA, in this dialog. You may also set options in the slave point in this dialog.


Video ID: ZKLKrgMr5n4
Once you've added the OPC client and connected to a server, you need to add your data points. To do this, right-click on the OPC client and select Add OPC Client Item. This then opens a browser window which allows you to browse the server and find the points that you want. Once you find those points, you can double-click them, or you can click and say Add Item. When you've added all the items you want, select Done.


Video ID: x1OhgkvpD5I
This video series provides an overview of the new SafeNet-based licensing scheme, which is being used on Triangle Microworks applications. There are several benefits to the new licensing scheme. When requesting a license, you have 24-hour access to an online licensing service to manage licenses. This includes license activation and transfer without needing to contact Triangle Microworks support. The new licensing scheme greatly reduces the need for USB licenses that have presented a few issues over the past, including theft, loss, and some reliability problems. Virtual machines can now be licensed directly without requiring a USB license. Licenses can now be transferred from one machine to another without the need to contact Triangle Microworks support. A new network license option is available, which allows you to share licenses across multiple users without needing to pass a dongle or USB license back and forth between users. The network license allows a license to be checked out for remote use, for example, if a field technician needs to check out the product to carry off-site and work without internet access. Network licenses are very easy to set up. This video series provides an overview of Triangle Microworks licensing, including how to install a new license, both on a PC with internet access and on a PC without internet access, how to transfer a license to a different PC, and how to upgrade from previous versions, including how to select the proper license type and actually performing the upgrade. Visit www.trianglemicrosoft.com to learn more.


Video ID: ghUfxbSV-m4
Welcome to the Triangle Microworks web-based DNP protocol training course. This course provides an introduction to the DNP3 protocol. Although the course is designed to be completed in sequence, you may use the navigation bars in the left pane to move forward or backward to a topic of interest. You may also click on hyperlinks to advance to a related topic. By the end of this course, you will understand the purpose of a SCADA communication protocol, understand the role of the DNP user group, have a high-level understanding of DNP, understand the fundamental operating philosophies of DNP, know how to identify and interpret DNP messages, and be able to use test equipment such as the Triangle Microworks communication protocol test harness to analyze and troubleshoot DNP communications, and understand recently added features and know how to use them. This course begins with an overview of the purpose of a SCADA communication protocol. It then presents an overview of DNP3 fundamentals, followed by an overview of the protocol itself. This is followed by a look at commonly used DNP messages. The course then presents a more detailed look at the application layer, the transport layer, and the link layer. Finally, benefits of industry standard protocols are discussed. Throughout the course, the communication protocol test harness will be used to demonstrate aspects of the protocol.


Video ID: pKfPu6ruDt0
Let's begin by taking a look at how to configure the master components. This section describes how to add and configure a DNP3 master, or MDNP, channel. To add a DNP3 master channel, right-click on Gateway and select the appropriate channel type. You can add an MDNP TCP IP channel, a DNP3 UDP TCP channel, or a serial channel. In this example, we'll add a TCP IP channel. After selecting Add TCP Channel, the Add a TCP Channel dialog opens. Select MDNP as the protocol, and the IP port number will change to 20,000, the port reserved for DNP. Enter an alias name and click OK. Next, the MDNP Session Editor will automatically open. Set the local, that is, the SCADA data gateway link address, and the remote, or outstation, link address. Then, configure the Link Confirm mode. The default value of Never can usually be used. Then configure the Absolute Response Timeout. This timer starts when a message is queued to be sent. The value is specified in milliseconds, so a value of 30,000 represents 30 seconds. Finally, set the Auto-Request mode. The default values can usually be used. To modify the Auto-Request mode, select the dot-dot-dot box, and a dialog will open to allow you to select Auto-Request options. The Auto-Request options include a variety of useful functions, such as clearing the Restart IIN bit when it is received, issuing an Integrity Poll upon receipt of the Restart IIN bit, or issuing an Integrity Poll when Buffer Overflow IIN bit is set. Finally, add the data types that you'll need. If you have just added the channel and session, the Add Data Type dialog should automatically appear. Otherwise, right-click on the session and select Add Data Type. You can select a data type from the drop-down menu in the Add Data Type menu.


Video ID: 0u4aKZSmDYw
This playlist will introduce you to the fundamentals of TestSuite Pro, so let's start at the beginning. What is TestSuite Pro? TestSuite Pro is a Windows application with the most comprehensive set of tools in the market today to cover all of your IEC 6150 testing and troubleshooting needs. 61850 TestSuite Pro was designed to make IEC 6150 easier to understand and help users maximize the benefits of 61850. TestSuite Pro can be used during the conceptual phase, the factory acceptance phase, commissioning, or ongoing maintenance in a substation, as seen here. For more information on this, check out the video in the description below. The interface has been designed to be intuitive, making it easy to filter and access data from IEDs and identify which IEC 61850 services are available for testing. It also contains the most comprehensive set of tools for both experts and new users. The many tools are useful for lab testing, troubleshooting, commissioning, and maintenance testing. Engineers that support IEC 61850 substations can start with the basic testing, like connection to IEDs and viewing IED data through reports and goose. Troubleshooting allows for the engineers to drill down to specific types of testing, such as debugging goose with a sniffer, event viewer, and publisher tool. 61850 TestSuite Pro also provides substation engineers and technicians a way to visualize tests, resulting in intuitive display. Data from multiple IEDs in a substation can be shown from goose, reports, and polling. The data is shown in a single line diagram format that is familiar to substation engineers. Data can be filtered to quickly find the type of data needed for a particular testing job. For example, protection data from goose messages can be shown for a relay test. Another example is finding alarm data for reports to help you escape a testing. Data that is needed for a test can be found quickly and then added to a custom display to view the values and the events as the test is carried out. 61850 TestSuite Pro also provides a logic analyzer to visualize signals as they are sent around your substation. Visualizing the timing in which events occur is a powerful troubleshooting tool for device behavior and performance.


Video ID: HBZjc5r7vws
Welcome to this overview of IEC 61850. I'm your instructor, George Schimmel. I've been active for over 15 years in these areas revolving around IEC 61850. I was active in the original EPRI-sponsored UCA initiative, which led to much of the underlying design work behind this. I've been active in various IEC technical committees, IEC TC-57 working groups, which defined 61850, including I was the convener of TC-57 working group 11, which is the station bus. I've been a member of now the TC-57 working group 10, which is now the place where these documents are published from. I was also the software architect behind the Triangle Microworks IEC 61850, GOOSE, GSSC, and ICCP source code libraries.


Video ID: cuMISvIJyiU
This iconic view shows how the unbuffered reporting model works. On the left we have data objects within a data set, which is the oval, circle, triangle, square. Within that oval shows the actual data objects that are named within the data set. As changes occur to that data, we see a flow from right to left of the values that are occurring as the change events occur. There's some logic within the 61850 model that does filtering and formatting of those changes. That filtering and formatting process turns that sequence of changes into a sequence of messages. Each message can contain the value of one or more changes, essentially the values that have changed just within that whole data set at one given point in time. So you'll see that the first two changes in the sequence are reflected in the first message. The next change goes in a separate message. The formatting of that data and that sequence of report messages is controlled by a report control block that sets the parameters for the filtering and formatting process. That report control typically can be get and set, in other words, read and written by the client to be able to change the report control. Notice that, again, a given report control can be reporting one data set. The messages go basically to the client that subscribed to it. You can have multiple instances of report controls and multiple instances of data sets or data objects within multiple data sets. So the actual report can be going to multiple places at once and may be formatted differently in the messages if each report control block has different parameters.


Video ID: QTL1x6wbzwA
Welcome to the Triangle Microworks free Introduction to IEC 61850. This course contains selected slides from our IEC 61850 web-based training in order to provide both an introduction to IEC 61850 and a sample of the complete course. If you find these sample slides to be an effective training medium for an introduction to IEC 61850, consider purchasing the full web-based training course. This course consists of excerpts from the full IEC 61850 overview course. The course is presented by George Schimel, who will introduce himself on the next slide.


Video ID: GD1SPo-hmGA
D&P 3 is an open, intelligent, robust, and efficient modern SCADA communication protocol. It can request and respond with multiple data types and single messages, segment messages into multiple frames to ensure excellent error detection and recovery, include only changed data and response messages, assign priorities to data items and request data items periodically based on their priority, respond without request or send unsolicited responses, support time synchronization and a standard time format, allow multiple masters and peer-to-peer operations, and allow user-definable objects, including file transfer.


Video ID: D24fjiv5pQ4
This video demonstrates how you can easily build or delete both persistent and non-persistent datasets using Hammer, the 61850 client component of the 61850 test suite. Here I have Hammer running and connected to a client, so I can expand this tree, right-click on Datasets and choose Add Dataset. This opens the Create Dataset dialog, which then allows me to choose the elements that I want in this dataset. Now there are several options here. For example, I can drill down into the tree and I can pick individual elements of an array. I can expand the array and pick, for example, the CVALs, or go all the way down just to pick up. So a lot of flexibility here. Of course, most datasets will be quite a bit simpler than this, but if you do have arrays, this particular example, an array of structures, we can pick certain elements from the structure and certain elements from the array that we want to be in the dataset. Once we've created the dataset, we can name it and save the dataset. Now when we expand the datasets, we see this new dataset being shown. Now we can also create application-specific or non-persistent datasets. To do that, we can track the tree, right-click in this empty space, and select Add Non-Persistent Dataset. Once again, the Create Dataset dialog opens and we can choose the elements that we want to include in this dataset. Deleting datasets is also quite easy. We simply find the dataset we want to delete, right-click it, and delete it. As you can see, Hammer offers a great deal of flexibility. It supports array indexing and also makes it very easy to build and delete both persistent and non-persistent datasets.


Video ID: n8Fo1T1-EOg
When you purchase an application from Triangle Microworks you will receive a product key. This product key must be activated on the PC that will hold the license for the application. The product key can be activated on the target PC through an internet connection or by using a manual offline process. In this video we'll demonstrate how to activate a license on a PC with an active internet connection. A separate video will show how to activate the license if you do not have an internet connection. If your product is not currently licensed when you launch it you will receive a license error dialog. Click on open license manager to open the license manager dialog. In this dialog we'll be able to enter the new product key. If you have a currently licensed product, for example if you're using the 21 day evaluation version, you can reach the license manager window by clicking help, license. In the license manager window click on activate product key and in this window activate online. Now you can paste in the product key that was sent to you by Triangle Microworks when you purchased the product. Select the correct radio box, in this case the product key is for a new license, and click submit. I now have a dialog that shows the license was successfully installed. However I must restart the application in order for the changes to take effect. So now I can restart the test harness. And when I go to help license I can see that I have successfully installed a permanent license and I have a current maintenance plan. So that completes the license installation process when you do have internet access. The next video will show how to install the license when you don't have internet access.


Video ID: s7t-OCkihRk
In this video we're going to look at the built-in front panel simulators that ship with the Communication Protocol Test Harness. The Test Harness ships with three DNP front panel simulators and one Modbus front panel simulator. So let's begin by looking at the DNP3 front panel simulators. To open a simulator we just open a channel. We're going to select DNP Slave and then on the session we're going to choose the appropriate simulator. We'll start with the generic slave and open. This opens the generic slave which has checkboxes to control the binary inputs, has sliders for the analog inputs. Binary outputs and analog outputs can be controlled from a master and it has some built-in counters that simply count the number of output commands that have been issued. Next let's look at the ABB PCD front panel simulator. Once again we open a DNP Slave. This time we choose the ABB PCD simulator and here's the front panel. The front panel has hotspots for buttons that work so we can open and close the relay, remote block, ground block, and reclose block. We have sliders for the voltage and the current and notice if we slide the current up too much there's some simulation built in. When the current gets too high the relay opens. Next let's look at the Schweizer 351S relay. Again we open a DNP 3 Slave. This time under the database we'll choose the 351S and here's our front panel. Like the ABB simulator with the trip and close buttons work. The status shows on the little LED there. We can enable ground, reclose, and remote. This also has the simulation so if we move the current up too high the breaker will open. And finally let's look at the Modbus simulator. This time we'll open a Modbus Slave and we'll choose the generic Slave front panel simulator. Like the DNP 3 simulator we have checkboxes for the discrete input, sliders for the input registers, and coils and holding registers can be controlled from the master. All of these simulators ship with source code so you are free to put in your own picture of the front panel, move the hotspots to the appropriate place for the buttons and LEDs, add any additional simulations you want, and make a nice front panel simulator of your own device.


Video ID: pJ-_M45owsw
The 61850 communications architecture is defined in terms of two buses. These are logical buses. They may in fact share the same media and infrastructure. The station bus is used for high-level communication between IEDs and back to the substation host, which would be the pathway back to the control center. The process bus is used for streaming data from things like current transducers and voltage transducers. These samples can then be shared by the various IEDs in the substation.


Video ID: SznUIGjTuAc
Congratulations, you have completed the free high-level overview DNP3 course. For a more in-depth look at the DNP protocol, consider taking our cost-effective web-based training course. For information on this course, please visit our website at trianglemicroworks.com. You may also contact us by email at sales at trianglemicroworks.com, or by phone at Country Code 1, Area Code 919-870-5101. Thank you.


Video ID: 1ZP-ide7O00
When analyzing or troubleshooting DNP3 networks, it's often helpful to view a protocol analyzer. DTM can easily show the protocol analyzer for a device or for multiple devices. First, let's look at the protocol analyzer for a single master. To do that, we right-click on the device and select Show, Protocol Analyzer. A new tab opens with the Protocol Analyzer display. If we look at the filter settings, we can see that DTM has already configured the filter to show only this device. Similarly, we can view the analyzer for multiple devices by right-clicking on the folder and selecting Show, Protocol Analyzer. Once again, a new tab opens with the Protocol Analyzer. If we look at the filters, we can see that DTM has automatically configured the filters to show each of the devices that are in the folder. Note that if you want to look at several devices, it may be easier to just keep one tab open with the Protocol Analyzer and use the filter to select the devices of interest. Now that we have a Protocol Analyzer configured, we can easily monitor communications. In the next video, we'll show how to configure polling and how to send other DNP3 commands.


Video ID: iSKI8d5_ik8
The XML device profile can be used when mapping DNP3 to IEC 61850 object models. Standard models allow a consistent database interface in a multi-vendor product. Device profile will include fields for mapping DNP3 point values, flags, and time stamps to IEC 61850 data attributes. This picture shows the Triangle Microworks ICD editor. The left column shows an IEC 61850 ICD file, and the right column shows a DNP3 XML file. This example shows mapping an analog input, total watts, from DNP3 to the total watts magnitude data attribute. Once it is mapped, it appears in section 2 of the device profile document, and the ICD file is no longer necessary. The properties window shows the mappings. Note how a 61850 data attribute can be mapped to the time stamp, quality, or value on a DNP3 point. IEC 61850 configuration values, such as the multiplier or units, can be set to a constant, since this information does not need to be transmitted by a DNP3 data point. In IEC 61850, the CLN class is mandatory for all logical nodes. Attributes such as health, behavior, and mode were created as part of the IEC 61850 object model, and typically are not included in an existing DNP3 database. Therefore, this information will not be available through the real-time DNP3 data mapped to IEC 61850. If a DNP3 database is created specifically to be mapped to IEC 61850, then it may be designed to obtain 100% coverage. This diagram shows a typical substation network architecture. At the bottom, we see DNP3 and IEC 61850 devices sharing the same Ethernet connection in the substation RTU. The substation RTU is connected to the control center via DNP3 through a slow connection. The DNP3 XML device profile configures the mapping from IEC 61850 to DNP3 in the substation RTU. This same DNP3 XML device profile can be used by the control center to represent the data received via DNP3 as an IEC 61850 object model to the user or other applications running in the control center. The substation RTU can usually read DNP3, IEC 61850, and a range of other protocols, but if other devices in the substation need data from the DNP3 device, a protocol converter can be configured by DNP3 XML to translate from DNP to IEC 61850, instead of IEC 61850 to DNP3 as just described. It is generally not feasible to replace or upgrade all equipment in a substation at once. Thus, we need a strategy for migration that allows different protocols to coexist. If there are no IEC 61850 devices in the substation, you can still represent data as an IEC 61850 object model with DNP3 XML in the substation RTU and or the control center. If we want to display or organize the master database by IEC 61850 object model, in the past we would have used a protocol converter. DNP3 XML gives an equivalent solution without adding extra hardware to the system by providing the mapping as an extra layer in the software.


Video ID: IN7eV4vEvRU
Hammer, the 61.850 client component of the 61.850 test suite, makes it very easy to work with report control blocks. Let's expand the tree and look at a report control block. And notice that we can see all of the properties or attributes of that control block in one view. We can right-click on the control block and enable it. Once it's enabled, the prompt changes to disable. We can change attributes, but first we do need to disable the control block. And then we can double-click and type in a new value. For attributes that are fields, such as trigger ops, we can double-click and just change the checkboxes. That also works for the fields like op fields. We can also change or set the active dataset. To do that, we right-click on the report control block, choose Select Dataset Reference, and choose the appropriate dataset. Bear in mind that Hammer is a test tool and therefore will let you attempt to do things that are not valid according to the specification. For example, if we enable the report control block and then try to change a field, we'll receive a message telling us that the field could not be changed because it is temporarily unavailable. So as you can see, Hammer makes it very easy to work with report control blocks. We can enable and disable report control blocks. We can set and change values. And we can change or set the active dataset. We can also attempt to do things that are not valid so that we can test the server and make sure that it rejects the command properly.


Video ID: Xy22rX_1R5E
The ID Capability Description files, or ICD files, are delivered from the vendor to the utility along with their equipment to describe the capabilities of the device and all the configuration parameters, object models, and all that sort of thing. They're typically developed with an ICD editor. We supply one with our software for developing these files, and they're used to communicate the information to the system configuration tool.


Video ID: r3jbrOyO1hI
The command window has a series of commands, each with an icon or a button. If you click the button, it executes that command. So for example, clicking the button next to Integrity Pull causes the Test Harness to execute that Integrity Pull. Now if we double-click the command name, the Test Harness opens a configuration window and we can customize this command. So for example, for the Report by Exception Data Pull, we can choose the qualifier code. We could, for example, only pull for 20 events or pull for all events and include or exclude certain data classes. We can also customize this window to automatically repeat the command. We can adjust the repeat interval using the up and down arrow or we can just type in the window and set the value. When we press Start Repeat, the Test Harness adds this progress bar and you can see the progress of the delay between each command.


Video ID: lHHjRzoFcdI
This video will be talking about the IEC 61850 engineering process and how it relates to our tools here at Triangle Microworks. The first TMW tool used in the 61850 engineering process is SCL Navigator. SCL Navigator is a Windows application for creating, viewing, repairing, and verifying IEC 61850 System Configuration Language files, or SCL for short. SCL Navigator contains a 61850 aware editor that provides a structured environment for compliant SCL file creation. SCL Navigator has three main uses in the 61850 engineering process. 1. Creating an ICD file. 2. Navigating the SCL file quickly and easily. 3. Finding and fixing SCL file problems. During the conceptual phase, when the design is being fleshed out and the system configuration tool has been used to create an SCD file, DTM can use this SCD file to simulate all of the IEDs in the substation. TestSuite Pro can use this SCD file to monitor and troubleshoot the behavior of the new substation design simulated in DTM. At factory acceptance testing, some of the simulated IEDs in DTM are replaced by actual IEDs in the lab. These IEDs have been programmed by their corresponding IED configuration tool with the same SCD file used by DTM to simulate the missing IEDs. The same TestSuite Pro tools and displays used in the conceptual phase are used to validate the behavior of the newly added IEDs. During commissioning or ongoing maintenance in the substation, simulation from DTM is no longer necessary. The same TestSuite Pro tools and displays used in the conceptual and factory acceptance phases are used to validate the behavior of the substation. For more information on this or any other questions about 61850, please contact us using the information in the description below. Thank you.


Video ID: AQNyltSR47s
Again, these abstract service definitions are intended to isolate the object models and the applications from the communication specifics. The idea here is that these common utility services like get value, set value, select before operate are common across a broad range of utility applications and they can be mapped to almost any kind of protocol you want to. This will, we hope in the future, this idea of future proofing that allows us to adopt new technologies without disturbing the whole thing, it basically builds in a migration strategy directly into the way that we've defined 61850. Part 7.2 of the document set, Abstract Communication Service Interface, defines the services and their parameters. It defines how to build logical devices out of logical nodes and it defines how to organize the data objects that are derived from the logical nodes. Part 7.2 defines the abstract models for all these different pieces, a server, associations and logical devices and nodes. It also talks about these other types of abstract services like substitution and sampled values, which again are common across a wide range of applications within the utility industry. This diagram shows the organization of the ACSI models and their scope. The server is basically represents a communications capability at some address, which you can down below it shows the types of services you do directly with a server, which is to be able to associate or make a connection with it, to be able to do time synchronization and to be able to do file transfers. These abstract services operate at the server level. Within a server are multiple logical devices. These may or may not be different physical devices behind the server, depending on how you organize as a gateway or whatever. Within each logical device, there is a set of logical nodes and at the logical node level, you have a set of control capabilities, substitution, settings, parameter settings, which operate on a set of data objects within the logical node. These data objects within the logical node can be organized into data sets. These data sets can be arbitrary types and organization of the data, but they define a group of data objects for the purposes of the reporting and logging services, GOOS, the generic object-oriented substation event, and sampled values. Data objects within 61850 represent the real data, what you�re really after within the logical node. Controls are voltage measurements, current relay settings, various controls, that sort of thing. They typically aggregate attributes of different types, so they�re structures, structures containing a set of measurements, for instance, the Y class, you know, for each of the different phase measurements. They typically have independent access control, and we have standardized naming and typing for all of them. Data sets associate a single name with a collection of data object references. The reference is in that a single data object can be in more than one data set at a time. When you access the single name, then you get the values of all the data sets, so you can do a get values and read all of the members, set value and sets all the members. Data sets can be predefined, meaning they�re static and defined at boot time in the device, or they can be dynamic, where a client can create a data set, either temporary or permanent data set within a server to set up a different collection. Data sets can also be interrogated, meaning you can get the, you can, there�s services available to read, to get the members of the data set, not the values, but the members to know what the data set members are. The control model in 61850 applies to specific object classes, so there�s a controllable single point class and double point class, there�s control step position, analog set points. The controls can be either immediate or time activated. Time activated controls include a time parameter that specifies when the control operation should take place sometime in the future. There�s several different possible mechanisms within the control model. The simplest one is direct control with normal security. With this type of control, the client sends a simple request and the server sends an operate response once the action is performed, with either success or failure on the response. SBO control with normal security, SBO stands for select before operate, and it�s a two-step control where the client first makes a selection request and then makes an operate request. Each of those are separately responded to by the server, and at the point that the operate request is made, the action at the server is initiated and the response merely says that the action has been started. It doesn�t really give you a close on it. The final, typically within our models, the final action, when the action is completed, the client should be looking for the status point that actually is monitoring that control to see whether it�s completed. The select before operate control with enhanced security closes that feedback loop. Essentially it�s a two-step, again, selection and response, then an operate request and a response that says the action�s been initiated, and then after that there�s a command termination that is returned from the server to the client that says the action�s completed. The sort of mental model of this is contact closure, that the operate request begins the contacts moving, but the command termination says when they�ve completed and they�re stable again.


Video ID: iSwSDe0XyCw
This section describes how to add an OPC client to the SCADA data gateway, how to connect that client to an OPC server, and how to subscribe to data tags. To add an OPC client, right-click on gateway and select add OPC client. To connect to the OPC server, select the node name or IP address of the OPC server. For the local machine, you can use the loopback address of 127.0.0.1. Then click find OPC server. Select the server in the select OPC server dialog, and click OK to close this dialog. Enter an alias name, and if desired, set the reconnect time, read properties time, and reconnect retry count. Then click OK.


Video ID: HG3JD5aXYD4
To add an IEC 60870-5-104 slave, first right-click on Gateway, and since 104 slaves are always over TCP IP, select Add TCP Channel. Next, select the session protocol, in this case, S104 for Slave 104. Set the IP address. This is the address of the device that the slave will connect to. In our example, we'll use the loopback address of 127.0.0.1. Select the mode, a slave should be a server, and the IP port number. The IEC 60870-5-104 specification specifies that the standard IP port number is 2404. Next, enter an alias name, and say OK. This opens the session window. You can change the values of T1, T2, T3, or the K and W values. In this example, we'll use the defaults. And finally, a sector window opens, in which you can set the sector address, or the ASDU address. In this case, we'll take the default of 3. Finally, you can add data types if you wish. In this example, let's add a single point information, and then we can add additional data types by expanding out to the sector, right-clicking, and saying Add Data Types. So let's add some additional data types.


Video ID: 63UyUUNq2p4
The SCADA data gateway includes a protocol analyzer to monitor and help troubleshoot communications. The protocol analyzer window displays data that is being sent or received by the SCADA data gateway. Starting from the left, we see the timestamp, assuming the timestamp checkbox is checked. This is the time the message was sent or received and is shown in the local PC's time zone. Next, we see a line composed of a less than or greater than character and a series of other characters. These characters form an arrow that points into or out of the channel name. Note that the channel name is specified when the channel is opened. It can also be modified later. If the arrow points to the left, away from the channel name, then the data are being sent or transmitted by the SCADA data gateway. If the arrow points to the right, into the channel name, then the data are being received by the SCADA data gateway. The characters indicate the protocol layer being decoded. The key for these characters is in the filter section above the protocol analyzer window. In this example, the arrow consists of equals characters, so this is an application layer message. To the right of the channel name, we see the decoded data, followed by the raw data.


Video ID: sj4Qlm_ZpAk
Welcome! You have decided to build a device supporting IC61850. This tutorial shall be an initial support for that task. Remember, IC61850 has three main elements. The communication capability with the abstract communication service interface and the mappings on protocols like MMS, Manufacturing Message Specification and TCP IP over Ethernet. The semantic data model with the logical nodes and data objects and the system configuration language supporting the engineering process. While the communication stack that you have just purchased supports you with all the aspects of the communication, it is your task as a device manufacturer to create the data model for your device and to support the engineering process. This tutorial focuses on the data model. The tutorial is structured into five parts. In part one, I will give you an introduction on what IC61850 is and the different tasks you have as a device manufacturer. Part two then introduces the elements of the data model. In part three, I will explain step by step what needs to be considered when designing the data model for a device. Part four finally provides some modeling examples for typical substation automation and protection functions. While in part five, I will discuss what is required such that your device can at the end be used within a substation automation system. So let's start with the introduction and let's have a look at some of the IC61850 features that you probably already know. What is IC61850? First of all, it's of course a communication protocol, or at least it provides the capabilities of a communication protocol. IC61850 defines what is shown in yellow here, an abstract model for the information exchange using different kinds of service capabilities. In addition to that, IC61850 defines communication profiles using existing standard protocols like ISO 9506, which is the MMS protocol, ASN 1, TCP, IP, and Ethernet. All the blue part is not really defined within IC61850, it's only referred to in IC61850. But IC61850 definitely defines the methods for the information exchange in the abstract communication service interface, which is IC61850 part 7.2. What you can see as well on this slide is that we have different communication methods. We have client-server communication, which is using the full seven-layer stack, but we also have Goose and sampled-value communication, real-time communication, which is directly mapped on an Ether type. But IC61850 is more than just about communication. IC61850 also defines a domain-specific object model. If you look at the example of the switchgear here, we have a couple of models we call a logical node. These are building blocks for the semantic object models that are defined in IC61850. A few examples here, like the XCBR at the bottom, which is the model of the circuit breaker, XSWI for switches like disconnecting or earthing switches, current transformers, TCTR, voltage transformers, and last but not least, another example here, the gas density monitoring that we have in a gas-insulated switchgear, the SIMG. All these red blocks here are called logical nodes, and they provide all the data, all the information that we need to interface to our application, to the process. This is the interface to the process. But of course, in a substation automation protection system or in any automation system, we have more than just interface information. We have also software functions. The software functions also need some models, and for that we have more logical nodes, like the PDIS as a distance element for protection function, PTLC, time over current element, the measurement unit MMXU, and the CSWI, which is the control functions for switches like circuit breakers or disconnecting switches. So these logical nodes here represent, in fact, information that you have produced in a software algorithm in your automation system. So what you can learn here from this first slide is 6x50 has two aspects of data modeling. One is the models of the process interface. These are the process closed logical nodes. They are shown in red in my slides. The other one is the models with the information from your software algorithms. These are the green logical nodes shown here. This is the information produced by your software. When we talk about 6x50, an important thing is that we need to think in functions, not in boxes. 6x50 doesn't define how you have to structure your devices. 6x50 defines functional elements, logical nodes for functional elements. And as an example here, you see a typical model of a distance protection relay. We have the interface to the current transformers and voltage transformers at the bottom left. We have then multiple distance elements, three zones shown here. We have the scheme logic. If you have a distance scheme implemented, we have the trip logic, the PTRC. And at the end, we have the interface to the breaker to trip, which is logical node XCBR. All these elements are defined in 6x50 as logical nodes with the data objects, with the information. But 6x50 doesn't say how you have to allocate them to devices. The first example is shown here. This is a typical, conventional approach, like we have it today, where all these functionalities are implemented in one device, a distance protection relay in that case. We have the interfaces to the process, and we have the software functions with the protection algorithms. However, 6x50 also would support in the future an approach like this here, which is more process-based approach, where we have a merging unit as an interface to the current and voltage transformers, transmitting sampled values to a distance protection IED that is reduced now to software functions, the protection elements. And then again, we have an interface to an input-output unit that provides interface to the circuit breaker. So in that case now, the distance protection IED would not implement any physical inputs or outputs anymore. It's mainly doing software functionality, but the merging unit and the input-output unit to the circuit breakers are really doing the process interface. Also, such an allocation of the function to devices is possible with 6x50. The third element of 6x50 is, as we already have discussed, the system configuration language, which provides a set of files with configuration information. It has, when we start as a first step, we can produce a specification of our future system, single line diagram, with the functionality required. We can then produce a first output, a standardized file, which is called the system specification description. On the other side, we have from our devices that we would like to use, the IEDs, the intelligent electronic devices, protection relays, bay controllers. We have another standardized file, which describes the device capability. It's called IED capability description, or ICD file, device capability. That's where we find the data model of the device that we need to do our functionality. And that's where we also find a formal description about the communication capabilities, the engineering capabilities of the device. These two files are the initial input to the engineering process. We then can take an engineering tool, a system configuration tool, import these files, and do with the system configuration tool all our system configuration, instantiating devices, defining data flow, information exchange, configuring all the communication parameters. And at the end, the result that we get out of that device is what we call a substation configuration description. This is again a standardized file that includes all information required from a communication viewpoint to configure a substation automation system. So now, all the devices can import that file into their device-specific configuration tools, call that IED configuration tool, or ICT. They can do the local device engineering, and then, based on that, they can download the configuration into the device. And at the end, on the right-hand side, we have our configured system, our configured substation automation and protection system. This is also standardized as part of 61850. Then, 61850 has some specialties or additional services, capabilities. One of them is GOOS. GOOS basically is a communication service. GOOS, by the way, stands for generic object-oriented substation or system event. It's a communication service that allows to use messages to be exchanged between IEDs instead of wires. So typically, when you have protection or control functions implemented in a substation automation system, the devices have to exchange binary information. Traditionally, this is done with a bunch of wires, wired from one device to another, from one device output to a device input in the other device. With 61850 GOOS, this can now be replaced by a communication service. So GOOS basically is a communication service that allows a fast transmission of status information as a multicast over the communication network. So in a similar way, like in the past, this binary information was transmitted using wires between the devices. GOOS is now providing a communication service that does this functionality. An example of using GOOS is shown in this next slide. So this is a typical application where you have a reverse blocking functionality. So what we see here is a transformer bay. On the top, we have the high voltage or the higher voltage part. On the bottom, the low voltage part with the low voltage bus bar. Individual feeders going out, and it assumes that the feeders are radial feeders, so the energy flow is always going out to the feeders and cannot get back. The typical protection function here would be that you have your feeder relays that do the feeder protection on the low voltage side, each of them. But behind that, you have on the low voltage side of the transformer, the transformer protection device, which also implements a bus bar protection for the low voltage, the 16 kilovolt bus bar in this case. So basically, when you now have a fault on this bus bar, this protection relay will see the fault in that direction. These feeder relays will not see the fault in this direction. On the other side, when we have a fault on one of these feeders, the relays will see the fault and the protection will see the fault. So to avoid that the protection trips, in the case we have a fault on the feeders, we have this reverse blocking signal that is typically sent back to this device. As soon as this relay here sees the fault on the feeder, it will activate that signal and the relay will block. However, if none of these relays does see a fault out here, it can assume the fault is here on the bus bar, and then this relay will trip immediately. So this is a scheme typically implemented in conventional substations and using wires. With 61850, we can now replace this wire with goose messages. There may also be another wire that initiates breaker failure. With 61850, all these signals can now be implemented as goose messages. So we would have, as an example, a goose message with the start information of the distance element, which would be used to block the bus bar protection relay, and we would have a goose indication with the operate indication from the trip conditioning logical node, which is the physical trip to the breaker, and this would typically be used to initiate the breaker failure function. Another possibility of 61850 is the possibility to transmit sampled values. So here we have an example where we have intelligent devices, merging units, IEDs connected to the current and voltage transformers, and they are transmitting sampled values to the switch, and these sampled values are then distributed to break controllers, line protection as they are needed. So instead of having analog signals between the CTs and VTs to these different devices on the left side, we now have communication possibilities that transmit the communication signals to the devices on the left side. So these have been a couple of features from 61850. One of the key features of 61850 is the semantic data model. In legacy protocols like 6875101 or DNP3, information objects are typically identified by a point number and a data type like single point status information or 16-bit integer measured values. A DNP3 example is shown on the top of the slide. Extra documentation like a device profile is required to know that, as an example, the binary input with number 6 represents the open contact of a circuit breaker. In another device, that same information might be available in a single point status with number 25, or the open and closed contact could be combined to a double point status with number 315. With IC61850, the objects are defined with standardized names and data types. For our example, the standard defines that a data object with the name POS represents a switch position. The data attribute with the name STVAL contains the status value. So the value has a double point status in this case. And if the data object POS is within the logic node XCBR, it's the position of a circuit breaker. So with 61850, the information is not anymore identified through anonymous numbers. The information is now identified with names that have a well-defined semantic meaning. Taking the previous example, in DNP3, the object is globally identified through the object type, which is 1 in that case, the variation, and the point index, which was 6. So this would be 1, 2, 6 in the above example. In addition, of course, the ID needs to be identified through a unique address. The DNP3 master device would use this address information to read the value of the position. In IC61850, the object is identified through the hierarchical name. While the data object name POS and the data attribute name STVAL are unique within the scope of the logical node XCBR, there may be multiple circuit breakers and thus multiple XCBRs within the device. Therefore, a prefix and instance number are added. So our position would now have the name QA1 XCBR1.POS.STVAL. To make it unique within the system, IC61850 is using, in addition, a logical device name rather than a physical ID name. So one ID may have multiple logical devices. With a logical device name AA1Q1CTRL, the unique identification would then be as shown. We will discuss later how exactly logical device names, LN prefix, and LN instance number are created. For the communication, when IC61850 is mapped to MMS, the name shown now is used with a little variation as addressing information. The object reference for MMS would be AA1Q1CTRL slash QA1XCBR1.POS.STVAL. So basically, the dot is replaced with a dollar sign. In this slide, we have a first example of an IC61850 data model for a device. The device is a Bay controller, as shown on the left side. The circuit breaker QA1 and the disconnectors QB1 and QC1 are controllable. For each of these switches, we find a logical node CSWI for the control, CILO for the interlocking function. The circuit breaker has a logical node XCBR. As process interface, the disconnectors use the logical node XSWI. The logical node MMXU is used for the measurements. In the example shown, we find individual values for phase voltage and current per phase. The values are available as floating point value. To summarize this first part of the tutorial, here again the key elements of IC61850 in a different view. What do you need to do when you design a device that supports IC61850? The major part of the information exchange and the communication protocols used are part of the stack that you have bought. The first step you have to do is to integrate the IC61850 communication stack into your application design and to adopt the information transmission to the IC61850 communication services. Here you need to consider that IC61850 communication typically works event-oriented and that the status updates include the timestamp of the last change. The second step is to create a data model based on the information that is available in your device. The IC61850 data model does not only provide the operational information. As we will learn in the next part of this tutorial, it includes as well configuration attributes and parameter settings. Finally, you need to consider as well the IC61850 engineering process with the system configuration language, the abbreviation is SCL, that is defined in part 6 of the standard. You need to consider how to map your existing engineering functionality into this process and you need to identify what extensions may be required to your existing ID configuration tool. In the next part of this tutorial, we will now have a closer look on the elements of the data model.


Video ID: TzNmefSzSso
This section describes how to create an IEC 60870-5-104 slave channel, how to configure a session on the channel, configure a sector on the session, and create data types. To add an IEC 60870-5-104 slave channel, right-click Gateway and select Add TCP Channel. In the Add a TCP IP Channel dialog, select S104 as the session protocol, configure the IP address and node name, configure the mode, a slave should be a server, configure the port address, and enter an alias name, then click OK. Next, configure the session level parameters. The default values provided should work fine for most applications. And configure the sector, or ASDU, address. Finally, add the data types that you'll need. If you have just added the channel, session, and sector, the Add Data Type dialog should automatically appear. Otherwise, right-click on the sector and select Add Data Type. Then select a data type from the drop-down menu in the Add Data Type menu.


Video ID: M7FzC4ZxITM
In this video, we're going to demonstrate mapping data from 61850 to DNP3. We'll use the 61850 client component in the SCADA data gateway to read the three servers that we simulated in the previous example. We'll map those points to DNP3 and report them up to a communication protocol test harness that's simulating a master. For the DNP3 connection, we'll be utilizing DNP3-SAV5, or Secure Authentication Version 5, the current secure authentication version approved by the DNP users group. For this demo, the SCADA data gateway is running on an embedded PC. Here you can see the embedded PC. This particular one is made by AdvanTech. You can actually order this. It has a specific part number from AdvanTech, pre-configured, ready to run the SCADA data gateway. Here we see a remote desktop connection to the embedded PC that I showed you a moment ago. On that embedded PC, we're running the SCADA data gateway. We're running it in GUI mode. You would probably actually run this as a service in an actual application, but services aren't very exciting to look at because there's no user interface. For the demo purposes, we're running it in GUI mode. Here we see a remote desktop connection to the embedded PC that I showed you just a moment ago. On that embedded PC, we see the SCADA data gateway. In this window, we can see in the left-hand side a tree indicating the various connections that are configured. We have three IEC 61850 client connections. We've called them Breaker 1, Breaker 2, Breaker 3. These are each connected to one of the instances of Anvil that we ran in the previous video simulating that breaker. We also have a DNP3 outstation channel. This channel is configured running DNP3 secure authentication version 5. And then we're mapping points from those three IEC 61850 connections to the DNP connection. I should point out all of these names are user configurable, so in a real-world application, you can name them whatever makes sense for your application. If we look at one of these IEC 61850 client connections, we can expand it out and see the various report control blocks. And we can expand out below that and read all the way down to individual points. Now one nice feature of the SCADA data gateway is that it will summarize the points that are below. So for example, if we come here and click A, it will show us all of the A values below that phase A, phase B, and phase C currents. If we go one level higher and just click on the MMXU, we can actually see the voltages and the currents that are part of that remote control block. And if you notice, these values are changing every five seconds because we're running that same simulation in Anvil that's simulating the various states of the breaker, stepping through the flowchart, changing values every five seconds. And if you notice down in the protocol analyzer window, we're seeing periodic updates. As those values are changing, we have the gateway configured to send unsolicited responses to the master, sending those data updates. Back on my host PC, we have the SCADA data gateway running and also the Insight OPC viewer. In the test harness, notice the protocol analyzer log is showing the communications from the SCADA data gateway. And the blue lines that you see are indications of secure authentication events. So we can see secure authentication running as those values are updated. On the right side, we see the OPC XML DA client. This is showing the phase A current for each of the three breakers. And you can notice that they change as the gateway reports new values. This is being displayed via Insight, our new graphical interface tool. And it's an OPC XML DA browser, so connecting directly to the gateway that's running on the remote box and displaying those changes. Focusing back on the test harness, we're now looking at the data window in the test harness. And like the SCADA data gateway, the data window summarizes the points below. So here I'm showing everything coming in from that connection, all of the binary inputs, which represent breaker states, the analog inputs, which represent the various voltages and currents, and also secure authentication security statistics. If I scroll over to the right, you can see that the test harness has the option to name values. I've only named them for the first breaker. But you can store that description in the workspace with the test harness. And then you get that English text description of each of the points. If you so desire. Again, those descriptions completely configurable. You can configure them however you want for your device. So there you have it. In this demo, you saw, first of all, that we showed how you can create a real-world IEC 61850 device. We gave two examples, one using our NCC source code library for the IEC 61850 server. That was running on a Raspberry Pi board. We also showed using the .NET protocol components to write the breaker demo that we used in these videos. We showed Hammer connecting to those real-world devices and gathering states and writing out a CSV file, comma, separated value file that creates a simulation file for Anvil. We showed multiple instances of Anvil running that file. And we showed Skated Data Gateway connecting to each of those instances of Anvil, receiving updates, mapping them to DNP3, and displaying those values in a DNP3 communication protocol test harness simulating a DNP3 master. And we also showed the inside OPC XML DA viewer also reading values from the Skated Data Gateway. We hope you found this demo useful. We certainly hope that it sparks some ideas on how these products can be used in your application, whether it's for developing a new device, testing a device, or integrating protocols in your system. Feel free to contact us. Contact sales at trianglemicroworks.com for any sales-related questions. Contact support at trianglemicroworks.com for any technical issues. And we would appreciate hearing your feedback if you found these videos useful or if you have any questions about how you can use them in your products. Thank you.


Video ID: DLTzqHcrcU0
Welcome to the Triangle Microworks free DNP3 protocol overview. This course provides a high-level introduction to the DNP3 protocol. This course provides a high-level look at DNP3. More detailed information is available in our DNP3 training course. For information on this course, please visit our website at www.trianglemicroworks.com. DNP was originally created by Westronic Inc., now GE Harris, in 1990. In 1993, the DNP 3.0 Basic 4 Protocol Specification Document Set was released into the public domain. Ownership of the protocol was given over to the newly formed DNP Users Group in October of that year. Since that time, the protocol has gained worldwide acceptance, including the formation of Users Group chapters in China, Latin America, and Australia. In January 1995, the DNP Users Group Technical Committee was formed to review enhancements and to recommend them for approval to the General Users Group. One of the most important tasks of this body was to publish the DNP Subset Definitions document, which establishes standards for scaled-up or scaled-down implementations of DNP3.


Video ID: vrl78h2knZg
This video shows a breaker failure demonstration. This video is the fourth in this series. Please watch video 1 for an explanation of the demo and video 2 to understand the role of each Triangle Microworks product in the demo. This slide shows an overview of the demo. You may print a copy of this overview for reference while watching the video by downloading a PDF copy from the link provided. In this demo, DTM simulates six IEC 61850 servers defined in the substation SCD file. In addition, we're using DTM as an IEC 61850 client to monitor the network status directly and as a DNP3 master obtaining data from the IEDs through the SCADA data gateway. For the breaker failure simulation portion of the demo, a trip can be initiated remotely using a goose pioc.op message. The breaker attempts to trip and simulates a breaker failure. It then broadcasts a goose breaker failure message or rbrf.opx equals true. Upon receipt of this goose breaker failure message, each of the other devices will trip their breaker. Anvil, the server component of the 61850 test suite, simulates an IEC 61850 server. Two Texas Instruments BeagleBone boards running the Triangle Microworks IEC 61850 source code library each simulate two additional breakers. If we look at each of the device displays in DTM, we see that the breakers are closed as indicated by a red breaker state. On the GE device, the front panel shows the closed breaker and none of the fault indicators are lit. In the single line diagram, green indicates the breaker is open and red indicates the breaker is closed. A script on each server will set the current to a base value plus a random load component if the breaker is closed. These values are being sent by the simulated devices and are being read by IEC 61850 clients in DTM, which uses Insight to update the display. Now I'll send the PIOC reset goose message to close the breaker and clear test mode in each device. Let's look at the goose report log for DTM. This report shows the individual external references that were received in a goose message by each simulated device. Here I filter the log to only show data attributes with a value of true and we can see that each of the simulated devices received the broadcast message. Now I'm going to clear the goose report log and trip the Schweitzer breaker in failure mode. As I do, we'll hear a sound file play as each of the breakers trip. I'm going to pause the logic analyzer so that we can see the state change for each of the breakers being simulated in DTM. Now let's take a look at the front panel displays. Notice they all show the breaker has opened except for the Schweitzer display, which shows the breaker failure status. These front panel updates are driven by Insight, the graphing component available with DTM, the 61850 test suite, IRN, our ICCP TASI-2 test tool, the communication protocol test harness, and the SCADA data gateway. The breaker status and breaker failure indications reflect the values of the data attributes they are mapped to in the IEC 61850 object model. A server display will typically reflect the data attributes that are sent by IEC 61850. And a client display will reflect data attributes that were received by IEC 61850. Note the GE device which uses an image of the actual device. Insight controls and displays have been overlaid on this image. As the data attribute in the IEC 61850 object model is updated, the corresponding Insight display element is updated, providing a simulation of the device's display. Note that the elements include the breaker status, which is now shown as open, and a fault indicator, which correctly shows that the fault originated with the Schweitzer device. If we pause this script, we can edit the display, and we can see that the various display objects are simply Insight elements overlaid on top of the image of the device. For example, here we have a clear or transparent Insight button that has been overlaid on top of the image of the button on the front panel. Looking at the single line diagram, which shows the IEC 61850 data received by the client, we see that it is updated to show that each of the simulated breakers has tripped, except for the Schweitzer relay, which is showing a fault. Remember that the two BeagleBone boards in this demo subscribe to the same rbrf.opex goose messages, but use different in adder assignments in the extra f statements in the IED section of the SCD file to trip different breakers. As expected, the single line diagram shows that one breaker on each BeagleBone has tripped, and that they tripped different breakers. Notice the current is now shown as 0 for each open breaker. The logic analyzer shows the state changes. If we look at the logic analyzer, we can see that each of the simulated breakers has changed state. This is useful for seeing the relative timing of signals in the simulation. Looking at the goose report log in DTM, we can see that each of the simulated breakers received the rbrf goose message from the simulated Schweitzer device. Now let's clear the goose report log again, re-enable the logic analyzer, and reset the breakers. As we do, we'll see each of the breakers reclose. Once again, we can see the current values being updated as they simulate the variation from a random load on a base value.


Video ID: TdqNDh6-_zA
Let's begin by looking at the purpose of a SCADA communication protocol. We'll look at the purpose and objectives of a SCADA communications protocol and provide a few examples. The primary purpose of a SCADA communication protocol is to allow remote monitoring and control. To do this, the protocol must support the transfer of data between devices. One step of this process is to replicate the database from one device to another. What are the objectives of a SCADA communication protocol? First, the protocol should provide standardized rules for data transfer. This helps ensure interoperability among vendors. The protocol should ensure reliable data transfer. This is often accomplished via a cyclic redundancy check, or CRC, or by a checksum to detect and possibly correct errors that are introduced during transmission, for example, by noise on the line. A SCADA communications protocol is likely to need to support advanced features, such as timestamp data or freeze operations. Freeze operations allow remote devices to report data as of a specific time or event, regardless of when the data are actually read. The protocol should provide data quality indicators. These indicators may include such information as out-of-range readings, counter overflows, or forced values. Features to prevent, or at least detect, unauthorized use or monitoring of the data are often required as well. Finally, the protocol may be required to minimize overhead in order to allow optimum use of the available bandwidth. This can allow more economical communications options to be used, even though they may have lower bandwidth available. More advanced protocols support report by exception, or RBE. Instead of reporting all data in the device, RBE only transmits changes or events of interest. This can result in reduced bandwidth and less processing, because only changed points are transmitting, resulting in fewer points being sent. Attaching timestamps to RBE data allows the master station to recreate the sequence of events that occurred in the outstation. Note that RBE can be either polled or unsolicited. With polled RBE, the master pulls the outstation for events, and only changes are reported. Although some bandwidth is used for polling, this is still more efficient than protocols like Modbus, in which the master station must pull all data. With unsolicited RBE, the outstation sends the changes as they occur, without having to be polled. Polled and unsolicited RBE will be discussed in more detail later in the course. Let's look at an example topology. For this example, we'll be looking at a typical substation, although many other topologies are in use. The master station communicates with the substation RTU via DNP. The substation RTU, in turn, communicates with multiple relays, also using DNP. The connection between the substation RTU and the relays can be serial point-to-point connections, such as RS-232, serial multi-drop connections, such as RS-485, or via a local area network, or LAN. Relay engineers often have a terminal and some sort of remote access, such as a dial-up line. This connection is used for configuration of the relays. Although this slide shows the engineer's terminal connection via a dial-up modem, this is only one way to achieve this connection. With DNP, the virtual terminal feature can be used to carry the configuration communications over DNP.


Video ID: 2uTTn_alpVA
This section describes how to configure the OPC data access server in the SCADA data gateway. No special configuration is required in the SCADA data gateway to use the OPC DA server. As long as the OPC DA server is licensed, all points, including internal status points, equations, and actual data points, are available through the server. Simply subscribe to these points from your external OPC DA client. This is typically done by connecting to the SCADA data gateway's OPC DA server and browsing to the desired points. When an OPC DA client subscribes to a point in the SCADA data gateway server, the point will show up in the mapping window as shown here.


Video ID: T2GpfZ5jZUc
From a data point of view, we have our same similar to reporting model. We have data objects within a data set that are generating a series of change events. These are formatted based on settings in a goose control block, which is controlled by the client, and the difference is the messages always send the entire data set. They are fast multicast reports, and I'll talk about how that that multicast works to get the reliability that we were after.


Video ID: 1pRyjyhRHWw
This video provides an overview of tools that can be used in the IEC 61850 engineering process. This presentation was originally given by Triangle Microworks President Jim Coates at the 2014 IEC 61850 conference in Prague. Triangle Microworks has many tools which are used by IEC 61850 users to increase the interoperability of their devices and systems. We will highlight a few of the capabilities including system configuration testing, IED configuration to SCD comparison, creating and checking SCL files, and educational tools. Triangle Microworks supports the IEC 61850 engineering process in a few different areas. Starting from the IED template on the left-hand side of the diagram, the IEC 61850 test suite allows users to test client and server devices. Anvil can simulate an IEC 61850 server from an ICD file including building the object model and implementing the defined services and control blocks. Hammer can simulate an IEC 61850 client and connect to either real or simulated server devices. SCL Forge can be used to create the IED template for a device including the complete object model, services, and control blocks. The Distributed Test Manager, or DTM, can simulate the communication protocols and data models for hundreds of devices on a single computer or spread across a network of computers. Based on the SCD or any other SCL file, DTM simulates the communications and the object model for each IED. DTM also has the ability to model the behavior of each device through built-in scripting or state machine capability. The test coverage of DTM includes the configuration definitions in the SCL, the network used in the test, and any real hardware used in the test. This allows users to verify the configuration of publishers, subscribers, clients, and servers within a mix of real and simulated components. SCL files are used throughout the engineering process. Triangle Microworks offers a new tool called the SCL Consistency and Completeness Check which allows users to check SCL files against the IEC 61850 standard and check for any inconsistent configuration definitions. The SCL checking capability is built into all of the 61850 test suite tools. DTM can test the system configuration for an IEC 61850 substation. DTM can read a System Configuration Design, or SCD, file and instantiate the IEDs that the user would like to simulate. The communications of the station bus or process bus can be tested based on the configurations defined in the SCD file. This allows the configuration to be debugged in the lab before commissioning. Users can mix real and simulated components in the test system in order to test different devices in the system environment. One use case for DTM is testing the configuration of the process bus for a substation. As shown in this diagram, the process bus is used for goose and sampled values messages across a variety of devices including protection relays, merging units, and debate controllers. From this network view, it appears very easy to set up the connections between all of these devices with a networking bus. However, there are many different publishers and subscribers that must be configured correctly to ensure the right data is being received by the right device. Now we look at the Publisher and Subscriber view and we see the true complexity of the system. DTM allows the user to simulate all of the devices and their configurations based on the SCD file. This includes the object model, data sets, goose control blocks, external references, MAC addresses, and virtual LANs. With the simulated system in DTM, the user can then change data in one publisher device and verify that the data is received through goose by the correct subscriber devices. Here, we can walk through the goose verification process. The user instantiates the relevant IEDs from the SCD file with DTM's interactive guide. The user can change data in a publisher IED through one of many different data simulation methods in DTM. When data in a publisher is changed, a goose message is published if the data is part of a data set that is part of a goose control block. The published goose message is then received by all devices which are subscribed through external reference definitions in the Inputs section of the SCD file. The user can then verify the data was received by the subscriber through one of the available simulation methods or using the goose monitor. For example, the user can map goose data into the subscriber IED's object model and view the data changed directly in the IED's model. There are many options for data simulation. With the manual method, the user can directly change data in the server object model. With the automatic method, DTM can automatically set data to random values on a regular interval. Table-driven data changes allow the user to import and play back states from a CSV file. Each column in the CSV file represents a state and each row represents a data object in the server object model. These CSV files can be captured from real devices using the test tool Hammer. DTM also has built-in scripting through JavaScript or graphical scripting with flowcharts and state diagrams. This allows the user to create custom logical behavior for a device. For example, the IED behavior can be modeled to start a certain process when a goose message is received. Custom displays can be created to model different parts of the system including HMI displays, single line diagrams, or IED front panels. One feature of the Hammer tool in the IEC 61850 test suite is the ability to compare the configuration of a device with an SCD file. Hammer can load the configuration of a server device through the IEC 61850 Discovery Service. The user can then compare this against one of the IEDs in the SCD file. Hammer then highlights the differences in the two including the object model, control blocks, and data sets. This helps users identify potential areas where a device is not being configured as defined in the SCD file. SEL Forge has many SEL editing and checking capabilities. It is designed to allow users to build and edit ICD template files as well as edit the configuration of any SEL file. Users can add data sets, control blocks, services, logical nodes, and data objects. Edition 1 SEL files can be converted to Edition 2. SEL consistency and completeness checking is also built into the tool. SEL files contain a lot of different configuration information, all the way from IEDs up to the system level. There are many parts of SEL that need to be checked to ensure interoperability of devices and correct configuration of the system. At the lowest level, the XML schema defined in the IEC 61850 standard must be checked. Moving up a level, the entire object model must be checked to ensure that other devices can parse the model and resolve all elements. Also, the model must be checked against the standard to make sure that mandatory data objects are included in logical nodes. The configuration of the communications section must be checked for consistency with control blocks, access points, and logical devices in other parts of the file. Other aspects of communication configuration cannot be checked inside the SEL file alone and must be done with other system testing tools. The substations section must be checked with system configuration tools which have access to system level information. Under specific sections like private fields and GGIOs must be checked by vendor tools. All of this means that independent tools are needed. Triangle Microworks has created a free online tool for checking SEL files. The functionality of this tool is the same as what is built into the IEC 61850 test suite. The tool covers many different sections of the IEC 61850 standard relating to SEL. This free online service is offered in order to help the IEC 61850 community advance the use of the standard towards greater interoperability. Protocol training videos are now available for free on the Triangle Microworks website. These training resources are available in order to help improve the implementation and continued adoption of the standard communication protocols that Triangle Microworks supports. Addition 2 of the IEC 61850 standard includes external references which is a very important improvement to the subscription process. Within the SCD file, each IED can define external references in their input section of any logical node. This allows the subscriber to define which publisher, control block and data object will be used as an input. The benefit is that the entire publisher to subscriber configuration is now contained in the SCD file so that GOOS, sampled values, reports and other services can be configured with SEL and without private fields for greater interoperability. This has the potential to streamline the engineering process and improve the interoperability of IEC 61850 related tools.


Video ID: RfHsrDwzpS0
This section begins the intermediate level training. We'll take a look at what this DNP3 secure authentication look like and how does it behave. This section is targeted for a technologist or technician. In practice, the core of the DNP3 secure authentication mechanism is three new function codes shown here. Almost every security message will use one of these function codes. This permits non-secure devices to clearly identify that the secure devices are using a feature that it does not support and raise a configuration error. Authentication can take place in either direction. Not only can an outstation verify that a control operation is authentic, for example, but a master can also verify that the data it's receiving from the outstation is authentic. The specification includes a list of which items are mandatory to be authenticated. For instance, controls must be authenticated, but it's optional whether data responses from the outstation are authenticated by the master. Each individual device has the choice of what must be authenticated for its proper operation. There are eight new objects to find for use with basic authentication. Note that all the objects except those related to key exchange can be sent by either the master or the outstation. A challenge message is sent by a receiver to indicate that the previously transmitted message must be authenticated. The challenger supplies data to be included in the authentication calculation. A reply message contains the MAC used to authenticate a message. An aggressive mode request and a MAC object, HMAC or GMAC, are included at the start and end of a DNP3 fragment respectively to authenticate a message without exchanging as challenge and reply. There are two types of keys that are used in DNP3 secure authentication. The pre-shared key already discussed, known as the update key, and a session key in each direction. The update key is used to periodically change the session key using the three session key messages. An optional error message can be used for debugging a secure system to determine why authentication is failing. Devices can stop sending error messages at any time to avoid denial of service attacks. This slide shows the update key change objects that are sent as variations of group 120 as well as whether they are transmitted by the master, the outstation, or either. This slide shows the statistics objects. These objects are contained in groups 121 for static values and group 122 for events and are sent by the outstation. DNP3 secure authentication is by default based on a challenge reply mechanism. Either the master or the outstation can challenge any normal DNP3 message it receives. The receiver of the message can determine that any DNP3 function is critical requiring authentication. However, the specification does specify that certain functions must be considered critical such as control operations, select, operate, etc. This picture shows an example of a select operation. The master transmits the select as a normal DNP3 message. However, instead of transmitting a normal select response, the outstation sends a challenge message using the authentication response function code and the authentication challenge object. The authentication challenge objects contain pseudo random data that both ends must use in their calculation of the message authentication code or MAC. This random data along with extended sequence numbers in the messages help to protect against replay attacks. The attacker would have to not only determine the appropriate sequence number for the moment of the attack but also guess the random data to be used. This is exceedingly difficult because the receiver of the original message, in this case the outstation, chooses the random data. In this example, the master sends an authentication reply object and reply to the challenge. Since the DNP3 terminology of request and response still applies, this reply object is transmitted using an authentication request function code. This may be confusing, but it helps to remember that in DNP3, masters always transmit requests and outstations always transmit responses. The authentication reply object contains the MAC, which the outstation will compare to the one it calculates internally. If the MAC is valid, the outstation replies with the control relay output block or CROB response that the master was originally expecting. Note that DNP3 sequence number rules continue to apply. The entire select sequence uses the same DNP3 sequence number, in this case 8, including the challenge and the reply. Note also that the select was preceded by a read operation which the outstation considered to be non-critical and therefore did not challenge. Because the complete challenge reply sequence adds several messages to a DNP3 transaction, the secure authentication specification permits devices to use an aggressive mode in which the sender assumes the message will be challenged and transmits a MAC and the same message that is being authenticated. Here's an example of a select operate sequence using aggressive mode. The master sends a message with a select function code and a CROB object as before, but precedes the CROB with an aggressive mode request object and follows it with a MAC object. The outstation authenticates this MAC and sends a normal select response. The operate sequence is similar except that the DNP3 sequence number is incremented and an operate function code is used as usual. Aggressive mode must be supported by all DNP3 secure authentication implementations and will likely be very commonly used. However, it cannot be used exclusively. Before it can be used, the two devices must first perform a challenge reply sequence. This is required so that the master, in this case, has the appropriate pseudo-random data in order to calculate the MAC. This does not provide an equal level of replay protection to a complete challenge reply, but it's still probably adequate while significantly reducing the amount of bandwidth required. This diagram illustrates how objects and headers are arranged when using aggressive mode. The application header and the objects in between the aggressive mode request and MAC objects are standard DNP3 objects. The qualifier used for the authentication objects is 1b hex, which permits test sets that do not understand the security messages to skip over the unknown objects if they choose. There are two types of keys used in basic DNP3 secure authentication as described in DNP3 SAV2. Both must be at least 128 bits in length. The session key is the key used in the MAC calculations. A new session key is created every time two devices begin communicating and is periodically changed by the master. In a typical real-time SCADA system acquiring data from outstations every two seconds, session keys should be changed on the order of every 10 to 15 minutes, perhaps an hour at the most. Distribution SCADA systems using dial-up systems and gathering data every hour or every day would naturally change them much less frequently, perhaps once a week. The specification states that session keys may be renewed either when a configured interval has passed or when a configured number of messages have been exchanged. There are two session keys used, one in each direction. The update key is the key used by the master to encrypt the session keys each time they were initialized or changed. The update key is the pre-shared key previously discussed. DNP3 SAV5 provides a method to manage the update keys remotely. The update key uses the Advanced Encryption Standard, AES, to encrypt the session keys so they cannot be viewed by an attacker as they are downloaded to the outstation. This encryption, plus the frequent changing of session keys, prevents attackers from analyzing the traffic to guess the session key and time for it to be usable. The key change transaction uses a challenge reply sequence like the normal authentication mechanism. The master requests a status of the keys, the outstation replies with the status and challenge data, after which the master may choose to reset the keys. The key change transaction uses a challenge reply sequence like the normal authentication mechanism. The master requests the status of the keys, the outstation replies with the status and challenge data after which the master may choose to reset the keys. The final response from the outstation shows that the keys are okay and provides challenge data for the next exchange if desired. This diagram illustrates how the outstation changes the status of the keys and the master resets the keys after a communication failure occurs. This diagram shows how a master would typically initialize secure communications with an outstation. The outstation generates an unsolicited response to notify the master that it has restarted according to the normal rules of DNP3 unsolicited responses. Rather than confirm the unsolicited response, the master first initializes the session keys. Again, by the rules of unsolicited responses, the outstation is required to abandon the original unsolicited response in favor of the session key transaction. Then when the outstation reattempts the restart unsolicited response, the master challenges and authenticates the response before supplying a confirmation. The outstation does not require authentication of the master's confirm. However, the outstation does challenge the write operation to clear the restart IIN. Following this sequence, both sides are permitted to use the aggressive mode because a complete challenge reply sequence has taken place in both directions. Error messages report various types of failures and are often used for debugging. Error messages include a text message and a timestamp and can be sent on an alternate link. An error message usually means that one end was misconfigured, although they could indicate some type of an attack. Consequently, the specification requires that they stop being sent after a threshold has been reached. Most of DNP3 secure authentication is concerned with preventative measures. However, it also includes some detective measures in order to attempt to identify if an attack is underway. The specification defines three new objects for security, security statistic, security statistic change event, and security statistic change event with time. These objects are very similar to counter objects. DNP3 secure authentication defines a fixed set of point numbers for statistics. Each statistic point represents a different measured quantity, reports both static and event objects at the appropriate times, and has its events assigned to a class, just like other DNP3 objects. Statistic points also have a predefined transmission threshold for event objects because too many events may, but does not necessarily, indicate that an attack is underway. In addition, some statistics have predefined maximums that, when exceeded, cause the device to take some other action. Statistics may be reported on other links, which allows another DNP3 master to discover the problem. Note that statistics counters may wrap, but they are never reset to zero. This slide and the next slide provide a list of statistic point numbers, along with the default threshold and additional actions that may be taken. In DNP3 secure authentication version 2, or DNP3-SAV2, update keys were pre-shared. DNP3-SAV5 adds the ability to change the update keys remotely. This capability is optional and requires the use of an authority. The authority cannot be the master itself and must support symmetric key management. It may optionally support asymmetric key management and optionally be an asymmetric certificate authority. The authority must communicate securely with the master. The methods of implementing the authority is outside the scope of DNP3. The most likely solution to this impasse is that the DNP3 specification will identify symmetric cryptography as mandatory and identify public or asymmetric cryptography as optional for changing update keys. In either case, the sequence shown in this diagram is likely to be used. In the proposed method, an authority, which is not the master or the outstation, will certify whether users are to be accepted by the outstation. Communications between the authority and the master must be secure but are out of the scope of DNP3. They're shown in dotted lines. The certificate or token supplied by the authority will be passed through the master without alteration by the master. The outstation will first verify that the certification is valid and then authenticate the master that is sending it. Using the asymmetric option, the outstation would validate the new user using the authority's public key. Using the symmetric option, there would be a pre-shared key installed at the outstation that was only shared between the outstation and the authority and only used when an update key needed changing. In either case, the message sequence used is very similar and DNP3 objects will be used in common whenever possible. The reason this topic is so controversial is a matter of risk and cost. Risk in that it is much more likely that update keys would be compromised by someone leaving a utility organization than by an attacker cracking the keys on the link. Cost in that there are a large number of devices in the typical SCADA system and the distribution of new keys to all these sites by hand would be prohibitive. Therefore, an electronic mechanism is necessary. Public key encryption is currently the approved mechanism in the computer industry for reducing this risk and cost. However, the utility industry has topology and processing power concerns that may make public key cryptography excessively costly. This table shows a comparison between asymmetric or public key cryptography and symmetric or pre-shared key cryptography. The main argument against asymmetric keys is that they require 10 to 100 times as much processing power than symmetric cryptography on the device that uses them. The main argument against symmetric keys is that they must be kept absolutely secret at all times while public keys can be transmitted in the clear, posted on a website, passed in an email, or entered by hand. Only the private key of the private-public key pair must be kept secret. This gives asymmetric cryptography an advantage in key distribution. A white paper is available from the DNP3 technical committee discussing this comparison in more detail. The DNP3 specification permits remote key update changes using symmetric cryptography, asymmetric cryptography using dedicated DNP3 objects, and asymmetric cryptography using IEC 62351-8 or X.509 certificates. A master or outstation may choose to use configured update keys only and not permit remote update key changes. It may also implement only symmetric methods, or it may implement both symmetric and asymmetric methods. DNP3 secure authentication assumes that the master may have many users. Each user has his or her own currently active session key in each direction. In addition, each user has an update key for changing the session keys, a username, which is a text string, a public key if asymmetric encryption is used, and a role, for example, an operator or an engineer. Each role has its own set of privileges, such as reading data, operating controls, or executing file transfers. Each role assignment has an expiry interval as well. The authority decides which users an outstation must accept, which roles, and therefore privileges, a user may have, and how long the user's privileges last. The master passes the authority's choices to the appropriate outstations and cannot modify these choices. The outstation authenticates the master and the authority, enforces the authority's choices, and assigns a short user number to each user. A user number represents a particular person. However, many functions are performed by the master on behalf of all users. Note that a user number is valid only on one distinct association. That is, the same user can be assigned different user numbers on different links. The outstation doesn't know which user it is challenging. Some utilities may not want to track individual users. Their argument is that the master authenticates the user, the outstation authenticates the master, the master logs all operations performed by the user, and so it's not necessary for the outstation to authenticate the user. Therefore, single-user systems are permitted. In such cases, the master always uses the user number 1, default, and the user name, common. Authentication verifies that you are who you say you are. Authorization relates to which operations you have the right to perform. For example, are you permitted to only read data, or are you permitted to send controls as well? There are three factors. Something you know, for example, a password or personal identification number or PIN. Something you have, for example, a smart card or USB token. Or something unique to you, for example, a signature, a fingerprint, or a voiceprint. Ideally, systems use more than one factor at a time. For example, using a debit card requires both your card and PIN, or using a credit card requires both your card and your signature. Role-based authentication control is a concept in key management that is complementary to authentication. A role, such as administrator, operator, engineer, or technician, is assigned to each person. Each role has certain privileges. For example, some roles may only be permitted to read data, whereas other roles may also be permitted to send controls. Role-based authentication provides centralized management of roles, which helps simplifies authorization, and, since it eliminates the need to keep track of which users are assigned which rights, role-based authentication control also makes it easier to revoke rights and helps keep roles consistent across the organization. D&P 3 makes use of standardized roles and privileges. This slide shows a variety of user roles. These roles are aligned with those in IEC 62351-8. This table is from the Secure Authentication Test Procedures, which as of the time of this recording, July 2014, are still in process. This table shows items that need to be configured in order for secure authentication to work correctly, along with typical values. These items typically need to be configured the same way on both the master and the outstation. This table shows typical threshold values. The max items affect the behavior of the system and should be chosen carefully. The statistics thresholds only apply to the outstation, since the master does not report statistics. A master just needs some way to display them. Finally, let's look at some of the responsibilities of the utility. These include ensuring that D&P 3 secure authentication is used where it's required and ensuring that the correct version is used, that is, SAV 2 or SAV 5. They should use SAV 5 if it's supported. The utility is also responsible for making design and policy choices regarding whether to use pre-shared or remotely updated keys, whether to use symmetric or asymmetric, and if asymmetric, whether to use certificates or non-certificates, and whether to use single-user or multi-user mode. If a certificate is used, the utility is responsible for determining whether to use internal or external certificate authority. In addition, the utility is responsible for establishing a master-authority relationship communications and security policy, identifying critical functions to be protected, and assigning roles to users and determining expiry times and revocation procedures. The utility is also responsible for ensuring the configuration parameters, such as timers and key sizes, match, ensuring keys and certificates are installed for each outstation, ensuring all non-public keys are protected and kept secret, and for creating and deploying training for all staff.


Video ID: G9I6kJzYSRs
The Communication Protocol test harness is easily customized to provide your own functional tests. One way to customize the test harness is with .NET plugins using any .NET programming language like VB.NET, C-Sharp, or J-Sharp. The test harness can also be customized using TCL TK scripts. Another way to create custom functional tests is using the built-in command window. The command window has a series of commands, each with an icon or a button. If you click the button, it executes that command. So, for example, clicking the button next to Integrity Pull causes the test harness to execute that Integrity Pull. Now, if we double-click the command name, the test harness opens a configuration window and we can customize this command. So, for example, for the Report by Exception Data Pull, we can choose the qualifier code. We could, for example, only pull for 20 events or pull for all events and include or exclude certain data classes. We can also customize this window to automatically repeat the command. We can adjust the repeat interval using the up and down arrow or we can just type in the window and set the value. When we press Start Repeat, the test harness adds this progress bar and you can see the progress of the delay between each command. Let's take a look at some of the commands in the command window. One command is the Validate Points command. With the Validate Points command, you can set up the points in the test harness to have specific expected values or expected flags and the test harness will compare the current value in the database to those flags. Now, you can click this checkbox for a Class 1, 2, 3 event pull and before executing the compare, the test harness will execute in a Class 1, 2, 3 pull in order to get the latest changes. You can also delay the read. So, for example, if you do a CROB command and then a Point Validation command, you could delay to let things settle before you read the commands. Another interesting command is the Omicron command. If you have an Omicron test set such as a CMC 156, 256 or 356 connected to the test harness, you can control that test equipment through the test harness and specifically through this command. So, for example, we can set the status of binary outputs, we could set voltages or set currents. This is very useful for setting up a series of test conditions which can then be read by the test harness and those test conditions can be compared against expected values. The command window also includes a conditional command. The conditional command lets you set up a test condition and set up branching based on the result of that condition. When the conditional command window is open, this red bar indicates where the flow will jump to if the test expression evaluates to true. And finally, if there's not a built-in command to do what you want, you can always use the custom TCL command and write your own TCL TK script which will be executed from within this command window. This gives great power in customizing your functional tests. Note that all of the commands can be organized by simple drag and drop. So, we can put like commands together if we wish. Even more powerful is the run all commands option. When we select run all commands, the test harness will cycle through each of these commands in order and this is really where the conditional command comes into play because you can control that data flow with the conditional command and create some pretty elaborate scripts. We have some examples of this in workspaces that ship with the test harness. So, as you can see, it's very easy to create custom functional tests with the test harness. As I mentioned at the beginning of this video, you can write plug-ins using any .NET programming language or you can use TCL TK scripts. But you can also do a lot of things from the command window without having to program.


Video ID: Mmg_6BBKOA8
The following video demonstrates using the test harness to issue control relay output block or CROB requests as well as reading the CROB status. This demo illustrates DNP3 control relay output block or CROB requests. The test harness is connected to an off-screen microprocessor board. This board has four green LED indicators which are configured to be controlled via DNP3 CROB requests. To control the LEDs via DNP we've configured two commands in the command window. If we look at the configuration of the all green LEDs on command we can see that this command will issue a latch on or L on command to points 0, 1, 2, and 3. Similarly the all green LEDs off command issues an L off or latch off command to these same points. If we now issue the all green LEDs on command all four LEDs on the microprocessor board turn on. Although we can't see that in this demo we can see that the status reported back by the device shows that the LEDs are on. Now if we issue the all green LEDs off command once again we can't see the LEDs turn off but we can see the CROB status reported as 0 now.


Video ID: Qf61sbzcQ8c
The following slides provide additional information about the SCADA data gateway. The SCADA data gateway has two panels, a configuration panel and a protocol analyzer panel. The configuration panel is used to configure channels, sessions, and, for IEC 60870-5 protocols, sectors. Data points are also configured in this panel. You can map data for protocol translation or data concentration using drag-and-drop operations. The configuration panel also shows the current state of all data points as well as their quality indicators. Internal points show information about the status of the connection. The SCADA data gateway also supports modem pools. You can define modem pools via the GUI, add modems, and then assign channels to use the modem pools. The protocol analyzer panel shows communications between the SCADA data gateway and remote devices. The check boxes can be used to filter what data are displayed. The SCADA data gateway supports many features not shown in this demo. For example, it supports OPC data access server and client, OPC alarm and event server and client, OPC XML data access server, and IEC 61850.


Video ID: H5_0QJiSoN8
Note that in addition to emulating the front panel, the test harness also displays a protocol analyzer, showing communications between the SCADA data gateway and each emulator. We can use the filters in the protocol analyzer to control how much data is displayed. For example, we can see all layers, physical, link, transport, application, and user. We can also just show the user data and only see data values that change. We can even use the sessions filter to display data from specific devices. This video demonstrates how the protocol analyzer filters change the output of the protocol analyzer display. If you notice now, we have all of the devices configured to be displayed, and we're only showing down through the application layer. We can turn on additional layers, and the display becomes even more crowded as more data is shown in here. Typically what I'll do is start out at the application layer, and usually start out just monitoring a particular device. Let's just enable the Schweitzer simulator here. Now everything we see is for the Schweitzer simulator, and only the application and user layer. Now a nice feature of the test harness, once we pause the display, and we can scroll up and down and find a particular message that we want to look at. Now we can, after the fact, turn on these filters, and that data is still saved and still in here. So this is very, very useful for troubleshooting. You can start at a very high level, find an area of interest, pause the analyzer, and then drill down and see the additional details. Another very nice feature is we can say file, save protocol analyzer log as XML. This window comes up and prompts us to save the name. I'm just going to overwrite my old log file there. And that has saved this analyzer file out as an XML file. Now later I can come back and I can say file, open protocol analyzer log, and open that file. That opens a new display window. And here's that log file that we had just saved out. And I can come in after the fact, after this has been saved, and change the filter settings as well. So this is very, very useful for troubleshooting applications. You can save off a log file and then go back to your desk and study it in more detail. Finally, on the far left, we have another test harness emulating a DNP3 master station. The test harness ships with the GUI for configuration, issuing commands, and displaying data. The ease of use makes the test harness ideal for easily testing your slave devices. This master DNP3 station is connected to an outstation DNP3 component on the SCADA data gateway. All of the points in the triangle and each of the outstation emulations are mapped to this component and are reported to the master. We can view the master's database and see all of the points in each of the remote slaves. This slide demonstrates how to send commands from the test harness master simulator. Note that the commands are actually being sent to the SCADA data gateway, which in turn sends them to the outstation simulator shown here.


Video ID: rRFtKe-dGrM
In this video we'll be going over the quick start guide installed with SCADA Data Gateway 5.0. You can find this guide by going to help. After doing that you'll find the steps that we'll be following today. So let's take a look at what we're going to set up today. We're going to set up a Windows machine with test harness with a DNP3 outstation. That Windows machine will also have a UA expert OPC UA client. On our Linux machine we'll have SDG which is what we'll be hosting a DNP3 master and an OPC UA server. The outstation's data will then be sent over to the master. The master will then map to the UA server and the client will receive data from that UA server. That way you can see a full loop through the SCADA Data Gateway. We'll also be using a web client on the Windows machine to interact with all of the services running on the Linux machine. The first few steps in this is just installing the applications. So you'll already have SCADA Data Gateway installed on your Linux machine and you'll need to go ahead and install test harness on your Windows machine and an OPC UA client on the same machine as test harness. We're using UA expert for this video but you can use anything you're familiar with. To install test harness just click the download link or go to our website, log in with your account and you can download an eval of test harness. The same can be done for the OPC UA client. You just go to their website which looks like this, log in and download UA expert 151. So now you have all of your applications installed. Now we'd like to create the DNP outstation inside of test harness. To do that go to the tickle window and type open DNP slave. That'll create a device here with many data points. To see these data points go to open data window. From here you just open the test harness, click the device and here are the data points. So now that we have our DNP 3 outstation set up in test harness let's create the master in our SCADA data gateway. To do this right click on the node root node to add TCP channel master, select the protocol which would be mdnp and then put in the IP address from the machine that you have test harness running on. For me that's 10.1.1.106 and then we'll give it a name. I'm gonna name it DNP 3 master. Hit OK. The second configuration window is if you need to change some specifics in the DNP 3. Since we use the default and test harness we don't need to change any of this. And this will be the initial data type that we're going to add. So far we'll have binary inputs but I'm going to add everything else as well. So you go to your DNP master, click the drop-down arrow and then you can right-click on L4 and do auto create tags. That will create every data type that the DNP master sees in the DNP outstation. But if you want to add a specific one you could always right-click and do add data type and select say analog inputs. From here you can open up L4 and see all of our data attributes inside. And you'll also see that they are automatically mapped to our previously created or automatically created OPC UA server. The last thing we need to do is connect our OPC UA client. We're using UA expert to do this. You could click add new server but since I already have one set up I'll show you the configuration for that. So I'll right-click and do properties. What I'm using is the endpoint URL which essentially just has the IP address of where gateway is installed in it. So now I can right-click on this, say connect, and you'll see it pop up down here. From here I can open the gateway, our DNP 3 master, and then L4 and this will show you all of the different data points. So I can open up the binaries and drag one over. Now when I go back to our test harness and change that value we'll eventually see that map through to our OPC UA client. Thanks for watching.


Video ID: ji0K-QfNJsY
If you need support with industry standard communication protocols, Triangle Microworks is here to help you. We hope you enjoyed this presentation. For additional information, please visit our website at www.trianglemicroworks.com.


Video ID: xyluBc2x_Ms
To subscribe to tags from the OPC server, right-click on the OPC client's alias and select Add OPC Client Item. In the Add Item dialog, browse the tree and select the desired points. Select the OPC data type if necessary. You can select a point by either clicking Add Point or double-clicking the point. Note that the tree structure will vary depending on your OPC server.


Video ID: xObw3fk4DQY
When you purchase an application from Triangle Microworks, you will receive a product key. This product key must be activated on the PC that will hold the license for the application. The product key can be activated on the target PC through an internet connection or by using a manual offline process. This video demonstrates how to activate the license using the manual offline process. Another video demonstrates how to activate the license if an internet connection is available. If your product is not currently licensed and when you start it, you'll receive a license error message. Click Open License Manager and you'll be taken to the License Manager dialog. If your product is already licensed, for example if you're using the 21-day evaluation version, then you can access the License dialog by clicking Help, License. In the License Manager dialog, click on Activate Product Key and then in the Offline Activation section, click Create C2V. From this dialog, you can choose where to save the file. I'm going to save it on my USB drive and you can choose the file name. You can name it anything you want as long as you leave the extension as C2V. C2V stands for Customer to Vendor and is a file type that is recognized by the Triangle Microworks license server. Once you've selected a location and name for the file, click Save. You can then close the dialog and move to a computer with internet access to activate the license. Next, go to a PC that does have internet access and that can access the license file. Since I saved the file on USB drive, I can just take that file to another PC that does have internet access. From there, I will launch the browser and go to HTTPS licensing.trianglemicroworks.com slash EMS slash CustomerLogin.html. The browser then displays the Customer Login portal. Enter the product key that you receive from Triangle Microworks and click Login. Review the licensing information to be sure it's correct. There should be a non-zero value in the remaining field indicating that one or more activations are available for this key. If everything looks correct, click the Offline Activation button. Then, upload the C2V file that we previously saved and then click Generate. The license is generated on the server and now needs to be downloaded. Click on the Download V2C file link to download the file. Depending on your browser, the file may automatically be downloaded to a Downloads folder or it may open a browser and allow you to choose the location to save the file. Using Chrome, it's automatically downloaded to the Downloads folder, so I can show the folder and see my V2C file. I'm going to copy this file to my USB drive. Note that V2C stands for Vendor-to-Customer and represents one instance of the license file. I can now look on my USB drive and see that I have the V2C file. Now that a copy of my V2C file has been saved on the USB drive, I need to return to the PC that I want to license and install the license. On the PC where I want to install the product, I need to launch the product and go to Help, License. I then choose Activate Product Key and this time I'll click on Install V2C in the Offline Activation section. This opens a browser and I will browse to the V2C file that I saved. I select the V2C file and select Open. Now I have the dialog that says the license was installed correctly, but I need to restart the application in order for the change to take effect. So I exit the application and restart it. Now I go to Help, License and I can verify that I have a permanent license with an active maintenance plan installed. As you can see, it's easy to install a license on a PC that does not have internet access. However, you do need to be able to access another PC with internet access so that you can save off the C2V file, go to a PC with internet access, generate the V2C file and then return to the PC to be licensed and install the V2C file. Once you do that, your new product will be fully licensed.


Video ID: x_MUVwW_JCE
Thank you for taking the time to see this demo. For more information, please view our other demo videos, the product overview video, or our quick start videos. The SCADA Data Gateway and Communication Protocol Test Harness offer 21-day full-featured evaluations. Please visit our website to download the latest evaluation version.


Video ID: Bu2YMg72OdY
Let's take a look at configuring slave components on the SCADA data gateway. In some protocols, slaves are also referred to as outstations or controlled stations.


Video ID: GlQlCaj9hC8
Welcome. This presentation is provided by Triangle Microworks as an introduction to using XML for DNP3 self-description and mapping to IEC 61850 object models. It also serves as an example of our online training tools. More detailed information on DNP3 is available in our free Introduction to DNP3 video. Even more detail is provided in our subscription-based DNP3 training course. Free videos demonstrating our products and how to use them are also available. For more information on our training videos, please visit our website at www.trianglemicroworks.com. The device profile document was introduced in the original Basic 4 specification. Over the next several years, the tech committee worked on subset definitions, the land-land specs, certification procedures, and various other technical bulletins. In 2007, these were all rolled into the Version 2 document set. This set expanded the number of parameters in the original device profile document and defined an XML schema for a machine-readable version of the device profile. Beginning in 2009, a DNP3 device is required to have its device profile available as an XML file. The steering committee has approved a stimulus plan to fund creation of the first several outstation DNP3 XML files. The Triangle Microworks ICD editor can also be used to define XML-based DNP3 profile documents.


Video ID: c6KknqTKwgg
Do you need to learn industry standard communication protocols? We offer self-paced, web-based communication protocol training for DNP3 and IEC 61850. The subjects are organized so that you can pause, rewind, or even replay each segment. We include quizzes and interactive exercises using our test tools to reinforce key concepts. And the table of contents and hyperlinks allow you to use the training as a reference tool even after you finish the initial training. We also offer free introductory videos on communication protocols and our products. To view these, please visit videos.trianglemicroworks.com.


Video ID: 6JkBM5wSj2k
Do you need to integrate industry standard communication protocols in your project? If so, our SCADA Data Gateway product can help. The SCADA Data Gateway is a Windows application. It's compatible with embedded and industrial PCs, as well as desktop PCs and servers. It can serve as an OPC driver, communicating with external OPC clients and or servers. It can also act as a protocol translator or as a data concentrator. The SCADA Data Gateway supports bidirectional data translation. For example, it can pass data from an outstation to a master, as well as send commands from that master back to the outstation. The built-in protocol analyzer display simplifies debugging, and the event logger makes it easy to see your data changes. It also supports expressions and equations to perform data transformation. For example, if a device reports integer values, the SCADA Data Gateway can scale those values and report them as floating point numbers in engineering units. An ODBC client component is available for the SCADA Data Gateway, providing access to any database management system, such as SQL or Microsoft Access. The SCADA Data Gateway supports a variety of OPC options, including OPC data access, OPC alarm and events, and OPC XML data access for interfacing to PC-based data systems. The OPC XML DA component can even support non-PC-based SCADA systems. The Insight component adds data visualization in a scripting tool through the SCADA Data Gateway's OPC DA interface. For more information, please visit the SCADA Data Gateway page of our website at www.trianglemicroworks.com slash products slash SCADA-Data-Gateway.


Video ID: v3IecQAJv2U
Next, let's look at some use cases for DNP3 XML device profiles. How are DNP3 XML files created and how are they used? The most common uses are shown in this diagram. The file is ideally generated by the IED or by PC-based configuration software. Otherwise, any XML editor could be used. If read directly from the IED with DNP3 file transfer, it would probably contain only the current values to reduce the file size. A separate capabilities file would contain the capabilities and configurable methods columns. This would typically be a generic file for each device provided by the vendor that does not change with each installation. System configuration tools might take one of these files as input and instantiate it with values for a specific installation. It could also qualify devices and ensure required features are supported for a specific application. A DNP3 master can use this file to set all communication parameters to connect to the device and display data as 61850 object models or DNP3 point maps with descriptions and other information. The DNP3 users group provides an XSLT file that can be used to translate the DNP3 XML into HTML and viewed in a standard browser like Internet Explorer or Firefox. It can then create a PDF file or a printed copy. The XSLT translation to HTML is only in one direction, but all others could read or write the DNP3 XML file. For example, the DNP3 master could write the file to the IED to change some configuration parameters. The MS Word version that is in common use today is equivalent to just the HTML or paper version.


Video ID: TTxXChQx8VI
This video demonstrates the Goose Messages Viewer in Hammer, the 61.850 client component of the 61.850 test suite. To view Goose Messages, we first need to subscribe to Goose Messages. If we notice the Goose Message option in the menu bar here, it's grayed out, meaning there are no Goose Messages to view. If we go to the Goose Control Block and we subscribe, now we can begin receiving Goose Messages. Now if we go to the Goose Messages window, we can see those Goose Messages here. Much like the report window, we can see the header information in the left pane. Of course, we can scroll over and see all of the header information. And if we select a report, we can see the data in tree form in the right pane. So this lets us see all of the Goose reports. We can right-click and scroll to the bottom to see the most recent report. We can also select auto-scroll so that we stay at the bottom.


Video ID: ug3FYaK2054
There are several ways to get Anvil to send unexpected or invalid results, which is useful for negative testing. In this video, we're going to look at how to override report control block options. If we look at Anvil and find the report control block, in the properties panel, we can see the optional items to include in the report. And below that, we have the option of overriding those optional items. So let's start with the default, and then we will come back and override some and look at the result. So first in Anvil, I'm going to start simulating data. Then in Hammer, I'm going to enable the report control block. Before I do that, notice that the information reports is grayed out. But when I enable the report control block, this now changes color and indicates that we have information reports available. I'm going to expand this over so we can see it a little more clearly. So now we can see the reports. Notice the sequence number is continually incrementing. And let's expand the timestamp where we can see that a little better. Now let's go back to Anvil. I'm going to enable overrides. I'm going to pretty much mimic the correct report, except I am not going to send the timestamps. So now when we look back in Hammer, we see the reports are coming in, but no timestamp is being reported. If I go ahead and report those here, now we see they are reported again. In another example, we will not report the buffer overflow overrides. And now we see that that's not being reported in the reports. As you can see, it's pretty easy to override the reports and to send the client information reports that it's not expecting. The ability to handle reports that it's not expecting is one of the conformance tests for a client. So Anvil supports you in making sure your clients are conformant.


Video ID: CcVRDoIJI-g
In this video, we will configure multiple master devices to communicate with the outstation devices configured in the previous video. To create new masters in DTM, we right-click on DTM, select Manage, Add, and I want to group my masters together, so I'm going to add a folder. We'll name the folder DMP3 Masters. Folders allow us to group similar stations together. We can then take action on all of the devices in the folder at the same time. For example, we can set up data simulation on all of the outstations or set up polling on multiple masters. We'll show examples of this in future videos. Once the group has been created, I can right-click on the folder and select Manage, Add, DMP3 Masters. This opens a dialog which lets us configure the device. We can click through the various tabs and see the different configuration options. For this demo, we'll use the default selections on the Device, Channel, and Session tables. The Advanced tab allows configuration of items that are not usually changed. Once again, we'll just take the defaults. On the Multiple Devices tab, we'll select the Create Multiple Masters radio button. We want to communicate with six outstations in this demo, so we'll create six masters. Notice that the table is populated with configuration data for each device. Here, we could change the device name or other information. Note that by default, the devices start with the port specified on the Channel tab, in this case, the DMP3 default port of 20,000, and increment for each device. Since we're just using the defaults for this demo, we could select OK. However, I'm going to cancel in order to show how to configure devices using a comma-separated value or CSV file. Using a CSV file allows us to configure multiple devices at the same time, even if their configuration differs. Here, we see a sample CSV file. Notice the first row describes the contents of each column, and each subsequent row contains configuration information for the device. In this example, I'm configuring six devices with pretty much identical configurations. However, we could just as easily have different configuration values in each column, allowing us to configure multiple devices with different configurations. To load the CSV file, I click Load Configuration from CSV File and select the file in the subsequent browser. Once again, we can see each of the devices and the configuration as it was loaded from the file. Now, we can click OK. We now have six DMP3 master stations. If we look in the device tree, we can see that each of the masters and outstations now show a successful connection. In the next video, we'll look at how to display a DMP3 protocol analyzer. Subsequent videos will show how to initiate polling and issue other DMP3 commands from the masters.


Video ID: oZIJrgU6nns
So, the previous generation of SCADA protocols were designed around the idea of optimizing bandwidth and processor loading while maintaining data integrity and reliability. These protocols like 870-5 and DMP Modbus do a great job of optimizing these things, but the next generation of protocols, such as 61850, are trying to reduce the data management problem while also maintaining integrity and reliability. So, they're really focused on different problems because the cost drivers have changed now over the years.


Video ID: E20IiOD8GSE
The substitution model can be applied to either status points or measured value. The idea behind the substitution model is that you want to be able to, the client needs to be able to substitute a value so that the process logic in the server is using something other than its actual measured value. This model can be applied to a number of places within the object model. It also is reflected, when you do a substitution, it's reflected in both the value and the quality that's available, the quality flags. You can specify a value and then substitute or unsubstitute, which means switching between the live process value and the substituted value. Basically, the servers can include a substitution model wherever they want, so you specify at the server which data objects within its object model can have the substitution operation performed on them or not. 61850 includes the concept of a settings group. This model allows a set of parameters to be switched all together, so there will be a settings group, which is a group of parameters. Within the server, there will be multiple copies of each parameter. Exactly one copy of each is active at a time and one copy is editable. The client can then have an active group running, modify the settings within the editable group, and then later on make that edit group become active. This model is meant for things like relays where you really have to set a group of parameters simultaneously. You can't set them individually, you need to set them as a bank or as a group for the process logic to operate correctly. This icon view of the settings group model sort of shows how this process works. There's a control basically that you can do a get or set on the active group control that switches the process logic from one bank of values to another. At any given point in time, you can also set the edit group, which switches for which group is available for editing, and then you can do get and set requests on the editable group to modify the values so that next time you make it active, it's got a new bank of values. Typically, you'd have some multiple settings groups and you'd be switching between them as needed. These icons that we're using will come up in the rest of the presentation. The arrows on the left show the requests and responses. The little squares and triangles and circles represent data objects.


Video ID: m1gUGreAeQg
In this series of videos, we'll look at using the Distributed Test Manager, or DTM, with DNP3. We'll show how to configure multiple outstation devices using comma-separated values or CSV files to create the point list, how to configure DNP3 masters to communicate with and retrieve data from multiple outstations, using a simple insight display to show data retrieved by the master, monitoring communications using a built-in protocol analyzer, and several ways to simulate data in DNP3 outstations. For this video, we used a topography similar to the one used for IEC 61850 demonstration at Distributech in 2015. Multiple outstations are configured to simulate data. Data changes are sent to the master, which updates the insight display. For more information on the DTM, please visit www.trianglemicroworks.com slash products slash testing-and-configuration-tools slash DTM-pages. That link is provided below for your convenience. You may also go to www.trianglemicroworks.com, select Products, Testing and Configuration Tools, and Distributed Test Manager.


Video ID: NMYiV54FfEA
In this video, we'll create multiple DMP3 outstations in DTM. To create new outstations in DTM, I right-click on the DTM workspace, select Manage, and Add. I want to group my substations together, so I'm going to select Folder. We'll name the folder DMP3 Outstations. Folders allow us to group similar stations together. We can then take action on all of the stations in the folder at the same time. For example, we can set up data simulation on all of the outstations, or set up polling on multiple masters. We'll show examples of this in future videos. Once the group has been created, I can right-click on the folder and select Manage, Add, DMP3 Outstations. This opens a dialog which lets us configure the device. We can click through the various tabs and see different configuration options. For this demo, we'll use the default selections on the Device, Channel, and Session tabs. On the Database tab, we currently see the default database configuration. If desired, we can clear the database using the Reset Database button. We can also use the Add Points section to configure points in the database. For example, let's add two analog input points. We can also use a comma-separated value, or CSV file, to configure the point map of a device. Here we can see a CSV file. Notice the first row describes the contents of each column, and each subsequent row contains configuration information. In this example, we're only configuring two digital inputs, representing the breaker state and the breaker failure indication, and one analog input point, representing the phase A current. To load this file, I select Import Database from CSV File, and in the subsequent browser dialog, I select the file and click Open. Notice the table in the Database tab is now populated with the points described in the CSV file. The Advanced tab allows configuration of items that are not usually changed. Once again, we'll just take the default values. On the Multiple Devices tab, we'll select the Multiple Device radio button. We want six outstations for this demo, so we'll set the number of devices to six. Notice that the table is populated with configuration data for each device. Here we could change the device name or other information. Note that by default, the devices start with the port specified on the Channel tab, in this case the DNP3 default port of 20,000, and increment for each device. We'll just use the defaults for this demo, so we could select OK. However, I'm going to cancel, and show how we can use CSV files to configure multiple devices. Using a CSV file allows us to configure multiple devices at the same time, even if their configuration or database differs. Here is a sample CSV file. Notice that the first row describes the contents of each column, and each subsequent row contains configuration information. For this demo, all of our devices are configured the same, however we could just as easily enter different values in the columns for each device. Note that the final column links to a CSV file to configure the database. In this example, I've linked each device to the sample CSV file shown moments ago, however we could just as easily have different CSV files for each device, allowing each device to have a different point map. Let's select the Create Multiple Outstations radio button, and then the Load Configuration from CSV file, and select the configuration file. The devices and configurations from this file are now shown. When we click OK, the devices will be created. Now we have six DNP3 outstations with a customized point map configured from a CSV file. In subsequent videos, we'll configure Masters to communicate with each of these outstations, and look at various ways to simulate data in the outstations.


Video ID: zaqm1O8mWuM
The configured IED description files, or CID files, are basically generated by IED configuration tools from the substation configuration description. They basically will take out the information from the full substation description that�s specific to the IED, including all of the configuration parameters and addressing information, and put that in a smaller file, which can be downloaded to the IED. In a lot of cases, these files won�t be used. Instead, the IED configuration tool may, in fact, configure the devices using proprietary formats. This last step isn�t necessarily required, but eventually we think this is the way all devices will be done. It�s much cleaner to be able to use this, and it also stands as a documentation for what�s really there in the device.


Video ID: hqIWhzButd0
The following video demonstrates the use of report-by-exception, or RBE, data in DNP3 and with the test harness. This demo illustrates report-by-exception, or RBE, data reporting. DNP3 makes extensive use of RBE data to report only data that have changed, as well as to report all data changes. Note that RBE can be reported using either polling or unsolicited responses. In this example, we'll be using polled report-by-exception. In this demo, the test harness is connected to an off-screen microprocessor board. Binary inputs 0 through 4 represent five front panel buttons on the microprocessor board. First, note that none of the buttons are pressed, so the current state for each button is 0, or off. If I now press and hold one button and issue an RBE poll, we can see the status go to on, and if we look in the protocol analyzer log, we can see that value reported. If I now release the button and issue a new RBE poll, we see the transition go back to off, and again, we see the status reported. Note that for binary inputs in DNP3, the most significant bit, or that 80-bit, represents the current state, and the least significant bit, this 1, indicates that the point is reported as online. Now, if I press and hold the button, or press and release the button several times, and issue an RBE poll, notice that the current value is still off, because the button was released at the time the poll was issued. If the device only reported the current state, all of those intermediate transitions would have been lost. However, if you look in the protocol analyzer log, you can see that each of these transitions were reported, along with a timestamp of the time of change. With DNP3, RBE polls are usually issued on a periodic basis. We can do this in the test harness by configuring a repeat interval. To do this, double-click on the command and set up the repeat interval. In this case, we'll set up 3 seconds, so we click Start Repeat. Notice in the command window, there's a status bar indicating the status of the repeats, and if we look in the protocol analyzer window, we can see updates every 3 seconds, and as I press and release the button, we can see those transitions reported in the various RBE polls.


Video ID: LHs5Tg41bTw
Hammer, the 61850 client component of the 61850 test suite, provides several different methods for reading data. This video shows the Hierarchical Data View. In the Hierarchical Data View, the object model is shown as a tree, so we can expand the tree and drill down to see specific data points of interest. Once here, we can right-click and read the value. We can also right-click and start polling, specifying a specific polling interval. If the report control block is enabled, then we'll also see the live data updates in this view. For writable points, we can come in here and edit the value as well.


Video ID: 1VCSHV6fpYQ
To add an OPC client, right-click on Gateway and select Add OPC Client. Choose the node name or IP address. If you'll be talking to a client on the same machine, you can use the internal loopback address of 127.0.0.1. Next, click Find the OPC Server and find the server you want to communicate with. You can double-click this server or click it and say OK. Next, enter an alias name and say OK.


Video ID: KVSFj4IagTQ
This series of videos shows the various demonstrations from our booth at the 2015 Distributech Conference in San Diego. If you were able to visit our booth, these videos can be used to refresh your memory, view the demos in more detail, or see any demos you may have missed. If you missed us in San Diego, we're pleased to invite you to visit our booth virtually via these demos. The UCA International Users Group, or UCAIUG, held an interoperability demonstration in their booth at the SIGGRADE 2014 Conference in which vendor equipment was configured from an SCD file to implement a breaker failure scheme. This slide shows an overview of the demo. Triangle Microworks participated with the following equipment. Our Distributed Test Manager, or DTM, simulated IEC 61850 clients to read data from each device. Our IEC 61850 source code libraries were running on a TI BeagleBone board, and we had Anvil, the IEC 61850 test suite, simulating an IEC 61850 server. The following demonstration will simulate communication and breaker failure behavior for six of the devices defined in this SCD file using the Distributed Test Manager. For the breaker failure simulation portion of the demo, a trip can be initiated remotely using a goosePIOC.op message. The breaker attempts to trip and simulates a breaker failure. It then broadcasts a gooseBreakerFailure message, rbrf.opx="true". Upon receipt of this gooseBreakerFailure message, each of the other devices will trip their breaker. We've enhanced the demo to also show secure substation monitoring and control. The SCADA data gateway has been added to translate the IEC 61850 data from the devices to a TASI 2 or ICCP client and a DNP master. The ICCP client is simulated by IRNN, our TASI 2 ICCP test tool, and a DNP master is simulated by the Distributed Test Manager. All communications in this demonstration use security defined for the corresponding protocol. The following videos in this playlist show each of the pieces of this demonstration in more detail.


Video ID: S19glqVhn2Y
It completes the discussion of the abstract services, now we come to the data model describing how to build out the object model that those services are going to operate on. The common data classes is a key part, this is part 7.3, and it defines attribute classes for data objects that are used in part 7.4 to build up logical nodes. The common data classes have standardized names, standard data types, and each attribute of a common data class is classified by its use as a measure and status or configuration, for example. An example of a common data class is the measured value class. Here we have the magnitude of the measurement or the instantaneous magnitude, both of those are of type analog value which is defined elsewhere. The functional component that they belong to is MX, which is measurement. Instantaneous is optional, some devices would have it, some wouldn't. The quality is a standard type that's used that defines essentially a bunch of bits for quality bits that all vendors share. And timestamp, which is also mandatory, our times are in UTC time. The units here, talk about the units of the measurement, that's in a configuration CF functional component, also optional. And DB is the dead band, it's an integer value that's used for detecting changes for doing reporting. So the idea is that there's a configurable value for the dead band that controls the rate of reporting for any particular measurement. Here's another example common data class. The double point control, DPC. DPC contains a control value, which is a functional component CO, which is optional. It includes a status value, which is enumerated and it's basically a 2-bit status value that you may be familiar with. The idea is to model 2-bit so you have an invalid value, trip, close, and traveling. Those are functional component ST. The sub-enable is a Boolean value that controls whether or not to use the substituted value, which is subval, so that you can substitute for that status value. Pulse config allows you to control the pulse shape for this particular control, also a configuration value. Typically, the CF values are all specified in the SCL file and get their definitions there, but often they're writable in the end system. Basically, data objects within the model are instances of common data classes, and logical nodes are constructed by putting together groups of data objects to meet some particular application requirement. Here's a simplified example of the logical node for a circuit breaker. It contains a double point control for position, which has a pulse configuration. It has a single point control, SPC, for blocking open and blocking closed. Obviously, in the real models, there's many more attributes and many more configuration parameters, but this is just a very simple version of a circuit breaker. This is some examples of the logical nodes defined in Part 7.4. XCBR is the circuit breaker, XSwitch, circuit switch, current transformer, voltage transformer, gas measurement unit. There are many, many more, all defined with standardized control elements, status elements, and measurements. This is another set of instances of our logical nodes with circuit breakers, the switches here, gas measurement unit, and a process bus that is connecting all of the IEDs that implement these logical nodes.


Video ID: uOM9dVQv3xw
Do you need to test, troubleshoot, or configure industry standard communication protocols? If so, Triangle Microworks can help. We offer a variety of tools for this purpose, including the Communication Protocol Test Harness, which supports DNP3, Modbus, and the IEC 60870-5 family, IEC 61850 Test Suite Pro, which supports IEC 61850 and IEC 61400-25, IRN, which supports IEC 60870-6, also called TASI-2 or ICCP, the Distributed Test Manager, which supports IEC 61850 and IEC 61400-25, DNP3, Modbus, and the IEC 60870-5 family, as well as IEC 60870-6, also called TASI-2 or ICCP, and DNP3 Forge, which supports DNP3 device profiles and DNP3 to IEC 61850 mapping. The Communication Protocol Test Harness is a Windows application that supports DNP3, the IEC 60870-5 family, and Modbus. The test harness can simulate both master or controlling stations and slave, also called outstation or controlled station, devices. Depicted here are two outstation device simulations complete with front panel emulation and a master control window. The test harness can also be used to monitor communications on both serial and TCP IP channels. The test harness is easily configured using a Graphical User Interface, or GUI. And in addition to the GUI, complex custom functional tests can be developed using any .NET programming language, Python, or TCL TK scripts. An optional module allows you to automatically perform conformance test procedures. For more information on the test harness, please visit our website at trianglemicroworks.com slash products slash testing dash and dash configuration dash tools slash test dash harness dash pages 61850 Test Suite Pro is a Windows application that supports IEC 61850. Typical use cases include implementing the IEC 61850 engineering process, testing new implementations of IEC 61850, deploying or modifying SCADA systems across a large number of devices, individual IED testing and validation, as well as preparing for formal conformance testing. For more information on Test Suite Pro, please visit our website at trianglemicroworks.com slash products slash testing dash and dash configuration dash tools slash 61850 dash test dash suite dash pro dash pages. The Distributed Test Manager, or DTM, is a Windows application that simultaneously supports any combination of IEC 61850, 61400-25, IEC 60870-6 or ICCP or TASI 2, DNP 3, Modbus, and the IEC 60870-5 family. Typical use cases for DTM include automated testing of RTUs, IEDs, gateways, and SCADA systems, network load testing for SCADA systems, substation communication testing, and network and cybersecurity testing. For more information on DTM, please visit our website at trianglemicroworks.com slash products slash testing dash and dash configuration dash tools slash DTM dash pages. SCL Navigator is a Windows application that supports creating, viewing, editing, and verifying IEC 61850 substation configuration language, or SCL, files. With SCL Navigator, you can view the SCL file in either the raw or native XML format or as a data model. You can also easily create a clean data model using the built-in models and templates. And you can verify SCL files so that you can quickly and easily find and correct any issues in your SCL files. For more information on SCL Navigator, please visit our website at trianglemicroworks.com slash products slash testing dash and dash configuration dash tools slash SCL dash Navigator dash pages. Iron is a Windows application that supports IEC 60870-6, also called TASI 2 or ICCP. Iron can act as an IEC 60870-6 client and server. It supports bidirectional or peer-to-peer and unidirectional test applications. It fully simulates blocks 1, 2, and 5 for clients and servers. And it supports versions 1996.8 and 2000.8 of the specification. Iron supports multiple methods for defining client and server models. For example, it supports both XML and CSV model files, as well as Discovery. It supports building new client and or server models via a GUI. And it supports multiple simulation options for changing server-side data. The Iron client and server support read and write, reporting via DS transfer sets, controls, and dynamic data sets. For more information on Iron, please visit our website at trianglemicroworks.com slash products slash testing dash and dash configuration dash tools slash Iron dash pages. DNP3 Forge is a Windows application that supports DNP3 XML device profiles. Both free and paid versions are available. DNP3 Forge allows you to easily create or edit DNP3 device profiles. It also displays device profiles in an easy-to-navigate searchable tree. DNP3 Forge supports multiple versions of the DNP3 device profile schema, and it allows you to easily change the profile version. DNP3 Forge also supports mapping between DNP3 and IEC 61850. For more information on DNP3 Forge, please visit our website at trianglemicroworks.com slash products slash testing dash and dash configuration dash tools slash DNP3 dash Forge dash pages.


Video ID: ysJigSkQFSI
This demo shows several of our products in action, including our SCADA data gateway, the communication protocol test harness, and our DNP source code library. On the far right, the triangle represents a small RTU or IED running our source code libraries. The compactness of our libraries is illustrated by the fact that the small processor you see is running our source code library. In the middle, we have a SCADA data gateway that is acting as a protocol translator and data concentrator. Above the SCADA data gateway, we have two OPC clients. The one on the left demonstrates a small SCADA system, developed using Nikonics Genesis 32, that displays the current data in the triangle. On the right is a simple OPC client that displays all points in a tabular format. The simple OPC client was developed using Visual Basic and the source code, a few hundred lines of code, ships with the SCADA data gateway. On the left, we have a communication protocol test harness. The communication protocol test harness is a Windows application that acts as a simple master or slave device, and can also be programmed with an automated test sequence through a scripting capability. The test harness can be used to emulate a master and or outstation. In this example, we're running four outstation emulations that ship with the test harness. An ABB power control device, or PCD, a Schweitzer SEL351S, a generic DNP3 outstation, and a generic Modbus outstation. Each of these devices, along with the triangle, is sending data to the SCADA data gateway. We can press the front panel buttons to cause data changes and see the corresponding change in the SCADA data gateway and the OPC client. In this little video, we're going to show some data flowing through the SCADA data gateway. To begin with, let's look at the triangle. We can see in the SCADA data gateway configuration window, the green switch is on, the red switch is off, and we've got them displayed in the ICONIX OPC client as well. Interesting thing about the triangle, down is on and off is up. If I go over to the triangle and I flip the red switch, notice it just toggled to down, and the red status also updated, so it's now on. So, what happened here, we flipped the switch on the triangle, and that was reported up via DNP to the SCADA data gateway, and that was passed to an OPC client with a visual HMI to show the operator that the switch position changed. Let's take a look at another example. I'm going to bring in the command window from the test harness on the far left, and I'm going to bring in the ABB PCD simulator from the test harness running on the middle screen. Here, this master DNP simulator communicates to the SCADA data gateway via serial connection, and the PCD simulator is communicating to the SCADA data gateway through a TCP IP connection. So, we're seeing the gateway used as a data concentrator and also translating from a serial to a TCP IP connection. If we look in the OPC client, we see the current state 52A is on and 52B and 79LOA are both off, and if we scroll down a little bit and look over here at the currents, we see that there is current flowing through the relay. If we look at the relay itself, it says it's closed both in the front panel indicator and in the little side status here. So, I'm going to come back over to the protocol command window, and I'm going to issue a trip command. Now, when I issue that trip command, that's going to the SCADA data gateway, which passes it down to the PCD. Notice now the PCD is open. The state up here also shows it's open, and if we go back and look in the OPC client, all of the currents are now zero, and if we look at the top status, they've changed as well. 52A is now off and 52B and 79LOA are both on.


Video ID: mq8zUGOXOBI
DNP3 secure authentication is an enhancement to the DNP3 protocol. It's a set of additional function codes, objects, rules, and definitions to be added to the standard protocol. It's contained within a supplement to Volume 2 of the specification. It's designed to ensure the receiver of a message can verify that the message was sent by an authorized user and verify that it was not modified in transit. DNP3 experts would say this means it addresses the threats of spoofing, an attacker pretending to be an authorized user, and modification, an attacker changing the message in transit. In addition, DNP3 secure authentication mechanism includes features that protect against replay. Replay occurs when an attacker captures messages being transmitted on an open link, for instance a radio system, and plays them back at another time, attempting to cause havoc by making the system do something it wasn't supposed to. To protect against such an attack, DNP3 secure authentication includes information in each message that changes each time a message is sent. DNP3 secure authentication can also help to protect against the threat of repudiation, that is, a valid user claiming not to have performed an incorrect or illegal operation. Although logging is not part of the specification, the authentication mechanism gives the remote device the necessary information to store a permanent log of who performed what operations, providing some degree of repudiation. However, non-repudiation usually requires a digital signature and sometimes a third party. Secure authentication uses a MAC, not a digital signature. A signature requires PKI, so that only one user knows the key and could have a signed message. For these reasons, PSK systems cannot claim non-repudiation because two instances know the key at the same time. It's also important to understand what DNP3 secure authentication does not attempt to do. The mechanism does not encrypt the message. This may surprise some people who assume that all electronic security involves scrambling the message. It was decided by the DNP3 technical committee on the advice of utilities that it was not necessary to protect the message against eavesdropping, an attacker reading what data was in the message. Encryption requires significant processing power on the part of the transmitting and receiving devices, and this cost was not considered worth the benefit, except for encrypting the keys that are downloaded to the outstation. However, DNP3 secure authentication does play well with encryption methods, such as TLS, on IP networks. DNP3 secure authentication also does not protect against traffic analysis, the ability of an attacker to determine what a utility is doing with a remote device by watching the pattern of message traffic. Lastly, it does not protect against denial of service, that is, an attacker transmitting so many valid or invalid messages at a remote device that it's made unavailable to valid users or it's brought to its knees. Denial of service attacks can rarely be addressed within the receiving device itself and are best dealt with by an intervening device like a proxy server or firewall that can filter unwanted messages. DNP3 secure authentication was designed with the following principles in mind. Authentication only. It addresses authentication only, not encryption or other security measures, as discussed already. It does not rule out the possibility of such measures being added to DNP3 later or through the use of external measures, such as a bump-in-the-wire link encryptor or TLS when used over an IP network. Application layer only. DNP3 must be used over a variety of different physical networks and may be bridged from one to the other, as in the case of a TCP IP terminal server or IP radio. Only authentication at the application layer will ensure end-to-end security. Application layer authentication also permits the possibility of protection against rogue applications that may be co-resident with the DNP3 application and attempt to use the DNP3 link without authorization. Based on standards, DNP3 secure authentication follows the IEC 62351-5 specification, which also applies to the IEC 60870-5 family of protocols. The IEC 62351-5 makes use of various ISO, IETF, and NIST standards so that the security technology used has been well proven. The only new portion is its application to DNP3. Bi-directional. Either the master or the outstation can authenticate messages. Challenge reply. It is based on the common security concept of challenge and response. The device performing the authentication challenges the sender by providing data that must be included in the authentication calculation. This makes spoofing much more difficult. It also means the receiving device also determines which messages must be authenticated. Preshared keys. DNP3 secure authentication version 2 assumed the sender and receiver previously shared a cryptographic key, or string of numbers, through some method other than DNP3. DNP3 secure authentication version 5 provides a method to permit these preshared keys to be changed remotely and securely. Backwards tolerance. The specification attempts to make it possible for a non-secure device to identify that a secure device is using a portion of the protocol it does not recognize without an authentication message causing the non-secure device to fail. This principle is dependent on how well the DNP3 implementation on the non-secure device handles unexpected messages. Upgradable. Security technology is always changing, so the mechanism permits the sender and receiver to identify which algorithms, data, and key sizes that they are using and change these in the future. The specification also requires that any implementations support a mode in which security is turned off on a per-device basis for upgrading purposes. Multiple users. It assumes that there may be multiple users of the system located at the site of the master. It provides a method to authenticate each of the users separately from each other and from the master itself. The intent of this principle is to permit the outstation to conclusively identify the individual user, not just the device, that transmits any protocol message. The DNP3 secure authentication mechanism is based on the concept of a cryptographic hash. A hash is a function like a cyclic redundancy check, or CRC, or a checksum, that when performed on a message produces a much smaller string of numbers. The smaller hash value is very sensitive to changes in the message, but it is virtually impossible to determine the original message or the key if all you have is the hash value. Let's assume Alice is trying to send a message to Bob in a way that Bob can be sure the message is authentic. To make it work, Alice and Bob must have previously shared a key, or a string of numbers like a password, that only the two of them know. Alice performs the hash function on the message she wants to send, concatenated with the key. This produces what is called a Message Authentication Code, or MAC. DNP3 SAV5 and later also support MAC calculation methods that are not technically hashes, but the effect is the same. Alice sends the original message and the MAC value to Bob. She does NOT send the key, because it could be seen by an attacker. However, the message is not encrypted in this case. The attacker can see what the message is doing, but as we shall see, cannot modify it or send the false message of the attacker's own. Bob receives the message. Since he already has a copy of the key, he can now duplicate Alice's calculation. He hashes the message and the key together to produce a MAC. If Bob's MAC value matches the value that Alice transmitted with the message, he knows two things. First, the message has not been tampered with. If an attacker had tampered with the message, Bob's calculation would have been on a different message than Alice used, and therefore the MAC would have been different. The MAC calculation is carefully designed so that without knowing the key, an attacker could not modify the message in such a way that would produce the correct MAC. Secondly, Bob knows that the message came from Alice, or at least someone who knew the pre-shared key. Although the key was not transmitted on the link, it was intrinsic to the calculation, and without it, an attacker could not produce a matching MAC. Because of the way the calculation works, it is nearly impossible to determine the key from the MAC. When a hash is used with a key in this manner, it is known as a Message Authentication Code, or MAC. The mandatory hash function used for DNP3 secure authentication is defined by the National Institute of Science and Technology, or NIST, the Internet Engineering Task Force, or IETF, and the International Standards Organization, or ISO. It is known as the Secure Hash Algorithm, or SHA, or SHA. Two varieties of this algorithm, SHA-1 and SHA-256, may be used in DNP3 secure authentication. This diagram illustrates how IEC 62351 Part 5 provides a common standards basis for SCADA security worldwide. DNP3 secure authentication specification is compliant with IEC 62351-5, and the IEC 6070-5-101 and 104 standards will also be compliant with it. IEC 62351-5 is, in turn, based on International Standards Organization, Internet Engineering Task Force, and U.S. National Institute of Standards and Technology standards. DNP3 is challenging to secure because it can be used in a variety of networks, including radio systems, serial links, and IP-based wide-area networks. Furthermore, it's designed so that from one end of the network to the other, it may travel over more than one of these links. For this reason, DNP3 secure authentication is included in the topmost of the OSI layers, the application layer. There are three types of security that are commonly deployed in communication networks today. Site-to-site security includes the use of virtual private network, or VPN, routers and protocols such as IPsec to secure the link between two locations, for example, a corporate office and a home office, or a master station and a substation. It does not secure the networks at those two locations, and physical security measures like locks and guards are necessary to protect them. Device-to-device security includes the use of protocols such as transport layer security, or TLS, to secure the complete TCP connection between two devices, similar to when you access your bank through the Internet. However, TLS only works on IP networks and is therefore lost if DNP3 messages are forwarded over radios or serial links. It also does not address the possibility that rogue software applications may be installed on a device, making use of the fact that the device itself is considered secure. Application-to-application security ensures that individual users, not just devices, are authenticated by the remote devices and that the authentication information will be carried wherever the DNP3 message travels. It permits remote outstations to perform rule-based authentication and authorization so that the level of security changes depending on who is attempting to perform an operation. This diagram illustrates how DNP3 secure authentication may be used over a variety of networks. When used over the serial links or radio systems, the authentication function codes and objects are carried like any other DNP3 traffic. When used over IP networks, there are three different options. First, TCP with authentication only, recommended for use over wide area networks and mixed networks. Second, TCP with authentication and encryption, using transport layer security, recommended if messages must travel over a very insecure network, such as the Internet. And third, UDP with authentication only, used under specific circumstances when TCP cannot be used and the network is guaranteed to be reliable. Because the DNP3 authentication messages travel at the application layer, they are carried with the rest of the DNP3 message when terminal servers or IP-based radios are used. Some utilities are using these devices to avoid using routable protocols in some portions of their SCADA networks. Using DNP3 secure authentication ensures that there is a security solution even when terminal servers or IP radios are used. One advantage of DNP3 secure authentication is that it allows utilities to deploy DNP3 in a secure fashion over IP networks. Some utilities may be missing the benefits of deploying DNP3 over IP because they are concerned they will be required to deploy very expensive security measures if they do so. DNP3 secure authentication provides security in IP networks and thus provides an evolution path to other IP-based automation solutions, such as IEC 61850, in the future. Some vendors may permit such an evolution as a firmware change if the IP networking hardware is already in place. DNP3 secure authentication is suited for the following types of applications. New deployments. Because this is in addition to the DNP3 protocol itself, it will require new software or firmware at both ends of the connection. Therefore, it may be most convenient to deploy DNP3 secure authentication in new networks and investigate other solutions for legacy systems. Unprotected radios. Radio systems with serial link inputs that are not authenticated or encrypted represent one of the most vulnerable types of DNP3 deployments. Deploying DNP3 secure authentication permits these networks to be secured without hardware changes. Multiple users. In networks like the data concentrator system pictured here, DNP3 secure authentication permits a separate identifier to be assigned to each user at the master, for example, Alice or Bob, so that the operations performed by that user are tracked throughout the system. Data concentrators. The specification has rules defined to ensure end-to-end security even through a data concentrator such as the configuration pictured here. Varying protocol suites. As discussed previously, DNP3 secure authentication operates at the uppermost protocol layer and is therefore carried across networks that use different lower layer protocols. Mixed IP and serial. As discussed previously, DNP3 secure authentication works well with terminal servers and IP-based protocols. This slide shows what's addressed by DNP3 secure authentication. It addresses integrity, spoofing, man-in-the-middle, replay, authentication, authorization, message integrity, and role-based access. Eavesdropping is addressed only to the extent of securing cryptographic keys. The question that many utilities ask is, where does DNP3 secure authentication fit with respect to the NERC SIPs? Firstly, as of this writing, July 2014, the mandate of NERC applies only in bulk electrical system, that is, transmission. So in theory, distribution networks, where many DNP3 deployments are found, are not affected by the SIPs by definition. This may change in the future, however. Secondly, some people make the argument that DNP3 is not a routable protocol, and therefore does not need to be secured. That may be true of DNP3 over serial links, but it is certainly not true of DNP3 over IP, because IP, by definition, is a routable protocol. It's not clear how it applies when DNP3 is transferred from an IP network to a serial or radio network. Thirdly, some people make the argument that electronic security controls are not required for SCADA or data traffic, only for remote login to a device. Their argument is that the term external interactive access means remote login. It remains to be seen whether this position is defendable. Lastly, the location that DNP3 secure authentication may be deployed may vary depending on how your organization chooses to define its critical cyber assets and electronic security perimeter. This in turn may depend on how much power may be switched or shed on a given network. Having considered all these items, it may be that your organization chooses to define its electronic security perimeter in such a way that some of its access points carry DNP3 traffic. In these cases, DNP3 secure authentication is intended to provide a technical control to ensure the authenticity of the accessing party. DNP3 authentication version 5 improves on version 2 in several ways. It adds the ability to remotely change keys on all outstations without sending personnel to the site, reducing the cost incurred when an operator leaves the organization or a key is compromised. It adds the ability to control management of keys from a central authority using either pre-shared symmetric keys or security certificates using asymmetric keys, also known as public key encryption or PKI. The symmetric method is the mandatory default when asymmetric methods are optional. It provides better protection against denial-of-service attacks by continuing to operate as normally as possible even when suspicious events are encountered. It provides a simple means for detecting some types of attacks by requiring outstations to track and report statistics on the operation of the protocol. It supports cryptographic algorithms that were not available in the earlier version and changes some default algorithms because the older versions are no longer as secure. For instance, AES-256 is now the default for encryption instead of AES-128, and SHA-1 is discouraged, with SHA-256 being the default for hashing. SAV5 also supports the newer, less processor-intensive GMAC algorithm for calculating MACs.


Video ID: tCTwZsr7_Gw
Hello, I'm Don Downs, a Senior Technical Support Engineer at Triangle Microworks, and I'll be providing a brief overview of some of our products. This is actually the demo that we had running in our booth at the Distributech 2013 show in San Diego. So if you got a chance to come by and see us, this will provide a refresher of the things you were able to see there. And if you weren't able to come by and see our booth, we are providing this video so that you can see some of the capabilities of our products. In this video, we will demonstrate how you can use the 61850 test suite to simulate IEC 61850 servers, how you can generate the simulation files for use in those simulations. We'll show a couple of products using our source code library and .NET protocol components. We'll demonstrate mapping IEC 61850 to other protocols such as DNP3. We'll show the DNP3 communication protocol test harness and its support of DNP3 secure authentication version 5. We'll also be showing a new product we have called Insight, which is a graphical visualization tool so that you can get a visual representation of your testing. Here's a brief overview of our presentation. We start with an embedded processor. You see it in the graphic there, and here is the actual processor. It's using a Raspberry Pi board, which is a development kit. A little bit better view on the backside here, you can see the Raspberry Pi. This Raspberry Pi is actually running our IEC 61850 server NCC source code library. We've added some switches and some sliders so that we can simulate a real-world device. We'll also demonstrate a simulation of a device written using our .NET protocol components, and we'll be using Anvil running on an embedded PC to simulate those devices. We'll demonstrate Hammer, the 61850 client portion of the test suite. Hammer will connect to these two devices and generate simulation file, which will then be read by Anvil. We'll be running three instances of Anvil and using a flowchart of a part of the new Insight component to cycle through the various states and simulate different states of that breaker. We then connect Anvil to the SCADA data gateway using IEC 61850 edition 2. We map those points to DNP3 and connect to the DNP3 test harness using IEEE 1815 or DNP3 with secure authentication version 5. We also have an OPC client to provide a graphical view. This client uses OPC XML DA, and the graphical view was created using Insight, our new graphical tool. Insight can also provide graphical views in both Anvil and Hammer. We'll also demonstrate DNP3 Forge, which can be used to create and edit and maintain DNP3 device profiles. DNP3 Forge can also create mapping files for DNP3 to IEC 61850 per IEEE 1815.1 use case A. Those mapping files can in turn be used by other devices such as the test harness and the gateway to describe the mappings. In the next video, we'll demonstrate using Hammer to connect to remote devices and generate simulation files. Then we have a video showing Anvil using those simulation files to simulate real world devices. Next up is a video showing the use of the SCADA data gateway to map IEC 61850 to DNP3 and communicate to a DNP3 master, in this case simulated by the test harness, and in this example, using DNP3 secure authentication version 5. In subsequent videos, we'll provide an overview of Insight, the graphical tool showing graphical capabilities added to Anvil and Hammer, and we'll show the use of DNP3 Forge to create and edit DNP3 device profiles and DNP3 to IEC 61850 mapping files.


Video ID: Z0abVjWbS5I
Hammer, the IEC 61850 client component of the 61850 test suite, makes it very easy to work with log control blocks. Let's expand the tree and look at a log control block. Here we can see all of the attributes of the log control block in one view. We can enable and disable the block by right-clicking and choosing Enable Log Control Block. Once it's enabled, if we right-click, the prompt will change and give us the opportunity to disable the log control block. It's very easy to modify attributes simply by double-clicking and typing in the new value or by choosing the appropriate checkboxes. If the log control block is disabled, we can right-click it and select the appropriate data set reference. As you can see, Hammer makes it very easy to work with your log control blocks. You can enable and disable them as well as change attributes and select the appropriate data set.


Video ID: e9UmeZkOBMU
Thank you for your interest in our Communication Protocol Test Harness. This demo highlights the use of the Communication Protocol Test Harness to simulate a master and to perform testing. Both custom functional tests and the conformance test module are shown. The use of the Test Harness to simulate outstations is shown in our separate Skated Data Gateway and Test Harness demo video, as well as in our Test Harness Quick Start video. This demo uses the Communication Protocol Test Harness running on a laptop PC, a single board computer running our DNP3 Slave source code library, and the Nomicron CMC156 to generate test conditions under the control of the Test Harness.


Video ID: qqzIslQuUAc
The 61-50 reporting model can be pretty complicated. You essentially report members of a data set either by exception, typically by exception, in other words, only the changed values are sent in any given report message. The exceptions can be triggered either by data or quality changes or freeze updates for counters. There's also a periodic integrity report, multiple clients, multiple data sets, so you can have the same data going to different clients at different times within different data sets. There's a buffered and unbuffered mode that I'll talk about in terms of how that works, and there's various formats of reports that you can do to save on bandwidth and make things simpler for the client. Again, the model's a little bit complicated. I'll go into it in more detail in the next slide. This iconic view shows how the unbuffered reporting model works. On the left, we have data objects within a data set, which is the oval circle triangle square within that oval shows the actual data objects that are named within the data set. As changes occur to that data, we see a flow from right to left of the values that are occurring as the change events occur. There's some logic within the 61850 model that does filtering and formatting of those changes, that filtering and formatting process turns that sequence of changes into a sequence of messages. Each message can contain the value of one or more changes, essentially the values that have changed just within that whole data set at one given point in time. So you'll see that the first two changes in the sequence are reflected in the first message. The next change goes in a separate message. The formatting of that data and that sequence of report messages is controlled by a report control block that sets the parameters for the filtering and formatting process. That report control typically can be get and set, in other words, read and written by the client to be able to change the report control. Notice that, again, a given report control can be reporting one data set. The messages go basically to the client that subscribed to it. You can have multiple instances of report controls and multiple instances of data sets or data objects within multiple data sets. So the actual report can be going to multiple places at once and may be formatted differently in the messages if each report control block has different parameters. The buffered reporting model works much the same. The major difference is that after filtering and formatting, that sequence of changes or sequence of messages is stored in a buffer. And so the messages going off to the client, if they're delayed in any way, either because of flow control problems in the network or because the connection goes down, they remain in this buffer for some period of time, basically until they get overwritten. And there's signaling to tell the client that data has been lost if the connection is down for too long. Otherwise, it's the same model working also with a report control block. The basic concept is simply to allow some level of robustness to the model. The logging model within 61850 works very much the same as the reporting model. Basically, values from the sequence of changes of data within the data set are logged, uses the same triggering as the report model. It's logged to, again, a circular buffer, typically much larger than the reporting model. You can log multiple data sets into one circular buffer by basically using multiple log control blocks all pointing at the same buffer. Multiple clients can receive this log. It's open to requests from multiple associations at the same time. And you can retrieve values from that log either by entry number or timestamp. This diagram shows the log model. And again, other than the size of the log, the size of this circular buffer, it's the same as the buffered control or buffered report control model except for that instead of sending data on changes, the stream of data isn't actually sent to the client unless the client requests it. And they can request it, as I said, in multiple ways to retrieve the log entries which correspond to the reports in the reporting model. Again, there's a control block that controls this process, and the client can get or set the parameters to manage that. Now, I should mention that these parameters on some devices, limited devices, may in fact not be writable. They may be configurable through the substation configuration language file, in which case it's managed at config time, not at run time. And so, these operations wouldn't be the get and set requests on the control block would not be done real time in the network. It would be configured ahead of time.


Video ID: eFAok7For3c
The DNP3 application layer provides features required for data transfer and processing, control operations, and retrieval of data. The data transfer and processing features include time synchronization, time-stamped events, the ability to freeze and clear counters, and data groups and classes. DNP3 supports to-pass or select-before-operate control operations. Data retrieval can be accomplished via polling, either for all data, specific types of data, or data changes, as well as unsolicited responses. The application layer responds to complete messages received and passed up from the transport layer, and builds messages based on the need or the availability of user data. Once messages are built, they are passed down to the transport function, where they are segmented and passed to the data link layer, and eventually communicated over the physical layer. The total length of the received message is indicated by the transport function, as it appends data link layer frames, each with their own indicated length. When data to be transmitted is too large for a single application layer message, multiple application layer messages may be built and transmitted sequentially. However, each message is an independent application layer message. Their only association with each other is an indication in all but the last message that more messages follow. Because of this possible fragmentation of application layer data, each application layer message is referred to as a fragment, and a message may either be a single fragment message or a multi-fragment message. Application layer fragments from master DNP3 stations are typically requests for operations on data objects, and application layer fragments from DNP3 outstations are typically responses to those requests. A DNP3 outstation may also transmit a message without a request, called an unsolicited response. As in the data link layer, application layer fragments may be sent with a request for confirmation. An application layer confirmation indicates that a message has not only been received, but also has been parsed without error. On the other hand, a data link layer confirmation, or ACK, indicates only that the data link frame has been received and that it passes CRC error checks. Each application layer fragment begins with an application layer header, followed by one or more object header, object data combinations. The application layer header contains an application control code and an application function code. An application control code contains an indication if the fragment is one of a multi-fragment message. Contains an indication if an application layer confirmation is requested for the fragment. Contains an indication if the fragment was unsolicited, and contains a rolling application layer sequence number. The application layer sequence number allows the receiving application layer to detect fragments that are out of sequence or dropped fragments. The application layer function code indicates the purpose or requested operation of the message. While DNP3 allows multiple data types in a single message, it only allows a single requested operation on the data types within the message. Example function codes include confirm for application layer confirmations, read and write, select and operate for select before operate or SBO controls, direct operate for operation of controls without select before operate, freeze and clear, restart, both cold and warm, enable and disable unsolicited messages, and assign class. The application layer header function code applies to all these object headers and therefore all data within the message fragment. For outstations, the application layer header also includes internal indication or IIN bits. In DNP3, data are organized into data types. Each data type is an object group, including binary inputs, binary outputs, analog inputs, analog outputs, counters, time and date, file transfer objects, and others. For each object group or data type, one or more data points exist. A data point is a single data value of the type specified by its object group. Also within each object group, object group variations exist. An object group variation is typically used to indicate a different method of specifying data within the object group. For example, variations of analog inputs allow for transfer of the data as 16-bit signed integer values, 32-bit signed integer values, or as 32-bit floating point values. As described previously, an application layer message may contain multiple object headers. An object header specifies an object group, a variation of the object group, and a range of points within that object group variation. Some application layer header function codes indicate that object data follows each object header. Other function codes indicate that there is no object data in the message. Instead, multiple object headers, if present, follow each other contiguously. For example, a read request message fragment only contains object headers that describe the object groups, variations, and point ranges that are requested to be read and responded. A read response message fragment contains object headers and the request object data. DNP3 allows object point ranges to be specified in a variety of ways. For request messages, object point ranges may consist of a request for all points of the specified object group, a request for a contiguous range of points beginning with a specified starting point and ending with a specified stopping point, a request for a maximum quantity of points, or a list of requested points. For response messages, object point ranges typically consist of either a contiguous range of points beginning with a specified starting point and ending with a specified stopping point, or with a list of points. For response object point ranges that consist of a list of points, a point number precedes each data object. A number of points in the list is specified as part of the object point range. Next, we'll discuss data acquisition methods, data classes, and report by exception. In DNP3, static data refers to a point's current value. It is common for a class 0 poll to return all points in the outstation. This is not required, however. It is acceptable to have points that are not returned by a class 0 poll. In this case, these points can only be read by direct reads of the point. However, DNP requires that any points in class 1, 2, or 3 must also be in class 0. Data can be in class 0 and not be in class 1, 2, or 3. Assuming class 1 contains the highest priority change event data and class 3 contains the lowest priority change event data, a class 1 poll would ideally be performed as often as possible, a class 2 poll would be performed less often, and a class 3 poll would be performed even less often. For each class data response, only the class data that is changed will be returned, keeping the response messages small and efficient. The term static data refers to a point's current value. In DNP3, events are associated with something of significance happening. This is most commonly a state change or a change in the data's value. However, it could also be because a value crosses a threshold or exceeds a dead band. It might also represent a snapshot taken at a particular time. DNP3 supports three event classes called class 1, class 2, and class 3. The protocol specification does not assign significance to these classes. Classes may be assigned and changed via the protocol. Since these classes only return changed values, they are usually fast and efficient. DNP3 supports Report by Exception, or RBE. With RBE, the outstation only reports data changes. When an event occurs, the event is added to the event buffer. The outstation may add all events to the event buffer, or it may only record the most recent change. The DNP specification states that this must be configurable. Although it is often configurable on a per data type basis, it could be configured on other bases, such as per RTU or per point. With RBE, changes can be reported with event polling, typically called a class 1-2-3 poll, or via unsolicited responses. Events are not removed from the events buffer until it receives an application confirm from the master. This ensures that the master has received the events before they are deleted.


Video ID: GJQwPW9um2Y
The first step in upgrading your license is to determine the type of license that you need. This video provides an overview of the various types of licenses available. Users with a single-seat license can easily upgrade to a standard PC license. If you have an internet connection, the installation process will automatically connect to the licensing server and convert you to the new license system. Simply follow the instructions provided by the installation software. A demonstration of this process will be shown in the next video in this series. The new licensing system supports virtual machines with a standard PC seat license without the need for a dongle or a USB-based license. If you are using a USB license solely to support a virtual machine, then you can upgrade to the standard PC seat license. The conversion to the single-seat license is free and can be performed during the installation. If you have internet access, you can connect directly to the licensing server and upgrade during the installation. If you don't have internet access, the licensing process requires an exchange of files with another computer that does have internet access in order to activate the license. Simply follow the instructions provided by the installation software. An example of this process is shown in a separate video. The new licensing system eliminates the need for USB memory sticks. If you are using a USB license solely to support a virtual machine, then you can convert to the standard seat license. However, you do have some other options. Converting a USB-based license to a software-based per-seat license is free and can be performed during the installation. The software-based license works with both virtual machines and standard PCs. Each USB license with a valid maintenance plan is available for a free, one-time upgrade to a network-based license. The network license allows multiple users to share a license across multiple computers. Installation is easy because the network license server is built into the application. For use away from the office, the network license can be checked out to a PC and checked back in at a later date. If you are interested in a network-based license, please contact us at sales at trianglemicroworks.com for more details. For cases where a hardware-based license is still needed or preferred, we offer a hardware-based dongle license for USB licenses that are in active maintenance. This option is typically discouraged because if the dongle is lost or stolen, we require that the full license be repurchased before we can replace the dongle. In most cases, the software-based or network-based licenses will meet the requirements of a dongle license. Note that while the upgrade to a network-based license or hardware-based dongle license is free, your maintenance renewal fees for both of these types of licenses will increase for future renewals because the list prices are higher for these options. However, current maintenance plans will not be affected by the upgrades. If you have any questions about upgrading your license, please contact us at sales at trianglemicroworks.com.


Video ID: aZ774cbbCGQ
Welcome to the Triangle Microworks web-based DNP3 Secure Authentication Training. This course is divided into three levels. An introductory level provides information about what is secure authentication, and when should you use it. This level is typically targeted for an executive, manager, or purchaser. The intermediate level looks in more detail and answers questions like what does it look like and how does it behave? This level is appropriate for a technologist or a technician. And finally, the expert level explains why does it work this way and how would we implement it? This level is typically targeted for developers. We'd also like to acknowledge and thank the individuals who contributed to this training, including Grant Gilchrist of Internex, the lead technical content creator, and the reviewers, who included Eric Thibodeau of Gentech and Steve McCoy and Jewel Green of Triangle Microworks.


Video ID: ryEO68Qa4nc
In this video, we'll be talking about the concept of workspaces in TestSuite Pro. A workspace is a collection of IEDs you wish to work on. These IEDs can be instanced from one or more SCL files or discovered from active devices on the network. After opening a workspace, you can use our various tools and displays to interact with the data in your system. First, let's look at the backstage section of TestSuite Pro. Click File. Here, you can open existing workspaces, create new workspaces, close your current workspace, open the help, change configuration options, manage licensing, update TestSuite Pro, get information about TestSuite Pro, such as version number, start our example servers, and exit the application. While in the backstage section, we can create a new workspace, so let's click New Workspace. There are a few ways to add IEDs to your new workspace. You can browse to an SCL file by clicking Browse. Navigate to the SCL file you're interested in and opening it. TestSuite Pro will search through that SCL file and show you all the devices it located. It'll also show you if the devices you may want to add to your workspace have servers on your network it can connect to by showing a green checkmark. From here, you can add devices by selecting or multi-selecting the devices that you're interested in and dragging them over to your workspace. You can repeat this process with another SCL file if you want to instance IEDs from a different SCL file. You can also search for IEDs on your network by clicking the IP addresses and IP range radio button and typing in the range you're interested in. Then click Find Devices. Again, this will display devices TestSuite Pro found and will allow you to drag and drop devices onto your workspace. If you would like to simulate servers as well, click the Enable IED Simulator checkbox. You can also enable Goose Publishing to publish Goose messages without a server by clicking the Enable Goose Publisher checkbox. Once you have the devices in your workspace, you can edit their configuration by right-clicking on the device and clicking Edit Properties for this device. Here, you can edit the configuration data such as server IP address and local IP address. You can also add security settings by clicking the radio button of the type of security you'd like to use. Now, you can fill out the security configuration information. If you're planning to simulate servers as well as clients, you can go to the How to Simulate tab and configure the same options here, as well as Goose and Sample Values Adapters. After that, all you need to do is name the workspace and if you want, add a description. Click Save or Save and Open to get started. Now that you have a workspace, you can interact with many of our tools and help monitor and display the data that you're interested in. We'll go over this in a future video of this playlist. After making the changes you're interested in, you can save the workspace by going to the Workspace tool and hitting Save. To get back to a blank workspace, you can go to the Workspace tool and hit Close Workspace. Now, you can either build up a new workspace or load a previously saved one. To load a workspace, click File and mouse over the workspace that you're interested in. Here, you can load, edit, delete, or copy this workspace.


Video ID: nvzztwQwzlQ
In this next section, we'll take a look at self-description using XML. This functionality is defined in the Interoperability section of the DNP3 specification. Topics covered in this section include what is a DNP3 XML device profile, features of the DNP3 XML device profile, use cases, mapping DNP3 to IEC 61850 object models, and the use in the IEC 61850 engineering process. First, we'll look at what is a DNP3 XML device profile and some of its features. The XML device profile document is organized into four sections, configuration, points list, implementation table, and mapping to IEC 61850. The configuration section has four columns. The current value and configurable methods are not shown in this picture. Protocol capabilities indicates the option supported. The current value indicates which option is active. The next section, points list, describes the scaling in units and provides for a text description of the point. The implementation table describes which data types, function codes, and qualifier codes are supported. An IEC 61850 object model can be represented in the DNP3 XML device profile document by assigning each IEC 61850 data attribute to a configuration parameter, data point, or constant in the DNP3 XML file. An optional user data element is included in all main data structures defined in the DNP3 schema to allow user-specific data to be incorporated at various places within a DNP3 XML instance document. Note that no outstation code changes are required to support the new DNP3 device profile in XML. The XML file is just a new file that's supplied on disk or by a web download. This format file can be applied to existing devices that were placed in operation years ago. It does not interfere with real-time communications and is good for small DNP implementations as well. Note that file transfer is not required for any subset level. Generally, all the XML files would be stored in a central network location. If DNP3 file transfer is supported, then it could be used to read current values from the device or write new configuration parameters to the device.


Video ID: 3YrZa6CfUiI
The transport function is provided to reassemble link layer frames into application layer fragments. For each frame, the transport function inserts a single byte function code that indicates if the data link frame is the first frame of the message, the last frame of the message, or both the first and the last frame of the message. This includes a rolling frame sequence number which increments with each frame and allows the receiving transport layer to detect dropped frames.


Video ID: T_Ri0CCLX8M
In this second part, we will now introduce the various elements of the IC62850 data model. What are these elements? Some of the terms like logical devices, logical nodes, data objects, or data attributes you certainly have already heard about. But other terms are as well Common Data Class and Constructed Attribute Class. But what do they represent? First of all, they are elements in a hierarchical data model that we find within a 62850 server of an IED. The top level is the logical device. A logical device is a collection of multiple logical nodes. The logical nodes contain data objects. And finally, the structure of a data object is defined by the Common Data Class. In the example, INS is a Common Data Class for integer status information. The Common Data Class defines what data attributes are available for a data object. The data attributes are the leaves of the hierarchy. It is at the data attributes where we find the values. However, data attributes can as well have a structure as a data type. In that case, we have what we call a Constructed Attribute Class, shown here with the example quality. Now that we have introduced these elements of the data model, I will explain for what they are used. Let's start with a closer look to the logical node. Logical nodes are the key elements for modeling application functions. A logical node represents a functional element, like a distance element of a protection function or the interface to a switchgear equipment, like a disconnecting switch. The logical node groups all the data that belongs to that function element. In our slide, we see an extract of the logical node XSWI that represents the interface to an electrical switch, other than a circuit breaker. We have a first group of data objects defined here with the light blue background, which is as an example the data object POS, which represents the position of the switch, or a data object BLOCKOPENING, which represents that the BLOCKOPENING kernel is blocked. What we see as well is the indication M, mandatory, or O, for optional, whether the particular data object needs to be present or not. The second group of data objects that we see in this example, with the light grey background, are data objects that are more used to control the behavior of a logical node and less for the specific semantics based on the logical node type. Next, we introduce the data object and the data attributes. We use the example of the data object POS, which represents the position of the switch from the previous logical node XSWI. This data object is of the common data class DoublePointControl. A data object has multiple data attributes. The structure of the data object, or with other words, what data attributes are available, is determined by the common data class. We have a first group of data attributes representing the status information. Among them, we have STVAL, where we find the value of the position of the switch, which typically will be either open or closed, but can as well be in an intermediate or bad state. Then we have Q for the quality, indicating, among other, if the information is valid. And finally, T for the timestamp of the last change of the position value. These three attributes are the typical operational information of the switch position. But with the IC61850 data model, we attach as well non-operational information, for example configuration information, to the same data object. Here we have a few examples, like the possibility to describe the control behavior of the switch or the operate timeout. Further, we may have information related to substitution. Substitution will be described later. Also later, we will discuss controls in IC61850 in general, and how the control parameters, like the command to open or close the switch, are modeled. Functional constraints are used to classify the data attributes based on their purpose. In the example shown, the functional constraint ST identifies the status attributes, CF the configuration attributes, or SV the attributes related to substitution. What we see as well in the example is that the data attributes may be of a basic type, like the operate timeout, which is a 32-bit unsigned integer, or they may be further structures like the timestamp. This is shown in the next slide. A data attribute may have, in principle, an unlimited number of hierarchical structure levels. We have here an example where we have a constructed attribute class of a type vector, which can be a type from a data attribute, in that case Cval. This is a composite component where we will find two elements beyond, like magnitude and angle, which is again of a type analog value, which is again a constructed attribute class, and beyond that we may find the primitive components like I or F for an integer or floating point representation of the analog value. In principle, we may have multiple hierarchical levels beyond a data attribute until we get to the basic type, like integer 32 or floating point 32, shown in this slide. At the top level of our data model, we find the logical device. This is the top of the hierarchical structure of the model. A logical device is basically a collection of logical nodes. The standard does not define logical devices. They are typically defined by the IDE manufacturer when he creates the model for his device. A typical usage of logical devices is to group logical nodes for protection functions, for control functions, and for measurements. Logical devices are relevant for the naming hierarchy or for the management of the device behavior, like putting the whole device in a test mode or deactivation of a function. So far we have now seen the elements of the data model. Now let's have a look where they are defined in the various parts of the 61850 standard. What we see here is an overview on where we find what about the data model in 61850. At the top level we have the definition of logical node classes, with for each of them a class name, the semantic description of the logical node, and the detailed definition of the structure in terms of data objects with the names. We also have a detailed semantic description and type indication of the data objects. This is defined in part 7.4 for everything that is the basic substation automation, logical nodes, and in part 7.410 or 7.420 for logical nodes that have been defined for new domains. The part 61850-7.3 defines all the common data classes with the structure in terms of data attributes with names, the semantic, and the type of data attributes. We also define in part 7.3 constructed attribute classes with the structures in terms of subdata attributes with names, as well as the semantic and type of subdata attributes. And finally in the part 61850-7.2 we will find the specification of the basic types and of common ACSI types. Let's start now to have a closer look how these definitions and specifications are presented in the various standards documents. We start again at the top level with the logical nodes that are defined in part 7.4. For an easier handling, logical nodes are grouped based on their functionality. A logical node class name is always standardized and has four characters. The first character is reserved for the group indication. In this slide, we see the logical node groups that have been introduced by edition 1 of IC61850-7.4. The number indicates the number of logical node classes defined for that group. I will also provide a few examples here. We have the logical node PDIF, differential protection, which belongs to the group of protection logical nodes. We have the logical node RBRF, that represents a breaker failure functionality, which belongs to the group of protection related logical nodes. The XCBR, circuit breaker, belongs to the group switch gear. The CSWI, switch controller, belongs to the group control. MMXU, a measurement unit that represents basically calculated values, RMS values for voltage and currents, belongs to the group M, metering and measurement, or YPTR, power transformer, which belongs to the group of power transformers. With edition 2 of IC61850-7.4, and with the part 7.410 for hydro and 7.420 for distributed energy resources, we have today more than 200 logical nodes standardized. In the standards, the specification of a logical node consists of two parts, a textual description of the semantic that may as well include some modeling hints, and a table that defines the structure of the logical node. This slide here shows an example for the textual description of the logical node XCBR from edition 2 of IC61850-7.4. And the next slide shows the table that specifies the structure of that same logical node. So what we find here is the name of the data object, the reference to the common data class, a short explanation, and the indication if the data object is mandatory or optional. Finally, for each of the data objects specified, the standard provides, in addition to the short explanation that we already have seen in the table of the logical node definition, a detailed semantic description with all the information needed to understand the data object. As shown here, if the value of the data object is of a type enumeration, that description includes as well an explanation of the various values. So basically, if the value of the data object CircuitBreaker operating capability has the value 2, that means we can still do one open. If it has the value 3, that means we can do a close and an open again. As already mentioned, the structure of a data object is defined through the common data class. A common data class can be considered as a structured type definition. Common data classes are defined in part 7.3 of the standard. In the specification, common data classes are grouped according to their main characteristics. The groups are shown here with a few examples. Again, the number shows the number of common data classes that belong to the group that exists today in the standard. SPS single point status or ACT protection activation information are status information. MV is measured values, the same for Y, the three-phase measured values belong to the group measured information. DPC belongs to controls, SPG to status settings, and DPL to description information. These are just a few examples. A common data class is defined by a list of data attributes and a reference to either a basic type or to a constructed attribute type. This is provided in part 7.3 as a table for each common data class. In addition to the attribute name and attribute type, the table includes as well the information about the function constraint and the presence condition of each data attribute, which can be mandatory, optional, or based on a particular condition. In addition, value ranges may be indicated and trigger options. Trigger options are relevant for the reporting model and are not further discussed here. The next slide shows how the semantic definition of a data attribute is provided with an example of the explanation of a data attribute T, which is a timestamp. Here it also shows for each of the common data classes to which data attribute the timestamp applies. In most cases, a common data class is defined as a list of data attributes where the data attributes refer to a type. There are, however, a few examples where the elements listed as part of the common data class are again data objects referring to another common data class. A common example for this is the common data class Y that groups the relevant measurements of a three-phase power system. As you may see, we now have an additional section for sub-data objects where we have a reference to another common data class, which is in this case CMV for complex measured values. So a data object of a common data class Y has not only a couple of data attributes, like shown at the bottom here, angle reference, or a description attribute, D. Such a data object has again sub-data objects like phase A, phase B, phase C for the three phase values. Finally, the standard part 7.3 defines as well the constructed attribute class. This slide shows an example of the definition of a constructed attribute class, in this case the attribute class Vector, which is basically two components, magnitude and angle, each of them of the attribute type analog value, which is again another constructed attribute class. Finally, we will as well find in part 7.3 an overview of the various functional constraints used in the context of the common data class specifications in part 7.3. Just as a remark, there are additional functional constraints used for the control blocks models of the abstract communication service interface definition, but this is out of the scope of this presentation. And last but not least, the part 7.2 defines our basic data types listed here and the common ACS types listed in this next slide. So to summarize this part of the tutorial, the information available within an IED is exposed as an IC61850 data model. This data model is hierarchical and consists of logical devices, logical nodes, data objects and data attributes. Logical nodes with their data objects are defined in part 7.4. The data objects refer to common data classes as a structured type. The common data classes are defined in part 7.3. A common data class is a collection of data attributes. Data attributes are either of a basic type defined in part 7.2 or of a constructed attribute class. In the next part, we will now step-by-step explain how you can create a data model of a device.


Video ID: PrgR11ZbIQI
In this video we're going to look at how to send time tag data from the SCADA data gateway. For this video I'll be using an OPC client that's connected to an OPC server and we've subscribed to a few points and I'm going to map those points to an IEC 60870-5-104 controlled station or slave component. So the first thing I need to do is create the S104 component. To do that I can add the TCP channel, select S104 and I'm just going to name it S104. I'm going to take the default values here. I'll also take the default sector address and now I need to create data types. Now I want to send time tag data but if you notice in the data type list there's no time tag data types here. So what I have to do is select the base type. So I'm going to select T1 for my single point and I'm also going to create a double point and a couple of measure ends. I'll do one normalized measure end and one float. So now I've set up the data types on my S104 component and as I mentioned before they're all set up with the base type which does not have a time tag. So now that I've created my data types I'm going to go ahead and map my data to them for now. The only way to create data in a slave component is by mapping a point from a master component. So in this case we're going to map from the OPC client to the slave. I'm just going to make my single point IOA100. I'm going to take this next point and I'm going to map that as a double point. I'll just make that IOA200. Then I'm going to map a measure end to be a normalized value. I'll just make that 300 and then finally I'll map one of my measure ends to be a float. I'm just going to make that IOA400. So now we have our data points mapped. I'm going to save the INI file and I'm going to go ahead and exit the gateway. Now I need to edit that INI file. You can use any text editor. I'm just going to use notepad and we want to search for a field with the base type. So for example for the single point MSPNA underscore time format or you can just use time. This field sets the time format that will be reported by the SCADA data gateway. For a single point it defaults to CP56 which is a 56-bit time which is what we want in this example. Next I'm going to look for the double point and again it already defaulted to the CP56 time. I'm going to look for my measure ends. Here's my normalized measure and notice that it defaults to no timestamp so I'm going to change that to be a CP56 and then finally look for my float measure end and again it defaults to no timestamp and I'm going to change that to a CP56. So now I just need to save this file. So now after modifying the INI file I need to restart the gateway. When the gateway restarts it should read that INI file. Now I'm going to go to my OPC server and begin creating data changes and now let's bring up a test harness that's connected to the gateway. I've set my test harness to show only event data since event data should carry timestamps. And now if we pause and look for data, here we have a type ID of 30, single point information with 56-bit time tag. I have a type ID 36, measured value short float with 56-bit time tag. I have a type ID 34, a measured value normalized value with 56-bit time tag. And I have a double point information with the 56-bit time tag. So as you can see the gateway is now reporting data with the 56-bit time tag. So in summary when you configure your SCADA data gateway you use the base type even though it doesn't show a time tag. You create points in a slave component by mapping. You can simply drag and drop points from a master component to the slave. In this case we dragged them from the OPC client to the appropriate data type. And then if you want to send time tags on points that do not send time tags by default, you need to modify the INI file and set the time tag type to be sent.


Video ID: SwB_FZ4Vp3w
The communication protocol test harness includes a protocol analyzer, which is useful for monitoring communications. The protocol analyzer display contains a number of filters to make it easier to see exactly the data you're looking for. For example, we can filter by protocol layer. Here we see the application layer is turned on. We can turn on the transport layer. If you notice the little key here, application layer messages are preceded by equal signs to transport by the little tildes. So when I turn on transport, those became available. When I turn on data link, we'll see the data link layer with the dashed lines. We can turn the filters back off, and they're removed from the display. Likewise, we can filter by data. So here is a piece of event data. If I turn off the filter for event data, that data is no longer displayed. You can also filter out errors and timestamps and target layer messages. We can also fully customize the colors and fonts displayed in the test harness. Here we have the protocol output formatter. Notice by default, errors are red and received security messages are blue and transmitted security messages are green. But we can customize this any way we want. For example, we can customize received application messages. Let's use black text on a gold background. Let's make that italic, and let's indent it a little bit. Then we can modify our transmit. Let's make this gold text on a black background. And that gold doesn't show up real well, so let's bold it. Let's indent this also. And then I can format any of these layers that I want, but let's just do this for now. And here we see our application layer messages are now formatted as I just showed. Now I'm going to configure my master to start polling. I'm simulating an outstation here. I'm going to turn on polling from the master. So every two seconds we see a new poll come in, and we see the protocol analyzer buffer scroll. Now I can pause this display, and I can scroll back and see previous data. And I can change the filter layers in the paused output. So if I see an event I want to study in more detail, I can pause the analyzer display, find that event, and then turn on additional filtering or turn off additional filtering to make it easier to analyze that data. Now another really nice feature of the test harness is the ability to save these protocol analyzer logs for future analysis. To do that, we just go to File, Save Protocol Analyzer Log as XML. Select the file name. I'll overwrite the file I had there. So then I can come to the test harness later and open that saved log. This opens a new protocol log viewer, and I can see all of that saved data. I can also change the filtering in the saved data. So for example, I can turn on the transport layer and turn on the data link layer. This is very, very useful during testing. You can save off your protocol analyzer logs and then later go to your desk and review the logs in detail to make sure things are operating as you expect them to. And one other nice feature of the protocol analyzer is the ability to parse a set of bytes that were captured by some other means. To do this, we need to open a monitor channel. Open DNP Monitor. And we want to select raw input bytes for the channel type. And now we can paste those bytes in. And they will be decoded in the protocol analyzer. So this is decoding of this set of raw bytes. So as you can see, the communication protocol test harness offers a number of nice features with the protocol analyzer log, allowing you to capture real-time data, pause that data and view it, change the way the data are displayed, both with filters and color and formatting changes, and also the ability to save and restore logs, as well as to decode bytes that were captured outside the test harness.


Video ID: 7YjzMVTChx8
Thank you for viewing this Gated Data Gateway quick start video. Be sure to check back later for additions to this video.


Video ID: jObhNAQB7k8
Welcome to the Triangle Microworks IEC 60870-5 communication protocol training videos. This is the fourth video in the series. In the first three videos, we provided an overview of IEC 60870-5, we looked at the application layer and ASDUs, and dug into functionality like polling and controls. In this video, we'll dig into the link layer, the transport layer, and the physical layer. This section gets into some of the more serious details, which can be important, but they may not be important in your application because if you're using the Triangle Microworks source code library, it will handle most of that for you. Now let's take a look at the IEC 60870-5-101 link layer. As I pointed out before, the 101 link layer is different from 104. In some ways, you might say 101 has a link layer, whereas 104, the link layer is replaced by the TCP IP stack. So the 101 link layer has a checksum, or CRC, to make sure that the data is received correctly. It also has the capability to initialize or reset the link, and it has link confirmations to confirm that the primary and secondary stations are up and running. By the way, that primary and secondary refers to both master and outstation, depending on who initializes the message. And that leads us to a discussion of balanced versus unbalanced mode. In the balanced mode, which is equivalent to DNP, there's a request from the primary and acknowledgment from the secondary. This could happen in either direction. With unbalanced, the controlling station, or the master, always has to initiate the command sequence. This is because it's sharing, for example, in a multi-drop line, the physical layer, with lots of controlled stations. DNP has no equivalent of unbalanced mode. Note that this drawing is showing the link layer, so these are link layer frames and link layer acts. Also note that the confirmations are optional. You can configure the link not to use confirm data, which you might want to do, for example, if the link was known to be very reliable. If you use confirm data, it will retry at the link layer. So if you don't use confirm, then you just trust that it gets there, and you find out when everything comes through with the application later, whether or not you got all of the data. There is link layer addressing, similar to DNP, except there's a few differences with unbalanced versus balanced. There's a broadcast address for unbalanced, and it's important to point out that the balanced mode only supports point-to-point links, and that it has a destination address for the primary and a source address for the secondary. You can configure how many octets to use in the addressing, versus DNP3, which always uses two octets. Now let's take a look at the link layer frame. The specs have different formats, so this is the FT1.2 frame, which is used for 101. It's built up with all of the fields that you see here. To start, it identifies what type of frame it is. The frames can be either fixed-length, variable-length, or single-character. And in the next field is the length, so this tells you how many total octets are in the frame. This is really only needed for the variable-length frame. And then there's function codes for the link layer services. And then there's the session address, which is the address of the device, or the session of the target. Next comes the user data, which is the ASDU, and is put inside this LPDU. And finally, the checksum and an end octet, which is different for the different frame types. So the control field has several different bits. The first up is the direction bit and a frame count bit. A frame count bit is a very simple sequence number. It just toggles, so it would tell you if you missed a frame. This is a very simple mechanism, and there's no real sequencing. Just kind of, if you will, a one-bit sequence number. The secondary station can also warn that it's about to have a buffer overflow using the DFC bit. And then the ACD bit, which basically informs the controlling station that there's class one data available. So if the primary's requested class two data, the secondary can say, wait, I have class one data, which is higher priority. That's only used for unbalanced mode. And then the PRM bit tells you if this is the primary, or initiating, or the secondary, or responding. And then there are the function codes, which are also included. They basically tell you what type of request or response is being sent. And these are different for balanced versus unbalanced modes. They have different requests and different responses. So that's about it for the 101 link layer. So as we've said before, in 104, you remove the link layer and replace it with the standard TCP stack. The 104 adds a header to the 101 ASDU, and this is meant to replace the functionality that you had with the 101 link layer. When you add the header, it becomes an application protocol data unit, or APDU. And that's what this is. It's the APCI header plus the ASDU, and that equals the APDU. And so the ACPI has this control field. It has all the sequencing and control of starting and stopping transmission. It also has an indication of the three different types. These are sort of like the function codes in 101. They tell you whether they're information frames, supervisory frames, or test frames, which is the equivalent of the 101 link layer, but they're not called function codes. And they're not exactly the same. But in some degree, you could say they kind of replicate that functionality, or they're kind of the equivalent functionality of the function codes in 101. So this essentially allows you to transmit more. It becomes kind of a data window of things that you can transmit. TCP itself is very reliable and will detect lost or duplicated frames. So you don't have to have that functionality. So instead, this kind of allows you a transmit window. It's true we look for missing sequence numbers, but TCP should have told you that anyway. So as we keep saying, 104 maps to the TCP services. The controlling station is the TCP client, and the controlled station is the TCP server. It uses point-to-point mode only, and it defines the standard port as being port 2404. Many installations will allow you to configure a different port, but the default specified in the standard is 2404, just as it's 20,000 for DNP. A large part of the 104 standard talks about this mapping. There's some other differences with the ASDUs and the timestamps and things like that, but this is a big portion. If you read the 101 spec, it doesn't really repeat what's in the 101 spec, it just says what's added. So it adds some type IDs, some ASDUs, and it also talks about this mapping to TCP. It also talks about redundancy, and I think those are the main things that are added. But in order to look at 104, you often have to refer back to the 101 spec, because a lot of it is really only in the 101 spec. So to understand 104, you really have to look at both the 101 and 104 specs. So let's take a look again at the physical layer. 104 uses TCP IP protocol suite and is supported over Ethernet. The 101 spec points out a variety of serial standards that can be supported. So that pretty much wraps up all of the layers. In the next video, we'll look at security in IEC 60870-5, and then we'll wrap it all up with a conclusion.


Video ID: v1I1i_3BiIM
Three protocols are defined for use on the station bus. The first is a TCP IP stack containing some application layer things that we'll talk about shortly. The second is the goose generic object-oriented substation event and generic status substation event which are link layer protocols used to do things like protection tripping and that sort of thing across the station bus. The third protocol is the simple network time protocol that's a standard IETF. It comes with most of the TCP IP stacks which is used to distribute time across the station bus.


Video ID: 7SK1ejhEdr8
This video demonstrates the report viewers in HAMR, the 61850 client component of the 61850 test suite. I'm going to start by showing ANVL, which is generating simulated data. Now I'm going to bring up HAMR, which is connected to ANVL. If we notice, the reports window is grayed out because there are no reports yet. But if we enable a report control block, then as the reports are reported, the information reports window is no longer grayed. The report window makes it easy to inspect the header and data from each report. So here I'm viewing the reports for this control block. Notice that we have the header information shown. We can scroll right and see additional header information. And in the right, we get a tree view of the data. So this is the same data that we can see in the tree over here. For example, I can scroll down to T1, MMXUA, Phase A, CVAL. We see data here and we see it here as it was sent in the report. One feature of the report window is that you can compare any two reports. So if I have a report selected and I choose another report, then I can right-click and select compare. And that opens the compare window, which for each data set member shows the previous value and the current value for these two reports. You can also right-click and easily scroll to the bottom to see the latest report. We can also enable auto-scrolling so that the latest report remains visible.


Video ID: lRyRxFp-3HQ
In this video, we'll look at how to initiate polling and issue other DNP3 commands. To issue a command from a specific master, we can right-click on the desired session and choose Commands. We can then choose the desired command. For example, let's send a Class 1230 Integrity Poll. When we select a command, the dialog opens and allows us to configure that command. For example, we could send just a Class 0 Poll without polling for the Class 123 events. In normal operation, that's usually a really bad idea, so we won't do that. We can also select when we want this command to be sent. For now, let's just send it once. When I click OK in the command dialog, the command is sent. We can look in the tab with the Protocol Analyzer and see both the command and the response from the outstation. We send other commands the same way. For example, I can right-click on the session, choose Commands, and Control Relay Output Block. Once again, we get a pop-up dialog that allows us to configure the command. We can set the mode to be Select Before Operate, Select, Operate, Direct Operate, or Direct Operate NOAC. We can set the qualifier code to be 8 bits or 16 bits, choose the point to send the command to, choose the control code to be Latch On, Latch Off, Pulse On, Pulse Off, Trip, or Close, configure the Pulse On time, and optionally configure a feedback poll that will be sent a specified delay period after the command completes. Notice that each of these options has pop-up help that describes the option and the various values. In this example, there are no CROB points configured, so I'll just cancel this dialog. We can also right-click on a folder and issue the same command to each of the devices in that folder. For example, let's configure an event poll for each master device. To do that, we'll right-click on the DNP3 Masters folder, select Commands, DNP3, RBE Data Poll Event Class 123. In the dialog, let's choose Scheduled and set the period for 1 second. Now each master is polling every second. If we look in the tab showing all of the masters, we can see the polls being sent by each master and responses from each outstation. We can selectively filter which masters we want to see, or we could even just look at responses from particular outstations. In the next video, we'll look at how to view data in the master using Insight.


Video ID: vyJ8JrNBnwQ
Welcome to the Triangle Microworks IEC 60870-5 communication protocol training videos. This video will give a brief overview of the 60870-5 protocol, which sometimes I'll refer to simply as 60870, and will also contrast the protocol somewhat with the Distributed Networking Protocol, or DNP3, which I'll sometimes abbreviate as DNP. This is the fifth and final video in the series. In the first four videos, we provided an overview of IEC 60870-5, talked about the various layers of the protocol, and went over key functionality like polling and controls. In this video, we'll discuss secure authentication in IEC 60870-5, and wrap it up with a conclusion. Now let's take a quick look at secure authentication. This is the latest section that's been added to the standard, and it's basically defining a way to authenticate requests. This is very similar to the DNP3 secure authentication standard. So what this functionality provides is the ability to make sure that the message was sent by the master that you thought was sending it. It makes sure it hasn't been tampered with, and it addresses a few other things. It's basically trying to prevent spoofing, where you could pretend to be the real master. So secure authentication makes sure you can't modify a message and change it, like changing a read into a control, or changing a point that's going to be acted on in a control message. It also makes sure that you can't just replay a message, so you can't record what a control message looks like, and then send that same control, you know, say a week or a few days or something later. There's also a concept of repudiation that's not really addressed. It's kind of a controversial topic, whether or not there even is repudiation in this standard. Basically repudiation is being able to prove who requested a command. So the secure authentication partially addresses that, but not fully enough to meet some of the cybersecurity standards. And then there are some things which are not addressed. The biggest one is eavesdropping, because secure authentication does not include encryption. However, the spec does call out TLS, if you're using TCP, as a method to improve encryption. It's just not covered in the spec, not required by the spec, and technically not part of the secure authentication specification. So really what's being included in secure authentication is the capability to do authentication at the application level, and to address the threats we mentioned, like spoofing, modification, and replay. The secure authentication mechanism is based on the concept of a cryptographic hash. A hash is a function, like a cyclic redundancy check, or CRC, or a checksum, that when performed on the message, produces a much smaller string of numbers. This smaller hash value is very sensitive to changes in the message, but it's virtually impossible to determine the original message if all you have is the hash value. So in this example, let's assume that Alice is trying to send a message to Bob, in a way that Bob can be sure the message is authentic. To make it work, Alice and Bob must have previously shared a key, that is a string of numbers like a password, that only the two of them know. So Alice performs the hash function on the message she wants to send, concatenated with the key. This produces a small hash value. Alice then sends the original message and the hash value to Bob. She does not send the key, because that could be seen by an attacker. However, the message is not encrypted in this case. The attacker could see what the message is doing, but as we'll see, cannot modify it or send a false message of the attacker's own. In step three, Bob receives the message. Since he already has a copy of the key, he can now duplicate Alice's calculation. He hashes the message and the key together to produce a hash value. If Bob's hash value matches the value that Alice transmitted with the message, then he knows two things. First, the message has not been tampered with. If an attacker had tampered with the message, Bob's calculation would have been on a different message than Alice used, and therefore the hash value would have been different. The hash function is carefully designed so that without knowing the key, an attacker could not modify the message in such a way that it would produce a correct hash. Secondly, Bob knows that the message came from Alice, or at least someone who knew the pre-shared key that Alice knows. Although the key was not transmitted on the link, it was intrinsic to the calculation, and without it, an attacker could not produce a matching hash value. Because of the way the hash works, it's nearly impossible to determine the key from the hash value. When a hash is used with a key in this manner, it's known as a hashed message authentication code, or HMAC, sometimes called an HMAC. Security can be challenging in SCADA networks because the messages can be used in a variety of different networks, including radio systems, serial links, and IP-based wide area networks. Also, it's possible that from one end of the network to the other, it may travel over more than one of these links. So for this reason, the secure authentication is included in the topmost of the layers, the application layer. There are three types of security that are commonly deployed in communication networks today. First is site-to-site security, which includes the use of a virtual private network, or VPN, and protocols such as IPsec to secure the link between the two locations. For example, between a corporate office and a home office, or a master station and a substation. It does not secure the networks at those two locations, and physical security measures like locks and guards are necessary to protect them. Second is device-to-device security, which includes the use of protocols such as the transport layer security, or TLS, to secure the complete TCP connection between two devices, similar to when you access your bank through the internet. However, TLS only works on IP networks and is therefore lost if a message is forwarded over radios or serial links. It also does not address the possibility that rogue software applications may be installed on a device, making use of the fact that the device itself is considered secure. And finally, there's application-to-application security, which ensures that individual users, not just devices, are authenticated by the remote devices, and that the authentication information will be carried wherever the message travels. It permits remote outstations to perform role-based authentication and authorization so that the level of security changes depends on who's attempting to perform an operation. As we wrap up, we look at how to get more information on these protocols. As we mentioned earlier, there's no official users group for the IEC 60870-5. However, there is an IEC 60870-5 mail list, which is monitored by vendors, system integrators, utilities, and software developers. To sign up for this list, you can follow the link shown below. There's also an IEC TC57 Working Group 3 mail list, and the link for that mail list is shown as well. We hope you found this training useful. If you have any questions, feel free to contact us at support at trianglemicroworks.com.


Video ID: E66oxWosK6o
I want to talk a little bit about the substation configuration language. This is an XML-based language for configuring substations and substation devices. It's designed for the exchange of information between engineering tools. It includes descriptions of the application components, device models, logical nodes, communication infrastructure, and the relationship of all of that with the power system. This is a major innovation within 61850 that we think is going to have a really major economic impact in the overall substation automation world. This is already fostering a wide range of tool development that will significantly change the way we develop substations in the future. There's various pieces to an SCL file. It's all included in one grand schema, for those of you that are into the XML tool domain. There's as usual a header to each document. There's a substation section that describes the functional structure of the substation. This is not the communications model now. This is the actual functional structure. It contains the primary devices and electrical connections, and it associates initial logical nodes with the equipment. This piece of it would typically be built out during the substation design process, and it's essentially a text version of a one-line diagram within the XML file. The communications section defines the set of subnetworks, access points. The idea being here is that you can associate addresses with actual subnetworks and be able to figure out the physical connectivity, and then it shows where the clients and servers fit within that infrastructure in terms of their defined access points. The IED section of the SCL file defines the communications servers and their access points. These access points have references back to the communications section so that you can link up with the actual physical infrastructure. The IED portion also defines the capabilities of each IED, logical devices and logical nodes within those IEDs, as well as all of the initialization data for things like configuration parameters and settings and that sort of thing for each IED. The data type templates section of the SCL file describes the full definitions of all logical nodes, data objects, and data types across all of the IEDs. So this gives the final build-out of all the detail of all of the data objects and attributes within each IED. Now while there's a single SCL schema for describing the SCL language, the various pieces of the SCL file can be populated in different ways. This depends on the vision of the engineering process for laying out, designing, and configuring a substation. These next slides will show the relationship of the different file formats. They're basically all running off the same XML schema, but with different pieces filled in as it goes through the design and configuration process. The IED capability description files, or ICD files, are delivered from the vendor to the utility along with their equipment to describe the capabilities of the device and all of the configuration parameters, object models, and all that sort of thing. They're typically developed with an ICD editor. We supply one with our software for developing these files, and they're used to communicate the information to the system configuration tool. The system-specific description files, or SSD files, are typically created by a system specification tool, and it's used to describe the overall substation specification to the system configuration tool. Basically, this is the description of the single-line diagram of the substation and the required logical nodes to implement the functionality needed. The substation configuration description, or SCD file, is basically a combination of those two. It puts together the substation specification and the build-out of all the IEDs, and within a system configuration tool, this generates a full substation configuration description. This basically contains all IEDs, communications, configuration, and the substation description. It ties the IEDs into both the communications infrastructure and the power system description. It's used then to inform a number of tools, IED configuration tools, typically database tools and things in the substation and up in the control center eventually. This basically is the description of the entire substation. The configured IED description files, or CID files, are basically generated by IED configuration tools from the substation configuration description. It basically will take out the information from the full substation description that's specific to the IED, including all of the configuration parameters and addressing information, and put that in a smaller file, which can be downloaded to the IED. In a lot of cases, these files won't be used. Instead, the IED configuration tool may, in fact, configure the devices using proprietary formats. This last step isn't necessarily required, but eventually we think this is the way all devices will be done. It's much cleaner to be able to use this, and it also stands as a documentation for what's really there in the device.


Video ID: o3kKtBJJyCw
Welcome to the Triangle Microworks web-based D&P 3 secure authentication training. This course is divided into three levels. An introductory level provides information about what is secure authentication and when should you use it. This level is typically targeted for an executive, manager, or purchaser. The intermediate level looks in more detail and answers questions like what does it look like and how does it behave. This level is appropriate for a technologist or a technician. And finally the expert level explains why does it work this way and how would we implement it. This level is typically targeted for developers.


Video ID: roFZO9ZoUdM
There are several ways to simulate data in an outstation using DTM. In this video, we'll show some of these methods, including changing data manually, automatically changing data, changing data using comma-separated values or CSV files, using JavaScript, and using flowcharts. To change data manually, we first need to show a point list. I'm going to show the point list for the entire outstation folder. So to do that, I right-click, select Show Point List. To change a point's value, I can click on a point in the point list. For binary inputs, I can then select the desired value from the drop-down box. Let's change the breaker to be on. For analog values, I can type in the new value. Let's set the new value to 250. This will change the value in the outstation's database, which will be reported on the next poll from the master. The master will then update the insight display. We can also look in the master's point list. To do that, I'll right-click on the folder, select Show Point List. And here we see the points reported in the master's database. Automatic changes are useful for creating large amounts of data changes. With automatic data changes, DTM will toggle the binary inputs and change the analog inputs. To configure automatic data changes, we can right-click on a device or a folder. I'm going to select the folder, which will allow data changes to be applied to each outstation. Once I right-click on the folder, I can select Simulate, Configure Automatic Data Changes. This will open a configuration dialog, and in that dialog, we can select which outstation should apply the automatic changes. I'm going to select the top checkbox, which automatically selects each outstation. Next, we need to set the change period. We can set a period for each device, but I'm going to set the period here in this top box, which applies to each selected device. I'm going to stay with the default of 5 seconds. When I click OK, the changes will begin. Here we see changes being applied to the outstation's database every 5 seconds. Those changes are sent to the master. Looking at the master's point list, we can see the updates. The insight display is also updated by the master, as we can see here. We can also use comma-separated value or CSV files to simulate data. Here is a sample CSV file. This CSV file was created by editing the 61850 CSV file that we used in the 2015 Distributech demo. You can also use the Communication Protocol Test Harness to capture states and create CSV files. In the CSV file, each row represents a point. The first column describes the path of the point, and subsequent columns represent a state. In this example, I have three normal states with different values of current, and one fail state. I have separate CSV file for each breaker being simulated. To apply the CSV file, we first need to load it. To do that, we right-click on a session in the outstation, and select Simulate, Load CSV File. This will open a browser. We can import CSV file and choose the appropriate file. I'm going to load a CSV file for each simulated device. Once the CSV file is loaded, we can apply the various columns from the file. To do that, we right-click on the CSV file, select Apply Column, and then select the desired column. The slaves database is updated accordingly, and the resulting change will be sent to the master, which will update the Insight display. We can also apply columns to multiple devices at once. To do that, we right-click on the folder, select Simulate, Apply CSV File Columns. In the pop-up dialog, we can drag the column header to the top bar in order to sort by columns. We can then expand the column and select the desired devices. We can use multi-select, click and shift-click or control-click to select multiple devices. I'll select all of the devices using shift-click and apply them by clicking OK. Once again, we can see that all of these changes have been applied to the outstation's databases and reported to the masters, which have updated their database and updated the Insight display. We can also configure DTM to play back the data in CSV files. To do that, I right-click on the folder and select Simulate, Configure CSV File Playback. I want to select all of the devices, and I'll set a period of 2 seconds. I also want to check the Restart Playback On In The File box. This will create a continuous loop, going back to Apply Column 1 again after all of the columns have been applied. When I click OK, DTM will apply all of the points in the first column of each CSV file for each device. After 2 seconds, it will apply the second column. After 2 more seconds, it will apply the third column, and so forth. 2 seconds after the fourth column has been applied, the first column will be applied again, and the process will continue to repeat. Looking at the outstation's point map, we can see the data value changes. We can also see those changes in the master's point list as the master updates its database as it receives data from the outstations from each pole. The master also updates the Insight display. Viewing the Insight display gives us a visual representation of these changes. Although not shown in this video, you can also manage CSV files from JavaScript and Flowcharts. Using JavaScript to simulate data is very powerful. It's limited only by your imagination, and perhaps your ability to code in JavaScript. In this demo, we'll create a short JavaScript program to simulate a sine wave. First, we open the JavaScript editor by clicking the JavaScript editor icon. Then we drag the point we wish to simulate into the JavaScript editor. This creates an insight tag that associates a variable name to the path of the point. Let's simplify the variable name and just call it P0. Now we just need to write the JavaScript. We'll use a for loop to cycle through 360 degrees, stepping 10 degrees at a time. We can use standard JavaScript libraries, so here we'll use the math.sine function to create the sine wave. Note that math.sine uses radians instead of degrees, so we'll have to convert i from degrees to radians. We'll do that by multiplying by pi. We'll use the math.pi function and divide by 180. Finally, let's pause for a second between each update. Now let's check for errors. To do that, we build the script. Since it built cleanly, we can run it. Just click the run icon, and watch the output vary from 0 up to 1, back through 0, to minus 1, and back to 0 as it cycles through the sine wave. We can also use flowcharts to simulate data. Here I have a flowchart that creates 10 random values. It initializes a counter, sets a new tag value based on a random number between 0 and 150, delays for a second, increments the counter, and if the counter has not yet reached a value of 10, loops back to set a new tag value. When we set the tag value, we need to specify a path. We can get that path simply by dragging a point from the point list onto the setTagValue operator. I can run the flowchart in debug mode to verify that it works. In debug mode, execution is slowed and each element of the flowchart is highlighted as it executes. It looks like it ran okay, so now I'll uncheck debug, and run it again. This time we'll see that the value changes once a second. As you can see, it's very easy to set up flowcharts. Your ability to simulate data using flowcharts is limited only by your imagination, and you don't even have to write a line of code. So we've seen multiple ways to simulate data in DTM. We can simulate data changes on individual points, we can simulate large amounts of data using automatic data changes, and we can simulate real-world states by applying CSV files. Automatic data changes in CSV files can be applied to individual devices or to multiple devices simultaneously. We can also create changes via JavaScript and flowcharts.


Video ID: ADHZFGiH17k
I mentioned GOOS services several times in this presentation, now we'll come to define how the protocol works, how the models work, and the services. GOOS generic object-oriented substation event came about really in the early days of UCA. We needed a way to model basically tripping and breaker relay communications, which is typically done with a fixed hard wire. We wanted to be able to simulate that or emulate that kind of a system over the LAN with some reliable distribution of data from a single source to multiple subscribed peers, so it's a subscriber-publisher model. We needed to get high reliability in that kind of a scheme, but we wanted it over the LAN. We'll talk a little bit more about how the protocol works and how we actually implement that to get that reliability. To start with, GOOS is arbitrary data. The generic substation state event, which was the original UCA mechanism that was defined, works with the same mechanism, but it sends a fixed set of status bits. There's no data set involved, and the bits are somewhat defined between publisher and subscriber, but it also uses the fast, reliable multicast capability. From a data point of view, we have our same similar-to-reporting model. We have data objects within a data set that are generating a series of change events. These are formatted based on settings in a GOOS control block, which is controlled by the client, and the difference is the messages always send the entire data set. They are fast multicast reports, and I'll talk about how that multicast works to get the reliability that we were after. Multicast is a problem in any protocol. When you have that kind of requirement, there's some inherent reliability. They're typically unconfirmed. In our case, it was particularly impractical to use a confirmed service. We needed to have trip times and switch closure times in less than four milliseconds was our target requirement. If you think about it, using most protocols, if you were going to do confirmation, then by the time you wait for the confirmation to come back from the receiver, it's too late to resend because you're already outside your four-millisecond window. This is particularly a problem when you've got multiple subscribers coming to a single publisher, being able to resolve all of that. So instead, after some work within the UCA community, we came up with the idea of being essentially pessimistic. We assume that the scribers didn't receive the data, and we go ahead and retransmit it as if they had not received it. When the data is stable, basically, we retransmit very quickly to be able to get the times to meet the timing requirements that we had. As the data gets more stable, so you're farther away from a change event, then the retransmission rate spreads out so we don't overload the LAN. And basically, then it comes to some fixed rate, which subscribers can use to detect loss of connection, and new subscribers that come on after the change event will still get the current data. This retransmission policy is shown in this diagram. The red bars are the messages that represent the real change. That's when new data has occurred. Something has occurred in the process logic that caused the data to change. The message is sent containing the entire data set, and then there's a fast retransmission that occurs, which gradually, over some kind of a strategy, spreads out so that we're not burdening the network. Each message contains the time interval to expect the next transmission so that you can tell if a message is dropped, you know that you've lost connectivity with the publisher. The second red bar, then, is the second change event. We will then start the retransmission policy all over again, again, doing fast retransmissions until we reach some fixed amount. So, each message contains the time to wait for the next message, and so you detect dropouts, and the maximum retransmission rate is configured to be the time that is acceptable for new subscribers to get the data. The messages that are defined in the Goose link layer protocol are multicast link layer packets. In our case, it's VLAN packets on the Ethernet. Each packet contains identification data, basically the name of the Goose control block, the data set, and some other identification of the source of the data, the publisher. They all contain a data set name, configuration ID, and the values of the members of the data set. I mentioned these extra services. There's a service where a subscriber can essentially request back to the publisher to ask for the definition of the data set to be able to resolve the data set members to make sure that their configurations between publisher and subscriber are the same. As I said before, each message also contains the time to the next retransmission for being able to maintain the connectivity and know when you've lost a connection. They each contain a test mode bit so that we can do test Goose messages to be able to do in-circuit testing of devices over the LAN, and they also contain a security parameter which will contain, basically contains information to validate the source of the data. The 61850 Part 81, which is the only current implementation of Goose, the only SCSM that builds out Goose, uses VLAN over Ethernet, so that allows us to run over the Ethernet but still maintain another level of reliability because you can basically reserve the bandwidth in the routers and make it higher priority. The GSSE model, as I said before, is almost exactly the same as the Goose model except for that it's sending a fixed set of status bits instead of an arbitrary data set. This was the original UCA Goose, and it's been carried forward into 61850, but the model was extended in 61850 to do the full Goose with an arbitrary data set, arbitrary data. The sampled values model is similar to the Goose in that it's implemented, built out in Part 9-2 as another link layer protocol that coexists on the VLAN Ethernet. It's used at the process level to send synchronized samples, typically from current transducers or voltage transducers, that sort of thing. It's used at the bay unit level for synchro check. One of the issues about multicast in this VLAN Ethernet is that it can only really be used within one local network segment. Those multicast packets at VLAN aren't passed through routers. It's not a routable protocol. So the sampled values model includes a way of basically running them through a TCP pipe so that you can get a connection to a sampled values publisher across the network and be able to get the samples directly in a TCP directed connection. The ICONIC model looks very much the same as the Goose model except for that we're using arbitrary data and it's not event driven, it's sampled. Now these samples can get combined in various ways into the periodic reports that go out on the Ethernet, again controlled by a control block and formatted according to various rules.


Video ID: dil2FTweMkw
Thank you for taking the time to see this demo. For more information, please view our other demo videos, the product overview video, or our quick start videos. The Communication Protocol Test Harness and SCADA Data Gateway offer 21-day full-featured evaluations. Please visit our website to download the latest evaluation version.


Video ID: oC7mGACgLI0
Today, we'll be looking at DTM simulating nine IEDs from the UCA IOP demo in New Orleans. This workspace simulates a small substation and helps show exactly how DTM can be used to simulate, display, and organize data from a substation. Today, we'll be looking at three parts of this simulation, the single line diagram, the signal flow diagram, and the goose event viewer. All of the devices are simulated in the workspace tree area of DTM. The device behavior is simulated by JavaScript, which we've put beneath it. We've pulled out all of the parts we are most interested in and created a diagram to help convey their behavior. The display shows two line bays and two transformer bays, each with a protection relay and a bay controller. The buttons across the top show ways that we can create faults on the line. When a fault is created, the breaker will open and try to clear the fault. Then the reclosing function will close the breaker again. The breaker was green when it was opened. The reclosing function then caused it to close again. The toggle switches next to the buttons will convey whether the fault is cleared by opening the breaker or if it is a permanent fault. As you can see, the fault is permanent and the breaker will reopen after closing until the fault is cleared. There are two buttons next to each breaker. The first button will determine if the breaker fails to open during the initial trip, while the second one determines if the retrip signal also fails. If it does, this will cause a breaker failure event. Goose messages will be sent in the event of a breaker failure to the other breakers in the network. This will allow them to open and stop current flow. This is our signal flow diagram. Its purpose is to show the flow of messages between logical nodes. It also allows you to quickly see which logical nodes are publishing Goose and who is subscribed to them. This display was created by sorting the data from the entire workspace, seen here. Now to recreate this, you can go to the substation folder, right click, show data points. This brings up a pane with all of the data points in the entire workspace. You can filter or sort this, but we also have a view you can select, the Goose table view. This has filtered out all information that is not Goose related, grouped it by the publisher and the path. We use this pane to develop our signal flow diagram, specifically line 1PR and line 1BC. Now that we have this diagram set up, let's see it in action. I'm going to cause a fault that will result in a breaker failure event like we did before. This time however, we'll pay attention to the signal flow diagram. You can see that the lines have changed to a red color when there has been data that transferred between logical nodes. For a full history of what has happened, you can mouse over the line. You can return to the original state at any time by clicking clear history. The last thing we'll be looking at today is the Goose event viewer. This is the section where any Goose messages received are recorded. For this example, we've grouped by time and Goose ID. This allows us to see when the Goose message was sent and what it is containing. Let's go ahead and trigger the breaker failure that we did earlier. As you can see, in the Goose event viewer, all the Goose messages are logged and recorded. DTM was used to simulate the protection and automation functions for this substation. The IEC 61850 model from the SCD file was used to simulate the IEDs in this substation. Real Goose messages were published in this simulation so that it could be adapted to test real IEDs or other devices like HMIs, RTUs, or PLCs. We can see these real Goose messages using TestSuite Pro, another one of our test tools. In another video, I'll be going over how to monitor the Goose message streams and visualize the events created by this substation. When published, there will be a link to that video at the end of this one. Different diagrams and views can be configured based on your project needs, like the one seen here today. Please contact us if you'd like this example DTM workspace, or let us know if there are other tests you'd like to automate with DTM using a similar approach. Check out our website or email us. Information is in the description below. Thank you.


Video ID: zUsN_rsXL2s
In this video, we'll show how to automatically create data changes when using Anvil to simulate an IEC 61850 server. Here we have Anvil simulating an IEC 61850 server based on the SEL file that's been loaded. To turn on automatic data changes, I'm going to show the simulation pane. Notice we have the filter button, so we can show which functional constraints we're interested in simulating. Also, we can show data sets, the data model view, and timestamps. We also see the data model in tree form, and we can drill down and only show certain portions of the tree. So, for example, I'm going to only show the phase voltage. On the right side, we see the current value in the data type. We can change the step size, so I'm going to have it changed by 1 volt. I'm going to set a minimum value of 110 volts and a maximum value of 130 volts. And I'm going to set my interval to be 1 second. So now when I click start, we should see this phase A voltage change starting at 110, and every second it will go up 1 volt until it reaches 130. Here we see it. So the automatic data simulation is very useful when you want to send large amounts of data. You can simply use the default values if you want, or you can customize it as I've shown in this video to only show a certain range of values, set a certain interval, and control your step size.


Video ID: yfpA6U8aAQk
To summarize, then, the design objectives underlying IEC 61850, when we set out to develop the standard, we wanted to address the data management cost drivers using modern communications techniques. We wanted to achieve a high degree of application interoperability using standardized object models. We also wanted to simplify substation engineering by using a common configuration language. This also addresses a lot of the data management issues. One important piece to it is the online validation of the substation communications using both metadata and self-description. The self-description piece, we weren't looking to do plug-and-play, but we did want to be able to do online validation of everything. Basically everything that's described in the configuration language is also available through the underlying services. So you can find the object model of the device at the device. You can find the capabilities of the device both in the configuration files and in the end device through the self-description techniques. So this is an important piece that we can both configure these devices, and then once they're operating, validate that they, in fact, are talking to the systems that they think they are and that the data is organized in the way that it's supposed to be. So it's both offline configuration and online validation. This is different than plug-and-play. We're really not interested in trying to get into plug-and-play issues within the substation, but we did want to be able to plug-and-validate.


Video ID: tftaqOkDwZs
This video demonstrates support for the logical node for monitoring a goose subscription or LGOS or LGOS. It was part of our booth demonstration at the Distributech 2015 conference in San Diego. Please view video number one in this series for an overview of the entire booth demo and video number two to understand the role of each Triangle Microworks product in the demo. In this demo we'll be using the IEC 61850 client in the Distributed Test Manager or DTM to update a single line diagram created in Insight, the graphical tool available with DTM, the 61850 test suite, IRN, our ICCP TASI-2 test tool, the communication protocol test harness, our DNP3 Modbus and IEC 60870-5 test tool, and the SCADA data gateway, our protocol translator data concentrator and OPC driver. We'll also be using the Triangle Microworks IEC 61850 source code library which is running on two Texas Instruments BeagleBone boards. We'll be demonstrating LGOS, the logical node for monitoring goose subscriptions. As shown in this excerpt from the IEC 61850 specification, the LGOS logical node is used for monitoring goose messages through an MMS client connection. There is a separate LGOS logical node for each subscription to a goose publisher which facilitates diagnosing the subscription state of goose messages. Let's take a look at the object model for BeagleBone TMWA. To do that I can scroll through the tree until I find the desired device then right click and select show model. In this case I already had the model open in another tab so I'll just select that. Note that it contains an LGOS logical node for each goose subscription. Taking a closer look at LGOS 1, the gocbref.setsourcecb data attribute shows that it monitors a goose stream published by BeagleBone TMWB. The last stnum.stval is the last state number received in a goose stream in the gocbref.setsourcecb. The goose publisher will increment the state number when any data attribute in the data set changes. Let's put BeagleBone B in test mode. Now let's trip the breaker in BeagleBone B. As we do it will simulate a breaker failure and we'll send a goose message which increments the value of the last stnum.stval. The st.stval will be true if we receive the goose stream and the gocbref.setsourcecb within the time allowed to live or TAL contained in the last message. The LGOS status value can be tied to a display to indicate the status of a goose connection. Let's take a look at the single line diagram again. First I'll reset all the breakers just to clear up the display. The LGOS status value can be tied to this display to indicate the status of a goose connection. Notice if I hover over the indicator the pop-up help will show the link to the LGOS data. In this demo the 61850 client simulation in the distributed test manager monitors the LGOS status and displays it on the single line diagram. The status is normally shown as green. However if the LGOS.st.stval goes false indicating no goose messages are received before the timeout period expires the display will turn red. If I unplug the Ethernet cable to TMW BeagleBone B the goose subscriber, in this case BeagleBone A, will detect that messages are no longer being received from that source and will set the LGOS data attribute for status st.stval to false. This value in turn is reported via MMS to the 61850 client which updates the display turning the indicator red. Note that the MMS client connection status indicator for BeagleBone TMW B also turns red because this connection was also broken when the Ethernet cable was unplugged. Data for this indicator is provided by the DTM host statistics for the client instead of the IEC 61850 object model received from BeagleBone TMW B. A list of all the statistics available can be viewed by selecting the host tab, finding the desired device, and selecting show statistics from the context menu. The LGOS logical node is a useful tool for monitoring the state of a goose subscription through an MMS client connection to the device. This allows the client to ensure that the server is operating with valid and current information by providing awareness so that system failures can be addressed quickly.


Video ID: zedtjgWus9Q
All of the commands can be organized by simple drag and drop. So, we can put like commands together if we wish. Even more powerful is the run all commands option. When we select run all commands, the test harness will cycle through each of these commands in order, and this is really where the conditional command comes into play, because you can control that data flow with the conditional command and create some pretty elaborate scripts. We have some examples of this in workspaces that ship with the test harness.


Video ID: e5zbptjZhNc
Another nice feature of Anvil is the ability to display the object model as a list instead of a tree. To do that, we click on the flat plane view and here we see the list. The view contains filters based on functional constraints. So for example, we could select only BR and RP and see all of the report control blocks. Or we could select on just MX and see only the measure ends. Once you find a value in the flat view, you can right-click and say locate to see the location of that measure end in the tree. The flat view also contains columns to show the value and the data type of each item.


Video ID: wM59ZHM4GN0
In this video, we're going to show how to use table-driven simulation in Anvil. Anvil can be used both to generate the tables and to replay tables to simulate a device. I'll start by showing how to create a table from within Anvil. To do that, we need Anvil simulating data. So I'm going to use the automatic simulation mode, which is demonstrated in another video. I start simulation here, and now I want to capture states and store them in a simulation file. To do that, I go to Tools, Generate Simulation File. I'm going to take the default name and store it on my desktop, save it. I'm going to name it Column 1. I'm only going to save my measurements. Now I can create additional states by saying, Add Column to Simulation File. Name that column. Let's say the last one was 1, I'll name it 2. And continue creating more states. If I want, I can go back and recapture a state. To do that, I can do Tools, Overwrite Column. So I'm going to grab the current state and put it in Column 3. So I just edited Column 3, rewrote Column 3 with new values. So now we have a simulation file. Let's take a look at it. The simulation file is simply a CSV file. I'm going to open it in Excel so that it's easier to read and expand out my columns. So here we have a simulation file. Each column represents a notable event or a specific time. And each row represents a specific data attribute. So when we apply simulations, we'll apply a column of data attributes and apply them all simultaneously. So let's look at how to do that in Anvil. First I want to stop my automatic simulation so that my table-driven values aren't overwritten. Then I will go to Tools. If I did not already have the file loaded or open, I could choose Load Simulation File. In this case, it's already open because I've been generating it and adding columns to it. So I can just apply the simulation file. And I'm going to apply Column 1. When I do, notice the data attributes are updated with the values that were captured in Column 1. I can also apply each column as I choose. Then we go to Apply Column 3. We'll see that those values are the ones that we used the second time we captured Column 3 when we overwrote it. Then we can go back and apply Column 4. And of course, these columns can be applied manually in any order. Now another nice feature of Anvil is the ability to play back the simulation file using the timestamps that were captured. If we look at the file, we see here our first data point was captured at 11.02.49, basically, and the next at 11.03.06, which is about 17 seconds. And then the next one was captured 11.03.16, so approximately 10, 10.7 seconds after the next one. So now, let's play back that simulation file in order. I'm going to Play Simulation File, select my file. Column 1 was applied, and in 17.126 seconds, Column 2 will be applied. Then Column 2 is applied, and approximately 10, 10.6 seconds later, Column 3 will be applied. And then another 8.832 seconds, Column 4 will be applied. So as you can see, table-driven simulation is very powerful for simulating a device. You can capture the table using simulated states in Anvil, or you can even use Hammer and capture states from a real device, create the simulation file, and then Anvil can play them back. You can apply states manually, a column at a time, or you can use the playback feature to automatically change values based on the timestamps in the simulation file.


Video ID: uJ25OxeKqro
The following video demonstrates using the Test Harness command window to build and run custom functional tests. The video highlights branching via the Conditional command, controlling test conditions using external test equipment including the Omicron command, and validating conditions using the Validate Points command. It also shows how to run the test using the Run All Commands option. This demo illustrates how to use the command window to run a sequence of commands. This feature can be useful for setting up custom functional tests. In this example, the Test Harness is connected to an off-screen microprocessor board. This test makes use of the Conditional command. The Conditional command allows us to specify a test condition. In this case, we're looking at the status of Binary Input 0.6, which is the inner key on the microprocessor board. If the condition is True, meaning the inner key is pressed, execution will continue with the next command. If the condition is False, execution will branch to the specified command. In this case, the green LED1 On command, as shown in the configuration window, and by the red bar in the command window. This test also uses an Omicron command to control an attached Omicron CMC156. The CMC156 will set up the required conditions that are connected to inputs on the microprocessor boards. These will then be validated with the Validate Points command. The Validate Points command allows you to specify points to be validated and their expected values. If the conditions are met, execution continues. Otherwise, an error prompt is displayed. Finally, the test uses another Conditional command to prompt the user whether to repeat the test. To run the test, we use the Run All commands. Note that Run All commands and Update Expected Values can be used to set up the initial conditions in the Validate Point commands, if desired. When we choose Run All commands, each command is executed in sequence. Although we can't see the LEDs in the off-screen computer, we can see the CROV status, returned by this computer, and see that each LED is being turned on in sequence. If I now press and hold the Enter key, execution continues past the Conditional command. The Omicron sets up the expected binary inputs, and we pass the Validate Points command, indicated by the fact that the test continued. I'm now going to disconnect the connection between the Omicron and the microprocessor board, and I'll select Yes to repeat this test. In this case, the Validate Points command failed because two inputs were not at their configured values. I'll acknowledge this prompt, select No to repeat the test, and the test exits.


Video ID: ovfAKcqWaXY
The data link layer manages the logical link between the sender and receiver of information, and it improves the physical channel error characteristics. For DNP3, this is accomplished by beginning each data link frame with a data link header and inserting a 16-bit CRC every 16 bytes of the frame. A frame is a portion of a complete message communicated over the physical layer. The maximum size of a data link frame is 256 bytes. Each frame has a 16-bit source address and a 16-bit destination address, which may be a broadcast address. The address information, along with a 16-bit start code, the frame length, and a data link control byte is contained in the 10-byte data link header. The data link control byte indicates the purpose of the data link frame and the status of the logical link. Possible data link control byte values include ACK, NACK, link needs reset, link is reset, request data link confirm of the frame, request link status, and link status reply. When a data link confirmation is requested, the receiver must respond with an ACK data link frame if the frame is received and passes the CRC checks. If a data link confirmation is not requested, no data link response is required.


Video ID: bTXXcG_ihLk
In this video, we're going to look at how to use Hammer in order to capture states from a real-world server and save those states into a simulation file that can later be used by Anvil to simulate that server. The simulation file is just a CSV file. It has points in the rows and states in the columns, so you could create one of these yourself, but it's very, very easy to create them with Hammer, as we'll see in this demo. For this demo, we'll be using the 61850 demo server as a real-world device. The 61850 demo server was written using our .NET components, and it simulates a bay controller with a recloser and some interlocks. The ICD file and the algorithm for this bay controller were provided to us by Christoph Bruner of IT4Power. So here we see the 61850 demo controller running. If we click the Help button, we get a pop-up dialog that explains the algorithm and the interlocks that are used by the bay controller. We also have Hammer connected to the bay controller, and the first thing we need to do when we connect Hammer to a server is we need to enable the report control blocks. So now that our report control blocks are enabled, we should start receiving data updates, and if we drill down here and look at, say, the currents on phase A, we can see that the currents are updating, as shown in the bay controller demo. Another way to look at values in Hammer is using the flat view. With the flat view, we can filter by functional constraint. So let's filter and only look at the MX and STs. Again, if we scroll down, get down to the MX, and look at our voltages and currents, again, we can see them updating, as also shown in the bay controller demo. Now that Hammer is connected to the real-world device and receiving updates, we can capture the states from this device into our simulation file. To do this, we select Tools, Generate Simulation File, we can enter a file name, select Save, and we get a dialog which allows us to name the column. I'm going to name this column Normal because this is the normal state, and we can also select which functional constraints we want to save. I'm only going to save the MX and ST in order to keep the file size a little bit more manageable. You can certainly do all of them. For the sake of the demo, I just want to keep it down to the two points that I'm interested in, two types of points I'm interested in. So now we've generated a file. If we already had a file open, we could open it using Load Simulation File and open the file. And once the file is open, we can create additional columns. So let's open QA1, and now we go to Tools, Add Column to Simulation File, TMW Demo, and I'm going to call this QA1 Open. Now let's open QB1, and let's save that state as well. Add column, choose our file name. And that's really all there is to it. Connect Tamr to your device, make sure you're getting the updates, and then just save the states into the simulation file. In the next video, we'll look at how to load that simulation file into Anvil and simulate one or more servers using the same file.


Video ID: GRVyNNYDtzo
This video will be summarizing our DTM substation simulation, mapping, and test management functions as shown at Distributech 2019. Our Test Suite Pro demonstration will be shown in the video in the links in the description. DTM is our system simulation tool. We have created folders in our DTM workspace to organize our simulated devices by their functionality. Our substation folder includes line 1, line 2, transformers, and high voltage busbar devices, and those devices can be seen here. Each device has scripts associated with it to allow it to behave as it would in a substation. Since many of our device's logical nodes behave in a similar way, we have created global scripts seen here. These scripts are referred to by the devices in the substation folder. This speeds up the behavior simulation process. At Distributech, we also use our SCADA data gateway system on Linux as a data concentrator. For the sake of simplicity for this video, we are also simulating the SCADA data gateway's mapping and data concentration in DTM. This simulation is done in the SCADA HMI folder. The 61850 clients to the substation 61850 servers and the DMP3 outstations are stored here. The mapping between 61850 and DMP is done via this JavaScript. The final folder we'll be looking at today is the SCADA folder. This is the DMP master to the DMP outstation that is holding all of the data that was concentrated. Now that we've explained our substation, we can see a few of the features DTM can perform with them. The first feature that we'll be talking about today is a brand new feature that will be released with DTM 1.4. DTM now has a built-in test manager. This allows for running manual tests, automating test plans, reviewing functionality, and exporting results. For this video, we'll be showing off a test manager created to verify proper mapping in our gateway device. Each row in this window is a test. Specifically for this video, it is a 61850 data attribute point change that will occur and the resulting DMP3 point that should be changed. You can edit these tests, reload your test changes, reset test results, skip selected tests, run selected tests, or run all tests. I'll be selecting a few tests to run. After selecting these tests, I will click Run Selected Tests and watch them run. Once finished, I can group by results to see which tests were successful, which failed, and which have not been run. Once the test is finished, I can export the results as any of the options seen here. The last feature I'll be showing in this video is the single line diagrams demonstrating the behavior of devices in our substation. Our single line diagram was created using Insight, which can be used to build visual representations of devices you're simulating, including a single line diagram, device panels, and HMIs. With this single line diagram, we can create a temporary fault via PDIF1, PDIS1, and PDIS2. We're also able to cause permanent faults and breaker failure events. I'll show you all of these situations now. This button causes a failure on the line. As you can see, the breaker opened, and then the recloser closed the breaker. Since the fault was cleared, the breaker remains closed. The switch next to this button causes the fault to become permanent. This will cause the breaker to open again after the recloser tells it to close. I'm going to close the breaker now. The last event we'll see is a breaker failure event. In this situation, the breaker will not open when it needs to. This will cause a goose message to be sent out to our other devices in the substation, causing the other breakers to open and stop the flow of electricity. These events will be used to demonstrate TestSuite Pro's ability to monitor a substation. For more information on TestSuite Pro's Distributech 2019 demo, check out the video in the description below. For more information on DTM, TestSuite Pro, any of our protocol stacks, or our test tools, feel free to contact us by following any of the contact information also in the description below. Thanks for watching.


Video ID: 0htHEP_RuMk
Another nice feature of Hammer is the ability to display the object model as a list instead of a tree. To do that, we click on the flat pane view and here we see the list. The view contains filters based on functional constraints. So for example, we could clear all of the functional constraints and select only BR and RP and see all of the report control blocks. Or, we could select just MX and see only the measureands. Once you find the value in the flat view, you can right click and select locate and it will show you the location of that value in the tree. Notice that the flat view also contains columns to show you the value and the data type for each entry.


Video ID: Aq4UxAr16XI
This is the third video of the fundamentals of DTM for Release 1.3. In this video, we're going to be going over how to create a single device, multiple devices at once, and how to do an import substation, which is very helpful for 61850. To add a single device, you'll go to the workspace, right click, manage, add, the protocol of your choice, I'm going to choose DNP3, and then the master or outstation option. I'm going to choose outstation. The configuration window has multiple tabs, and we'll address each of them in detail. The first tab will decide where the device is going to be simulated. You can see in my network tab, in the background here, that I only have one machine as the master and one host, so it will default to that host on that master. If I decided I'd rather create it on a new host, I can always click new host on InGreenPC. We also have the device name, which we have defaulted to SDNP. You can change that if you'd like. In the channel tab, this will affect its behavior and configurations for connections. Remote.0.0 anyadapter is a setting we have for a local address that allows any device or master looking for this specific outstation to connect. If you'd like to make it more specific, you can choose a specific IP address and adapter it will only communicate on. The remote address is the address it's expecting to get communication from. Again, by default, we leave it at its most generic, star.star.star.star, to allow it to connect to any device that it's specifically looking for. For DNP, the outstation port is 20,000, so we'll leave it there. But if you'd like to play around with it, you can change the port number as well. We also have a sessions tab, where for DNP you can change the source and destination, but we've also left it as the default here. In the database tab, this is where you can change what the database of your specific device has. We do, again, have a default that we normally set it to, but if you'd like to change that, you can click reset database and create whatever sort of database you'd like to match your device. So if I clicked okay right now, it would go ahead and make one device. Let's see that. Now as you can see, I said on new host, it created a new host, dthost1, as the names have to be unique, and it put our SDNP device on top of that, or inside of that, dthost. Over on the workspace tab, you can now see the device that has been created. But if you needed to create more than one device at a time, you would go through the similar process. You would just address a tab that we didn't look into. You would right click, manage, add, DNP, outstations, and then you would go configure your device again. I'm going to increase the port this time, just so we don't have a overlapping port problem. Then you would go to the multiple devices tab. From here, you would select create multiple outstations, and you'll see it defaults to one, and that's the configuration information for our first DNP outstation for this configuration. You can increase this, and you'll create more instances of the device, as you can see here. By default, we increase the port number to make sure they all have unique configuration information. When you hit okay now, it will create all the devices that you asked for, and it will put them on the host that you set. Now these devices are up and ready to be connected to. If I wanted to connect to these devices, I could right click, manage, add DNP masters this time, and I could go straight to the multiple device tab and create six of these guys. Now, as you can see, I have created six masters and six outstations, and as mentioned in the previous video, all of the little Wi-Fi indicators have gone green, as they have had the correct configurations to communicate. In your system, you'll probably not want to simulate both the outstations and the masters in DTM, but while learning about the system, feel free to do it. In the second part of this video, we're going to be talking about a 61850 specific feature. This is import substation. This allows you to pull in devices directly from an SCD file all at once. To do this, you'll go to file, import substation, then you'll be presented with which configuration file you would like to use. If it's already in your resource tab, you could just choose it here under this editor. If not, you can browse for it using import configuration. I'm going to use substation SCD. After you hit okay, you will be prompted with the substation browser. From here, you can drag and drop any of the devices you want. So if I wanted this first one, I can drag and drop it onto the node I'd like. Another thing you can do is you can change the configuration to match your system. This can be done either manually by clicking and choosing an option that you'd like for the port number or the IP address, or if you'd like to do it all at once, you can click shift click to multi-select and come down to the group edit. You can choose what host you'd like to put it on and what IP address you'd like it to use. After you've selected the configurations you'd like, you can click apply and it will change all the selected values. Again, from here, you can drag and drop it onto the node you'd like and it will instance these devices. This is a very quick way to instance all of the devices that you need from an SCD file.


Video ID: JbGGS8Q8MIA
In this video, we'll demonstrate sending controls to a simulated substation using ICCP. This video is the third video in this series. Please see videos 1 and 2 for more information about the substation simulation and the components involved. In this section, we're using Iron to simulate an ICCP or TASI-2 peer-to-peer device. Iron is connected to the SCADA data gateway, which is acting as a protocol translator and running on an AdvanTech embedded PC. The STG translates from ICCP to IEC 61850 and sends the resulting command to the configured device, which in this case is a 61850 simulated server running on the Distributed Test Manager. The simulated device sends reports to two IEC 61850 clients, one in the SCADA data gateway and one in the Distributed Test Manager, or DTM. The STG translates the report to ICCP and sends it to Iron. DTM updates the single-line diagram via Insight. So here we have Iron on the left and the SCADA data gateway below it. To the right of the STG is a front panel simulation of a GE device. Please see video 2, Demo Components, in this playlist for more information about the front panel simulation. The Distributed Test Manager with the single-line diagram Insight display is on the right. We can see on the GE front panel that the breaker is currently closed. This status is also reflected in the single-line diagram, where red indicates closed and green indicates open. Notice in the STG, the 61850 client shows the GE breaker position is 2, which is the double-point representation of on. This point is mapped to the GE breaker position in the ICCP or TASI 2 client, where it is also shown with a value of 2, or on. To send a command, we open the VCC node and right-click on the GE breaker control point and select Operate. We can then set the value to 0 to open the breaker, or 1 to close it, and then click right. Let's open the breaker by setting the value to 0. As we do, we'll see the command value in the STG go to off. The STG sends this command to the GE device, where we can see the breaker is now shown as open on the GE front panel simulation. The GE breaker reports this change to both the DTM and STG clients that are connected to it. Here, I am viewing the DTM MMS reports, filtered to show only the GE device report control reports. If I expand the report, we can see that it received a data change report with a value of off. We can also see that DTM has updated the single line diagram to show the breaker is green or open. Looking at the XCBR status in the SCADA data gateway, we can see that the value is now 1, which is the double point representation of off, indicating the STG has received the report and updated its status. This IEC 61850 point is mapped to TASI2 or ICCP, where the value is also shown as 1. That value is reported to IRNN, where we can see that the GE breaker position is also reported as 1 or off. Now let's clear the reports in DTM and send a command from IRNN to close the breaker. Once again in the STG, we can see that the control point now has a value of on. This point is mapped from the ICCP server to the IEC 61850 client, where it is sent to the GE device, whose front panel now shows the breaker is closed. Looking at the MMS reports in DTM, we see a data change report with a value of on, indicating the 61850 client received the report from the GE breaker. DTM has also updated the single line diagram to show the breaker is red or closed. In the STG, we can see that the IEC 61850 client is reporting the BOS.STVAL as 2, which is the double point representation of on. That point is mapped to the TASI2 or ICCP server, where we also see the breaker position is on. The server sends the update to IRNN, which now shows the breaker position is on. So we have seen the full cycle. A control command to open or close the breaker is issued from IRNN, representing an ICCP or TASI2 control center. The command is mapped from ICCP to IEC 61850 in the STG and sent to a breaker simulated in DTM. The breaker updates its status and sends the appropriate reports, which results in updates to DTM. The reports are also mapped to ICCP in the STG and sent to IRNN, where the updated value is displayed.


Video ID: uEPC1LbRffs
SCADA protocols have evolved over many years, always with the design objective of optimizing bandwidth and minimizing the impact on the hardware and RAM and ROM of the end devices. DNP, Modbus, 870-5 all do a very good job of this. These are designed around what have traditionally been the cost drivers in the system. Bandwidth was expensive, hardware was expensive, particularly in the embedded systems and the end devices. Over the years, since these SCADA protocols were developed, we've had some really radical changes in the bandwidth constraints. Serial technologies have moved from what were typically 1,200 baud links some years ago to a minimum of 56 kilobaud. The LAN technologies have gone from the megabyte range to the gigabyte range, and we've developed a lot of new technologies like frame relay that really change the picture for substation communications. At the same time, we've also had even more drastic reductions due to standardization. Things like Ethernet and TCPIP have really changed what computer communications is now. Because of this, we've had much expanded markets. Devices are communicating now that we didn't conceive of 20 years ago. Also, this same explosion has caused really major advances in hardware capabilities. All of this has basically reduced the cost of communications in terms of the bandwidth and the hardware. Because of this evolution of technology, we now have a wide variety of communications technologies available, and bandwidth is no longer the cost driver that it once was. The cost is greatly reduced. We can move an awful lot more information for the same dollar that we used to. We also, on the other hand, expect greater intelligence out of our substation applications, and that means that there's been an explosion in the amount of data that's monitored and moved around and an explosion in the complexity of the applications. This means that the cost now is changing our focus from being worried about bandwidth constraints to being worried about how do we manage this huge data and how do we manage the datasets that are spread throughout the system that have to be configured, maintained, and updated in order to make all the applications work together. So, we're shifting from moving data to managing data as the main cost driver in the overall substation work. As the volume of data has increased and the application complexity has gone up dramatically, we've now moved to higher costs for things like commissioning costs and commissioning both the end devices and the applications, maintaining the databases. Deregulation is added to this because there's a lot more data sharing now across organizational units, and that makes, again, this data configuration and management even more critical to the utility. Because traditional SCADA protocols were defined to move data with a minimum amount of overhead, the meaning or semantics of data as it's spread through the system has to be maintained in different ways. So, typically, you'd have configuration of RTUs and IEDs, configuration of databases and configuration of applications that all have to be done to make sure that the right data is getting to the right place. So, instead of moving phase A voltage from some particular line, you're moving point number 3,850, and someone's got to figure out all along the system whether that point, as those point maps occur, that the right data is flowing in the right places. This validation is costly and time-consuming and requires a lot of staff time to be able to maintain this. These trends aren't specific to the utility industry. When you look at all of the communications world, we've moved towards more object-oriented communications capabilities or protocols that organize the data by function. This greatly simplifies distributed applications and managing the applications. Similarly, we standardize object models that allow for a high degree of interoperability between applications. We also include things like self-description and metadata that allow for online validation. Now, this concept, the concept of metadata comes from things like Corbo, where as the objects are moved around, you're not simply moving the bytes of data, but you're moving description to talk about type descriptions and allow for type checking as it goes along and identification of data. The semantic meaning of data goes along with the raw data. This is basically trading off a little bit of bandwidth to allow for a higher degree of interoperability and validation of application connectivity. So it's an important piece that's, again, it's not specific to the utility industry. That's the way all of communication is moving because really connecting the applications to the data has become a bigger chore than trying to buy the bandwidth to move it. So the previous generation of SCADA protocols were designed around the idea of optimizing bandwidth and processor loading while maintaining data integrity and reliability. These protocols like 870-5 and DMP Modbus do a great job of optimizing these things, but the next generation of protocols, such as 61850, are trying to reduce the data management problem while also maintaining integrity and reliability. So they're really focused on different problems because the cost drivers have changed now over the years. To summarize, then, the design objectives underlying IEC 61850, when we set out to develop the standard, we wanted to address the data management cost drivers using modern communications techniques. We wanted to achieve a high degree of application interoperability using standardized object models. We also wanted to simplify substation engineering by using a common configuration language. This also addresses a lot of the data management issues. And one important piece to it is the online validation of the substation communications using both metadata and self-description. The self-description piece, we weren't looking to do plug-and-play, but we did want to be able to do online validation of everything. So basically everything that's described in the configuration language is also available through the underlying services. So you can find the object model of the device at the device. You can find the capabilities of the device both in the configuration files and in the end device through the self-description techniques. So this is an important piece that we can both configure these devices and then, once they're operating, validate that they, in fact, are talking to the systems that they think they are and that the data is organized in the way that it's supposed to be. So it's both offline configuration and online validation. This is different than plug-and-play. We're really not interested in trying to get into plug-and-play issues within the substation, but we did want to be able to plug-and-validate. IEC 61850 was developed by three different working groups within TC57. It's working groups 10, 11, and 12. The three groups worked in task forces that spanned all three groups so that the work would be highly coordinated and ended up developing really one document set. The work was derived originally by combining submissions from Sea Gray and the EPRI-sponsored UCA project. Most of the work is pretty much backward compatible with UCA. The scope of 61850 is much broader than anything that's been attempted before for substation automation. It spans all areas of the substation, includes multiple protocols for various special purposes. It has a large and extensible object model, including standardized naming and standard data types. It also has, for the first time, a standardized configuration language that can be used for all the devices in the substation.


Video ID: KePw-mcVM_w
In this video, we're going to upgrade a product which had a seat-based license using the old licensing scheme to a seat-based license in the new licensing scheme. In this video, we'll be using Anvil as an example. So here I have Anvil already running. If I go to Help, About, we can see that I'm running version 3.6.3.0, which uses the old licensing scheme. And if we look at Help, License, we can see that the product is currently licensed with a seat license and it has a current maintenance plan. So now I'm going to exit Anvil to get ready to upgrade it. To perform an upgrade, you need to log into your portal and download the installer. For the purpose of this video, I've already downloaded my installer and have it here on my desktop. So I'm going to run the installer. The wizard verifies the contents of the setup package, prepares the package for the installation, and brings up the installation screens. Be sure to read this important upgrade notice. Depending on your license type, you may need to make a decision on how to license your new product. That process is covered in a separate video. For now, I've read the upgrade notice, so I'll click Next. Then we get a screen with special notes for USB license users. If you currently use a USB license, you'll need to decide how the product should be licensed once you upgrade. The various license options are covered in a separate video. If, after reviewing the licensing options, you feel that you need a hardware dongle or want to upgrade to a network license, then please stop the upgrade and contact us at sales at trianglemicroworks.com. Now I'll confirm that I've read this note and click Next. I need to uninstall the previous version, so I'll click Next. The installer then verifies my license with the license server and begins the uninstall process. Note that this may take a few moments, so be patient. So now the old version has been uninstalled, and we begin installing the new version. I can now click Next, agree to the license agreement, click Next. I'm going to accept the default folder for the installation and click Next. Next we confirm the components to be installed. I want all of the components installed. I do want to create an application icon on the desktop, and so I'll click Next. Finally we're ready to start the installation, so we click Next again. And now I'll need to restart Windows. So now I'll start up Anvil. If I look at Help About, I have the new version 3.7.0.0, and if I look at Help License, we see the new license information and can confirm that the license was converted from the old license to the new license. So as you can see, converting from a current seat license to a new seat license is a very straightforward process. If you do encounter any problems, please contact us at support at trianglemicroworks.com.


Video ID: v6vpwUIFZPA
Hello, and I welcome. This is Herbert Falk, Vice President of the UCA Users Group. I'd like to welcome you to this webinar about strategies for using 61-850 Addition 2 Amendment 1. I hope you will find it useful and get your questions answered. I'd like to have Beth Capellas kind of give some instructions regarding the webinar, and then we can move on with the presentations. Thanks, Herb. Good morning, everyone, and again, thanks for joining us today for today's webinar. As Herb mentioned, my name is Beth Capellas. I'm the Marketing Manager at Triangle Microworks. Before we get started, I did want to go over some housekeeping items. The webinar is scheduled to be one hour long. We will be taking a couple of breaks to answer questions throughout the webinar. The audience is on mute due to the large size, so please submit your questions online using the webinar toolbar under the section titled Questions. You can submit questions as soon as you have them. Don't feel that you have to wait for a break or wait until the end. We will read and answer some questions at the breaks, and we will have some time in the end to go over questions and answers. We're also happy to stay on the line after the webinar is over and continue to answer questions if they're still coming in. So, if you've got some extra time and you want to hang on the line with us, we're happy to just keep answering questions as they continue to come in. I also wanted to mention that the webinar will be recorded. When you sign off of the webinar, you actually will receive a short survey. Once you answer the survey, you will get a follow-up email with a link to the recording. It will also be posted at trianglemicroworks.com on our website early next week, as well as the UCA's website, so you can access it there as well. With that, I think we're ready to get into the content, so Herb, I'll hand it back over to you. Thank you, Beth. I'd like to take this opportunity to introduce Stuart LaValle. He is the current and new chairman of the board of the UCA International Users Group, and he works for Duke Energy, and he has some introductory comments. Thank you, Herb. Welcome. Good morning, or good afternoon, for those who are overseas. I appreciate the opportunity to introduce the UCA, for those who are not familiar with the International Users Group. We are a nonprofit organization. We're served by our members, and it's a bunch of contributors, really trying to drive the organization forward to enhance interoperability through the industry. Clearly the main focus in this presentation is the 61950 user group content, but UCA also supports some of the other kind of user groups, such as the Common Information Model, as well as the Openfield Message Bus. I'm also the co-chair of the Openfield Message Bus user group as well, and all three we feel are very complementary and are based on standards to really move the industry forward for interoperability and also the testing of these standards. Next slide. So the mission of UCA is to enable integration through deployment of open standards, providing a forum in which the various stakeholders in the energy industry can work cooperatively together as a member of a common organization. Really the focus is on influencing, endorsing a lot of these open and public standards that are appropriate to the utility market and really addressing the needs of the members. We also specify and develop testing certification programs to really help accelerate the adoption of interoperability products and systems based on these standards I just mentioned. And also outreach is a big focus, driving awareness to really help promote and educate the users and the community on these important interoperability standards. Next slide. And the last slide is UCA is spending a lot of time really improving its focus to enhance some of its services that are appropriate to the members, trying to improve the experience. We've made some investments recently to improve the website collaboration spaces so that the tools for interactions between members virtually are effective. In light of the COVID we've had to adapt and we're making those investments to make it a better virtual user group experience as well as when we can meet face-to-face next year we also plan to resume those very vibrant interoperability plug fests and other user group meetings. Last but not least, the testing certification program is very important and paramount to UCA and we plan to continue to invest into improving the quality of that experience and standing up some interoperability testing certification authority programs to support the conformance testing of 61.850 STEM and OpenFMB. So that concludes my section and I'll pass it back to you, Herb. Thank you, Stuart. The reason why we are all here is to discuss 61.850 Addition 2 Amendment 1 and the impact on testing and the industry at large. This slide is intended to show you the intended relationship between how UCA has structured its conformance testing to align with the IEC cadence of version releases. IEC typically has a stabilization period of every five years but only has the view of one standard release. So when Addition 2.1 came along, Addition 2.1 by IEC and it's really Addition 2 Amendment 1, Addition 2 Amendment 1 is now viewed as IEC by IEC as the only Addition 2. Now amendments are not supposed to introduce compatibility issues within the same version and that's how we basically structured our cadence for conformance testing. And our basic policy was to provide stable conformance testing for each addition for a minimum of 10 years so that the industry could plan on their migrations from one addition to another. And as I said, we expected incompatibilities to be based upon release, releases and not amendments. And originally the end of life date for conformance testing on Addition 2 would be evaluated upon the release of Version 3, which isn't on the horizon, by the way, so I don't want people to be concerned about that. What actually happened in Addition 2 Amendment 1 is there were some changes, some intentional, some non-intentional that created forward compatibility issues between Addition 2 and Addition 2.1 systems. We queried the user community, and you'll hear from Carlos later on, and they came back and said that they have RFIs and RFPs calling out Addition 2 and not 2.1. And since there's a compatibility issue, they wanted us to do something to provide them the capability of continuing conformance testing of Addition 2 devices, not based upon the normal one-year rollover policy that we had in our Quality Assurance Plan. So Addition 2.1 broke the UCA IUG assumptions for our conformance testing cadence, and because of that, we spent a significant amount of time to put together a white paper that documents the compatibility issues from Addition 2.0 to 1.0, Addition 2.0 to 2.1, and so forth and so on. Within that white paper, we provide recommendations on how users could address migration into 2.1, and it's a white paper that's available to UCA IUG members. That white paper is the basis for most of what you're going to hear in the rest of this presentation. What we also determined based upon this white paper was we needed to modify our testing policy. So basically, this graph shows that right now, you can actively, vendors can actively submit IEDs and applications for Addition 2.0 testing, and you'll see the keyword only. We have a lab that is currently scheduled to undergo accreditation tests, accreditation for Addition 2.1 testing. Once they get accredited, which should be soon, vendors can choose to do Addition 2 or 2.1 conformance testing or both. We're going to continue to allow Addition 2 conformance testing only till January of 2024, and after that, we're going to allow people to get conformance certificates for Addition 2, but only after that device or application has passed conformance for Addition 2.1. And we think that this provides a migration path for the industry forward. Obviously, the actual date of January 2028 depends on future amendment and release cadence of IEC. And with that, I am going to turn it over to Richard Schimmel of DNB to talk about the impact on the actual testing. Yes, thank you, Herb. This is Richard Schimmel. You can hear me? You can see my screen, I think. Yes. I've been working for the same company for over 30 years. It's just that the company keeps on changing the name. It used to be Kyma, then DVKyma, DVGL, and now it's DNV. I will talk about the testing and certification of Addition 2 Amendment 1. One moment, yeah. Testing is part of the quality assurance process. The quality assurance is specified in the standard in IEC 6150 part 4. There's a product development quality assurance, mostly for manufacturers. And we have, on the right, we have project execution. The project is like automating a substation automation system, so that's done by the utilities. So UCA and we at DNV are involved in the middle, especially in the part of conformance testing. So conformance testing, why do we do conformance testing in the first place? So yeah, to reduce interoperability risk and to provide maximum customer confidence that the certified device will interoperate with other certified devices now, but also in the future. The conformance testing needs a reference. You need a reference where you base your testing on. If the testing is passed or failed, you need a reference. The reference is done in 6150 part 10 conformance testing, which is just on a high level where we define abstract test cases. And the UCA IUG works out the abstract test cases into detailed test procedures. So I'm the editor of both the part 10 and I'm also one of the editors of many UCA conformance test procedures. So what is the role of the UCA international user group related to 6150? As mentioned, UCA sets up conformance, maintains 6150 conformance test procedures. The UCA accredits 6150 test labs and we have level A labs and level B. Level A is vendor independent, and level B is vendor specific. So also vendors can have a test lab and certify their own devices. Level A is D&V or Kyma viewer level and we are level A. Once we do a test and the device passes the test, we issue the certificate to the UCA and UCA will review and when okay, it will post it on the SharePoint, on the UCA website. Over here you can see it will change in a while, but here we have the test certificates. We started this already in 2005, so over 15 years and at the moment 1195 certificates are posted, are available on this website from all the labs, not just our lab, but from all level A and also all level B labs. All certificates are, you can find and you can search for a specific device from a specific vendor. There's some search possibilities. Very interesting to see what firmware versions, what devices have been certified. UCA also hosts and organizes a bi-yearly interoperability event starting from 2011. So this year, there's another one. Yeah. Okay. Coming back to performance testing, where we started, it was edition one or first edition. Very easy. No migration at all. There was only one edition. We started with testing, certifying a server with the MMS interface and Goose publish and subscribe. And then we moved to clients, so we can also certify the client MMS client interfaces. And also merging units, merge unit that publishes sampled values according to the 9-2LE specification. So it's only the publishing, we don't do subscribe. Again, edition one, and it has ended already. So no edition one certificates will be posted anymore on the UCA SharePoint. So of course, we do edition two for quite some years. And as Herb mentioned, we will keep on doing it till 2024 and also beyond. We still have MMS server and client and Goose publish and subscribe. What is new in edition two compared to edition one, that we also can certify the configuration tool. So we have a configuration tool for an IED, the IED configuration tool, or ICT, we can do it. We have conformance test procedures for that. We have also test procedures for a system configuration tool, the SCT. I must say the server and the client are very popular. There are many certificates available in the database. Testing the tools is quite rare. I think there are only a few, less than five certificates for tools available in this database. It's a bit of pity, I think. So edition two has no sample values, as mentioned, only after 2024, only combination with amendment one. So now we are moving to amendment one. And what has been shown, and I told you that the tooling, so that the server and client, is all the same, the tooling has changed. So we have seen in the UCA interoperability testing, that there are quite some issues related to the SEL files and also to the engineering tools, the ICT tools. And yeah, that's not good. So based on that, we decided, okay, let's do some basic ICT testing, add it into the server test. So it used to be separate, now it's integrated. So the ICT tool and the server are integrated. So importing an STD file and exporting an ICD file and IAD file are mandatory test cases. And now, if you want to certify an amendment one server. For time sync, we also have, we added, we support PTP time synchronization, the precision time protocol, and we support sample values. Sample values based not on the 920, but now based on the IEC 61869-9, both the publishing as well as the subscribing. So as mentioned, we, to improve the quality of the ICT tooling, we mandate the basic ICT functionality as part of the server test. So having a certified server is a bad ICT tool. It will still, it may have a certificate in edition two, but for sure in amendment one, it won't happen again. Okay. So what has changed in amendment one? Okay. The ICT tool has become part of the server test. That's as I showed you in the picture. We also verify the backwards compatibility to edition one and edition two. So if you want to certify a client, it shall work with an amendment one server, of course, but also to edition two server and edition one server. So this, it will be backwards compatible. We also will check the forward compatibility to future edition. So to edition three or four. So the existing features will be compatible for in the future. And also now that we know how amendment one is working, we will update the current edition two test procedures to also be ensure forward compatibility to amendment one. Yeah. There's a white paper. It will show you the details, but actually the functionality was already in edition two. It was just an optional feature. And the only thing we need to do is to change that optional feature into a mandatory feature. So it's nothing new. We don't add anything. It's just that there's an optional feature will change to mandatory. Okay. If you look forward to security, cybersecurity is also part of amendment one and is being worked on by the workgroup 15. And so we will have secure MMS, we will have secure goose and secure sample values as well. There will be the routable goose. So those gooses can be routable, can pass through a router. So if you want to use goose outside the substation, you can use a routable goose. And by definitions, these are secure. And same for sample values. So if you want to use sample values outside the substation, you can add security. It's being worked on. We didn't want to wait until security was available. So we already got started with amendment one, but the security will be expected next year, 2022. To summarize, amendment one, we add functionality for ICT. So the ICT tooling will improve the quality and improve the durability. I hope we will see the results soon at the next interoperability testing organized by the UCA. We will address backward and forward compatibility. Security will be added when available next year. And yes, at the moment, we have available the server test procedures, the clients, the ICT tooling, and the complete ICT tooling and SET are in progress. UCA endorses vendors and utilities to implement amendment one within a reasonable number of years. So we discussed a long time what is reasonable. Certainly not one year. That's not reasonable. But yes, 2024 is good. OK, this concludes my presentation. We can have a look at. Please keep your slide up, Richard. Sorry. If you would. So this is our break for answering questions. There are a couple of interesting questions for you, Richard. And I just want to run through them real quick. How can you test 61869-9 compliance when it's based on addition 2.0? And I'll take a swag at this. 61869-9 actually requires a data object that's only in addition 2.1 officially. So it is aligned with 2.1. Do you have any other comments, Richard? Yeah, the 61869-9 refers to amendment one. That's the whole issue. The IC61869-9 does not refer to addition 2.0. It refers to addition 2.0 amendment one. And it also adds a new accommodative class, which is only available in amendment one. So that's why the 61869-9 is on amendment one. Okay. Is routable goose need to meet the performance requirements of layer 2 goose? I guess that's the question. I have experience in a system with both layer 2 goose and routable goose. And there's no significant performance degradation between those two protocols. But when you do turn security on, the layer 2 goose and routable goose, they're equivalent. But with security turned on, routable goose does cause a slight delay, but it's implementation-specific. Well, the other issue there, of course, is that the network is constrained for layer 2. If you're trying to run routable goose across, say, the full internet, then you can't make those guarantees. Yep. The other thing is a lot of people use L2TP to route layer 2 goose, and they think that's going to avoid NERC SIP regulations. The regulations are changing. And basically, I think that type of an approach is going to be viewed as a routable protocol in the future. I've answered the backward compatibility issue, I think. Herb, you've got time for one more question. Okay. So, as part of the tools testing, only ICD and IID and SCD files shall be validated. No SCD files, question mark. Richard? No CID files. Yes, the CID file is only used internally, so we don't care about the CID. It's the ICD, IID, and SCD, but the CID is out of scope. So, with that, we have an interesting question that I've intentionally delayed. So, if you will hang on to the end of the presentation, hopefully you'll get your answer. So, at this point in time, I'd like to introduce Joel Green of Triangle Microworks. He's the Director of Development for them for IEC 61850. Take it away, Joel. Thanks, Herb. Good morning, good afternoon, good evening, everyone. In addition to my roles at Triangle Microworks that Herb mentioned, I'm also an active member of several of the IEC working groups and editor of the core parts of 61850. So, I'm going to discuss a little bit about how we maintain the standards and progress the standards and what impacts those have on users and systems. Moving forward with 61850, as we move into Amendment 1, similar to as we did with the move to Addition 2, we've got a few different use cases where this comes into play. We've got the case of a brand-new substation, we've got the case of replacing a substation, and then we've got the issues of expanding or maintaining existing substations. In the first two categories of the newer replacement, when everything's new, life is fun and easy. We build a system to the latest standard, everything interoperates, there's nothing to worry about here. This is what we'd all like to be doing. The other couple of cases where we're mixing in new devices with older systems are where things get a little interesting. We've got a couple different situations here. Richard talked about Addition 1 a couple of times. We're not certifying any new Addition 1. Hopefully, no one's installing any new Addition 1 systems, but there are Addition 1 systems out there that have some life left in them. Those have to be maintained. Addition 2 is now moving into that category of later in the life cycle systems, but those systems in particular have to be maintained going forward. We've got a couple possibilities. We've got sometimes we need to expand systems, add new bays, and then, of course, we have maintenance, we have replacement of equipment. Those two situations are very similar from the perspective of how to move forward with the standard. We've got a couple different options on how to move forward with these systems that are in place. If we don't need any new functionality, we can simply add or replace the devices that are currently in the system. Couple paths down that road, of course, certification and the standards have nothing to do with products that are on the shelf. When the system's installed, your vendor has some commitment to life cycle to produce those same devices throughout the life of the system. Typically, those devices should be available for replacement, no problem. In some cases, we see devices go obsolete because parts are no longer available and so forth. In those cases, the vendor can certify new devices to the existing standard, and as Herb went through the testing procedure, UCA is going to allow a process where new devices certified to addition 2.1 can then also be certified to addition 2 to handle that replacement life cycle issue. So we've spent a lot of time in analyzing that problem. There's a clear path there for those maintenance situations. On the other hand, our job as the working group, as IEC, is to continue to meet new customer needs, to improve the standard, in some cases to correct it, produce new models, new features as required. So we've got to keep evolving the standard. Technology improves. We've got to work with it. So we've added new features to the standard, new services in some cases. In the case where now we have an existing system and we need to integrate those devices, which are built to the current rev of the standard, we've gone through and analyzed these cases also. There's an annex in Part 6 of 61850 that goes through the rules for this. At a high level, it boils down to some pretty simple concepts. Our philosophy is that the critical thing here is for any devices that need to exchange information to be able to understand each other. So what that means is that servers typically operate at one level. Clients and tools that operate with multiple devices have to understand all previous revisions of the standard up to the current. So when that gets to implementing the actual system, here's what we get into. If we're happy with the 2.0 system, we can integrate the new devices as in the first option. But if we need to use some of those features, we need to update our system configuration tool. The system configuration tool needs to understand all the devices in the system and the interactions between the two. So the system configuration tool needs to be the latest revision of the standard used in the system. This gets us to a, in this case, 2.1 system. The next thing we need to do is look at our clients in the system. We've talked a little bit about one backwards compatibility, so I'll go into detail on that in a later slide. But we need to look at our clients, our HMI, our gateway, our control center, and see if it correctly handles the reservation control box. If we have a client that doesn't do that, the HMI and the gateway, then that device needs to be updated in order to have a 2.1 system. The big consideration that I keep alluding to is this reservation. Now, this reservation concept has existed since Tissue 453 of Addition 1 in 2007. From that point on, in 2007, any devices that implemented that Tissue 453, which changed reporting from Addition 1 to Addition 2, essentially, from that time forward, should be able to handle these concepts. Unfortunately, for backwards compatibility reasons, that reservation system in the server was added as optional. And because it was optional, a lot of servers didn't implement it. And unfortunately, a reservation system that's not mandatory turns out to not be very helpful. So in the amendment, we finally took the step of making that reservation system mandatory. What this has exposed is that a lot of clients didn't implement the reservation scheme. Even though it was optional in the server, a client needs to be able to interact with any server out there, so that the proper client implementation should have had this reservation support since 2007. Some have not, but this is what has to be updated as we move forward in 2.1. On the left here, we've got a screenshot from a UCA certificate. Herb mentioned a little bit about the certificate database, and Richard talked about how many devices have been certified and so forth. The database is currently being overhauled. Herb's doing a lot of work to make this process easier for users to get to. You can go to the UCA website, and you can get access to all certificates for all the devices the UCA certified. And on this certificate, we list every test that that particular device has been tested and passed, of course, if it has a certificate. And this is what you see here on the left side of the slide. We've got the mandatory test cases in the center column. Those should be there for all devices. On the right-hand side, we have conditional tests. So these are the services that the client can opt to provide. And this CBR 33 is the test that we're looking for here that will indicate that this client has been tested with the reservation concepts and should be completely compatible with new Amendment 1 servers. Beyond that change to the services, the Amendment 1 brings a lot of new data models, data objects, data attributes. Which are supporting new data, especially in the DER world. We have a lot of new information available. But older devices that didn't know about those information yesterday still won't need to know that information to get their job done. So when we put these devices in systems together, the reason the SCT has to know all of this, the SCT knows which devices understand which information and can create the data sets and communication paths between the devices, ensuring that both ends of the communication exchange only information that they understand. So older devices not being able to use the new data does not affect the older device at all, can still do everything it's always done. It doesn't do the new features, obviously, but it didn't need to do those before. As long as our SCT is able to configure these devices correctly, there should be no trouble intermixing these different levels. So wrapping up here with some of the basic ideas going forward and as we migrate these systems, the standard continues to evolve and improve. We need to keep going with that. We do our best to minimize impacts on the users who are adding features. We very carefully consider any cases that we need to cause a speed bump. This reservation, we spent many, many hours debating this and concluded that it was worth the minor amount of trouble here where we have implementations that were not the best before. But we have a mission to continue the standard and make those decisions going forward. We've got limited impacts here. We've got new features in Davis that cannot be used with older devices, but they don't impact the existing functionality of the systems at all. In case of the highlights, key points to keep in mind with Amendment 1. That's what we need to look at from a standard perspective. Thank you, Joel. I'd like to turn this over to Carlos de Castillo-Rodriguez from ELWIT. He is going to give a user perspective and then we'll have some final thoughts. And if you can stick around, we can answer some more questions. So, Carlos, I'm going to make you presenter. Thank you. Thank you. Good morning, everyone. Good afternoon here from Spain. This is Carlos Rodríguez de Castillo. I work in RedElectrica. Currently, I work in ELWIT. That is the innovation company of the group, of RedElectrica group. But I have been working in the substation engineering department of RedElectrica for 15 years. RedElectrica is the sole transmission agent and operator of the Spanish electricity system. So, I will try to give a user perspective from migrating from 2.0 to 2.1. So, let's start taking a quick look at the market. At first, we have utilities. Utility world is complex because each utility is independent from each other. There are some tries to have a common requirements and common solution for different utilities. It's not easy because each utility have different and specific requirements, different scopes of 60, 20, 50 implementations, process bus, station bus, different services used. And also, each utility has its own roadmap and procurement strategy. On the other hand, we have providers that they are trying to sell their products as much as possible. So, providers need to have flexible solutions and specific implementation with specific management in order to fulfill utility requirements because they have to handle different specifications. So, that makes more difficult the patch and improvement strategy and also affects the roadmap decisions. And in the middle, we have the system integrators that sometimes they are also providers and sometimes they are also the same utility. But in the end, the system integrators are looking for the best solution from a business perspective. So, these three big actors, I think, create the market and of course, we need the market to go to 2.1. If we take a look about possible patch pushers or stoppers for doing the migration to Edition 2.1, in the utility context, the procurement strategy is a potential stopper. There are some – if there are contracts that last several years, it's very common to freeze specification during this period of time. So, during this period of time, it's not possible to go to a different edition of the standard. But on the other hand, if the utility wants to catch the best technological improvement, then there could be a pusher for adopting new editions. But also, there could be some restriction if the utility applies a normalized solution for all of its projects. So, again, that depends on how many projects you have and different policies. If we're talking about the provider's context, providers are usually pushers for adopting new editions of the standard. It's just going forward to reach the technological leadership in the market. But in the end, they have to give the continuation of the service to clients. So, they have to adapt and accommodate the adoption of the new editions to the utility requirements. And finally, the system integrators. If they get cost savings or time reduction with the new edition, they can push to try to introduce the new edition as soon as possible in the market. But of course, they have to see a benefit from that. Oops. And now, some key points that I think the utility needs to – some questions that need to be answered in order to take a decision. So, how mature is edition 2.1? So, I think that with the certification test of 2.1, we can consider that this edition is mature. So, let's see when this is possible. Another question is, are there enough certified products in 2.1 in the market? Not today. We have to wait a little bit. But it will be something interesting to know how many certified products are in the market. Are they compatible with edition 2.0? We have seen in this presentation that there is an issue with the client. But in general, if you have your product certified with 2.1, it's going to be compatible with 2.0. Does the utility have solutions in edition 1.0? There is a question in the Q&A section that said that there is still a lot of edition 1 systems in the world. I really don't know. From the point of view of Reelectrica, we only have one. But it's something that you have to take into account if you have a lot of system with edition 1.0 and you have to upgrade them, then move to 2.1 in order not to create an intermediate step. And also, it's important to know if your products are 10-key projects or the utilities, the system integrator and the projects are, let's say, made by the utility. Because in the 10-key projects, usually you could be more flexible. You could give more flexibility to the system integrator. But if the utility is its own system integrator, then the policy maybe could be more restrictive. And of course, if you have a unique global solution for your project or you have case-by-case or project-by-project, if you have a unique global solution for all the projects, of course, you are going to take more time in order to move to 2.1. If it's case-by-case, you can adopt 2.1 in some pilot projects and move on going forward. And of course, the scope of the solution is if you are using process bus or station bus and all the services that you are using, it's something that could be also a driver. I will try to explain a little bit about an improvement in 9-2 later in my presentation. And also, how many new substations are planned? If you don't have many substations with 6,150 planned, then it's not the big issue to decide if you're going to 2.1 or remaining 2.0. And also, how many refurbishments? It's easier to adopt the new standard in the new edition of the standard in new substations. With the refurbishment, maybe you have to study a little bit more. And finally, if you have in your roadmap a centralized protection control solution, if you have this in your roadmap, think in 2.1 and moving forward, because it's going to be the best option to take the most updated edition of the standard. From a technical perspective point of view, I think it's good to see how many tissues have been included in the new edition of the standard. So I selected part 6, 7-2, 7-3, 7-4, and 8-1, and I took a look on all the issues that have been included in the standard and all the issues that have been in the tissue database. And I found that 335 tissues have been included in this edition 2.1. So that means 71% of all of them. And the ones that have not been included have been categorized as future improvements, not applicable, or they are still in red. So I think this is a big number. The 71% of the tissues created with edition 2 has been included in the amendment, provoking not a major change in the standard. Because all the tissues included in edition 2.1 are solved. So you don't need to explicitly name them in your specifications. And you don't have to review the provider's documentation to be sure they are included, because if the ID is conformance to 2.1, you know that all these tissues have been included. And just to finish this technical perspective, in 9-2, improves the sample synchronization information when using PTP. That I think is the present and of course the future for sample values. And there is a field in the APDU that means sample synchronization that in the edition 2 is true or false. So now you have three values and the old value 2.1 means local synchronization. This is important. That means that you have lost the synchronization with the grandmaster clause, but the local clock of the imaging unit of the device sending sample values is still valid. And you have a new value 2 to be sure that the device is synchronized with the grandmaster clause. So I think this is important. If you are planning to use 9-2, I think this is a good improvement. And if you think in centralized protection and control system, I think this is a big driver to say, okay, in the future, I think I have to move to 2.1. Taking into account the roadmap that has been presented today about the certification test. So what to do? It depends, of course. It depends on each utility, how many substations the utility has and so on. And moving forward to edition 2.1, I think is the best option. But of course for new substation, always the policy procurement strategy and the roadmap allows you to do it. And for existing substation, if you need tissues to be included, try to go to 2.1, taking into account that no need to update existing server IDs 2.0 don't need to be updated. But remember that maybe you have to update the client. That is not the big number on the equation, but you have to take into account and you have to know if you have to upgrade the clients that you have in your substation. For me, the question is when to do it. And now I will try to explain the point of view of Red Electrica that I think is also focusing that we require level A certificates for the IDs that we use. So what is the situation in Red Electrica? We don't have too many substation with 6.1.850. It's around 10. Up to date, it's around 10. And we have now a compromise of using 6.1.850 in at least 10% of the new substations in the next, from 2021 to 2025. And the solution that we now are, let's say, standardizing inside Red Electrica is using IDs near the field to control breakers and deep connectors. But we don't have sample values yet. We have a two-year roadmap for a solution that is ongoing. And also, we have an innovation project to study centralized position and control detectors that is also ongoing. And as I said, we require level A certification of IDs during our own certification process. So when we think in moving to amendment 2.1, we have to think that we have today two providers of HMI gateway. We are studying with them if the client is now compatible with 2.1 or we have to do an upgrade and how much is it, of course. But remember, we don't have too many substations. So we don't have too many clients. So it's not a big number. And protection IDs, we have certified protection IDs with Edition 2. And our idea is to study the upgrade to 2.1 as soon as the IDs that we are using today that are certified in our own certification process functionality talking, I would say. If these IDs have a level A certificate in 2.1, then we will study the upgrade of these IDs in order to use 2.1. And for the breakers and switches ID controllers that we have with our solution 61850, because we have two substations with this kind of solution, but two pilot projects, of course. So we are now planning to have a certification process for next year. So let's see if it is too soon or not to use 2.1 at that moment for this kind of ID. And that's all from my side. Thank you very much. I give back to Herb. Thank you. Herb, you're on mute still. Thank you, Carlos. I just want to give some final thoughts. And this, so let's get started. So one of the things that is happening is we now have a new site at UCA at UG called RedMine. And you'll see that it has several projects, but one of them is the User Feedback Task Force. And Carlos is the chairperson of that task force. So if you want to see any of the future issues that are being vetted by the standards organization that have been inputted by the users, that's the place to go. If you want to go and see how testing is being changed, you can see the issues with the 61850 conformance test. It's totally transparent. You don't have to have a login. I am actively working on migrating the SharePoint certificate database into RedMine. And since this reservation topic is important between Edition 2 and 2.1, we'll be adding a filterable field so that people can click, can filter on it and see which clients have already been tested to conformance for that optional conformance block that Joel was talking about. I wanted to announce that we have a 61850 interop to test our findings. It's been bifurcated into a virtual SPL testing week that's scheduled for November 8th through 12th. The face-to-face will be June or July in Europe somewhere, potentially Italy. And if you want to participate and you have particular test cases that you want addressed, please drop me an email and I'll get you enrolled so you can help us out. So some final thoughts. Users can continue with Edition 2, but planning needs to begin on how to migrate to Edition 2.1. As you've heard, migration to 2.1 requires thought, planning and potential research. It is possible to have MIX 2.0 and 2.1 applications and devices in a single system. And if you need to have some questions answered, UCIUG is here to support its members. And with that, I think we are done because I don't think we have any unanswered questions. No, we've got two. Carlos, there's a question to you about why does REEE require a Level A certificate specifically? Yes, yes, I was trying to, I was answering by writing, but yeah, of course. Hi, Andrew. This is a good question. And I have to say that it's a decision of REEE in order to have a certificate that has been done by an independent lab. So we think and we believe that it's important for us that the IEDs have been certified and tested by an independent lab that doesn't have anything to do with the provider. So that's the reason why we require Level A. But this has been seen since Edition 1. We have only one session with Edition 1, but we require Level A at that moment. I just want to comment that that's a Red Electrica policy. UCA, IUG, we view the accreditation of both types of labs to be equivalent. Yes, yes, I think this is a decision of the utility. It's not, I wouldn't say it's going to work better or worse because the two type of certificates are valid, but it's a decision of the utility. And then, as I tried to explain at the beginning, each utility has its own requirements and some specific things. Should we quickly explain the difference? Well, Level B is a lab that is associated with a vendor. Level A is an independent, third-party lab that can test any vendor's equipment or applications. Both of which are accredited by the UCA to the same standards. That is correct. Level B is an internal vendor lab. Yep. Yep. We have a question for you, Richard. Any conformance tests for Edition 2.1 already done at DNV? Yes, I see the question and I already responded that this is confidential information. I cannot share it. But what I will, what I can share, I believe, and I'm going to share it, is DNV is undergoing accreditation for 2.1. Yes, so we, yeah, that's, so we're working on it. We have a test tools. We, yeah, we need, we have devices. So it's all in process. But we, first, we have to go through the accreditation of the UCA. So that, yeah, so at the moment, the current, you expected something like August 2021. So that seems like a reasonable schedule. So at the moment, there are no certificates available for Amendment 1 in the certificate database. But, yeah, keep track on it. And in August or September, you will see, I hope you will see that there is a, the first certificate issued. Yeah, and it will not be on the SharePoint database, by the way. Oh, it's in the red line. Yep. Okay. We have a question about SCT to be certified, but not available yet. Can server be certified alone in the interim? ICT functionality is mandatory to be part of the server testing in 2.1, server certification in 2.1. I don't know if that answers your question or not. Okay. So, the second question. The SCT certification is separate. SCT is a separate certification. It's independent from any device. But ICT is connected to a device, server device. Right. And it is required that that function get tested along with the server. We're in discussions if that function. Sorry, he came back. There is slightly more coverage to get an ICT certificate. So, this is the server testing doesn't require you to get an ICT conformance certificate. I hope that helps. Okay. There's an overlap and it could be efficient to do both. But it's up to the utilities to require it. It's up to the utility to decide, okay, I'd like to have the devices certified. Yes, but, yeah, how about engineering tools? So, not many utilities have any requirements related to engineering tools. So, that's. We will see how it will work out. And for Amendment 1, there will be testing involved for ICT as part of the server testing. So, I hope that the interoperability of SCL tools will improve in the next UCA-IUG interoperability test. Go ahead. So, we have a question, Richard, about tool testing. ICD, IID, and SCD shall be validated only if the tool is certified. For Addition 2, as part of conformance. I believe that's a true statement. Yeah, so, we don't use the CID. We use the ICD, IID, and SCD file. So, that is the importing and exporting of those files as part of the server conformance. Okay, let's see. I see the last question is about the Redmine. I don't, Redmine is open, yeah, for everybody, yeah? Yep. You don't have to be a member. That is correct. How secure Goose, how secure MMS in Goose is going to work? Any PKI involved? Yes, PKI is involved. Okay. Maybe we should have a, Beth, do you have the questionnaire ready to go? Which questionnaire? The survey at the end? The survey at the end, yeah. Yep, they'll get that. Once you sign off, you'll get prompted to do a survey. So, we appreciate your feedback in answering that survey. It's only five questions. It won't take much time at all. And then, once you submit that within 24 hours, you will get a follow-up email with a link to the recording. And the presentations? The presentations will be sent out in a follow-up next week. Yep, so, the reason I brought Beth in is because we're looking for new topics for future UCA IUG webinars as part of that survey. So, if you have ideas, and if you want to talk about security, you know, put that into the survey. And I think I'm seeing a new topic, you test the ICT, this keeps changing. There's a question, at which point can we start SCT conformance testing for addition 2.1? Richard? Yes, at the moment, we started with a server, and the next step is to set up test procedures for client. And then, the third step is to test, set up test procedures to go back to the server. Client. And then, the third step is to test, set up test procedures to complete the test procedures for an ICT tool. Yes, and then the last activity for UCA is to set up test procedures for the SCT tool. I'm the editor, and I'm alone, not alone, but I get support. Yeah, my resources are limited, so I have to prioritize the editing work. So, SCT is a bit on a low profile. The last, it will come, but probably next year. I think we have cleared all of the questions. Shall we just give it a minute in case more comes up? We got a question from Edwin. Will the Q&A also be disseminated, Beth? Yes, I can include that in the follow-up, both the presentation and a copy of all the questions and answers here today. And with that, I think we can close the webinar. I'd like to thank everybody for their time and their questions. I hope you have found this useful, and I guess we can close it off, Beth. Yep, that's it. Thanks, everyone, for your time, and like I said, there'll be some follow-ups sent out so you can get a link to the recording, and next week you should see the presentation and the question and answers. Again, thanks, everyone. Take care. Thank you. Bye-bye. Bye. Bye-bye. Thank you.


Video ID: qfgdQvbBYAM
In this video, we'll be showing off three features of TestSuite Pro. The signal flow diagram, the logic analyzer, and the custom mode. We will be doing this by repeating the demonstration we gave at Distributech 2019, so if you missed us at the booth, would like to revisit some of the elements of our demo, or you're just interested in some of the new features of TestSuite Pro, stay tuned. During this demo, TestSuite Pro will be monitoring a substation simulated in DTM. This substation will simulate a few different events, including a fault on line 1 and a breaker failure. For more information on this simulation, check out the DTM Distributech 2019 demo in the video in the description below. This is our signal flow diagram. You can create this type of diagram by dragging logical nodes from the data miner to this display. When a logical node is dropped on the display, it will pull all other logical nodes it needs to communicate with via goose automatically. Reporting and internal connections can be added as well. Once you have this diagram set up, you can simply click record, and it will start monitoring these values. When a signal is sent, you will see it and any resulting signal travel around your diagram. Goose messages will be seen as yellow, reporting will be blue, and internal logical node functionality will be seen as gray. When a logical node sends or receives a signal, it will flash. After recording the entire event, you can stop recording. Afterward, you can review this event in multiple ways. You can do this by using playback, which will play back the signals of the event. Each signal is also logged on the timeline, so you can see these specific signals by clicking their respective markers on the timeline. If you're only interested in a specific logical node, you can click that logical node in the diagram and only see the markers involving that specific logical node. You can also click the next or previous event buttons to see the signals in order at your own pace. Another way to view the fault on line 1 event is with the logic analyzer. After dragging the values that you're interested in to the logical display, you can click record and monitor the event. This gives you a straightforward view of how signals triggered responses in your system. If timing is something you're interested in, you can use the cursors to find exact response times. The last feature I'll be showing off in this video is the custom mode. We used the custom mode at Distributech 2019 to create a single line diagram. You can drag and drop values you're interested in to this display to form a single line diagram. You can also use the send signals to feature to send signals from one display to another. This greatly speeds up the process when creating multiple displays for viewing similar events. I will trigger the breaker failure event in the simulated substation from DTM. When the event is finished, you'll see all the breakers open to prevent energy loss in the substation. You'll also see any other signals being sent in the data cache below the single line diagram. For more information on DTM's Distributech 2019 demo, check out the video in the description below. For more information on DTM, TestSuite Pro, or any of our protocol stacks or testing tools, feel free to contact us following any of the contact information in the description below. Thanks for watching.


Video ID: kTJJp3Dq2EM
In this video, we're going to look at how to use Anvil to simulate an IEC 61850 server. Actually, we're going to simulate three different IEC 61850 servers, all using the simulation file that we created in the previous video by capturing live states from a real server using Hammer. In our booth demo, we actually used an embedded PC to run Hammer. Now, Anvil is just a Windows application, and typically you would run it on a PC. However, there's a couple reasons why you might want to use it on an embedded PC. One reason is scalability. Although Anvil can run multiple instances of a server, it was really designed for testing and implementation, and due to the GUI overhead, it eventually reaches a limit of how many devices it can simulate. So with these embedded devices, which are available relatively inexpensively and don't take up much space, you could build a rack of these in a lab, for example, and run multiple instances on each one and very quickly simulate a pretty good-sized network. Another reason you might want to use an embedded PC is for the ruggedness. So for example, a field tech can toss one of these in his toolbox, and when he gets to the substation, he can just expect it to work, because it's a pretty rugged device, unlike a laptop, which needs to have a little bit more care taken. This particular device is available from Advantech. There's a part number available, preloaded with Windows Embedded, ready to run our products, including Anvil and the SCADA data gateway, both of which we ran on an embedded device in our booth demo at Distributech. Since Anvil's running on an embedded PC, we're connecting to it via remote desktop for this demo. Now when we connect to Anvil, you'll see that we already have three instances of the server running, and there's a couple of ways that we can get these servers to simulate data. One way is to manually set data. So for example, we can scroll down to a voltage, click on the voltage, and enter a new value. We can do the same thing if we're looking at the flat view. Again, find the particular voltage you want to change in this view, and enter a new value. We can also use the simulation pane to simulate values at a specified frequency. So if we turn on the simulation pane, see we have an option to change data. Here we're doing it every second. When we click start, Anvil will automatically start changing data values, and it will change values for all of the points, moving them between the minimum and maximum values specified for the point. In this demo, we're going to demonstrate another way using flowcharts, which gives you complete control over the values that are going to be simulated. To create a new flowchart, choose view, flowchart IDE, and then new to start with a blank flowchart. And then we can just drag and drop actions onto the flowchart. So let's apply a column from a simulation file. Notice it initially comes up in this color to indicate there is an error. The error is that it hasn't been initialized. So we will indicate which server we want to run on. We'll load a simulation file, and we'll choose a column. We'll start with the first column, which is the normal state. Then we can apply another column if we want. Drag it on here. Once again, we want to run this on breaker one. Choose the simulation file, and this time we'll use the next column where QA1 is open. And we can continue this process. If we want to run these automatically, perhaps we want to insert a delay. So we can drop the delay on here. And let's put about a five-second delay in here. And so we can continue this process until we've built whatever kind of flowchart we want. Here's the flowchart that we used in our booth in the demo. It basically walks through, for Bay Controller 1, each of these states with a five-second delay between each one, and has a conditional command at the bottom to loop back and repeat the process. To run this, all we have to do is right-click the Start node and say Execute. And it will now step through this flowchart, executing each step. So now if we go back to Anvil and look in the flat view, we'll see these values periodically changing. So as you can see, it's pretty simple to build a flowchart and to use that flowchart to simulate a server. So we built a simple flowchart that loaded each of the columns in our simulation file, the simulation file, a simple CSV file that we had captured using Hammer. And then we walked through that file, applying each of the states to simulate the server. In the next video, we'll look at using the 61850 client in the SCADA data gateway to read the values that are being served out of Anvil, and to map them to DNP and send them to a DNP master using DNP Secure Authentication version 5.


Video ID: e2RmjuQtAUw
This video shows how to transfer the license for Triangle Microworks products from one computer to another. With our previous licensing scheme, you needed to contact the Triangle Microworks support team after killing your existing license in order to request a license for the new machine. With our new SafeNet-based licensing system, you no longer need to contact us when you want to transfer the license. As we'll show, to transfer the license, you first need to install the product on your new computer. Make sure the computer you're transferring from and the computer you're transferring to are using the same version of the product. Next, on the new computer, you'll need to run the RUSTMW application and create a recipient information file. This file will be needed on the old computer, so you'll need to copy it to a thumb drive or a network drive or have some other way to make the file accessible by both machines. Next, you'll need to run the RUSTMW utility on the old computer and add information to the file that was generated on the new computer. Finally, you'll use the newly modified file on the new computer to install the new license. Let's walk through those steps. As an example, in this video, I'll be transferring the Distributed Test Manager, or DTM, license to a new computer. Here I have my new computer. Note that DTM is already installed. I can run DTM using the 21-day trial license that comes with the installation. I can then go to Help, About Distributed Test Manager, to get the version number. In this case, it's version 1.1.0.3. Now I want to exit DTM and run the RUSTMW utility. To do that, I need to open Windows Explorer and browse to the bin directory of the DTM installation directory. By default, that's located in C colon, Program Files x86, Triangle Microworks, Distributed Test Manager, bin. In this directory, I browse for RUS underscore TMW dot exe, and double-click it to run it. Then click on the Transfer License tab. I'm going to save my file on the desktop and then copy it to a network drive. So I click the dot dot dot next to the file, I'm going to browse to Desktop, and name the file, I'll just call it License Transfer File. Then I need to click Collect and Save Information. Read the dialog and select OK. Now if I look on my desktop, we can see my license transfer file, and then I want to copy it to a network drive. In this case, I'm going to put it on my Microsoft One drive. Now I can exit the RUSTMW dot exe application. Next I go to my old PC, where DTM is currently licensed. Just to double-check, I'll start DTM, go to Help, About Distributed Test Manager, and confirm that this version is also 1.1.0.3. Now I can exit DTM and launch RUSTMW. Once again, we'll do that by starting Windows Explorer and browsing to the bin directory of the installation directory, which by default is C colon, Program Files x86, Triangle Microworks, Distributed Test Manager, BEP. Then we'll scroll down and find the RUSTMW application and double-click it to run it. Once again, we'll click on the Transfer License tab. This time, on the old PC, we're on Step 2 to generate the License Transfer File. First we'll select the license to transfer. I'm going to choose my DTM license. Then we need to find the recipient information file that we created on the new PC. To do that, I'll click the three dots, remember I put it on OneDrive, and here it is. And I need to generate a new file, which I'm going to put on my desktop. And again, I'll call it License Transfer. This time it creates a .h2h, which stands for Host to Host File. So now we've selected the license to transfer, the information file to read, the license transfer file to store, and so we click Generate License Transfer File, read the dialog box, and select Yes. Then read this dialog, and select OK. Now if I look on my desktop, I have this new .h2h file. Note that at this point, the license on the current PC, or the old PC, has been invalidated. And the valid license information file is in this .h2h file. Do not lose this file. This file contains the license, and if you lose it, you will also lose your license. So if you put it on a thumb drive, protect that thumb drive carefully. So what I'm going to do is I'm going to copy this file, and then I'm going to move it to the network drive that I can access from both PCs. The reason I'm copying the file instead of moving it is just in case some kind of corruption happens on the network drive, I still have the original file back here on my desktop. So at this point, we've removed the license from the old PC. The next step will be to move to the new PC and install that license there. The third and final step of the license transfer is to install the new license on the new PC. So here I'm back on my new PC, and I'm going to launch the RUSTMW.exe application. So I launch Windows Explorer, browse to the location of the file, which by default is C colon, Program Files, Triangle Microworks, Distributed Test Manager, BIM, and I'll scroll to that application and double-click it to run. This time, in step three, I'm going to click the tab that says Apply License File. I'll click the three dots and go to that license file we created, which I put on one drive, the H2H file, so I will load that, and click Apply Update. And here we can see the update has been applied, and it was written successfully. So now I'll launch DTM. And if I go to Help, Manage License, you can see that my license has been transferred. I now have a perpetual license on this machine. So once again, to summarize the process of transferring the license, you begin by creating the recipient file on the destination PC, that is the PC you're moving the license to. Then you go to the source PC, that is the PC where the license currently resides, and you create a file, the H2H file, which combines the license with the information in the recipient file. Then you go back to the destination PC, or the new PC, you apply the license file, and then your product will be licensed on the new PC. If you have any questions on this process, please go to www.trianglemicroworks.com, log into your portal, and submit a support request.


Video ID: w1DiVR7SJA4
Welcome to the Triangle Microworks IEC 60870-5 communication protocol training videos. This is the third video in the series. In the first video, we gave an overview of IEC 60870-5 and talked about stack layers and a little bit about the application layer. In the second video, we dug into details about the ASTUs. And in this third video, we're going to talk more about the application layer, including polling and controls. Okay, so let's dig in and look in a little more detail at the application layer. Within the application layer, the standard identifies something known as application functions. These are basically the requests which can be sent either from the master or the outstation. We'll quickly go through some of these. We're not going to cover all of these in detail, but there's an initialization sequence, which the master controls. There's a data acquisition by polling, which is a little misleading because this is just for 101 unbalanced mode. The other common ways for the outstation to send data are through cyclic data transmission, which is for static data. And then there's acquisition of events, which would be for sending event data or data changes. And then, of course, there's the general interrogation that we briefly talked about before. That's the equivalent of the class 0 poll in DNP3. The master can request a clock synchronization. It can send commands to the outstation. There's a way for sending integrated totals or basically counter measurements. There's a parameter loading, which is a way for the master to send certain parameters to the outstation. There are testing requests, file transfer requests, and requests to acquire the transmission delay. Some of these are control requests from the master. Some are from the outstation. And I'm just going to go over a few of these that are some of the more important ones to know. So, the first one is an interrogation. This is basically how the outstation can send current values to the master. The master can request a general interrogation, which sends back all of the points, or you can define subsets of groups of points, which are set up on the outstation, and the master just requests that particular group of points. Those groups of points can be totally customized. And when the master sends an interrogation, it can include the QOI, or the Qualifier of Interrogation. And the QOI determines whether this is a general interrogation, or an interrogation for a defined group of points. A GI is performed after the initialization sequence, so that the master can synchronize its database with the data in the outstation. So this is basically like a class 0 poll in DNP3. The standard also specifies what will be included in this interrogation, and basically it includes all of the relevant cause of transmission. So it includes periodic, cyclic, background scan, and spontaneous. So pretty much all of the data. So let's take a look at a typical interrogation sequence. Here we can see the controlling station sending a GI, or ASDU-100, with an ACT, or a COT, or cause of transmission, of 6. The control station responds with an ACT-CON, which is a cause of transmission of 7. It then sends the monitored data. As you can see, it will send multiple monitored ASDUs in response to the GI, each of which has a COT or cause of transmission of 20. And then it wraps up the general interrogation sequence by sending an ACT-TERM, or a COT of 10. There's also the capability to request a read of a particular point or a particular ASDU. This is supported through the read command. This ASDU type is type 102. It's important to point out that this is not one of the application functions per se. It's one of the commands which is supported. So only a single object can be read with each read request. So really, in order to read a lot of different objects, it's better to do an interrogation, either as a subset or as a complete general interrogation. One other basic function is the ability for the outstation to have either cyclic or periodic or background scan. This is a way for the outstation to send sample data to the master on a periodic basis. This is really for static data which needs to be sent over. So even if the data has not changed, it's still sent with a new sample and could include a time tag with that. So the outstation tells the master the cause of transmission. So it was a one or a two. One is for cyclic. Two is for background. They are very similar, but cyclic is typically used with a faster sampling time. The specification doesn't really say why you would do a cyclic versus a background scan. But generally, the background scan is just a slower period than the cyclic. The important thing here is that the master is not sending a pull request at the application layer. The outstation is directly sending this data to the master on a periodic basis. So in some ways, this is kind of equivalent to an unsolicited response in DNP3. However, it's done on a periodic basis. In DNP3, the unsolicited response always contains event data or data changes, whereas this cyclic and background scan, it's still static data. It's just being sent on a regular basis without the need for a pull from the master. Another function is called the acquisition of events. This is where the outstation can identify events or data changes and sends them to the master with the cause of transmission of spontaneous. So the specification defines which ASDUs can be spontaneous, and in a similar way, it defines which ASDUs can be cyclic. Once again, the master does not send a request for this at the application layer. The outstation just sends it as the event occurs. This would be very equivalent to unsolicited messages in DNP3, which also contain event data. Note that although spontaneous transmissions and DNP3 unsolicited responses are similar, there are a few differences. In particular, with DNP3, the master can suppress unsolicited responses. So it can send a command telling the outstation to stop sending unsolicited responses. You cannot do that in 60870-5. There is no disable for a spontaneous message. The outstation can choose to send them or not, but the master cannot tell the outstation to stop sending them. Another function talked about in the specification is polling. Polling is a little bit of a misnomer. This is what we talked about before. There's no real application layer polling like there is in DNP3. The polling they're referring to here is to support multi-drop links for IEC 60870-101. It is the link layer which is doing this polling. It's only for the unbalanced mode where the master is requesting user data at the link layer and basically telling the outstation to go ahead and send me anything you have. So this allows the 101 unbalanced mode outstation to send things as it has them. It defines this as class 1 and class 2 data. Basically the spontaneous or event data is class 1 and class 2 data is the cyclic or background data, which is static. After any requests on the link layer, the outstation can say that it has class 1 data and then the master can give it permission to send that. Note that the class 1 and class 2 data at the link layer in IEC 60870-5 is not at all related to the class 1 and class 2 data referred to in the DNP3 specification. In the 60870-5, if you have an unbalanced link layer, there's two classes of responses or they're priorities of responses. There's the high priority and a lower priority. And the responses are assigned to those two different classes. In the Triangle Microworks library, we don't even let you choose what class they're assigned to. Theoretically, you can choose what class they're assigned to, but it has a little real value because no matter what the master asks for, you can always send the high priority response. You really don't need to think too much about this. It's implemented in the library. We do it according to the specification and things are assigned to one of the two classes. But it's important to know because somebody's going to start talking about class 1 and class 2 and confusing it with DNP3 classes, which is completely unrelated, and you're going to want to know the difference. The specification also describes controls. This is basically where the master is making a request for one of these application functions. So if it's doing interrogation or read or sending a command, then it uses the ASTU type to identify what type of control it is. It also uses a few of the cause of transmissions. For example, when the master sends a request, it sends an activation, COT6. When the outstation sends back the confirm, it's the ATCON, which is COT7. The outstation then can send an ACTERM or a TIN to terminate that control. Once again, the specification defines which of these COTs can be used for different ASTUs. Also note that the specification allows multiple command requests of the same type ID to be sent simultaneously before a response is received. This is in contrast to DNP3 in which multiple simultaneous requests are not supported. The control address specifies the particular ASTU that's being controlled. Note that the point being controlled and the monitored point that is reading back that the control happened will have different addresses. This is handled in the Triangle Microworks test harness by defining an offset between the control point and the monitored point. What we use by default in the sample database is we have an offset from the control object from the monitored object by a fixed offset. It's convenient in our sample database that we use, but it's not required by the specification. The only requirement in the specification is the two addresses have to be unique. So for example, you could have control point 7 and monitor point 102. They don't have to be an even offset, they just have to be different. If you're using the Triangle Microworks library and the default database, there is a fixed offset and the points will be offset by that much. The control point and the monitor point will be offset by that fixed offset. Again, you don't have to do that in a real database. You can choose whatever makes sense. So the final application function we'll talk about is what's called a command transmission. This is basically when the master wants to control an output on the outstation. So this could be, for example, a single point output or a double point or even an analog set point. IEC 60870 supports what they call select and execute, which is similar to the select before operate in DNP3. Basically it's the same thing. The master sends a select first, the outstation confirms that the control point has been selected, then the master sends the execute, and the outstation confirms that. Then the status of the output can be sent. Sending back the status of the output can be done unrequested back to the master. It uses a different cause of transmission than a spontaneous, but for all intents and purposes it looks very much like a spontaneous message. It is an event-based response, in other words. It's just an event that says that the point you just sent me a control for has actually changed like you asked for it to. If we look at the qualifier of command, you see short pulse, long pulse, and persistent. The persistent is similar to the latched in DNP3. So if I turn it on, it goes on and it stays on. Short pulse and long pulse will be a pulse output. If you tell me to pulse it, I turn it on for a preset period of time, then turn it off. In DNP3, that pulse time can be sent as part of the message. In IEC 60870-5, however, the pulse lengths are predetermined and are configured as a short pulse and a long pulse, and then the master just sends which one it wants. Does it want a short pulse or a long pulse? So again with DNP3, you could say, oh, send me a pulse of 1,000 milliseconds. In IEC 60870-5, you could only do that if 1,000 milliseconds was configured for either the short pulse or the long pulse time. So let's say, for example, it was configured as the long pulse time, then the master can send the request with the long pulse and the output would be pulsed for that 1,000 milliseconds. OK, so let's go back to this example of a command ASDU. This is the single command, which is type 45. The cause of transmission, when it first sends the request, would be an activation, and then it would get an activation confirmed back from the outstation. Then in this SCO, you would have the bit for select or execute. In the full sequence, this would first be sent with the select and the single man with a cause of transmission of activation. Then the actcon comes back with the same information. The execute would be sent with the act and then the actcon and then the actterm after that control is complete. And then optionally, the event can be sent to show the output status. So that's pretty much everything we wanted to cover here for the application layer. Next up, we'll go into the link layer. Next section gets into some of the more serious details, which can be important, but they may not be important in your application because if you're using the Triangle Microworks source code library, it will handle most of that for you.


Video ID: xX3kU7BymOc
In this video, we'll be going over ways to display data in DTM. The three ways we'll be looking at in this video are the model of one specific device or points list of one specific device, a more high-level view of data points, and an in-site display which can be similar to an HMI. So let's start with 61850, so you want to navigate in the workspace to the device you'd like to see. I'll currently be using this channel. You right-click, show, model. Now what you'll have here is a fully collapsed model of this 61850 device. You can open it up by clicking the arrows, and this will help you navigate to where you want to go. If there are specific areas that you like to look at and you'll be using frequently, you can create bookmarks. We've already preset a few for you. This will jump you directly to where you want to be. If there is a different area that we do not have by default, you can create your own by navigating to the place you'd like, right-click, add bookmark. Now in the future, if you go to bookmarks, you can scroll down to user-defined and click your specific bookmark, and it will take you directly to it. Very similar to 61850, you can go to another device, like a 101 device, go to its channel, right-click, show points list. In this area, you can see the full points that are laid out inside of this device. You can scroll through and look for the points that you're looking for. You can filter by clicking the little filter symbol here to filter on a very specific type of data you're looking for, or you can grab any of these headers here and group by that specific type. This will put them all into groups and you can open them up by clicking the arrow. The second way I wanted to look at data today would be on a higher level. This would allow you to group multiple devices into one display. For this video, I'm going to just go ahead and do it from the top level node, although it will warn you that if you have too many points or you have too many devices, this could take a while to display it all. So in a similar vein, you do right-click, show, data points. This confirmation window will pop up here and basically say, hey, if you have hundreds of devices, this could take a while, are you sure you want to continue? So now we're going to have something very similar to the points list we just looked at, but we're going to have each of the devices in our workspace. To show this, I'm going to go ahead and group it by device. Now that we've grouped it, it's a little bit easier to see. You can then go inside any of the points, the devices that you like, and see their points and continue to group from there or filter. The last thing I was going to show you on how to show points in this is to create something more like an insight display or something like an HMI. To do this, you go to the workspace where you'd like to place it, right-click, insight, add display. You'll need to name the display. I will name this one video. After that happens, a completely blank canvas will pop up. For reasons you'll see in the future, I'm actually just going to put this right alongside what we were just looking at. So what you can do from here is you can actually drag and drop points from this point list I was talking about onto this display, and it will show the values. So if I come in here and change this to true, you'll see it change over here. That way, you don't have to monitor points all throughout different devices and keep an eye on them. You can just drag them straight over. Alternatively, if you're looking for something more along the lines of an indicator or an analog view, you can come over here and do a drop-down on our user palette. In this situation, I'm going to get the indicators, and I will drag over an LED. This will make it so if I drag the value directly onto this LED, it'll light up because it's true. If I set that value to false, it then will go false. You do not need to have this anymore in the workspace if you do not want to.


Video ID: WRjnU_AnhqY
This section provides information for the expert level, answering the questions, why does it work that way, and how do you implement it? This section is primarily aimed at developers. This section includes advanced topics, including symmetrical versus asymmetrical cryptography, the use of certificates, sequence numbers, algorithm choices, using with TLS, using with data concentrators, redundancy, and state machines. Let's begin by looking at symmetrical encryption. Symmetrical encryption uses the same key for both encryption and decryption. This requires that the key be pre-sheared, that is, the key must be exchanged using out-of-band communications. Examples of out-of-band communications include using the telephone, a floppy disk or USB drive, the sneaker net, or some other protocol. In the example shown here, an asymmetric key provides encryption. That is, using one's public key encrypts data in a way that only the user of the private key may recover the message. In parallel, the asymmetric key provides authentication. If the data is signed with the private key, recovering it successfully with the public key means it really comes from the expected source. Digital signatures can be provided using asymmetric encryption for authentication, using a one-way hash that is similar to a CRC or checksum. In this example, Alice hashes the message and signs it with her private key. She then sends the message and the signature to Bob. Bob uses Alice's public key to decrypt the hash and also hashes the message. If the signatures match, it's the same message that Alice signed. A message authentication code, or MAC, is a number appended to a message to verify its integrity. The MAC may be, but is not necessarily, a hash. A good hash function provides an output that is much smaller than the size of the original message and produces an output that is very sensitive to changes in the message. Even a single bit change in the message should produce a new result. With a good hash function, it's nearly impossible to reconstruct the original message from the output and it's difficult to find two messages that produce the same hash. A good hash function also requires very little processing power, especially compared to encryption. When you include a secret key in the hash, it becomes a MAC. As we discussed earlier, a digital signature verifies that the document was written by the owner of the public key. But how do you know that the public key really belongs to who you think it does? For that capability, we need a third party, the Certificate Authority, or CA. The CA must be trusted by both the sender and the receiver. A certificate includes the public key, the issuer's digital signature, and the distinguished name of both the public key's owner and the certificate issuer. The certificate ties the key to a person or organization and often has a limited lifetime. An interesting question, then, is how do you know that the CA's public key is correct? In theory, you could have layers of certificate authorities, as described in ITU-C X.509. The CA's public keys change much less frequency than individual keys, and note that a utility can act as its own CA. In practice, it all comes down to trust. The Certificate Authority is much like a bank. Its name and reputation is their product. DNP3 secure authentication prevents replay attacks using both random challenge data and incrementing sequence numbers. Two sequence numbers are used, a challenge sequence number, or CSQ, and a key change sequence number, or KSQ. The DNP3 application layer sequence number is not sufficient. It's too small and wraps around too quickly. Additionally, it may not increment at the right time, and the rules over its use are somewhat vague in order to promote backward compatibility. The challenge sequence number, or CSQ, identifies when new challenge data are being used for a MAC. It can be incremented based on actions from either the master or the outstation. On startup, each device sets the CSQ to zero. Each device then increments the CSQ each time it transmits a challenge, and sets the CSQ of the reply to that of the challenge. In addition, each device increments the CSQ each time it sends a reply aggressive mode request between receiving challenges, and increments the CSQ each time it receives a valid aggressive mode request. The CSQ wraps after 32 bits, and the same CSQ is used for all users, but there is a separate CSQ for each direction. Challenge data is another mechanism for preventing replay. Challenge data is used any time a device verifies the validity of a MAC, including when challenging a critical function, sending an aggressive mode request, changing the session key, or changing the update key. Challenge data avoids the need to calculate the same MAC for two similar operations, and consists of random data created according to a standard algorithm, and is created by the device that will receive the MAC. Separate challenge data are used for responses and unsolicited responses. This drawing provides an example of challenge sequence number usage. The first CSQ is set to one. It was reset to zero at startup, and incremented for the challenge. The CSQ of the reply equals that of the challenge, one in this example. The CSQ is incremented twice for the aggressive mode request, once for the select, and once for the operate. Note that the DNP3 application layer sequence number, which is not shown in this example, remains the same during the challenge reply sequence. Incrementing the challenge sequence number prevents replay, even though the challenge data used in the MAC remains the same for the aggressive mode requests. Note that the challenge data for aggressive mode comes from the receiver. Here's another example of the challenge sequence number. The unsolicited response includes new challenge data, B, for the MAC on the confirm. Note the use of separate challenge data, C, for the MAC on the direct operate. The CSQ increments with each new challenge. The aggressive mode request is acting like a reply to the challenge, so the CSQ does not increment. The key change sequence number, or KSQ, is much more straightforward. It is always changed by the outstation. It's set to zero on startup unless an AES-GMAC is used. If an AES-GMAC is used, the KSQ must be maintained upon restart. The outstation increments the key change sequence number when it receives a session key status request, and when it receives an update key change. The master uses the KSQ in the MAC or digital signature. This page shows the cryptographic standards used by the DNP3 secure authentication mechanism. The specification follows the proven security philosophy that all security algorithms should be well-reviewed open standards, and that only the keys should be kept secret. The DNP3 specification describes only how these proven algorithms are applied to the DNP3 protocol. This page shows the NIST designations for these standards, and almost all of them also have corresponding ISO and IETF designations. Viewers familiar with security issues may note that a valid cryptographic attack has been found against SHA-1. It is included because the attack can be mitigated by changing session keys often, and SHA-1 implementations are much easier to find and require less processing power than SHA-256. This slide provides an overview of various algorithms used in DNP3 secure authentication. For MACs, HMAC is well-proven. A GMAC is trickier to implement, but provides better performance. For hashing, SHA-256 is the default. SHA-1 is suspect. If implemented, there must be an option to disable it. In the current specification, DSA is the only choice permitted for signing. For key transport, AES keywrap is the only option permitted for symmetric, and RSAES-OAEP is the only choice permitted for asymmetric. Elliptical curve asymmetric algorithms show promise, but they have intellectual property concerns. This slide provides an overview of symmetric key change methods. Note that it must be possible to disable SHA-1 algorithms. This table provides an overview of asymmetric key change methods. If you use DNP3 secure authentication with transport layer security, or TLS, there are some rules that must be followed. You must adhere to IEC 62351-3 for use of TLS in power systems. This means you must use TLS 1.0 or higher. You must use TLS RSA with AES-128 SHA, the mandatory suite for TLS version 1.2, at a minimum. You must renegotiate the TLS keys in approximately the same time as the session keys. You must use a MAC, although TLS states this is optional. You must support up to four certificate authorities. You must support TLS certificates of up to 8192 bytes and keys of 1024 bits or bigger. You must support the ability to accept either any certificate from a CA or specific individual certificates. You must check for revocation of certificates every 12 hours by default, and must terminate connections if they're revoked, but not if they're expired. When using DNP3 secure authentication with data concentrators, each upstream user must have a downstream user number, or USR. Non-DNP3 protocols have a single USR, and local applications have a USR. USRs are unique within an association, and not necessarily unique across associations. USRs are not necessarily the same upstream and downstream, and not all users may use all outstations. Concentrators must map between USRs upstream and downstream, and a concentrator may make requests, for example, pulls, on behalf of multiple users using the one default USR. There are three parts to any protocol. The services provided, for example, authentication, authorization, and detection, the message definitions, and the procedures or rules for exchanging the messages. The DNP3 secure authentication specification uses state machines to describe the protocol procedures. This provides a non-platform specific way to describe software behavior, and requires very little logic descriptions or if statements. State machines can be defined using a diagram, a table, or both. The DNP3 secure authentication specification uses both, with the disclaimer that the table is definitive. The state machine treats the software like a black box process. Events come in, for example, messages, timers, or something else, and messages go out. They may be secure authentication messages, or normal DNP3 messages. And actions take place, for example, starting timers that cause events later. The process has a state. Depending on the state, different events cause different actions. One of the actions may be to change to another state. This state machine describes a simplified cell phone. States are each waiting for one particular event. For example, the talk button is pressed, or the call fails, or the call is connected, or the call hangs up. All other events cause error handling or queuing. The DNP3 secure authentication state machines are much more complex than the simple example just shown. There are actually four state machines. The challenger, the responder, the master, and the outstation. Any device can act as up to three of these at once, and the machines are assumed to run simultaneously. Note that the master and outstation state machines include additional states and events if they support remote update key changes. In practice, the master and outstation are for key changes, and the challenger and responder are for when the keys have been established. This slide shows a sample state machine table. Events are listed in the left-hand column, with the various states listed across the top. Actions and state changes are described inside the cells of the table. This slide shows state machine relationships. The master and outstation machines establish the keys, and once the keys have been established, the challenger and responder run, authenticating messages in either direction. This concludes this tutorial. For more information, we recommend reading Application Note 2013-002, DNP3 Secure Authentication Tutorial. This application note is available to DNP3 users group members via the DNP.org website. The application note covers the highlights of this course, plus key design decisions, implementation issues, and responsibilities when deploying DNP3 secure authentication. For more information about DNP3, DNP3 Secure Authentication, or Triangle Microworks, please visit the Triangle Microworks website at www.trianglemicroworks.com.


Video ID: BoqNL8OLz-E
This concludes the presentation. We would strongly encourage you to contact Triangle Microworks here, support at trianglemicroworks.com if you have further questions, things we haven't answered in this presentation. It's a huge range of topics here that we've covered and it always generates a lot of questions typically. We also would like to point you at some other sources of information. The UCA International Users Group is really the users group for 61850. They've been active in all the development of these protocols. A lot of utilities have been involved. There's a lot of discussion groups there that discuss implementation topics and different projects. They are also active in the process of generating errata, so as we find problems in the field that gets fed back in through the UCA users group. They also have an active conformance testing program. The UCA International Users Group has been working on developing the specifications and the process for making conformance testing be high quality. So there's a quality assurance program for conformance testing that ties this all together with you with the vendors and utilities. So there's a lot of information at that site. We'd encourage you to go there. The IEC website includes a lot of discussion of 61850. It's a good source for information, so we strongly encourage you to go to those websites to get further information as well. Thank you very much. I hope this has been helpful.


Video ID: BzVdZYtp55s
deserves our love and care here in butt 느낌내고 있어 anyone who waves at anyone, how is this possible? are you too comfortable in your own house? aren't you afraid to get to people's door? anyone invited to any party? are you afraid of calling yourself a Masterchef, or are you afraid of wanting to be a part of it? let us see depressed in jealousy even you can complain were you depressed because you couldn't try.. you set that one as the agenda just like this you tried and there was no reply flattered but you still want our help thank you cool best


Video ID: x4lj5IfW9a0
This is the fourth video for the fundamentals of DTM for version 1.3. In this video we'll be going over data simulation via auto data changes, CSV file playback, and scripting. We'll also briefly touch on how creating a command for a master or client is similar to how you'll do auto data changes. I've gone ahead and loaded my three of each workspace which I've showed in a previous video. To get started I'm going to go ahead and display the data points of one of my 101 devices. This will give us a reference for later. So you right-click, show, data points. That will add a pane to the middle of the DTM administrator tool and allow you to see what data points are contained in that device. To add a auto data change node to that specific device you will right-click, simulate, add auto data changes. This will bring up a configuration window. Here you can change the name of the auto data change node, add a description, select the devices that it applies to, look at the period in which the data will be changed, and change the percentage of the data being changed per interval. By default we have filled out some points. Now if you'd like you can customize this yourself. To dump it all you could click reset or you can drag individual points here to this trash can. If you decide that you only want certain data groups you can come in here and select the data groups that you'd like. Or alternatively you can drag points from the points list that you'd like back into the section here and those will only be the points that this is changing. Afterward you can click OK and you'll see a little hammer appear underneath the device where you put the auto data change node. If you'd like to apply auto data change to multiple devices you can go up to a node that they're all underneath. For this example I have three S101 devices underneath the 101 slaves folder. If I right-click on the folder, simulate, add auto data changes. It'll ask me if we want to do a default data points for all three devices. I'll say yes. Again you could drag in the points as you like, insert different groups, and set it however you like. Now when you configure this one it will apply to all the devices. This is where the select device comes in. If you decide you only want to do two or three of the devices you can keep them all checked or check them as you'd like. This is very helpful when you have hundreds of devices and you decide that you only want to do say 99 out of the 100 or 75 out of 100. When you click OK now it'll apply the hammer to the folder level but also if you drill down you'll see a new auto data change node at the bottom of each device this applies to. You can tell it is a reference because it has a little arrow. One interesting feature of auto data changes is you can apply an auto data change node not just across similar protocols, say the 101s we just did, but also across multi protocols. So I'm just gonna go to my servers folder which has 101, 104, 61850, DNP, Modbus, and TASI. If I right-click, simulate, add auto data changes, it'll ask if you want to apply them to all 18 devices. Say yes. The main configuration is the same across all protocols but if you need protocol specifics we have different tabs allowing you to access the different protocols. Again hitting OK a hammer will be applied to the top level node and then if you drill down into the individual devices you'll see the reference beneath it. To run an auto data change node you can just right-click and say start auto data changes. When I do this you will have seen that these values are changing every five seconds because that's the duration that I have set. I'm going to go ahead and turn that off. Now that we finished with auto data change we're going to talk about the second form of data change in DTM and that's CSV playback. CSV playback allows you to configure specific states on devices that you'd like to trigger within a certain time or in a certain order. This is much more flexible than if you did auto data changes which generally just counts up and toggles inputs on and off. This can be done by going to the device that you would like to put a CSV file on, right-click, simulate, add CSV file, then you'll need to go and locate the CSV file you need. Since I'm on a 104 device I'll choose 104 and then click OK. This will bring up the configuration window. There's two things you can do in the configuration window. The first is you can set how frequently you'd like to play back the states, so would you like one second in between each, two seconds, etc. The second thing you can address is would you like the states to start over after it reaches its final state, meaning if you have four states when it gets to the fourth state which you'd like the CSV playback to stop, would you like it return to state one and continue on. That's done via this checkbox. Go ahead and click OK. Now if I scroll down you can see that we have added a CSV file down here. If you right-click on that you can do a next state which will allow you to choose which state you'd like to go into. You can right-click, say apply next state which will just move it on to the next state, or you can right-click and do start playback which it will cycle through the four states that I had and then it will stop. Now if I had selected playback it would continuously cycle through those states. Just like auto data changes you can come to a folder level node, right-click, simulate, add CSV file. I'm going to use the same CSV file and then apply this to a more global level. When I hit OK it's going to tell me that it needs to have a unique name which makes sense because I already created a CSV file. I'll just say underscore. Now when you hit OK it will apply one to the top level node where you're at and then linking nodes to each device it applies to below. You can still come down to the bottom level node, right-click, and do the same steps as before or if you'd like to do them all at once you can go to the CSV file and do the exact same thing from the top level node. This will make the apply next state apply to all three devices rather than just the one. If you'd like to know exactly what data points are changing you can do show data points and that will show you the data points that are changing in all the states as the top level node will reference all three devices. So then if I right-click and say apply next state you'll be able to see the values changing without having to dig through the points list itself and find out exactly where the points are that are changing. The last thing we're going to be talking about in this video when it comes to data changes is scripting. Now scripts can be the most flexible out of all the ways to change your data but can be a little bit more complex. To add a script you can right-click on any node in the workspace, though we advise you should put it close to the devices that it is changing just for organization's sake, and then you go to insight, add script. You'll want to name the script. I'll name it 104 data changes. Click OK. This will bring up a JavaScript editor. The best way to get points into this JavaScript editor is just simply drag and drop them from the points list. Now that we have the point we can change the name. So the syntax for this is insight tag, which is the variable type, variable name, and then what point it maps to. Let's change the variable name so it's not something so complicated. From here you can use that variable name to set values. So if I run this JavaScript we should see that point turn false, and sure enough it did. Of course since it is just JavaScript you should be able to pull in most of the commands you know from JavaScript and run it that way. So you can create while loops and arrays and all sorts of things to allow you to have a very flexible interaction with the data change, and you can pull in as many points from as many different devices across the workspace as you'd like. This is a very powerful tool but can take a little while to get used to. As I mentioned earlier in the video I'd like to draw some comparisons between CSV files and auto data changes and commands that can be done on clients and masters. So let's go ahead and go up to the clients folder I made and let's look at the 104 masters. In this folder just like an auto data change you could go to the individual device, right-click commands, and create a command to put on that one specific device. And as you can see the command is on the device and then there is a reference node off that one individual site. But very similar to auto data changes you can actually go to the top level like a folder and right-click and do a command. Now since we're at a folder level it's showing us that we have commands for all the different protocols but inside this specific folder there are only 104 devices so you can only create commands for 104s. We'll do another one and change the name. Now when I apply this it will not only be put at the top level on the folder but also be put down on every single 104 device underneath this folder. If you go even higher up in the tree and right-click commands you'll see since I have each device in its own specific folder that I can do the commands across all the protocols. That being said if I decide to come in here and do a general interrogation for 101 you'll only see the child nodes on the 101 devices and nowhere else through the workspace. This is a quick way to create commands across multiple masters or clients.


Video ID: 8Y4rA5ni-B0
In this video, we're going to demonstrate DNP3 Secure Authentication version 5 and user management via a DNP3 authority. So here we have the distributed test manager simulating a master talking to two different outstations, outstation 1 and outstation 2. And the DTM is also simulating those two outstations. Now let's bring up the DNP3 authority. And we see the DNP3 authority is connected to that master and knows about the two outstations. And we've defined three users, Dave, Jim, and Joe. Dave is defined as a viewer. Jim and Joe are both defined as operators. Now let's map Jim to outstation 1. And when we do, we see a series of messages beginning with the secure authentication user status change as the master sends the user status change message down to outstation 1. Let's assign Joe to outstation 2. And again we see the user status change sequence, but this time it's to outstation 2. And now let's assign Dave to both outstations. And again we see the user status change messages as Dave is assigned. So now Jim can operate outstation 1, Joe can operate outstation 2, and Dave can monitor both outstations but can't send any controls. So now let's see that in action. If we go to the DTM, go to outstation 1, and we're going to send a CROV command. We'll send the latch on command. And we can choose one of the assigned users. Let's choose Jim since he's assigned to be able to send controls. He's an operator. Say OK. And we see that the command succeeded. Now let's send another CROV command, but this time let's send it by Dave. Remember Dave's a viewer. Let me clear the log and then we'll send the message from Dave or try to send the message from Dave. And we see that the command fails because of the challenge reply, because Dave is a viewer and not allowed to send controls. And notice we have an error message now telling us that the CROV command failed. Let's clear that error and go take a look at outstation 2. And again we can send the CROV command. We'll send the latch on if we send it by Joe, the operator. The command succeeds. But if we send the CROV command from Dave, the viewer, the command fails. So now let's suppose that Jim didn't complete his proper training certification and so he is downgraded from an operator to a viewer. Again we see the user status change messages updating that change in his authority. And let's go ahead and assign Joe to outstation 1 so that someone can control this outstation. And again we see those user status change messages. So now if we try to send the CROV command here on outstation 1, if we send it from user Jim, who's been downgraded to a viewer, it fails. But we do have the option now to send the message from Joe, because he's been added as a user and an operator, to this outstation. And here we see the command passed. So now let's say Jim regains his certification, we'll make him an operator again, but let's suppose Joe leaves the company. So we're going to remove Joe, and we can even remove Joe from the list of users. And now when we go to execute a command... Joe is no longer in the user list, but Jim once again is able to execute the command. So this is a very simplified example, but it does show some of the power of being able to control users and user permissions via a DNP3 authority with DNP3 SAP5. Note that all of the user updates were done securely, without having to drive out to the substation or have direct access to the devices.


Video ID: bpAxh5XoPCY
Congratulations! You've completed the free Introduction to IEC 61850 video. For a more in-depth look at the IEC 61850 protocol, consider taking our cost-effective web-based training course. For information on this course, please visit our website at www.trianglemicroworks.com slash videopage.aspx. You may also contact us by email at sales at trianglemicroworks.com or by phone at Country Code 1, Area Code 919-870-5101.


Video ID: EPETHW62Nmk
Good morning. My name is Beth Capellas, and I'm the Marketing Manager here at Triangle Microworks. I'd like to welcome everyone to our latest webinar, Strategies for Upgrading to IEC 61850 Edition 2.1, Learn How to Prepare and Test. Before we get started, I'd like to go over some housekeeping items while everyone is still joining. The webinar is scheduled to be one hour long. We will take a couple of breaks throughout the webinar to answer some questions. The audience is on mute due to the large size, so please submit your questions online using the webinar toolbar under the section titled Questions. You can submit questions as soon as you have them. Don't feel that you have to wait until the end. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly to you. We will read and answer some of those questions at the breaks, and then we will take some time in the end to go over any remaining questions. Before we get started, I would like to introduce the folks that are behind the scenes actually answering those questions. Today we have with us Jim Coates, who is the President of Triangle Microworks. We have Dave Gokenauer, our Senior Developer for Distributed Test Manager, one of the tools that we'll be seeing today, Eric Fabre, our Senior QA Tester, and we also have Sri Angula, our Senior Developer for TestSuite Pro, another tool that we'll be touching on today. Next I would like to introduce our main speaker, Joel Green from Triangle Microworks. Joel has worked with communication protocols for over 15 years, with the last six years on a focus in the power industry. He is the Lead Engineer for IEC 621A50 Product Development at Triangle Microworks, and is heavily involved with 621A50 industry groups. He also works as the Co-Editor of IEC 621A50 7-2. He's a member of the UCA Test Committee and Test Procedure Working Group, and is heavily involved in the IEEE Standards Association. So with that, I'd like to go ahead and hand it over to Joel to get started for today. Thanks, Beth. Good morning, good afternoon, good evening, everyone. Thanks for joining us today. We're going to talk a little bit about the new amendments to 621A50 that have been published in the last year or so, and go into how those might affect your systems and work, and how to plan for that. First of all, a little bit of background on Triangle Microworks. We've been around for a little over 25 years now. We're based in Raleigh, North Carolina, and we serve a number of folks in this industry, primarily equipment manufacturers, but also end-users, utilities, and system integrators. We serve all those worldwide. We build products based on open standards published by a number of different groups. In this case, we're focused on IEC 621A50, which I'm heavily involved in, but we also support DNP, OPC, Modbus, IEEE, and the UCA users group to support testing in 621A50. Because of our heavy involvement in all these groups, we're able to represent any interests our customers have, contribute to forward development of the standards. We've got a couple of different product lines. Our core business is the source code libraries. This is the basis of the protocols we work on. We provide those in ANSI C, C++, and .NET interfaces for our easier implementation so that vendors and tool developers can focus on their value add and not have to learn all those details. On top of those source code libraries, we've built a set of test tools for implementations of these libraries, originally focused at the device vendors that were incorporating our libraries into those tools, but we've expanded on that expertise to build system test tools, help the end users understand the protocols and test their communications without having to know all the intricate details. In addition to that, we have a protocol gateway that will allow translation between all of the various protocols that we support. Today's webinar is following up on one we participated in in July on behalf of UCA International Users Group. When 61850 edition two was published approximately 2010, so quite a while now, that was a pretty major jump from edition one to edition two, and there were a few backwards compatibility issues. It caused a lot of uncertainty in the market and delayed a lot of adoption of those new features. So, when the working group went to release these amendments, we went to a great deal of effort to make sure that we didn't repeat that mistake, I'll say. A lot of careful analysis went into any compatibility issues moving forward with the amendment, not only within the working group, but also UCA. UCA has two major missions. They provide conformance testing certification verification for 61850, and they certify the test labs that do conformance testing of 61850 products. And also, as a users group, of course, they have an objective to provide guidance to users on the standard. So, UCA produced a white paper. If you're a member of UCA, it's available on the website, after thorough analysis by quite a number of experts who went through and verified all the changes in this amendment, and have consolidated guidance on the issues that need to be considered moving forward with the standard. So, UCA participated in that webinar, produced that webinar, gave a lot of advice on how that works, and also the updates to the testing procedure involved there. We've adjusted the quality assurance program with UCA to be sure that the life cycle of devices is still in place, that existing systems can continue to be maintained, yet the testing procedure is going to evolve to no longer certify Edition 2.0 devices only, because it's important to the industry to move forward and adopt new improvements to the standard. Also with that webinar, we had participation by DNB GL. DNB is the oldest test house doing conformance testing for 61850. They are formerly known as KEMA, and there was a lot of expertise in that group to guide vendors through the conformance test process. So, they gave us an in-depth overview of what those changes are in the coming test procedures updates. Red Electrica is a utility in Spain. One of my colleagues there joined us to give the utility perspective. Updates such as this always carry some risk that has to be carefully analyzed. So, those guys did a very thorough analysis of the changes and what the risks are and what the benefits are of updating to the standard. In their conclusion, the overall message they brought was updating to Amendment 1 is a when, not an if. There's no question that it needs to be done, that systems need to move forward. But the planning needs to be in place, needs to be done carefully, make sure the few things that need to be checked are checked, and those plans are rolled out in a methodical manner. And then Triangle Microworks presented some guidance on adoption strategies, and we're going to go into that a little bit more and show how to test really one place that you need to be careful in updating to Addition 1 and how to mitigate your risks and make sure whether or not there's anything you need to be concerned about in your current systems. So, the Amendment 1 IEC process, the documents are all maintained individually. So, when we're talking about the Amendment 1 here, we're really talking about updates to the handful of core parts of the standard. There's some 50 different 16.850 documents in production, in publication now. And each of them is actually independent, but the core group is generally maintained together. So, when we say Amendment 1, we're talking about updates to the core parts. Part 6, which is the description of the system configuration language, the XML format that describes the 16.850 systems. Part 7.2 is the abstract communications interface. This describes the abstract services, read, write, data sets, all that. Part 7.3 describes the common data classes for 16.850. These are the basic data objects that make up the model. 7.4 gives us the description of the logical nodes in 16.850, which are the collections of data objects from 7.3. And these are the basic functional building blocks. These are minimum sets of function in 16.850. And then 8.1 is the concrete mapping, in this case to MMS. So, we take the communications abstract services in 7.2 and map them to a wireline protocol that gives us 8.1. So, all of these documents were published approximately 2010 and have been in the field for quite a while. They've got plenty of miles on them. They're well-exercised at this point. And they've been updated a bit over this 10-year period. The IC process prior to 16.850, the standards documents were delivered in PDF. They were updated on a process developed in the paper world where standards were produced and distributed in paper format and then later in PDF. 16.850 was a huge effort to modernize communications protocols and move us into the digital substation world. And as that was started with Edition 1 originally, 16.850, some 25 years ago, it was identified that we needed a faster process to handle, essentially, ERATA and the standards. So, 16.850 defined what was called a tissue process. This is technical issues. And that tissue process we're still using with some updates today allows us to respond to any issues in the standard when we find a place where there's an ambiguity in the standard where two vendors don't come to the same understanding of how to read the standard. We can, at a reasonable pace, identify the issue, propose solutions, and publish that. There's an online database that collects all these. Those get fed into the UCA test procedure process. Producers know what they have to do to produce a device to the current standard. So, Edition 2 has evolved over the last 10 years through this ERATA process that allows us to respond to those issues relatively quickly. So, the vast majority of the Amendment 1 of these parts is the collection of those tissues so that we have a consolidated document of all the changes that have been made throughout this life cycle. The vast majority of changes in Amendment 1 have been in the field for years. But at a certain point, we need to collect those into a single document and ease the maintenance cycle of checking through the database. In addition to collecting those corrections throughout the years, we do have a few other new concepts in the amendments. One of the main ones is routable goose and sample values. The IEC process to generate standards has a mechanism called a technical report. The technical report allows us to produce or propose a suggestion on how to solve a new problem. What we have in routable goose and sample values, 61850, was designed from the perspective of a substation. In addition to what it was, communications for substations and substation configuration language. And the perspective was, we're inside a fence, we're inside a fixed network. That's a closed system. And so, we implemented goose and sample values with Layer 2 Ethernet multicast. Layer 2 Ethernet multicast will go to every device within a subnet, but it will not cross routers and switches. It's not routable. And that was fine in the substation world. Well, in the last 10 or 15 years, 61850 has expanded outside the substation. In fact, they should too change the entire name of the standard series from substation to system, acknowledging that we're having broader scope, particularly in the distributed energy resources world. The EDR world is much more dynamic, much more distributed, as the name says. So, we need to be able to reach these systems outside the substation. One of the major drivers for this was synchrophaser devices between different utilities, between different grids. There was a major outage in New England and the United States in the early 2000s that turned out to be caused by lack of synchrophaser information exchange. And so, routable goose and sample values was produced to do that. We've moved the functionality of goose and sample values as it always was, but wrapping it in a UDP multicast frame so that we can route that. And that's been published for six or seven years now. It's been tested in the field. It's been updated. Problems were solved. The beta tested, if you will, and proven out. And now, Amendment 2. Amendment 1 incorporates the information, TR 90-5, there's some good use case and background information in there, but as far as the implementation, that's deprecated at this point, and we've absorbed those definitions into the core parts of the standards, specifically 8.1 and 9.2. So, that's the maintenance lifecycle. We've pulled in those changes that came from that technical report format instead of from a tissue process issue. We also did a lot of updating to the models behind the scenes. We've gone to a UML model generation format. This is entirely invisible to the user. This is entirely a document maintenance thing, so nothing any user will ever know about, but it helps us validate the models before we produce the documents and the schemas in 61850. Another big chunk of the work that got added with the amendment were compatibility rules. As I mentioned, we had a huge issue with some backwards compatibility changes throughout Edition 1, a lot of uncertainty in the market. And so, the working group went to a great deal of effort, detail, to analyze any possible issues with compatibility as we move forward, not only to Amendment 1, but to Edition 3 and other updates in the standard. And the various documents have these published, and the beginning of 7.1, so 7.1 gives an overview of the 61850 standard. And that gives us an overview of the compatibility rules, how they all fit together, and then each part includes an annex that describes how the compatibility rules apply to that particular part. And we'll go through the compatibility rules in just a minute here. First, let's see if we have any questions at this point. We do have a couple that came in. First question, where can the tissue file be found for reference purposes? The tissue database is available online. If you Google 61850 tissues, you'll find the webpage immediately. I believe it's tissuedb.61850.com or something along those lines, but Google 61850 tissues will bring you right there. Okay. And I have one additional question. Can we get NWNVAR signals in GOOSE protocol to SCADA, which supports 61850, and it has in parentheses SCADA client and IED server? I'm going to say yes. I think we're talking about sending analog values from 61850 servers to essentially the HMI and control center. If I understand the question, then absolutely, that's a normal feature of the protocol. Well, and if you want to, the person that sent that question in, feel free to send in some follow-up details, and we can address it later on in another question and answer session if that didn't answer it for you. Definitely. Definitely. Anything more specific there we can go into? Okay. And then if you want, Joel, you can go ahead and move on. Okay. So, we're going to go a little bit about specifics of the compatibility rules. As I mentioned, 7.1 gives in great detail the concepts of the compatibility rules. The overview of how they work and what the reasons for each of the possibilities of changing the standard are. For the most part, we fix errors, we add enhancements, we make clarifications majority of the time. We're adding additional functionality that won't affect any – that won't change anything in existing systems. For example, devices simply can't understand the new functionality, but it doesn't bring any impact on existing functionality, and that's all okay. So 7.1 gives us the overview of what those rules need to look like. This is a snippet from 7.4, as I mentioned, the collection of logical nodes from the standard gives us the basis of the modeling. And so this lists how each rule from 7.1 applies to that part of the standard, and it goes through and tells us every case in this document, in this amendment, where that rule was applied and what the changes are. And we've got a snippet here showing you a little list. Here we've got new data objects based on a new common data class added to existing LNs. So this is a list of the ANCR added, a call change, and so on. So we've got an explicit list of everything that changed in the standard. Again, the majority of these are modeling changes that are going to have no impact. There are going to be additional things added to new models completely interoperable with anything in the field now. The other major chunk of this work in the compatibility issues is not how the standard evolves, but how devices need to behave in order to guarantee forward compatibility. And this is one excerpt from 7.1. There's quite a few of these tables. There's one for every one of those rules listed. And what this talks about is for the device vendor, the tool vendor, how do I need to program my product in order to not have any problems in the future when the standard's evolved? And as you can see, this goes through each type of device in the system and talks about the impact and the recommendations on it. So in this case, we're, like the last example, we're adding a new data object based on a new CDC. So for instance, 7.4 is updated. And that existing LN adds a new data object from a new CDC in 7.3. If I'm building a device in order to be sure that I don't have a problem with this situation when the next update, 6250, comes out, if I'm a client, this is achievable. And as long as I can use the get data definition, which is one of the discovery services in 61850, I can go ask the server what it provides. Or I can read the SCL file and interpret the data type templates from the SCL file. And I can learn what that CDC looks like. And I can work with that. As a client, that's all the information I need to be able to understand the model that's out there. If the data object itself is something I don't know what to do with, my client application doesn't need it anyway, it's not a problem. My forward compatibility is achievable here. There's nothing that will prevent me from working with a system that makes this change or device that makes this change. Similarly with the subscriber, forward compatibility is achievable. I have a mechanism. I can analyze the data structure. I can see the data set members. I can see the list either by interrogating the device or by reading the SCL file. I can get access to the data type templates. I can understand the structure of the CDC. I know the data types of each member of the CDC. I can achieve forward compatibility if I design my product understanding that I may have to interpret these things in a newer edition. We also list here backwards compatibility. In this case, backwards compatibility is guaranteed. There's no possibility of changing anything in this direction that would break existing systems. If I'm the IED tool, so each IED in the 61850 system has a configuration tool that interprets the SCD file, configures the device, again, essentially the same situation, forward compatibility is achievable as long as I design my tool to understand the possibility that comes here. So in this case, we may have to be able to ignore a DO that we don't understand. That's fine as long as my device is capable of ignoring a configuration it doesn't understand, we can achieve that forward compatibility. Again, backwards compatibility guaranteed in this case. At a system tool level, there's no problem in either direction. The system tool has to understand everything that every device in the system can do. This compatibility is guaranteed in both directions. And then for downgrading, downgrading is a concept where a system configuration tool can put out an SCD file at an older revision. So in some cases, if I'm moving a system to, for instance, 2.1, and I've still got, for example, edition one devices in the field, and I'm trying to use those devices in my updated system, that device that's been sitting there for 20 years is never going to be able to understand a 2.1 SCL file, the scheme has changed, et cetera, et cetera. So the SCT will produce, based on the rules, these compatibility rules, the SCT can produce an edition one SCD file that describes the information that those devices need to interoperate in the system. Now that means that the goose messages that that device publishes and subscribes are limited to the set that the older revision can understand, but the system tool knows those rules, knows how to do that, can make that all interoperable. Okay, are you ready for a couple more questions? We have some there. I wanted a little bit more detail on the standard, so see if we have any more on detail questions there. I do have a couple of questions. So the first one, which part of the 62850 standard would be a starting point to understand the differences between edition one to 2 and 2.1? And that kind of follow questions at any specific sequence of 62850 standard part numbers. So the place I would recommend to start there is 7.1. 7.1 gives a lot of the background and overall concepts of 62850. And that annex in 7.1 is the sort of the root of the compatibility rules. Every one of the core parts has an annex that tells how the 7.1 rules apply to that part, but 7.1 is the background concepts for all of that. The other part that has a lot of, a lot of information on how to put all this together is part six. Part six describes the system configuration language, which in one sense is the syntax of the SCL files themselves, but there's also a lot of background information in there on the system engineering process, which did get some clarifications with this amendment. So there's a lot in there about how the file types work and how the round trip engineering process works, how all those pieces fit together, including some forwards and backwards compatibility information. Additionally, as I mentioned, the UCA white paper goes through some of those details as well. Okay, next question. Like the list of data model changes listed in the annex of 7-4, is there any document which lists out the exact feature-wise changes from addition 2 to 2.1? So the exact changes are produced in documents, in the document that's called the amendment. The IC produces two different documents when there's an amendment. The amendment document itself is literally the change set. It's the diff between the old document and the new document, and very painful to read, but it explicitly lists the changes. And then IC produces what's called the consolidated version, which is the document with the changes applied. So it is the new standard, although technically the amendment is the normative version. The CSV, the consolidated version, if there's any error between the two, the amendment's actually the truth. But it's impossible to use, so we have the consolidated version, but the amendment document itself is an explicit list of the changes. Okay. We do still have a number of questions coming in, so I want to encourage those to still come in. We will try to answer them online, but Joel, go ahead and continue on so we can just stay on schedule, and then we will get back to answering some questions and answers live. Okay. So we're going to go through quickly our 628.52 toolkit. I'm going to go through at a high level some of the overall features of two of the tools that we're going to use to show some of this testing and how you can examine your systems and make sure everything's as needed. First of all, we have TestSuite Pro, which is our diagnostic test client. This is a tool designed to analyze originally devices built with our stack, but now more 628.50 systems, substations, all that, so we'll go through a little bit about how to investigate your systems and how to be sure everything's working correctly. And then we'll look quickly at Distributed Test Manager. Distributed Test Manager is our system simulation tool that can simulate not only 628.50 devices but all the other protocols we do, DNP870, Modbus, TASI-2. With 628.50, we'll import the SCL file and be able to generate a simulation from that. And we also have SCL Navigator. We won't look at that today, but that's an ICD editor and SCL verification tool. I think we've gone through a couple rounds of questions. We're going to go on into the quick overview of the tools here. So, this is the TestSuite Pro. This is our diagnostic client tool. It's got a number of things here to let us analyze our system. At the moment, we've got DTM running in the background. We're analyzing. We'll show you DTM in just a moment. We'll go through the highlights of this tool quickly. On the left side of this tool, we have a set of tools. To begin with, we have a system status. And the system status is showing that we're connected to 19 IEDs in this case. It just shows us that a high-level view that everything's happy, addresses and so forth. We've got another set of tools here. And you can see on this side, we've got a red light, green light thing. The green check says everything's happy. This tells us we've got an issue here somewhere. So, I can come into this file. And what I can do with this compare model is let's me discover the IED and compare it to the SCL file I've got. If we have a mismatch between client and server, SCL files in the field, that can cause a lot of problems. It can be very expensive to track down. So, this lets us go and very quickly see if the device we're hooked to matches what we expect it to be. And lots of green here. Everything's good. No problems. If I found anything here that was different online from what I expect, I would see a list. I don't. Goose tracker allows us to monitor all the goose on the wire. This is sniffing the wire, checking the goose that we see on the wire versus the SCD file we loaded. In this case, I've missed a couple of retransmissions on the wire, so I've got warnings about that. Otherwise, we're very happy. If I had any goose on the wire that I didn't expect from the SCD file, those would show up here, show an error. If I had anything in the SCL file that did not show up on the wire, I would see that as a warning as well. Additionally, monitoring goose traffic, we have an ALGOS. Monitor ALGOS is a logical node that a subscriber can provide to give an indication and supervision of that ALGOS stream. And so this matrix here is telling us which servers and subscribers are in the network and which of the subscribers provide ALGOS nodes. And if it has an ALGOS node, whether it's happy or not. In this case, we're running the simulation here in our local device, in our PC. There's no problems going on. In every case that I have, a publisher and subscriber gives me that indication. In this case, this device over here, test subscriber, does not provide ALGOS functionality. That's optional in the standard. So this red is telling me that I don't know what this device is doing because he won't tell me. And there's a whole lot of other statuses available that can tell me a lot about what's going on in the system. But that gives me a quick overview. If my devices aren't getting the signals they need, my system's not going to work. So that's a pretty high-level check. And I've got warnings here, these ones that don't provide ALGOSes. And then my high-level indicator here gives me a summary of how many issues I might have. Similar to that, I can monitor the mode and behavior and simulation status. These are test features in 61850. When I'm testing a system, it's important to understand how each of my devices are configured with respect to these test setups. So this matrix gives us a quick overview of what those look like. We've got a SetCL Verifier. We won't go over that today. Data Miner is a way to create these displays on the right-hand side that we'll go to. Helps me find stuff in the model. And then we have an Advanced Client. Advanced Client, the reference here is the advanced user. This is a very raw interface. I can get to the server and have almost no limitation of what I'm allowed to do here for testing purposes. I've got a set of negative tests I can run against the device. I intentionally let you do bad things if necessary for testing. I have a couple other particular tools here for testing particular services. When a server is being implemented, all those need to be tested. And then we have a Goose Publisher. 61850 provides for simulating goose and sample value messages. This replaces jumper wires and test boxes to inject signals in the old copper wire. Substations, Goose Pub can publish messages with a simulated flag. And from the test tool, I can manipulate the data set members and the headers of all these in order to create tests. On the left-hand side here, I've got a series of displays. This is a custom display. This allows you to quickly build an HMI for various testing reasons. All these signals here are live. This is showing me the status of my simulator. This is all live data coming in here. So I can drag objects from the model and set up this display, see what I need to see quickly for any test I need. And I can keep a number of these displays from the list. We've got a logic analyzer. This allows us to monitor the signals on the wire and plot those in a pretty typical logical analyzer format. We've monitored a successful reclosed sequence here, and we're able to display that. In a similar way, we've got signal flow. This monitors the same signal instead of a time graph. This is an animation. It allows us to show all these signals. The yellow lines will show the sequence of events as we step through. Light that up, make it easy to visualize what's going on in our system. And we likewise have goose and report viewers over here that we can see the messages on the wire. So that's a high-level overview of TestSuite Pro. We'll go look now at our distributed test manager that this is connected to. This is our server simulator, and this tool allows us to simulate any system, any number of devices out there quickly and easily. We see here a one-line diagram. It's tied to all the devices in the system. So we're simulating all these IEDs. We've got a list over here. This is our workspace on the left-hand side. It identifies all the resources in this workspace configuration. On the bottom here, we've got the list of IEDs. These are all the servers that we're simulating. TestSuite Pro has a list of those in the other display. To set this up, we load IEDs from an SCL file. For other protocols, we can do it from a spreadsheet in the list, or we can add them manually. One of the features that we have here, if we're trying to test a device, we can set up the whole system. And if I'm trying to test my particular device, once I have the system proven in simulation, I can come in here and disable one device and bring my hardware in and test that hardware against the rest of the system. So in order to test communications, we need to be able to manipulate data in the model. So we've got a number of features here to be able to simulate data in the model. To begin with, we've got a manual change. I can come in here in any one of these. I can show the data model. And I can open this up, and I can come click through the model. And then I can change any of the data in here manually if I have specific things I need to test. It's pretty slow going, so we have some automated ways to test data. We have an auto data change node here. This generates a list of changes, and I can come in here and set this up with a periodic timer and a function to change the data, particularly for analog values, so I can change the ramps and all that. And I can set this thing up and tell it to go change that data by whatever format I need my tests. In addition to that, if I need a more specific way to change data, if I want to be able to reproduce a particular sequence of events or series of events, I can drive those data changes from a spreadsheet, a common separated value list. And that looks a lot like this. I've just got a simple spreadsheet. I've got a column of paths, and my rows are states. So each of these states exists here, and at this state, the value for each attribute in our list is here. I can apply that. I can have it run through and change that. I have a method here where I can start that up and step through the particular states. So I have a list here of the states from the columns in here, and I can set every device in the system to that state with one click. In addition to that one click there, I can also manipulate the states from this table based on scripts. We have a script engine in here right now that is based on JavaScript. We can JavaScript. We've got a simple script here. The JavaScript editor is in here, and we can program just about anything behind the model here. This is just a simple script that changes a value in there, but the JavaScript has full access to all the data model behind DTM. Anything that needs to be done can be scripted there, including referencing those states from the CSV files. In our upcoming release, we're adding to the JavaScript the ability to use 61.131, so that's structured text from the PLC world. So if you've got programmable devices in your systems that use structured text for their configuration, that structured text can be directly imported into DTM, and we can simulate the exact same logic from the definition of those devices. Another test we're going to show quickly is our test manager. We can run lists of tests that allow for repeating specific tests. We can set up conditions and tests here and quickly run through each of these tests. One of the particular functions that DTM is useful for is gateway testing. Because we implement all the various protocols that we support in here, a gateway typically needs to be configured and tested. If it's, for example, you've got DNP outstations in the substation, and we need to get that data up to a 61.850 client, an HMI in the control center, we have a gateway that has to be configured to map the DNP points into the 61.850 model. That's typically a lot of annual labor to test all that. Since we have that mapping information known, DTM can simulate the IEDs in the field, manipulate data changes there, simulate the client at the control center, and programmatically verify that the correct changes are recorded on both sides and test the gateway configuration very quickly. So that's a high-level view of the two tools. Okay, Joe, did you want to take a couple of questions, or are you going to go on to the... Yep. Do we have any questions on the tools there before we go on? We do have a couple that are coming in. How many IEDs can you simulate at one time? That is a resource and licensing question. I believe I did fail to mention that DTM is called Distributed Test Manager because it can distribute the application over multiple PCs and test across networks. It is licensed in numbers of connections, but those are the only constraints. With the licensing and with enough PCs, we don't really have a hard limit of what can be done there. The system does scale as needed. And I'll just add that question was answered online by Dave, so his additional response was we routinely simulate hundreds of IEDs depending on the IED and the hardware, as you mentioned, Joel. We have simulated as many as thousands of IEDs. Next question, can you show the remote control commands from an HMI? I assume that's referring to DTM. I'm sorry, can you? Can you show the remote control commands from an HMI? If we're using DTM to test an HMI, if the HMI is issuing commands to the simulated server in DTM, we would be able to see that in the logs in DTM, as well as reports in GOOSE of the effects of that. I hope that was the question. Yep, and if that does not answer your question or did submit that, please feel free to send in some follow-up information or questions and we can address it at the end. And I think, Joel, with time in mind, go ahead and continue and we can get to these questions towards the end. Yep, very good. So, getting into how do we move forward with adopting the amendment. As I mentioned earlier, UCA and the working group and many experts have analyzed this in great detail. We're confident that there's very little concern here. But at a high level, there's a few different situations where we could be needing to implement addition 2.1. The first of which is a greenfield. We're building an entirely new substation. In this case, there's nothing to worry about. You want to implement the latest and greatest. There's no compatibility issues at all because it's all new. Away we go, nothing to worry about there at all. The second situation we have is an existing system that meets all the user requirements. We've got a 2.0 system or even an addition 1 system in the field and then everything's working and nothing new is needed. There's no problem there. As I mentioned, the UCA has updated the conformance test procedures, the quality assurance program behind the conformance test procedures. We will be end-of-lifing addition 2.0 testing. It is important to the industry to move forward as the standard advances. So it's a couple years out, but we will be end-of-lifing addition 2.0 only testing. We've already end-of-life addition 1 testing. Ten years too late, but we got it done. However, we need to separate in concept the conformance testing from the lifecycle management of these devices. If you've got a 2.0 system in the field, your vendor's not going to be, in a couple more years, not going to be able to produce a new product tested only to 2.0 systems. He will, however, be able to continue to sell you replacements for the devices you have now for as long as he cares to maintain that lifecycle. So that's a huge distinction between end-of-lifing testing of new devices and availability of replacements on the market. So there's nothing, there's no risk to ongoing maintenance of existing systems. So in that case, everything's fine. There's nothing to worry about. We can continue to maintain existing systems throughout the lifecycle of that system, no change. Where it gets interesting is where we have a mixed system. We've got a substation in the field. We need to expand it. We need some newer functionality. We want to get up to date. We want to take advantage of some of these improvements. And so we end up needing to expand the system with what's on the market now. We've got a couple of options here. We can always add devices at the same revision as the existing system. As I just mentioned, the vendors can continue to produce the devices that are on the market today for as long as they care to do that. The other thing that we've added in the quality assurance plan with UCA is the ability to certify devices for older revisions. We're going to end-of-life edition 2.0 testing a couple more years out. I believe it's 2023, but somewhere in that timeframe. However, after that point, a new device coming in for conformance testing, once it's been conformance tested to the current standard, because the IEC has one standard at any particular time, edition 2.0 now includes the amendments. That's, according to the IEC, the only version of the standard. Once we've conformance tested the device to the current standard, we can then, if the market demands, certify that same device to be configured in a compatibility mode so that it can behave exactly like a 2.0 server. There is a forward path. If the market needs those devices and the vendors see the value in producing them, we will certify new devices to meet both versions of the standard going forward. So there is a path to provide updated devices continually. The other option we have at this point is to add 2.1 devices. So this is where we get into the mixed system. And this, the concept in 61850 is, if we have mixed systems, the SCTs, the system configuration tools, and the clients need to be able to understand all the devices in the system. Servers only have to deal with one other device. They only have to understand their rev level, but clients and system tools need to understand them all. And we've gone through and we've investigated all the possibilities with Amendment 1, and really the one issue that's come into play is we made a breaking change in reservation of control blocks. Reservation's been in the standard since late in Edition 1. It's been there a long time, but it was optional in the server. We've gone and made it mandatory with this amendment. It should have always been. Lots of history, but it's obvious that had to be. We went ahead and did it, acknowledging there's a backwards compatibility issue. In addition, two clients should have always implemented the reservation services, but because it was optional in the server, there's some fear that some clients may not have realized they need to do that. And so we need to test that scenario. If the client, your HMI, your gateway, does not properly do the reservation scheme, then it will need to be updated. And so we produced a simple tool to test for that scenario. And we're back to our distributed test manager, which doesn't want to move to the other screen at the moment, but we're going to convince it to. There we go. So you can download this from our website. We've got a workspace here that allows us to test your system. So what you will do is bring your system configuration description file, your SCB file, into 6180.50. Simulate one or more of the servers in it, and then have your client connect to it and attempt to enable or control blocks. So I've loaded this workspace. It's on the start page in 6180.51. Click under the short list of workspaces there. And from here, we've got some basic instructions. We're going to come in here, and we're going to load a file. So I'm going to go Manage, Add, 6180.50 Servers. And I'm going to go find my SCL file, import my configuration. And this is an SCB file I'm using here. DTM allows me to monitor external files for changes. I'm going to say yes, go through that. At this point, I'm picking an IED. I can reassign my IP address. This red here is telling me that IP address doesn't exist on my PC. There we go. And I can simulate that device. So DTM's going to load that up. It's waiting for it to come online. And there it is. Our server's up and running. Workspace here is telling me I've got one server. I could have loaded many servers out of that file. I've just got the one. My one server has a whopping 216 report control blocks. I think we can find one to test out of all that. Nothing's enabled. So at this point, you're going to bring up your client, your HMI or your gateway. My client happens to be TestSuite Pro again. And with TestSuite Pro, I can come to my advanced client. This is that raw interface we discussed. And what I'm going to do is discover this device. I could load my SCD file in here. But in this case, I'm just going to go query the server and find out what's out there. It's on my same PC, so I'm just looking at the loopback address here. And we'll go out and find our device. So we found all that. We're back here. We're going to go into one of our devices. I'm going to find my report control blocks. I'm going to find one of them. And the reservation system, it's a mutex. It allows a client to claim that control block. And I'm going to go in here and I'm going to find my report control blocks. And then configure it and then enable it so that it claims the control block for itself and prevents anyone else from using it. So if I come in here and I say enable report control block, my workspace in DTM is going to say it didn't get reserved. We're looking at an addition to server in this case because the majority of the files around addition to server would let me enable that. So I'm explicitly saying here that it didn't reserve it. And I can see here that it's enabled. So if this is the result that comes out from your client trying to enable a report control block, it's not going to work with an addition 2.1 server. It needs to be updated. On the other hand, if I come in here, leave that there and start another one. So reserve TMS is the time in seconds that this control block is reserved to my client after I disconnect. So I'm writing that. And we see over here in DTM that the control block was reserved. And now I can enable it. And it's successfully enabled. And everything's happy. My client behaves like that. I have no trouble with addition 2.1 at all. So it's pretty much that simple to verify that your system's ready for 2.1. As I mentioned, this tool can be downloaded from our website. And I think I have run the clock out. Have I not? We are right at the hour. That was your final demo. Yep. We're happy to stick around a few minutes. There are more questions. But I think we're. We do have a couple more questions. So let me just kind of wrap up here. Again, we are at the hour. We're happy to stay on, whoever would like to stay on with us. Just so everyone knows, this webinar will be posted on our website in the next couple of days. Once you sign off from here, you will also receive a survey. We would love to get your feedback on what you thought of the webinar, if you'd like additional information on what you saw today. And of course, to answer any other questions that perhaps you didn't get to address during this hour. And you can certainly download any evaluation licenses of the tools that you saw here today. That workspace is available in DTM that Joel just showed you. And of course, I've left my contact email address there on the bottom of the slide, if you want to email me directly for any additional questions or answers. So with that, again, we're right at the hour. I will just go into a couple of more questions that we've got coming here. And there's one that's been answered online, but he's asked it a couple of times. So Joel, I do want to throw this one out to you. Is it possible to communicate both addition 1 IED and addition 2.1 IEDs using one SED file for all different versions? Or do they have to create different SED files and clients for each edition? So this is the downgrading I mentioned. It depends on the exact mix of devices in the system. There were some changes to some of the CDCs through addition 1 and addition 2. And so you do have to be careful. All the devices that need to communicate that information need to be able to understand the same set of data objects. There's actually only a couple of cases where there's a significant difference there. So if your system doesn't need those particular data objects that changed, then there's no problem. There's a mapping for the couple of cases that can make that work across that. And then the system configuration tool needs to be able to understand the revision level of each of the devices. Each of the devices in the SED file has its own IED section. And so it gets configured appropriately. Then depending on what the devices in the system can interpret as far as the SED file themselves, in some cases, the SCT has to publish, for instance, an addition 1 SED file and an addition 2 SED file of the same system. Remember, the SCL is an exchange format for files. The system configuration tool manages the database of all the information. So if devices can only consume one scheme or the other, the SCT can produce the versions of that file needed for each of the devices in the system. So it depends. There are a few corner cases that have to be managed carefully. But in general, it all works just fine. OK. Next question. And you did kind of touch on this in the presentation, but it came in kind of in the last couple of minutes, so it might be good to elaborate on this a little bit more. Is the TestSuite Pro capable for client testing? TestSuite Pro is a test client. So it's designed for testing servers and publishers. DTM is the server simulator primarily. DTM does client functionality as well, but it's primarily a server simulator. So if you're looking at testing a client, testing an HMI, testing a gateway for the client side, then DTM is the tool to simulate the number of devices you need to exercise the client. OK. Kind of another question along the lines of TestSuite Pro. You had, again, answered this earlier, DTM, and how many IEDs DTM can simulate. This question is coming in saying, how many IEDs can TestSuite Pro simulate? TestSuite Pro has a simulator. It's designed to simulate one or two IEDs in a lot of ways to exercise TestSuite Pro itself. But sometimes there's a need to simulate an IED, so we allow that in the tool. It's not at all designed to simulate numbers of IEDs. That's functionality for DTM. OK. I'm going to go back to a couple of questions that came in a little earlier. So this might be more on the content you addressed in the beginning. Is there any compatibility issues with Edition 1 and Edition 2? And does Edition 2 still support Edition 1? Yes. Again, the tissue process I went over was very active in Edition 1. 61850 was largely invented out of whole cloth to meet the needs of this industry. And so there were a lot of revisions over the lifetime of Edition 1. To some extent, Edition 2 collected a lot of those tissues. So the question is actually more when in Edition 1 a device was produced. Early Edition 1 device is very different from Edition 2. A late Edition 1 device is hardly different at all because those tissues were applied over that life cycle. I strongly recommend against any new development of Edition 1 systems. But it gets a little more complicated with some of those early devices. There are some systems in the field like that. But other than that, compatibility is possible. But there are some issues to work through to make that all work. And that's the reason we went through the effort we did to identify any problems going forward. OK, next question. Is there a security recommendation for goose traffic when it is leaving the substation with Argoose? Yes, there is. Argoose includes encryption and authentication. We're currently working on an implementation of key distribution center GDOI client that works with a GDOI server to manage all those keys. It's an interesting problem when you're securing multicast traffic across a network, across multiple networks. But absolutely, encryption and authentication is defined in the standard and is coming to the market very quickly. We'll have a webinar about that coming up, as a matter of fact. OK, next question. Can DTM handle a collection of CIDs rather than a single SED file? Absolutely. DTM can import any combination of the above. OK, next question. At some point in the future, will the SCADA be seen as another application using IEC 61850, or will it continue to be seen as separate functionality? So that's an interesting question. I mean, there's definitely a lot of SCADA using 61850 now. The channel from the substation to the communication center is slowly migrating. We've got a couple of technical reports. One's in the process of being revised and moved from a technical report, which I mentioned earlier, to a technical standard for substation control center communications. So yes, it's entirely possible to run the complete channel So yes, it's entirely possible to run the complete network up to the SCADA system with 61850. OK, well, I think that covers all our questions. So again, I'd like to thank everyone for taking the time to come and join us for this topic and ask as many good questions as you did. I appreciate if you answer the survey that you get once you sign off from here. And certainly, please feel free to reach out to us directly if you have any additional questions. Thanks, Joel. Thanks, everyone. Thank you very much. Have a good day.


Video ID: r50FJSMqVaY
Hammer, the 61850 client component of the 61850 test suite, also makes it easy to manage your goose control blocks. Let's expand the tree and look at a goose control block. Here we can see all the attributes and properties for the control block visible on one screen. We can enable the goose control block by right-clicking and selecting enable. And once enabled, we can subscribe. We can also unsubscribe and then disable the block. And with the block disabled, we can select a data set reference. As you can see, Hammer makes it very easy to manage your goose control blocks. You can enable and disable the blocks, subscribe and unsubscribe, and view all of the data attributes in one view.


Video ID: ov8Ly3HqD10
This video describes how to manually create data changes in an IEC 61850 server being simulated by Anvil. First we'll find the data values we want to change. So I'll expand my tree and I'm going to begin by changing the STVAL which is an enum. When I double-click on an enum, Anvil opens up a drop-down menu and I can choose the appropriate value. For numerical values, such as the voltage, I can simply double-click and type in my new value. We can also change the quality. When we double-click a quality value Anvil presents a list of quality values and we can select the appropriate ones. We can also change the time. When we double-click on the time, Anvil presents this dialog which allows us to change the date as well as the time and the quality flags associated with the time. I can also manually change data values using the flat view. So let's turn on the flat view and I'm going to widen it a bit to make it easier to see here and I'm going to filter and only show my MX values. So now I can scroll down and find the value I want to change. For example, here is my phase A voltage that I changed earlier. I can click in here, type in my new value, change my quality flags, or change my time. As you can see, manually simulating data changes in Anvil is quite easy. Be sure to check out our other videos to see other ways to create data changes in IEC 61850 servers being simulated by Anvil.


Video ID: 1eTASsi6-Kg
Hello, everyone, and welcome to today's webinar, DER Solutions Enabling Interoperable SCADA Communications for PV Inverters through Embedded Controllers. My name is Beth Capellas, and I'm the marketing manager here at Triangle Microworks. Before we get started, I'd like to go over some housekeeping items and some introductions. The webinar is scheduled to be one hour long. We will be stopping for questions a couple of times throughout the webinar, and we'll stay on afterwards to answer as many questions as possible. The audience is on mute, so please submit your questions online using the webinar tool under the section titled Questions. We will try to answer questions online during the webinar, and we'll read and answer questions during the breaks and at the end. Next, I'd like to introduce our speakers. We have two folks with us today from National Renewable Energy Lab, also known as NREL. First, we have Kumar Prakhabar, who is a senior research engineer. He leads research projects targeting improvements in distribution system protections and interoperability of distribution system assets. He's a technical contributor in multiple microgrid controller evaluation projects and advanced distribution management system evaluation projects. We also have with us D.P. Bigaminton, who is a research engineer at NREL with a focus on energy system integration and grid modeling and simulation and performance optimization for scientific software. Lastly, we have our own Joel Green from Triangle Microworks. Joel has worked with communication protocols for over 20 years, with the last 10 years in the power industry. He is the lead engineer for 61850 development and products at Triangle Microworks and is heavily involved with 61850 industry groups. He also works as the co-editor of 61850-7-2. He is a member of the UCA Test Committee and Test Procedures Working Group and involved in IEEE Standard Association. So now that you know everyone, I'd like to give you a little bit of background on Triangle Microworks before we get started. We were established in 1994 and headquartered in Raleigh, North Carolina. We have customers in 70 countries and work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support standard SCADA protocols and fall into three categories. The first category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs and their development time. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab and the field. They are designed to troubleshoot, simulate, and automate testing. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. We are heavily involved in the standards committee that supports SCADA protocol, like UCA Group for 61A50 and DMP User Group and Technical Committees. Through our participation, we help define the protocols, stay up to date on the changes that take place, and also provide feedback based on our customers' needs as the protocols continue to evolve. So now that you know a little bit more about Triangle, I would like to go ahead and hand it over to Kumar, who's going to kick off the topic that we're here to learn more about. So Kumar, I'm going to go ahead and make you the presenter. Okay. Thank you, Beth. Let me see if I can show my screen. You should have control of the screen now. Okay. Can you see my slide? We're not seeing your slide. We're seeing your Windows screen. Oh. Let me move my – I think it's here. Okay. Can you see my screen now? Yes. Yep, we're seeing it. Okay. Double checking one more time so you can see my screen, right? Yep. We can see it. You're good to go. Awesome. Thank you. So hello, everyone. I'm Kumar Guruprabhakar from National Renewable Energy Lab, and we're going to present primarily on a couple of projects. Work came out of a couple of projects. The first one is additively manufactured PV inverter, and the second one is technology commercialization funded project, which is enabling interoperable SCADA coms or PV inverters. Both of them are funded by DOE, Solar Energy Technology Office, and we have cost share from Triangle Microworks for the second work. So for the contents today, first I'll talk about the smart PV inverter control capabilities, a little bit of background on the protocols, the interoperable protocols that we have chosen to use here, and the steps that are involved in enabling this interoperability protocols, and also background on the software tools that we used to achieve this, and then the ICD file design, which is a critical component in the interoperability, enabling interoperability on the microcontroller, and finally, a demonstration of data exchange between a client and a server using a real-life microcontroller board. So first, I want to give some background on NREL and the type of work that we do. So I work primarily in the Energy Systems Integration Facility. ESIF is a national user facility located in Golden, Colorado. We call this a user facility because the primary goal behind our facility is to enable resources for researchers and engineers around the world that focus on renewable energy integration into the grid. So we have calls every year. I think it's now called as an ARIES user call, where anyone in the world can ask for resources, high-performance computational resources or lab resources to help with any renewable energy integration challenges. So primarily, the work that I do is based on controller and power hardware in the loop. We do have megawatt-scale power hardware in the loop capability. We can go all the way up to two megawatts on the DC, two megawatts on AC, and the lowest we can go is a few kilowatts. And we also have multiple controller hardware in the loop capabilities, including multiple digital real-time simulators that we can use, microcontrollers, high-performance computational facility, and most relays and protection devices for CHIL. For the work we are presenting today, we are focusing on interoperability for PV inverter. So the slide that I'm showing, it shows the smart inverter controls that we built a few years ago and built additively manufactured PV inverter in the lab. So we took the smart inverter capabilities that we built for this inverter, specifically volt-watt, frequency-watt, and then other controllable things like ramp rate and voltage and frequency right through, and tried to enable interoperability and allowing clients like a microgrid controller or ADMS to talk to the inverter and change these settings in real time. So these are the curves that we used originally in the paper for the three different settings and also the write-through settings borrowed from Hawaiian Electric Company, and this was in 2017. So we used data from probably 2016 or 2015 on this work. Since we did this work, a lot of things have changed in 1547 and dual standards, and it's always evolving, but the fundamental is still the same. Basically these are curves, XY curve points, regardless of the volt-watt or frequency-watt or even the write-through settings. So in this demo we'll show how you can update these XY curve points to an interoperable communication from a client to a server. And I'll hand it off to Joel for the presentation. Okay, Joel, I'm going to make you a presenter. All right. Good morning, good afternoon, good evening. This is Joel Green of Triangle. Okay, Joel, you're presenting. There comes my screen, and we'll tell you a little bit about the background of 61850 standard that's used in this project, and then we're going to talk a little bit about the Triangle tools and products that are involved with this product, or with this project. 61850 is a collection of standards. It's built a stack of protocols so that we can have different components and how they interrelate to build a complete communications infrastructure and data modeling. This is an overall view of the core parts of 61850. Our implementation here in orange is our, in this case, our PV inverter controller. A number of the basic parts of the standard come into play to make this project work. We have concrete mapping called SCISM. This is what gives us the information about how 61850 maps to MMS, which is the wireline protocol used underneath. That comes out of Part 8.1 standard. On top of that, here, we have Part 7.2, which is the abstract communication services layer. 61850 is based on using existing technologies as much as possible and separating those technologies out in order to future-proof, so 7.2 gives us the abstract communications requirements. The 8.1 gives us the concrete requirements so that wireline mapping can be replaced in the future as technology evolves. On top of the communication services, then, we have the data models. The data models start with common data classes that collect the data attributes, the actual information in the device into structures of data. Those are defined in 7.3. Then we have 7.4, which groups those data objects from the common data classes into what's called logical nodes. Logical nodes are the small building blocks of functionality in 61850. We'll look at a few of those shortly. Those logical nodes create logical devices and up into IEDs. There's a couple other parts here. 7.1 gives some overview of the background principles of all this and other parts to give background information, but these are the core parts that build up the communications and modeling information. Then all of that information about the device and the systems is described in a language called System Configuration Languages, described in IEC 61850-6. We'll be looking at that and how the device is described in a few minutes. This is a quick overview of the client-server architecture, and in a later webinar we'll be talking about multicast protocols used in 61850, but today we're focused on the client-server perspective. In this case, we've got a physical device, which is our PV inverter. Our PV inverter provides read-write access from the client. It can also be configured to generate reports based on specific events in the device and communicate that information back to the client. The physical device, in this case, we're implementing a photovoltaic inverter. This IED can also be a battery inverter, charging station, wind plant, windmill, or several other types of intelligent devices used in the DER space. On the client side over here, in our demonstration here, we're going to use TestSuite Pro, a Triangle's test client for the client interface, but in a production system, this could be an ADMS interface, could be a microgrid controller, could be a control center HMI, and that's the general client functionality. A quick overview of the Triangle products that are involved in this project, SCL Navigator is our SCL editor. It has two primary functionalities. We're going to go through today a little bit about how we create an ICD file for a device. ICD is an IED capabilities description. A device that comes from a manufacturer needs to provide this file. It's a template file that explains the data model and the services available, the capabilities of their all devices that comes from the factory. The vendors need to provide that file, so SCL Navigator's primary purpose is to ease that process, help guide vendors through creating that ICD file. It also has a functionality as a SCL verifier. We find that SCL files are not always perfect. SCL is an exchange format. One of the core tenets of 61850 is interoperability, so we have devices and tools from different vendors that are able to interoperate with each other. To enable that, the SCL files are all often passed between devices and tools of different vendors. If there's any question about whether the file is valid or not, that can waste some time and money as folks figure out who's responsible for any issues there, so we find that having a third-party tool that can verify those files against the standard helps everybody be more efficient. Those are the two main functions of Navigator. We're also going to look at TestSuite Pro very quickly. DT's going to use that later in the demonstration to interrogate, interact with the device under test here, the PV inverter controller. I'm going to go quickly over the highlights of that tool. And then, of course, we have our source code libraries, which are the foundation underneath all the Triangle products. In this example, in the PV inverter controller here, we're using the TMW ANSI C-Stack. DT will go into a little more detail on that, but we provide that. Compatible on virtually any platform imaginable. We also have C++ and .NET wrappers on top of that library available for more specific applications. So now we'll go quickly into TestSuite Pro. Just like I said, it's our diagnostic test client. It's designed to help, first of all, our library customers implement devices, test the devices, and make sure they're compliant to the standard. It also provides a lot of functionality for helping system integrators and end-users, utilities, and in this case, DER system integrators, diagnose problems in the system, high-level communication issues, modeling issues, and so forth. So I'll quickly go through the main features of the tool. Here we have a Goose tracker. We're going to go through Goose in a later webinar, but this monitors the signals on the line and compares them to that SCL file, the system configuration description file we have, and would highlight if the packets we see on the wire are not what we expect from the configuration file. We've got a tool here for comparing models. This SCL is good. We need to have that and know what we're doing, but there are times a device doesn't get updated as it should. We end up with an outdated SCL file somehow. This tool quickly lets us attach to the device and compare what the device exposes online to what the SCL file tells us it should be. Saves a lot of time and confusion if there is any discrepancy in the files being used. We have some other tools here that help identify what's going on in the system from a testing perspective. There's some advanced test features built into 61850 that we provide some nice interfaces to. Then we go down and we have what we call the advanced client. This is a very low-level interface. It allows advanced users to get in and interrogate the device in a fairly low-level way. We allow just about anything to be done here because the device vendors need to do negative testing. This does allow a lot of cases violations of the standard to test the device against. So in this one, this is a very raw interface. We've also got Goose Publisher. Again, for some of the test features in 61850, a test tool can publish Goose streams directly. There are ways to tell the devices to interpret test data simulated Goose messages instead of the messages coming from the devices in the system. This supports that. And we do have an IED simulator in here. We're not using it in this example, but the TSP will allow you to simulate an IED. In some cases, that's helpful to prove out the system and verify what we need. On the right-hand side here, we've got a couple of different display options that can help see what's going on in the system. In this case, we've got a demo example that's got five IEDs out there running bay controllers, breakers, and disconnectors in our CTs and BTs. This custom display allows a drag and drop of objects to quickly build a simple HMI as you design test cases and need repeatable situations. We can build a quick interface here to see what's going on. We've got a signal flow analyzer. We can drag data out of the data model, set up dynamic displays here. As events happen in the system, we can track those in real time. Similarly, there's a logic analyzer that allows that data to be placed and viewed easily. We've got Goose viewers and report viewers over here that allow for seeing the communications on the wire. So, again, Dipthi will look at this a little bit more in the demo, but that's the highlights of that product. Then we're going to move into SCL Navigator. SCL Navigator is our SCL editor. We do have the verifier. We're going to focus a little bit on how the model is created. For the PV inverter, we put together a data model based on IC61850-7-420, which is a recently published edition two of that. It has become the basis for data modeling in the DER space, not only in 61850, but 1547, 30.5, and a number of other protocols in the DER space that have concluded that a harmonized data model is good for everyone. So, based on that, we'll look at the completed file a little bit, but we're going to go into quickly how to build this file. So, I come up here in my file menu. I'm going to create a new file, and I'm going to create an edition two file. So, I'll just select that for a new file. I need a file name. This brings me up a little dialog box that lets me preset some basic information about the file. There's a file ID. It's an arbitrary string related to the product. We have a vendor name and software version number, and then we have a setting here to determine how we want to view analog values generally. 61850 provides for analog values to be represented as floating point numbers or as integer numbers. It's talking about the exchange of data on the wire. In most cases, we want that to be a float. There are two exceptions to that. We still have smaller devices in the field. It's possible to run 61850 on a processor that's not capable of floating point math. So, in that case, we can use integer representations, and also sample values typically uses integer representations for efficiency, but we can select these at any analog value in the model, but this lets us set the default for most cases in the model. So, starting that brings up our template file here. This is about the simplest IED description we can come up with. We've got the STL header. We've got a little bit of a communication section. We really don't have that in ICD, but there's a stub for it here, and then the IED section. And then the IED section. The IED section indicates the services that are provided by the IED, and this is a good starting point. These are typical services that an IED provides. This can be edited here as needed for the particular device, but this is a good starting point. Then under each IED, we have an access point. Access points are basically your Ethernet interface, and under each access point, we can have a server. And then, as I mentioned in the overview of the standard, we get into the logical devices and logical nodes. These are data structures. They're fairly arbitrary constructs to organize the data that we need in our device, but they are standardized in 7.4 and 7.3 and 7.420 and so on. A logical device is required to have an LN0. LN0 collects the system data for 61850, our control blocks for communications, and some other information that applies to the whole logical device. Our IED has to have one LPHD. LPHD represents the information about the physical device of the system, nameplates, and that sort of information. So, we prepopulate that. That's required for the device here. And so, this is our basic minimum set for an IED. SCL is constructed of data type templates. So, we've got templates down here that indicate, that build up these two logical nodes. This is all the information we need to find those. We start with our LN0 types, our data object types. Our data objects are constructed of data attributes, and those get down into our constructed of basic types from 7.2, our booleans and enums and so forth. So, that's how all that's constructed. For the most part, Navigator relieves you of having to worry about data type templates, but it's important to know they're there. Sometimes you have to think about how to make the best use of them. So, once we have our basic model here, we have an LN library. This is apart from the standard. And so, this comes with the different models that the LNs are broken up into groups designated by the first letter of the LN, group in our measurement units. In this case, we're going to go back to part 7.4 for the basic logical nodes. We're going to start in this device with an MMXU. MMXU is the most common measurement unit. This is how we provide information about the currents and voltages and so forth in the system. So, to build the file, I just drag this over to my LN, my logical device, I'm sorry, drop it on my LD. I get this dialog box. This brings us up adding new instance of MMXU. We don't have any in our model yet. If we had any in our model, we could reuse the templates and that would show up in a list here, but we don't have that, so we've got to build it from the standard. We can have a prefix, application-specific prefix on the logical node, completely user dependent. The instance attaches to each instance of an LN class. The tool tracks that for us, gets us a one there. By default, we'll get a two if we start to do another one. Again, the analog type definition, we can override the setting we put here. Once we do that, we then bring up a list of all the data objects available in the MMXU. We've pre-selected the ones that are mandatory from the standard, and there aren't many. The MMXU can provide an awful lot of different information. Typically, we're going to see current and phase voltage in one of these, so I'm going to go ahead and select those. That puts those two data objects in our model. Each of those is built from a common data class, and so I could further click into here and I can customize the data attributes that are optional in the common data class. For each of these, I'm going to go ahead and go with the defaults here. We typically go through and be a little more rigorous in selecting those, and then I finish and I have an MMXU type. I'm going to go ahead and throw in one more thing. We've also got some protection equipment in this inverter. We have a PTOB, I believe. This is a protection time over voltage protection unit, so we drop that in. We have the same ability here to customize the optional components of this, and we allow it there. We've got a couple other tools available in here. We're going to stop adding now and look at our examiner device. This is going to encourage me to enter history. I'm going to ignore that for now. We have a tool called model view. This is built up from these data type templates. It can be difficult to visualize what the model will look like online, so this window builds up the model view from those templates, assembles all that, and presents us a view of what a client will see when it connects to this server. Very helpful to the engineer to be able to see easily what's going on there, and we've got navigation pane, so we can go through here. We have a lot of features in the model to navigate back and forth around these templates. As I select an LN, I can quickly navigate to the template. I can find the components and go to make it up, find what I need to do there. If I need any instance data in a model, the model structure comes out of those type templates, but if I need to pre-configure anything in here, I have a tool that adds the instance data here. We have to build out the whole tree, and if I need to come in here and add any information in my model, for instance, a nameplate, I can come in here in my vendor, say this belongs to NREL, and once I fill in a value, then I need to strip all that extra information out, or all the unused fields out, because we have a motivation that SEL is expected to be as compact as possible, so we don't want any additional file size that we don't need, so we can come in here and delete all this unused instance data. So, that makes it very easy to add the pieces we need in here and get rid of the pieces we don't. That's very difficult to do in a text editor. So, the other tool we talked about and read through quickly, the SEL Verifier, this is the tool that just analyzed the file we have here, and it goes through and tells us, this is telling me that I skipped that history indication, the dialogue box that popped up, and I said I wasn't going to do, I should have done it, and it caught me. That's what verifiers for. We've got a couple little warnings down here, because our file is nowhere near complete. It's telling us that I said this device will do reports, but I don't have any report control blocks configured. Same for logging and goose. This is the tool that helps us verify files as they come through. So, that's what that looks like. So, those are the highlights of that. Detail shows a little bit about the model she built with it here in a few minutes. If there's anything else, if you want any more information on any of those tools, please reach out to Triangle. I think we have some questions now. Normally, I would take a break now for questions, but we're good on questions right now, and a little tight on time. So, I'm going to go ahead and switch it over to DP. DP, let's go ahead and share your screen so we can get into your part of the demo. Okay, DP, you should have the screen now. Yeah. Can you see my screen back for the presentation? Great. Hi, everyone. Hi, everyone. Today, I will be presenting a use case where we have used the IEC61850 server that runs on a BeagleBoard to show how we can use that to connect to a PV inverter. Let me go to the next slide. For this setup, we will be using a BeagleBoard microcontroller, which we chose because it has a similar pump factor and compute power as a lot of the PV inverter controllers that are available. And our development environment basically consists of a Linux subsystem on Windows. We'll be using a Linux host. We will also need a cross-compiler and a GCC compiler if you want to compile for the host, and cross-compiler if you want to compile for the BeagleBoard target. And we will be using the TNW's IEC61850 source code library for this application. And we will also use the TestSuite Pro, which will be used to simulate the IEC61850 client or the distribution system operator. And then we will also use the SCL navigator in order to edit the ICD file in order to create a mapping between the server and the ICD file. So, for the TNW source code, you can download this from Triangle Microworks' website. It has instructions on how to unpack and set up the environment. The only requirement is that we do need the GCC compiler to run the examples. And the source code also comes with a set of makefiles for different architectures, like Joel mentioned. So, you can run it on Linux systems, Windows systems, 32-bit, 64-bit, all of that. As long as you use the right makefiles for your application, it should be good. The use case that we will be looking at today is an IEC61850 server. And we will show how you can get voltage and current measurements. And then a simple example where you can send out multiple volt bar curves where you can update it and how the total bar changes based on your curves. There are five steps that we need to go through in order to develop an application. The first one would be the ICD file development. Just opening the SEL Navigator here to go over the ICD file. As Joel mentioned, we're using the SEL Navigator to go over the ICD file. As you can see, for a PV inverter, we have the multiple logical devices that we've added in here. There's one for protection, measurement, for write-through curves, and then also volt bar and frequency watt curves. So, in this example, we're just going to go over the ICD file and how you can update it. So, in this example, we're just going to go over a simple volt bar curve and how we can do the mapping for this. We use something called the S address in order to map between the ICD file and the server code that we would be developing. So, you can go into the volt bar curve and then add something with a prefix. So, I'm using the prefix FA in this case. You can use any other prefix. You just have to make sure that it maps to the source code as well. And once you have that set here, you can then go to the next step where you would define these S addresses within the source code itself. So, in our case here, we're actually calling it the float array. So, the volt bar curves are just arrays that have floating point numbers. So, we can define them in here. And the third step would be how to actually map the data type that you just defined in here. For that step, we would be looking at – there's something called a demo or primitive XML file where you can define newer data types that you would be using. So, here we are defining what the float array would do. You need to define five different handlers for any new data type that you declare. The first one would be the read handler, which basically needs – which gets triggered every time a read instruction is done on the particular parameter. And then there is the write handler, which handles any write instructions that the parameter receives. Then there is the attribute handler, which just tells it what type of an attribute this is. And evaluation handler, which lets you validate if it's the right value and within bounds. And then you have an index handler, which is basically just used to index the S addresses in your source code. So, once you define that within the primitive.xml file, you can then define the actual source code for all of these handlers within the file itself. So, I have a really simple handler for the float array since I only have Boltzware in here. Just showing an example where this function gets called when a value gets returned to the Boltzware curves. So, you can see that we can get the index from the pointer to index handler. And then we should be able to get the Boltzware curves as well. I'm just printing out what the current Boltzware curves are. And once you have that value, you can use that value to perform your application within your code. Over here, I'm just changing the global bar value, which is just the bar value for this based on the curves that I get. I just check to see the x for the voltage is within a certain bound. And if it is, I multiply that by the bar value. And this is what I do. This just shows another example that is available where you can take a look at that. Once we have the application ready, we would need to compile the code for the host. And then you can cross compile it for the microcontroller as well. If you are going to be using the host for this, you would just use your GCC compiler. If you would like to cross compile it for the microcontroller, you would just use the If you would like to cross compile it for an ARM architecture microcontroller, you can use the GNU ARM compiler for this. So the compiler makefiles come with the option to choose either of these so that we don't have to go and select them ourselves. This is an example of the compiler makefile that we have in here that comes shipped with the source code library itself, where you can go in and edit if you're going to be using a compiler or cross compiler if you want to use GCC or if you want to use the Clang compiler. And you can also add paths to other libraries that your application needs in this makefile. For example, if you want to see mq, json, anything, all of those libraries can be added in here. So this takes care of compiling the source code and then integrating that with the application code that you have. So once that is done, we can move on to actually launching the code on the host or the microcontroller. The one thing to keep in mind is we will be using networking ports, so it is good to have root privileges so you can have access to the Ethernet ports on the device itself. Let me go over this. Over here, I just have the tester pro that is connected to the server code where you can go in here and try to connect it. And then within the advanced client, if you go in there, it should show you what is connected and not. And you can then select the inverter function, which is where the volt bar curves actually live, and you have the ability to change these curves. So, for example, I can go in and change one of my curves to have a different value, which should change the bar value in here. I apologize for my computer being a little bit slow. So you can see how the bar value basically changes based on updating the curves. When our voltage is 120, we're within the range of between 0.93 and 0.95. And then when it falls into that, we just use that as a multiplier and we can change this. So we are using the client server architecture in order to do this. And I think later, once we would cover the other messaging patterns that we can use. But overall, you can also do other things with what we have in here. I'm also going to show an example of what is running in here. So you can see my volt bar curve basically updates with whatever value I give it, initially with 0.1. And you can see what happens to the bar. All of that gets updated on the server side, and that gets sent back to the reporting. So the reporting basically tells you what is the latest value when there is a change, and it also tells you if everything is within bounds, and it is also timestamped. So you can use that in your application as well. And that's it. So you can launch your applications and try this out. Any questions? Thank you. Thank you, DP. Right now, we're good on questions and caught up there. So I'm going to go ahead and change the presenter back over to Kumar. So you can do your technical wrap up and have some closing comments. Thank you, DP. Thanks. Can you see my screen? Yeah, we're good. OK, sounds good. Yeah. So these are our other webinars that we gave in IEEE Smart Cities and IEEE Smart Grid newsletter, and we have a technical report on this, which is available in open source. So please feel free to go through these technical reports and newsletters, and let us know if there are any questions. OK, yeah. And if you can keep it on this slide, Kumar, I can just wrap up with some of the final thoughts here. So this webinar will be posted on trianglemicroworks.com. It'll be there on the homepage in about a week. We also do publish it on our YouTube channel. Everyone that attended will get a follow-up email with a link to this recording. You can also find the download evaluation licenses on trianglemicroworks.com for the tools you saw here today for Navigator as well as TestSuite Pro. And then Kumar, I don't know if you want to spend a couple of minutes on just talking about if you want to collaborate with NREL going forward for future projects. I know our contact information is here. Yeah, I do want to talk 30 to 40 seconds on the follow-on work. I think PV inverters are definitely evolving every day, and the next generation of PV inverters, I think they'll have grid farming capabilities. So what we presented here today is based on the inverter that we built three years ago, which is a grid following inverter. But going forward, there's going to be more grid farming inverters, and we need to understand how to have interoperability for them. The needs for grid farming inverters will be different, like exchanging peer-to-peer comms, high-speed comms between grid farming inverters or even grid farming and grid following inverters. So we may have to use other fan-shared communications like sample values or goose messaging to make sure that they understand what's happening in the system. So definitely, there's going to be a lot of follow-on work, and we are more than happy to work with industry members, universities, or anyone interested in this. So please reach out to us. And some of our work are in open source, so we can share part of this, but I think it will have to go through Triangle Microworks. But we are more than happy to share any information that we can on our work to anyone who needs it. Okay. Yeah, and as I mentioned, our contact information is there, Kumar and myself, if anyone is interested in working on some future projects. And this PowerPoint will actually go out to the attendees. Kumar had a slide prior to this one that had several links to different reports and articles and webinars. So you will get those links in your inbox shortly. There are also two handouts that are available to you on this webinar. You should see that in your toolbar. One of those is a product overview of Triangle's products, but there's also a download of the actual technical report. So you do have access to that for joining. So, again, I do appreciate everyone's time. All the information is available on the different websites you're seeing here. We do hope to hear from some of you if you're interested in future projects. I will open it up to questions and answers. If you don't have anything else, Kumar or Deepi, that you want to share. I think we do have one or two questions. I covered what I wanted to cover. So, yeah, we can go into questions. And please feel free to ask more questions if you have. We're more than happy to answer them. Yep. So if anyone wants to start to enter in some questions online, we will be on for a minute or two. I'm going to just read off the first one here and I'll open it up to the group. Is there a possibility to use LNs with the BeagleBone? And I am specifically referring to addition to logical nodes. I think this is a question probably for Deepi or Joel. Yeah, definitely. The model in the BeagleBone is constructive logical devices and logical nodes. Okay. I do have another question here. Historically, have there been any issues with security related to SCADA, such as stuck next malware or similar malware attacks? Have there been concerns related to security or not so much? I'm not sure if this falls a little bit outside of this topic, but I'll open that up to the group. Security is definitely a growing concern. I'm not aware of any malware attacks, such as Stuxnet, but there are the occasional minor, minor, I think I don't know if hacks is the right term. We had a water system in Florida attacked two years ago, I want to say. Of course, we're probably all, at least in the U.S., familiar with the Colonial Pipeline incident this year. So there are a few. We're certainly taking a closer look at it every day. I'm involved with the standard organization that creates the security protocols around 61850, 62351 out of Working Group 15, handles those using TLS and certificates and encryption and authentication, all that's layered on top of 61850. Definitely a lot of concerns, a lot of work going into how to secure these devices, especially once we get into DER. 61850 traditionally is inside the substation where you're counting on an air gap network. DER is bringing those issues to the forefront as we start deploying this out in the field. Kumar, do you think you have any thoughts on that? No, I would have to say some of the security issues are beyond my expertise. So I think you covered it pretty well. Okay. Okay, well, we don't have any other questions coming in, so I think we're ready to wrap everything up. Again, thanks, everyone, for your time. Look for a follow-up email with the recording and the presentation so you can access those links. Please take advantage of the handouts that are available to you and downloading some emails from our website, and look forward to seeing you at the next webinar. Okay, thanks, everyone. Thanks, everybody. Have a good day.


Video ID: hllyZl5LSu8
DNP3 Secure Authentication is an enhancement to the DNP3 protocol. It's a set of additional function codes, objects, rules, and definitions to be added to the standard protocol. It's contained within a supplement to Volume 2 of the specification. It's designed to ensure the receiver of a message can verify that the message was sent by an authorized user and verify that it was not modified in transit. DNP3 experts would say this means it addresses the threats of spoofing, an attacker pretending to be an authorized user, and modification, an attacker changing the message in transit. In addition, DNP3 Secure Authentication mechanism includes features that protect against replay. Replay occurs when an attacker captures messages being transmitted on an open link, for instance a radio system, and plays them back at another time, attempting to cause havoc by making the system do something it wasn't supposed to. To protect against such an attack, DNP3 Secure Authentication includes information in each message that changes each time a message is sent. DNP3 Secure Authentication can also help to protect against the threat of repudiation, that is, a valid user claiming not to have performed an incorrect or illegal operation. Although logging is not part of the specification, the authentication mechanism gives the remote device the necessary information to store a permanent log of who performed what operations, including some degree of repudiation. However, non-repudiation usually requires a digital signature and sometimes a third party. Secure Authentication uses a MAC, not a digital signature. A signature requires PKI, so that only one user knows the key and could have a signed message. For these reasons, PSK systems cannot claim non-repudiation because two instances know the key at the same time. It's also important to understand what DNP3 Secure Authentication does not attempt to do. The mechanism does not encrypt the message. This may surprise some people who assume that all electronic security involves scrambling the message. It was decided by the DNP3 Technical Committee on the advice of utilities that it was not necessary to protect the message against eavesdropping, an attacker reading what data was in the message. Encryption requires significant processing power on the part of the transmitting and receiving devices, and this cost was not considered worth the benefit, except for encrypting the keys that are downloaded to the outstation. However, DNP3 Secure Authentication does play well with encryption methods, such as TLS, on IP networks. DNP3 Secure Authentication also does not protect against traffic analysis, the ability of an attacker to determine what a utility is doing with a remote device by watching the pattern of message traffic. Lastly, it does not protect against denial of service, that is, an attacker transmitting so many valid or invalid messages at a remote device that it's made unavailable to valid users or it's brought to its knees. Denial of service attacks can rarely be addressed within the receiving device itself and are best dealt with by an intervening device like a proxy server or firewall that can filter unwanted messages. DNP3 Secure Authentication was designed with the following principles in mind. Authentication only. It addresses authentication only, not encryption or other security measures, as discussed already. It does not rule out the possibility of such measures being added to DNP3 later or through the use of external measures such as a bump in the wire link encryptor or TLS when used over an IP network. Application layer only. DNP3 must be used over a variety of different physical networks and may be bridged from one to the other, as in the case of a TCP IP terminal server or IP radio. Only authentication at the application layer will ensure end-to-end security. Application layer authentication also permits the possibility of protection against rogue applications that may be co-resonant with the DNP3 application and attempt to use the DNP3 link without authorization. Based on standards, DNP3 Secure Authentication follows the IEC 62351-5 specification, which also applies to the IEC 60870-5 family of protocols. The IEC 62351-5 makes use of various ISO, IETF, and NIST standards so that the security technology used has been well proven. The only new portion is its application to DNP3. Bi-directional. Either the master or the outstation can authenticate messages. Challenge reply. It is based on the common security concept of challenge and response. The device performing the authentication challenges the sender by providing data that must be included in the authentication calculation. This makes spoofing much more difficult. It also means the receiving device also determines which messages must be authenticated. Pre-shared keys. DNP3 Secure Authentication version 2 assumed the sender and receiver previously shared a cryptographic key, or string of numbers, through some method other than DNP3. DNP3 Secure Authentication version 5 provides a method to permit these pre-shared keys to be changed remotely and securely. Backwards tolerance. The specification attempts to make it possible for a non-secure device to identify that a secure device is using a portion of the protocol it does not recognize without an authentication message causing the non-secure device to fail. This principle is dependent on how well the DNP3 implementation on the non-secure device handles unexpected messages. Upgradable. Security technology is always changing, so the mechanism permits the sender and receiver to identify which algorithms, data, and key sizes that they are using and change these in the future. The specification also requires that any implementation support a mode in which security is turned off on a per-device basis for upgrading purposes. Multiple users. It assumes that there may be multiple users of the system located at the site of the master. It provides a method to authenticate each of the users separately from each other and from the master itself. The intent of this principle is to permit the outstation to conclusively identify the individual user, not just the device, that transmits any protocol message. The DNP3 secure authentication mechanism is based on the concept of a cryptographic hash. A hash is a function like a cyclic redundancy check, or CRC, or a checksum, that when performed on a message produces a much smaller string of numbers. The smaller hash value is very sensitive to changes in the message, but it is virtually impossible to determine the original message or the key if all you have is the hash value. Let's assume Alice is trying to send a message to Bob in a way that Bob can be sure the message is authentic. To make it work, Alice and Bob must have previously shared a key or a string of numbers like a password that only the two of them know. Alice performs the hash function on the message she wants to send concatenated with the key. This produces what is called a Message Authentication Code, or MAC. DNP3 SAV5 and later also support MAC calculation methods that are not technically hashes, but the effect is the same. Alice sends the original message and the MAC value to Bob. She does not send the key because it could be seen by an attacker. However, the message is not encrypted in this case. The attacker can see what the message is doing, but as we shall see, cannot modify it or send a false message of the attacker's own. Bob receives the message. Since he already has a copy of the key, he can now duplicate Alice's calculation. He hashes the message and the key together to produce a MAC. If Bob's MAC value matches the value that Alice transmitted with the message, he knows two things. First, the message has not been tampered with. If an attacker had tampered with the message, Bob's calculation would have been on a different message than Alice used, and therefore the MAC would have been different. The MAC calculation is carefully designed so that without knowing the key, an attacker could not modify the message in such a way that would produce the correct MAC. Secondly, Bob knows that the message came from Alice, or at least someone who knew the pre-shared key. Although the key was not transmitted on the link, it was intrinsic to the calculation, and without it, an attacker could not produce a matching MAC. Because of the way the calculation works, it is nearly impossible to determine the key from the MAC. When a hash is used with a key in this manner, it is known as a Message Authentication Code, or MAC. The mandatory hash function used for DNP3 secure authentication is defined by the National Institute of Science and Technology, or NIST, the Internet Engineering Task Force, or IETF, and the International Standards Organization, or ISO. It is known as the Secure Hash Algorithm, or SHA, or SHA. Two varieties of this algorithm, SHA-1 and SHA-256, may be used in DNP3 secure authentication. This diagram illustrates how IEC 62351 Part 5 provides a common standards basis for SCADA security worldwide. DNP3 secure authentication specification is compliant with IEC 62351-5, and the IEC 6070-5-101 and 6070-104 standards will also be compliant with it. IEC 62351-5 is, in turn, based on International Standards Organization, Internet Engineering Task Force, and U.S. National Institute of Standards and Technology standards. DNP3 is challenging to secure because it can be used in a variety of networks, including radio systems, serial links, and IP-based wide-area networks. Furthermore, it's designed so that from one end of the network to the other, it may travel over more than one of these links. For this reason, DNP3 secure authentication is included in the topmost of the OSI layers, the application layer. There are three types of security that are commonly deployed in communication networks today. Site-to-site security includes the use of virtual private network, or VPN, routers and protocols such as IPsec to secure the link between two locations, for example, a corporate office and a home office, or a master station and a substation. It does not secure the networks at those two locations, and physical security measures like locks and guards are necessary to protect them. Device-to-device security includes the use of protocols such as transport layer security, or TLS, to secure the complete TCP connection between two devices, similar to when you access your bank through the Internet. However, TLS only works on IP networks and is therefore lost if DNP3 messages are forwarded over radios or serial links. It also does not address the possibility that rogue software applications may be installed on a device, making use of the fact that the device itself is considered secure. Application-to-application security ensures that individual users, not just devices, are authenticated by the remote devices and that the authentication information will be carried wherever the DNP3 message travels. It permits remote outstations to perform rule-based authentication and authorization so that the level of security changes depending on who is attempting to perform an operation. This diagram illustrates how DNP3 secure authentication may be used over a variety of networks. When used over the serial links or radio systems, the authentication function codes and objects are carried like any other DNP3 traffic. When used over IP networks, there are three different options. First, TCP with authentication only, recommended for use over wide area networks and mixed networks. Second, TCP with authentication and encryption, using transport layer security, recommended if messages must travel over a very insecure network, such as the Internet. And third, UDP with authentication only, used under specific circumstances when TCP cannot be used and the network is guaranteed to be reliable. Because the DNP3 authentication messages travel at the application layer, they are carried with the rest of the DNP3 message when terminal servers or IP-based radios are used. Some utilities are using these devices to avoid using routable protocols in some portions of their SCADA networks. Using DNP3 secure authentication ensures that there is a security solution even when terminal servers or IP radios are used. One advantage of DNP3 secure authentication is that it allows utilities to deploy DNP3 in a secure fashion over IP networks. Some utilities may be missing the benefits of deploying DNP3 over IP because they are concerned they will be required to deploy very expensive security measures if they do so. DNP3 secure authentication provides security in IP networks and thus provides an evolution path to other IP-based automation solutions, such as IEC 61850, in the future. Some vendors may permit such an evolution as a firmware change if the IP networking hardware is already in place. DNP3 secure authentication is suited for the following types of applications. New deployments. Because this is in addition to the DNP3 protocol itself, it will require new software or firmware at both ends of the connection. Therefore, it may be most convenient to deploy DNP3 secure authentication in new networks and investigate other solutions for legacy systems. Unprotected radios. Radiosystems with serial link inputs that are not authenticated or encrypted represent one of the most vulnerable types of DNP3 deployments. Deploying DNP3 secure authentication permits these networks to be secured without hardware changes. Multiple users. In networks like the data concentrator system pictured here, DNP3 secure authentication permits a separate identifier to be assigned to each user at the master, for example, Alice or Bob, so that the operations performed by that user are tracked throughout the system. Data concentrators. The specification has rules defined to ensure end-to-end security even through a data concentrator such as the configuration pictured here. Varying protocol suites. As discussed previously, DNP3 secure authentication operates at the uppermost protocol layer and is therefore carried across networks that use different lower layer protocols. Next, IP and serial. As discussed previously, DNP3 secure authentication works well with terminal servers and IP-based radios. This slide shows what's addressed by DNP3 secure authentication. It addresses integrity, spoofing, man-in-the-middle, replay, authentication, authorization, message integrity, and role-based access. Spoofing is addressed only to the extent of securing cryptographic keys. The question that many utilities ask is, where does DNP3 secure authentication fit with respect to the NERC SIPs? Firstly, as of this writing, July 2014, the mandate of NERC applies only in bulk electrical system, that is, transmission. So in theory, distribution networks, where many DNP3 deployments are found, are not affected by the SIPs by definition. This may change in the future, however. Secondly, some people make the argument that DNP3 is not a routable protocol and therefore does not need to be secured. That may be true of DNP3 over serial links, but it is certainly not true of DNP3 over IP because IP, by definition, is a routable protocol. It's not clear how it applies when DNP3 is transferred from an IP network to a serial or radio network. Thirdly, some people make the argument that electronic security controls are not required for SCADA or data traffic, only for remote login to a device. Their argument is that the term external interactive access means remote login. It remains to be seen whether this position is defendable. Lastly, the location that DNP3 secure authentication may be deployed may vary depending on how your organization chooses to define its critical cyber assets and electronic security parameter. This in turn may depend on how much power may be switched or shed on a given network. Having considered all these items, it may be that your organization chooses to define its electronic security parameter in such a way that some of its access points carry DNP3 traffic. In these cases, DNP3 secure authentication is intended to provide a technical control to ensure the authenticity of the accessing party. DNP3 authentication version 5 improves on version 2 in several ways. It adds the ability to remotely change keys on all outstations without sending personnel to the site, reducing the cost incurred when an operator leaves the organization or a key is compromised. It adds the ability to control management of keys from a central authority using either pre-shared symmetric keys or security certificates using asymmetric keys, also known as public key encryption or PKI. The symmetric method is the mandatory default when asymmetric methods are optional. It provides better protection against denial-of-service attacks by continuing to operate as normally as possible even when suspicious events are encountered. It provides a simple means for detecting some types of attacks by requiring outstations to track and report statistics on the operation of the protocol. It supports cryptographic algorithms that were not available in the earlier version and changes some default algorithms because the older versions are no longer as secure. For instance, AES-256 is now the default for encryption instead of AES-128 and SHA-1 is discouraged with SHA-256 being the default for hashing. SAV5 also supports the newer, less processor-intensive GMAC algorithm for calculating MACs.


Video ID: b-cLy5knoBs
In this video, we'll be focusing on TestSuite Pro's ability to locate and organize data with the Data Miner, and various ways that you can display that data and the sequence of changes that happen during an event using the Logic Analyzer and custom displays. The Data Miner tool provides a configurable grid of key processed data objects from any IED in your workspace. These data objects can be grouped and sorted, or filtered by any of the columns listed at the top. You can group by grabbing a column header and dragging it into the group by section. You can remove this grouping by mousing over the header in the group by section and clicking the X. You can also sort the data alphabetically by clicking on the header. You can do reverse alphabetically by clicking again, and one more click will return us to the default. And if you're only interested in certain types of data, you can click the filter icon in the header, select the type you're interested in, and click filter. Now you'll only see that type of data in the Data Miner. To remove this, go back to the now red icon, and click clear filter. Using these three methods, you can quickly get down to the information you're most interested in. Once organized in this manner, you can save these as presets to quickly get back to this filtering option. We have also provided users with a few TMW presets, like the goose data preset, which will sort by data objects used in goose, and measure and values. Once you have data organized in a way you're interested in, you can then drag them over to the displays, which we'll see next. In the Logic Analyzer, we can drag values over from the Data Miner, and track the signals directly via goose and reporting. Let's go back to the Data Miner and get some values involved with a fault on a breaker line. I've made a preset to speed this along for the sake of this video. Since I'm interested in line 1, I'm going to go to those IEDs, and open them. Then I'm going to grab values associated with a fault, specifically looking at pdif, ptrc, xcbrposition, and the recloser. I'm going to drag these over to the Logic Analyzer. Now you can see that the Logic Analyzer has four graphs for the values I drag over. Now that we have the Logic Analyzer set up, let's start recording. I'm going to trigger a fault on the line from our IOP 2017 demo in DTM. Once these events have come through, we can stop recording and investigate what happened. I'm going to zoom in a bit, so that way we can see this event in more detail. If you're interested in specific values, you can click the cursor and bring it to your current time. You can also move this cursor around to see where the nearest event is and what the value was at the time. Following the logical flow events, you can see the pdif that was sent out by our simulation, which caused the trip signal and ptrc to be published. This caused the breaker position to open as seen here. Afterwards, the recloser sent a signal, causing the breaker to return to the original closed position. Now that we've seen these values in the Logic Analyzer, let's send some to the custom display. To send signals from the Logic Analyzer to the custom display, you select the signals that you're interested in sending, go to Options, Send Signals, Selected Signals to the custom display. Then you can organize them as you see fit. I'm also interested in creating a single line diagram. As before, I've already created a preset that will help me set this up, and I will grab all of the data objects in line 1bc and drag them over to the display. These tiles can be used to make the single line diagram. I will also add a bus bar and a load, and rotate them into their proper orientation. This time, I'm going to trigger a permanent fault in my simulated substation so that you can see the values change in the single line diagram. Note the event counters will increment each time the corresponding data attribute changes. This can be cleared before new tests to catch transient events. You can also right-click on these tiles for their history of all their changes. You'll also see the breaker open and close and open again since the fault did not clear. The data miner tool provided a configurable selection of data objects which was grouped, sorted, and filtered. These data objects were then dragged to the logic analyzer and custom display. The logic analyzer was used to chart real-time values from the IEDs. This allowed for us to monitor and troubleshoot the behavior of the IEDs in the substation. A custom display was created to view the available data from goose and reports in a single one line diagram. If you'd like more information on how I generated the goose signals that were displayed in the logic analyzer or triggered the permanent fault for the custom display, check out the link in the description below or the I in the top right of the video. Thank you.


Video ID: UpjJxrEVHrI
In this video, we're going to be going over the fundamentals of DTM, Distributed Test Manager, by Triangle Microworks. The first thing I'm going to talk about today is how the DTM network is set up. This includes master and slave DTM machines, the DTM services, and how licensing works for DTM. So the first thing you want to do is when you launch DTM, you should have DTM Admin, which is this GUI you're seeing right here, launch. And either the services, DTControl, DTManager, and DTHost, launch either as services in the background or as windowed applications. To configure each type of computer, you'll want to go to Tools, Configure DTM Services. That will bring up the DTM services configuration window. On your master, you'll want to have this radio button clicked. That will let this machine know that it is the master for the DTM network. There are some other optional configurations that you can click here, but I would recommend that for more advanced users once you're more comfortable. If you are trying to connect a slave machine for the DTM network, you'll want to click this radio button here and put in the name or IP address of where the master is sitting and make sure they are able to communicate. The second thing you'll want to look at is the second tab, the Adapters tab. This is where your DTM network traffic will go across. There is a small amount of information that is required to be sent back and forth between master and slave machines and they will communicate on this adapter. If you click the dropdown for this, you'll see all the adapters that your local machine has. Some people like for testing purposes, if you just have a trial or something for DTM, you could just use the loopback, but if you're planning on talking to any slave machines out there, you'll want to use one of your other IP addresses. The last two tabs are something you can look into once you're a little more comfortable. The Services tab is what allows you to run the applications as windowed applications, so like this is DTControl running as a windowed application. Once everything is set up and you're happy with how it is, you can run them as services in the background. The last tab is for logging information. It allows you to decide where you'd like to keep the DTM logs and how long to keep logs before they're deleted. This is really only necessary if you run into an issue or you need our support to help you troubleshoot something. The only other thing that I'm going to mention in this part of the video is the licensing. So if you want to see licensing, you can go to Help, Manage License. It will inform you that in order to look at the license, you may have to restart your DTM services. So only the DTM master needs to be licensed. All of the slaves work as additional resources for you to use for your DTM network. We use Sentinel SafeNet, so you'll see this window brought up. There are many different things that you can configure with your license, but the most important is how many DTM connections you can have. This is how many devices that you can simulate, and depending on if you have our legacy license or one of our new 1.3 license, you can either get all protocols or individually license different protocols. Other options can include things like DNP secure authentication.


Video ID: aQ3UPT_w6J4
Welcome to this overview of IEC 61850. I'm your instructor, George Schimmel. I've been active for over 15 years in these areas revolving around IEC 61850. I was active in the original EPRI-sponsored UCA initiative, which led to much of the underlying design work behind this. I've been active in various IEC technical committees, IEC TC57 working groups, which defined 61850, including I was the convener of TC57 working group 11, which is the station bus. I've been a member of now the TC57 working group 10, which is now the place where these documents are published from. I was also the software architect behind the Triangle Microworks IEC 61850, GOOSE, GSSE, and ICCP source code libraries.


Video ID: XQeVvSDpQ54
One of the features of Anvil is that it supports negative testing, or returning results other than those that are expected, or even invalid data. For example, Anvil can force the return value of a control command. We will demonstrate that feature in this video. To do this, we want to expand the model to find a control. Right-click on the control and select Configure Control Errors. And that opens the Control Errors dialog. In the Control Errors dialog, we see blocks for the three functions that make up a control operation, Select, Operate, and Cancel. On the left side, for each of these functions, we can set the MMS return value. So for example, in an Operate, we can force the return code. Let's set it to IOHardwareFault. And on the right side, we can send the last application error message, or send a command termination message. Both of these have the option to set a trigger value, or a check value. I'm going to leave them at no check, but let's set an application error. Let's set OperatorTestField, and let's add a cause of BlockedBySwitchingHierarchy. So now we're going to force this error when the Operate command is received, and we say OK. To test this, we go to our 61850 client. In our example, we're going to use Hammer. So we'll come into Hammer, and we will send the Select, and then we'll send the Operate. And since I set those fail conditions, we would expect this Operate to fail. Sure enough, it does. Operation failed with an error of IOHardwareFault, just like we set. And then there's an additional cause, BlockedBySwitchingHierarchy. Now if we go back to Anvil, we also notice that Anvil issued an error message indicating that the operation failed. As you can see, it's very easy in Anvil to force an error response to a control message. So that's just one example of using Anvil for negative testing.


Video ID: roNY0fHfWvE
This is the second video to the fundamentals of DTM for 1.3. In this video I'm going to be going over workspaces, how to save and edit them, and how to share them with other computers. I'm going to go ahead and load a workspace I've been previously working on just to fill it out a little bit and give you some more details. To do this I go to File, Load Workspace, and choose the workspace I'd like to load. Now if this is the first time that you've installed DTM or you haven't saved any workspaces previously, you will only see our sample workspaces in here. Of course you can load them up, play around with them, and learn from that as well. But I'm going to load this one. The first thing that's going to happen is that it's going to populate the workspace with all the devices that were simulated the last time you were working on this specific workspace. You can see this over in the Workspace tab. Very important indicators to pay attention to in the Workspace tab are these Wi-Fi symbols. If it is orange, that means that there is a device or devices beneath that node that are not connected. Whereas, like the DNPMasters folder, if it is green, that means all devices within that node and underneath it are connected to a respective partner. So you can see here that all three of these MDNPs are green as well, and that propagates up to the folder. Other things that could be in the Workspace tab are scripts, displays, commands, and different types of data changes. Pretty much anything you'd want to interact with is going to be in the Workspace tab. Another tab is the Resource tab. This is where all of the configuration information or other resources are stored for the workspace. This can be CSV files, displays, ICD files, any sort of SEL file, XML files, anything along that lines can be stored here. If you ever need to update them, you can right-click and update the resource and find the new updated ICD file or resource from anywhere on your drive. I currently have in the middle section here in this tile a points list of our M104 device. This is just so I can monitor some points to make sure they're changing in the ways that I expect and make sure that my test results are correct. You can have multiple tabs up here as well. I only have one as of now. The next tile to pay attention to is the Network tab. The Network tab will show any machines that are in the DTM network, followed by any hosts that are on that machine. Now if I had a DTM slave machine, it would be listed below and all the hosts that are on that machine would be listed there as well. Currently all I have is the master and the hosts on the individual master. Now on each host, you can open them up as well and see any of the devices that are being simulated on that host. For very large workspaces, we recommend distributing hosts across different machines and devices across multiple hosts. The output is by default down here at the bottom. This allows you to see any of the changes that have happened on the workspace in chronological order. Here you can see that devices are being set up and connected. Once you've made any changes to the workspace and you're happy with the results, you can save your workspace by going to File, Save Workspace. That will overwrite what I just opened. Alternatively, if you don't want to overwrite it, you could do a Save As feature and just rename it. If you want to share a workspace between multiple computers that aren't on the same DTM network, you can do File, Export Workspace, and save it to a location. This will save it as a zip file that you can share with another computer. Once you get to the new computer, you can go to File, Import Workspace, and choose the file that you just loaded off. If you already have a workspace with that name, we'll ask you, are you sure you want to overwrite it? For this, I'll say yes. Then, as before, it will go ahead and populate the workspace. All the hosts will be brought up if you have those options set, and you'll be good to start working on your workspace.


Video ID: Vv8yC7F-xsM
This video describes how to use the 61850 test suite to send and receive Goose messages. In the 61850 test suite, Anvil is always the Goose publisher and Hammer is the Goose subscriber. So here I have Anvil running and the first thing I need to do is make sure I have a Goose adapter configured. So I go to Options, Configure, Servers, select the server I'm running and scroll down to the Ethernet adapter section and look at the Goose adapter. Now this adapter needs to be in most cases a physical Ethernet port. You most likely don't want to run Goose over a wireless connection and usually can't use a virtual port. So in my case the correct port to use would be the Intel Pro 1000 MT network connection. While you're in the configuration option, you probably want to look under Application and look at the Ethernet adapter and set the default adapter here as well. That way when you create new servers they will use this default and you don't have to set it with each server. Now I say OK. If I changed the Goose adapter, the server has to be restarted in order for the change to take effect. So I'll go ahead and demonstrate this even though I didn't actually change it. We would simply close the server and then restart it. Also in Anvil, we can look at the Goose control section and notice the min time and max time. In order to set the min time and max time, we need to disable the Goose control blocks. Now they show up as editable. When there are no status changes, messages will begin being sent at min time and they will decay down to max time. So let's start at say 10 milliseconds and decay down to 5 minutes. So Anvil is now configured to send Goose messages. So let's look at the Hammer configuration. So here I have Hammer running and connected to Anvil. Like Anvil, I need to check the status of my Goose adapter. I'm in this server and again we want to look at the Goose adapter. In this case, mine was already set up for this network and we also can look under application and set the default adapter so that in the future we'll default to the correct adapter. Also, if Hammer and Anvil are running on the same machine, we need to look at the loopback. For Hammer, the Goose loopback setting should be true. For Anvil, the Goose loopback setting should be false. Those are the default values, but it's probably worth taking a look to verify that they're still correct. So false in Anvil and true in Hammer. Going back to Hammer, once again, if I changed that adapter, I would need to restart the client. So I'm going to close the client connection and then I'm going to restart it. So now Hammer is connected to Goose. If we go look at the Goose control blocks, in order to receive Goose messages, I first need to enable the block and then I need to subscribe. Notice this symbol here for the messages. Right now it is grayed out up here because we have not received any Goose messages, but once I subscribe and we start receiving Goose messages, we'll see this turn this yellow gold color. There we are. So now I can click here and look at my Goose messages. And we see we got a message. Now we can generate more messages by coming into Anvil and going to the simulation pane and starting simulation. Anvil generates Goose messages based on data change. Now in a real-world application, not all data changes may produce Goose messages, but Anvil, being a test server, does this so that it's easy to generate Goose messages. So now if we go back to Hammer, we can see lots of Goose messages being sent. So as you can see, it's fairly simple to send and receive Goose messages using Hammer and Anvil.


Video ID: GHidvA6CDsQ
Anvil has several features to support negative testing, that is, returning invalid or unexpected values so you can test how the client responds. In this video, we're going to give an example of forcing read or write errors. So to do that, I'm going to expand the object model and find a point that I want to change. So I'm going to select the stval for a pause, and I'm going to change the read response and so instead of returning success, I'm going to return type inconsistent. That's in this TMW specific properties here. So now when my client, in this example hammer, comes down to that same value and reads it, we see we return an error and it's type inconsistent. Just as another example, let's change it to object undefined, and again we'll go read the value from the client and see that Anvil is again returning the error type that we set. As you can see, forcing invalid or unexpected return values from read and write operations is very easy to do from Anvil, which supports your negative testing.


Video ID: adU0cn0-EkU
The following video illustrates using the Conformance Test Module, both with automated test equipment and by manually setting up the required test conditions. This next demo demonstrates the Conformance Test Module of the Communication Protocol Test Harness. The Conformance Test Module is an optional module that automatically performs the conformance test specified by the DNP Tech Committee and approved by the DNP Users Group. This is especially useful now that the Users Group allows you to list a device as self-tested. What we actually recommend is that you use third-party testing every few years, every two to three years, third-party test your device and then in between with every release self-test it to ensure that your device remains compliant. For this demo, we're going to run an excerpt of the test. We're going to run some of the binary input tests and one of the double-bit binary input tests. With this demo, the binary input tests are connected to Omicron CMC156 to automatically generate the input conditions required by the test. So this demonstrates running the test with automated test equipment. For the binary test, we don't have automated test equipment connected. To run the test, start by resetting the state in case any tests have previously been run and then simply select Execute. The test begins and you notice in the right-hand pane it shows the tests that are running and shows a green band as each test passes or it would show a red band if the test failed. This is a fun test to run in person because you can hear the Omicron fire up and the relay start clicking. I'm going to let you hear a little bit of that. So what's happening is various input conditions need to be set up to run the test and the test harness is automatically configuring those input conditions through the Omicron CMC156. So the binary input tests have completed now and we've gone to the double-bit input test. As I mentioned with the double-bit input test, we do not have this connected to a piece of automated test equipment. So since the test harness can't automatically run that test, what it does is it issues a dialog box like this indicating the conditions that you need to set up in order to run the test. So if you don't have automated test equipment, you can still run the test. It's a little slower than if you have full automation but still much easier than any other way to set up and run these tests. I'm simply going to cancel the test here and when I do, this test is going to fail. Notice we have a red check mark here and the double-bit input change test failed. We also got an orange band warning us that the tests were canceled and the final bar shows the result of all of the tests and in this case, the combined test failed because the last test failed. Now if we go look at the main window of the test harness, we can look in the protocol analyzer log and notice that it shows is each step occurring, what happens. It shows the communications that occur as a result of that step and it indicates the result of the test. At the end of your test, you can save the log. We recommend using file, save protocol analyzer log as XML. If you select this, then a little browser pops up and you have the opportunity to choose where you want to save your log file. The advantage of saving the log as an XML file is that it can be loaded back into the test harness later and then the filters still apply to this loaded file so you can drill down and look at additional layers or turn them off and look at fewer layers.


Video ID: iFVxpV1nFi8
In this video, we're going to discuss how you can manage your DNP3 device profiles and your IEC 61850 to DNP3 mapping files using DNP3 Forge. DNP3 Forge is a free tool to manage these functions, but also has a couple of add-ons to provide even more functionality, for example, scripting and the ability to export CSV files. So here I have DNP3 Forge, and the first thing I'm going to do is show how to create and edit a DNP3 XML device profile. To create a new file, we go to File, New File, and we can choose the device profile from January 2010 or from July 2012. Let's take the latest approved one, the July 2012, and it opens up a profile. Now within the tree here, I can click on any value and get more information over in the Properties pane. If I drill down to the lowest level, then I get a property grid where I can fill out the information. So in this case, I'm creating an outstation profile. I can specify the vendor name, the device name, hardware version, software version, and so forth. So let's go make a few changes. Maybe we want to change the data link layer address. This would be a really bad idea in practice, but I am going to fix my data link layer address at four, and I'm going to never validate addresses. So I can continue on making changes like this. At each phase, notice as I make a change, when I click on the value to change it, I have the property grid over here where I can set the capabilities. I can set the current value, and down here we have a description field that indicates the purpose of this field and indicates a reference to the Triangle Microworks source code library and the TriangleMicroworks.net source code library to show where this value is set in the library. So as you create your device profile, you get pointers of where to make the corresponding changes in the source code library. Let's go down and create a few points. I can come to, say, the binary points, right-click on data points, and add a data point. And I can fill in over here the point index, name, description. I can add another point, and so forth. Let's come down and create a few analog input points as well. So now let's take a look at how we can map between 61.850 and DNP3. To do that, I'll open an ICD file, so I say File, Open File, and load my ICD file. Mapping is easily accomplished by drag and drop, but that's easier if I can see both my ICD file and the device profile at the same time. So I can grab the ICD file and rearrange my display so that the DNP device profile and the ICD file are side-by-side. So now I can expand down into my ICD file and find the point I want to map. So for example, let's look at the phase A current. Since I am mapping this point, I want it to be required, so I will click this checkbox, and then I can just drag the point and drop it on my analog value. Edit Mapping dialog pops up and lets me verify the mapping and shows me what rule is being used. So that mapped the value. I can also require the quality and time and map them as well. And of course, I would do the same thing to map the phase voltage, and I will put them on analog 1. As you can see, it's very easy to map points by drag and drop. I'm going to save this file, then I'll close it and show another way to create device mappings. So here I have the same ICD file I was working with before, and notice I still have checked the values that are required to map. I'm just going to go to Tools, Create New Device Profile by Mapping Selected IEC 61850 Attributes, and it creates the new mapped file. If I go look at my points list, look in my analog points, I'll see that it's created analog 0 and analog 1, shows the points over here, point name, and fills in the description what point it was mapped from in 61850. Now I'm going to save this file and move on to show some additional features of DNP3Forge. I'm going to go to the Tools menu and select the Validation and Test window. This window demonstrates scripting capability of DNP3Forge. DNP3Forge allows you to customize it using Python scripts and includes some predefined scripts to do DNP3 level capability validation. So for example, if I want to take the device profile I just created and validate it against level 4, you see in red any errors for items that are not supported. In orange are places where the device generates responses that are not part of the subset, and then in blue it shows additional responses that are created that are not part of the subset, but that's okay because there is the capability to configure to only return responses that are within the subset. I can create my own scripts by clicking the green plus button and adding a new Python script here. I can also compare device profiles. So let me open up the first profile that I saved off. I can go to Tools, Compare DNP3 Device Profile Documents. I'm going to compare Map 1 to Map 2, and these yellow circles indicate areas where there are differences. So if we drill down, remember I made changes in the data link layer, I had it fixed at 4, turned off address capabilities, so those differences are showing. If I want to set it back, I can match them up and then hit Apply. Now they match, and notice the yellow circle went away. If we drill down and look at the points list, we see the primary difference in the points is that the automatically created file included a description describing where the points were mapped from. DNP3 Forge also has the ability to display the device profile in human readable format. To do that, we go to Tools, View DNP3 Device Document, and it will run the XSLT and provide us the profile that looks like the old document generated device profiles. This is very nice for providing a human readable format, for example, to include in a request for proposal or in a response to a proposal to indicate the capabilities of your device. Another feature of DNP3 Forge is the ability to export CSV files. This is one of the optional features of Forge. To do that, I go Tools, Export Mappings, or Export Data Points. To a CSV file, I'm going to export all my points. I get the ability to define the format of the CSV file. When I say OK, it will create the export, and I'm going to save this off as an exported file. Now I have a workspace in the test harness that's designed to read that file, so if I launch my test harness, and I'm going to load this workspace, configure master from the CSV file. Look in the data window. We see that no points are configured. I'm going to go to the command window and read the configuration from the CSV file. Browse to the file I just saved off. Now we see the data window updates, and I have the points that I created from that mapping. Notice that the description that was in the mapping file comes through and is shown in the description in the test harness. So I hope you found that useful. Once again, DNP3 Forge is a free tool that's available to manage your DNP3 XML device profiles as well as to manage DNP3-61850 mapping files. And then it has the optional components to add additional features through scripting such as the ability to validate a file or the ability to compare two device profiles. And it has another optional feature to import and export CSV files. Please feel free to contact us if you have any additional questions. For sales questions, contact sales at trianglemicroworks.com. For technical questions, contact support at trianglemicroworks.com. Thank you, and we look forward to hearing from you.


Video ID: VZCIbOuRABo
Let's talk about our SCADA data gateway. Our SCADA data gateway, or SDG as I might refer to it later, was first built as a protocol translator almost 20 years ago. It was built with the same source code libraries that we license and sell. Over the course of these 20 years, we've added support for new features and use cases. We've added support for new protocols including IEC 61850, DNP3, ICCP or TASI2, IEC 6870-5-101, 103, and 104, Modbus, OPC Classic, and OPC UA. And today, our SCADA data gateway is used by hundreds of customers all around the world. Recently, we did an enhancement to our SCADA data gateway product as it transitioned from version 4, our Windows application, to version 5.1, our new service and web application. Version 5.1 is now compatible with both Linux and Windows. So let's take some time to look at some similarity between the old version and the new. First, I'd like to point out that if you're familiar with any of the previous versions of our SCADA data gateway, you'll feel familiar in version 5.1. We've designed both the GUI and the workflow to match the previous versions, although we have added additional features to aid in productivity. It's also important to know that you can upgrade from any version of our SCADA data gateway to version 5.1 via the normal installer. To support the new workspace feature, we've added a migration tool to convert your current SUG configuration into a workspace. This tool is included in the installer and will launch automatically. If you have any concerns about this, we have created a video tutorial to walk you through this process. This video is on our website and on our YouTube channel. So now let's talk about enhancements. One of the first features I want to talk about is that the new SCADA data gateway is always running as a service. You no longer have to stop and restart the application in order to reconfigure it. Configuration can be done on the live service via a web browser and a secure connection. We've also added additional security features, including user roles and logins. For example, you can set up a person to act as a viewer, who can view all the data in the SCADA data gateway, while others might have the role of operator or even super user. To support this feature further, we've added in audit logs. This will allow you to see who is logged in, when they were logged in, and what else is going on in your system. We've also added features to streamline the configuration process, and this includes the ability to create multiple points and create multiple mappings at once, and the ability to save and load different configurations just with a click of a button. This allows you more flexibility when you're working on multiple configurations at a time. A few more improvements for our SCADA data gateway can be seen on this slide. We've added in new special views to our GUI to allow for checking of errors and warnings. You can also check in the overall health of our SDG, or check on system and protocol statistics. We've also added a system information bar, which is always visible, to allow the users to quickly make sure the system is up and running and how many resources the host computer is using. You might see performance issue if either your CPU or RAM is close to 100%. We've also added a way to navigate around your logging system easier. You can filter down directly to the information you're interested in viewing. And my favorite feature is that the mapping information is now in line with the point itself. This allows you to quickly see what is mapped to where without needing to navigate to another window pane. Now that our SCADA data gateway can be configured remotely, it raises some questions about networking and security, so let's address some of those. Our SCADA data gateway does not require access to the internet, and it can be configured via a secure local network connection, and can be isolated for security purposes. This includes limiting configuration to a specific IP address or a list of IP addresses. Our SCADA data gateway communicates over HTTPS and is completely encrypted. If internet access or configuration is a requirement, we suggest getting a third-party CA certificate, and if not, SEG also supports self-signed certificates. Our user guide can help you through how to update these certificates. Alright this will be the last slide we look at before we look at the demonstration. For this video, I'll be finishing up the configuration of a system that looks like this. Our SCADA data gateway is in the middle here in blue, and the rest of the network can be simulated by DTM. DTM is our system simulator tool, and will be a stand-in for both our IEDs and our SCADA master. DTM has been designed specifically to test gateways, data concentrators, and protocol translators, and this makes it a great fit for demonstrating our new SCADA data gateway. If you are interested in the automated testing that DTM can provide, we do have videos on our website explaining this automation testing further. So let's go ahead and take a look at our applications today. The very first one we'll be looking at is obviously our SCADA data gateway. On the top here, you'll see our configuration view. That's where you're going to see the devices that we have, so we currently have a 104 database that's going to be supplied to our SCADA master, and a 61850 client, which is getting information from a 61850 server. In this middle section here, you'll see where all of the data points are going to be at, and this includes things like action masks, how we're getting the values, but we'll get into that a little bit later. At the bottom, you'll see our logging. This is where you can see everything going on on our system live as it's coming through. In the background, I have my DTM tool. That's our system simulator tool. So here's the 61850 server in the IEDs folder that we're connecting up to, and we also have a DNP3 server here, which is an outstation that we're not connected to. I also have my SCADA master here that's ready to receive the values being sent out of my gateway. So we need to get the values out of my DNP3 outstation and into my gateway. So we can do that by adding a device, go to the root node, right click, go down to TCP channel master, and then we'll go ahead and add this master. Now if you have any questions about the configuration that you're currently working on, you can mouse over the node that you're interested in, and that will give you some information on that specific node. If you need more than that, if you see a blue question mark, you can click that on any window and get full information on each of the things that you'll be looking at on that window. I'll go ahead and hide these away. I'll go ahead and select a DNP3 device, mdnp3, and I know that I am working with DTM and our SCADA data gateway on the same machine, but you can provide IP addresses. So for example, the remote address that I'll be interacting with today is 192.168.1.210, I could leave this at 0.0.0.0, that just means the gateway can use any network adapter and IP address that it has to access that device. Or I could come in here and give it a unique IP address as well. From here I just need to give it a name, I'll just name it masterdnp, and hit OK. At this point we'll get to our second window when it comes to configuration. You can come in here and customize things like local link addresses, we can look at the different timeouts, retransmission rates, application layer things, or by default you can enable or disable unsolicited event class polling. There's also things like authentication which you could come in here and edit as well, and an advanced area that you might need to look into in the future. Luckily for this video I'm just using the default everything and the defaults for the protocol are automatically put in here, so I can just hit OK. Then it's going to ask me what type of data attribute do I want to be able to add to this device. Now, if I click this drop down you'll see that there's a bunch of different options in here, and I'm only going to be able to select one right now. But don't worry, we'll be able to add more of them later. So it doesn't matter, I'll just add binary inputs. So now I've created a masterdnp data device inside of my SCADA data gateway, and I can connect up to the outstation connected in my DTM. But if I look inside my binary inputs I don't have any, so let's go ahead and start adding data points. I'm going to start from the smallest and work to the largest. So we can do one at a time by right clicking and doing add MDO. MDO stands for master data object, and this is a master database, so we can click that. And I know I'm going to start at zero, because if I start at zero that's where my outstation starts for its point number. So this will be our T1P0, or our binary input 0.0. You could give it a name or a description which can show up in the tooltip or nearby it, but I'm just going to leave everything default for now. If I hit OK, we'll see that that point was then added to our database. Of course you could come in here and right click and do add multiple MDOs, and that's going to follow you through the same process. Now it knows I already added 0.0, so it's starting out at 1, and I think I'm just going to go up to 5 here, and that's going to add all of the points in between 1 and 5. So now if I hit OK, we'll see now that we have all six of those points within our SCADA data gateway. And we can take this one step further to being able to add even more devices at once. You'll notice we only have our binary inputs here. If I wanted to add more types, I could come in here and add a data type, but I can take that even one step further and go even all the way to the very top level node and add all the points across all devices at the same time. So if I do import or export points here, and then I select input points, and hit OK, it'll bring up a browser. And all I need to do is find the points that I want to import into the system, hit open, and it will show you very quickly that all of these devices have had their points added. And you can scroll through and make sure there's no errors or warnings or anything like that. Now if I go back to my device, you'll see not only do I have a lot more binary inputs, but I also have all my double boots and output counters and all those things as well. If you don't have a CSV file in order to auto add points, you can actually just right click on a master DNP and do auto create tags, and it will reach out to the outstation and just pull everything in. But you lose a little bit of your ability to customize there because you're pulling in absolutely everything. It is worth pointing out that the feature that uses importing and exporting of CSV files is a SCADA Data Gateway 5.2 feature, but that is slated to be released in the near future. So keep an eye out for that. So now we have a database within our SCADA Data Gateway of all these different points, but we need to make sure that the points are actually coming from the outstation when it's changing and get the values into our SCADA Data Gateway. To do that, you have to update something called an Action Mask. So if I go to L4 here, you'll see we have Action Mask 0 through 4. These are the different ways that we can interact with our master DNP and have it communicate to the outstation. So I'll just take a look at Action Mask 0 right now and click this. That's just how you edit it. And if I drop down what the Action Mask is, you can see here it's doing a read of class 1, class 2, and class 3 on the change events. And how frequently it's doing those is listed below. So 2,000 milliseconds, every two seconds, it's pulling these values in. You can have multiple of these Action Masks running at once like I do here. So I have another one. I'll just go down to Action Mask 1 here. And you can mouse over it and see it's doing the exact same thing, but it's also reading class 0 static data. Class 0 static data is basically going to pick up for everything that we possibly could have missed over that time. So this read is going on over the course of an hour. So it's going to be doing the exact same thing, reading class 1, 2, and 3 change events. But in case anything gets left behind or we get out of sync, the class 0 static data will take care of that for us. So I'll hit OK. From that point of view here, we've taken care of everything we need to configure on the master DNP side. We're able to get the database from the outstation into our master DNP. But now we need to figure out how to actually provide that as a 104 database to our SCADA master. So let's scroll down and take a look at our 104 a little bit. So I'm going to go ahead and open him up and see that he's already got some of these containers allowing you to add points. If you are very familiar with our previous SCADA data gateways, you'll notice that there's only one container for all the points. For convenience sake, we've taken two different folders. One used to contain MDOs and one used to contain the SDOs and combined them into a similar container. That allows you just to have one place to look when you're looking for certain type of data points. So let's go ahead and start mapping. Again I'll stick to binary inputs like we've been doing. And I'll just go ahead and grab one and just drag and drop it down here to this single point. And sure enough, it's going to pop up and give you a configuration for it. So I know that I want my T1P0 to map over to my single point at object address 100. So I'll go ahead and make that change and I'll hit OK. It'll give me a success message here letting me know that everything went properly. And when I hit close on it, you can see within the database of our 104 now, we have that T1P100 and it's being mapped from our T1P0. If I happen to fat finger something or the configuration needs to change, I could always come in here and click delete mapping and that'll go ahead and get rid of it for me. Just like last time, if we want to do more than one at a time, you can actually come in here and multi-select all the ones that you're interested in and drag them all over at once. This will give you the same type of layout that we just had before, but it will allow you to do multiple at the same time. So I've got the last one I mapped over was 0.100. So the gateway can assume that I might just want to increase from there. So by default, it's giving me 101, 2, 3, 4, 5, which is actually a really handy pattern because it kind of goes hand in hand with 0.1, 2, 3, 4, 5. But again, this is all completely editable so you can come in here and change it to whatever you want as you go through. So I'll go ahead and hit okay. And what we'll end up seeing is if I come here, now I've got all those points mapping. Now just like last time, you could come to this very top level node here and right click and do import and export points. But instead of doing points this time, we can do mappings. Now this is actually really interesting and I do think it's important to explain how to build up these CSV files. What I'll generally do is I'll do a handful of points, maybe one of each different types or one of each different types of mappings I'm planning on doing, and then I'll actually export it as a CSV file. That'll give me the base foundation of how I can expand it. Open it up in Excel and then I can expand it out however I want by copying out the columns and what's necessary in there. So to show you what that'll look like, I'll go ahead and hit okay. But instead of automatically importing this mapping, I'll actually open it up and go to open with, just use Excel here. And then you can see exactly how the layout is. So you've got all of your column headers here. And if you have any questions about any of these column headers, it's all described within the user manual. But you can pretty much infer what's going on. So the tag name here, this is the point that's being mapped and where is it being mapped to? Well, the map name. So you can use any of the features built into Excel to actually build up these CSV files. I just really like making a baseline that I can export out of the gateway first. So I'll go ahead and close this out and I'll go ahead and hit open on the mappings. And lucky for us, we just see a success here. Now, if anything had gone wrong, you would get a warning or an error message here in the results. It might say something like, hey, you've already mapped this one point to the other. So I mapped those first six binary inputs. If those had been in my CSV file, I would have got a, we didn't map again because it was already mapped there. So I'll go ahead and close this down. And then we can scroll down and see all of these points have now been mapped automatically for me. What's really interesting about that mapping file I showed you is that there had been any points inside of my master DNP that hadn't already been created like we did in the previous section I was talking about. It would have automatically created those points inside of the master DNP and then created the point where it would be mapped to. So I could have skipped that entire section, but I did want to show you how you can do it manually to start with in case you want to create a database and map them to different places and you don't want to use the CSV file process. Again, the CSV file process is for our gateway 5.2, which at the time of posting should be out shortly. If this video has been out for a while, it may already be out and that feature could be built straight into your gateway. So now we've gotten the values from our SCADA data gateway in our DTM. The values in DTM in our outstation are then being pushed to our SCADA data gateway via the action mask I showed you earlier, and then automatically being mapped over. But let's take a look at exactly how this is happening inside of our log. So let's say I'm interested in what's happening in the master DNP. So if I just type in master DNP here in the message, it's going to filter out everything not having anything to do with the master DNP. You can also see the arrows and the directions in which this communication is going. So if I go down here to DTM and just change the values of, say, all of our binary points and come back over here, we should see that there is some traffic going on, but there's still a lot that we're kind of looking here. I know we just did binary, so if I type in binary, you can actually see all of these values coming across as binary inputs. Since I just did an auto data change and just changed them all, they all went from on to off or off to on, so it's not super exciting to look at, but you can see this traffic coming through. Again, if I just clear that out, go to DTM, make that change one more time, we can see these values coming across the line. Of course, another way you could do this if you wanted to prove that your master DNP was getting these values is actually go to the database itself. They're all on on, and I will change them all to off. And then after the values come through, you can see in the log, sure enough, all these values then go to off. So now let's verify the mapping over to our 104. Again, just like before, you can look at it at the database level. So you can go to DTM, right click, do change values once, go back, you can see the values coming in, and then they're immediately mapped over to our 104. But just having the values in the database in our SCADA data gateway isn't all we need to accomplish. We need to also make sure that our SCADA is getting a hold of this information as well. So I can look in the logging for that as well. I can come here and type in COT. What that's going to do is it's going to show me if spontaneous messages are being sent out like we expected, and sure enough, they are. Another convenient thing that you can do to see what's going on in your system is if you go over here to category and type in event, you can actually not only just see the event data from the spontaneous message we were just looking at, but also the event data from our DNP3 device. So you can actually kind of go through and watch all of these values coming through, being sent in, and then pushed back out. Just like our 104 database, we do have an OPC server and an OPC UA. So if I click on the UA server here, you'll see all of our points have been mapped over. We've been looking at the binaries today, so if I just scroll down, you'll see there's our T1, P0, but it goes all the way up to 400 different points here. So if I go to, say, 100 or 200, we'll see some more of our values here. These are our binary, I think, output controls. And if I created an OPC UA client and connected it up, you'd have access to all that information as well. If you start to add a bunch of devices inside of your SCADA data gateway and you need to be able to find them and you don't want to scroll a bunch, it's been very easy so far for me since I only have three. But just like you can search for points, you can actually search for nodes. So if I type in master DMP here and hit search, you'll find that device right off the bat. And just like you can do that, if I go up to, say, this gateway root node here, and say I want to map over all my P0s, right, well, I can click search, I can type P0, and then do a deep search here, and that'll look through all of my devices and all of my points list and find all the P0s for me. This will allow me to kind of quickly search down to the points that, say, I want to map. Now I've got them all in one place and I can just drag them one over a time to actually map over. So that really addresses a lot of our ability to build up and configure our gateway. Now let's talk about security a little bit. Well, one of the security features we talked about was users and roles and the audit logging. So let's go ahead and enable that. We can go over here to settings and go to system settings, and then scroll down to the bottom. And under security parameters, you'll see enable user authentication. And under the log parameters, you'll see enable audit log. As long as these are checked, these features are going to be built into your gateway. Not all implementations of our SCADA data gateway is going to need both of these. But if you do want them, make sure these are checked. It's the first place you need to look. Then what we'll do is we'll actually make some new users. So if I go up here to settings again, I can go to users, and you'll see I already have some users. I have admin, which is what I'm currently working on. Then I have someone I call Tom viewer. He's just someone who has the role of viewer. And then I've got Sally super user. She has basically the same powers as the admin. But if I wanted to create a new one, or if you want to create a new one, you go up here to add new user. You give them a name, just like I did previously. You give them a temporary password. And then the first time they log in, it'll prompt them to replace their temporary password with a password that only they will know. Optionally, you can also add their email address. So if I was going to configure our gateway and make some changes, I could send an email out to everyone I know that's interested in that specific gateway and say, between one and two o'clock today, I'm going to be changing these parameters. If you see any strange activity during that time, it's probably just me changing things. So it gives you the ability to kind of reach out to everyone who has shown interest in this specific gateway. And of course, you have access to your different roles. That's your super user. That's your admin level. They can create points, create devices, create mappings. They have access to all the security features such as adding and removing users. Then if you go down a step, you've got your configurator. That is someone who can configure our SCADA data gateway, but doesn't have access to the security parameters. So they can come in and add points, add devices, remove mappings, things along those lines. The operator is one step lower than that. They can come in and execute commands and controls on different points within the SCADA data gateway, but they're not able to actually remove any mappings or add any devices or things like that. And then the viewer is the user with the least permissions. They're able to come in and see everything going on within your gateway, but not allowed to make changes to things. Now the viewer actually has a special thing that none of the other ones have, which is the ability to have multiple viewers at once. You can have as many viewers logged on looking at your SCADA data gateway at the same time without any problem. But these other three groups up here, the ones that can actually make configuration changes and operation changes, you can only have one of those total across the full system. So let's take a look at what different users can actually see. So I'll go ahead and log off. And I'll go ahead and log on as top viewer. Hit log in. Now, you might notice this pop up on my screen, and I did leave it this way as kind of a reminder to touch on this when I did log in. Without HTTPS, you could see everything being sent across the networks with something like Wireshark, and that would include your username and passwords. So it wouldn't really be that beneficial to have roles if anybody could just pick up those username and passwords. So by default, we use our self-signed certificate, and we ship that with our SCADA data gateway. SCADA data gateway supports both self-signed certificates and official CA-signed certificates. We highly recommend switching off of our self-signed certificate as soon as security becomes a requirement. And the resource to do this is included within our user guide. So I'm now logged in as Tom viewer. You can see that up here. And if I go into our points here, you can see I have access to see the values and where things are being mapped to, but I don't have a way to delete the mappings or delete the points. If I go into our binary output controls, right-click, left-click, I can't actually issue any controls or commands, can't open any breakers or anything. So it really just lets you look at things and know what's going on without necessarily or maybe accidentally messing something up. But if I log off of here, and I'll log in as Sally superuser, type her password in, now you'll see I'm logged in as Sally. And if I go to the exact same place Tom was just looking in, I can actually come in here and delete the points, make some changes. I can right-click here and change the values, edit the different points. So I have access to all the things that the admin that we were logged on previously can do. You can also go into the audit log here as a superuser and see everything that's happened over the course of your gateway being up. If you scroll all the way down to the bottom, you'll actually see where I logged on as Sally. There's where I logged on as Tom. And here's everything that I've done as the admin. Now let's say that you're interested in a specific user. So you can actually come in here in the username and type in Tom and see what everything he's done. Granted, he's just a viewer, so all he's really done is logged in and look at things. Or I could type in admin and see everything that was going on, that changes have been made. If you're looking for, oh, something went wrong yesterday at 5 o'clock or whatever, you can actually come in here and make changes and filter down to the specific time and date that happened. So it really just comes down to what you're looking for on how you use this audit logging. But you do have ways to kind of quickly search down to what you are looking for. And we've talked about using HTTPS to hide your information. We've talked about the security added by adding roles and users and audit logging. It's also important to reference the fact that our protocols also support different types of security, like TLS or secure authentication. Things along those lines are also built directly into our gateway and into the protocol. So those are all working too. So now that I've configured everything that I am interested in having set up, I can go ahead and save this off as a workspace. The easiest way to do it is right here at the root node. You can just click save workspace. And then once I've done that, it's going to take all the configuration we've done today and save that to a file. And then anytime I want to bring that back, I can just load that file back. To do that, you go to settings and system settings. And then I've got a little dropdown here and I can select any of my configurations I've worked on previously and bring them back up. If I need to send this configuration to another one of my coworkers or someone who's purchased a SCADA data gateway and they're putting inside their system and you've been configuring it for them, you can actually download the workspace after you save it and ship it off to them, whether it's through flash drive, email, anything like that. And actually they just go through and import your workspace and they'll have the exact configuration you had set up. So you can sit in a situation like I am right now, where I've got my SCADA data gateway, I'm trying to configure it all up. I have it in a testing environment with DTM being the SCADA master and all the downstream devices. I get it configured properly. I'm happy with how it's working. And then all of a sudden I can just sub out DTM with the actual physical devices and make sure things are still working properly. Then I can send that configuration out on site into the system that's actually been deployed and verify everything still works properly there too. So that's basically all of the new features that we have right now within our SCADA data gateway. So today we have taken a look at the productivity enhancements that we've added into our SCADA data gateway, 5.1 or greater. We've shown off how it's easy to create and configure devices and databases. We showed all the different types of improved searching that we've provided, whether it's in the logs, searching for points, searching for nodes, anything along those lines, and the ability to faster map points. We showed off all of the new views that we looked at today, including the special views. We showed off workspace management and how to import and export workspaces. And for the security features, we showed off the ability to remote configure. We showed off our user roles and audit logging, and we showed off the different ways that you can support the HTTPS in the SCADA data gateway and the certificates. If you have any questions about migrating to 5.1, we do have another video on our website available that can step you through this process. If you're interested in either of the application tools that you saw today, you can get an evaluation for both of them at trianglemicroworks.com. And in the future, we're going to have a DTM release and webinar completely focused around the new tools we're adding to automate the testing of your gateway configurations. And of course, for any questions or comments, you can contact us at sales at trianglemicroworks.com.


Video ID: St_qhLxe7ew
Welcome to the Triangle Microworks tutorial on IEC 61850 data models. This training is led by Christoph Brunner. Christoph graduated as an electrical engineer from the Swiss Federal Institute of Technology in 1983. He is a utility industry professional with over 30 years of industry experience with knowledge across several areas both within the utility industry and in the automation industry. He is a well-known expert on IEC 61850 and is the president of IT4Power in Switzerland, a consulting company to the power industry. As such, he has been a consultant in many projects for substation automation and projects involving IEC 61850. He has worked as a project manager at ABB Switzerland Limited in the power technology products business area in Zurich, Switzerland, where he was responsible for the communication architecture of the subsystem automation system. He is covener of the working group WG10 of the IEC TC57 and a member of WG17, 18, and 19 of IEC TC57. He's an IEEE fellow, a member of the IEEE PES and IEEE SA, and is active in several working groups of the IEEE PSRC, that's the Power Engineering Relay Society Relay Committee, and a member of the PSRC Main Committee and the Subcommittee. He's also the international advisor to the board of the UCA International Users Group. So without further ado, here's Christoph. Welcome. IEC 61850 has three main elements. The communication capability with the abstract communication service interface and the mappings of protocols like MMS, manufacturing message specification, and TCP IP over Ethernet. The semantic data model with the logical nodes and data objects, and the system configuration language supporting the engineering process. While the communication stack that you have just purchased supports you with all the aspects of the communication, it is your task as a device manufacturer to create the data model for your device and to support the engineering process. This tutorial focuses on the data model. The tutorial is structured into five parts. In part one, I will give you an introduction on what IEC 61850 is and the different tasks you have as a device manufacturer. Part two then introduces the elements of the data model. In part three, I will explain step by step what needs to be considered when designing the data model for a device. Part four finally provides some modeling examples for typical substation automation and protection functions. While in part five, I will discuss what is required such that your device can at the end be used within a substation automation system. So let's start with the introduction and let's have a look at some of the 61850 features that you probably already know. What is 61850? First of all, it's of course a communication protocol, or at least it provides the capabilities of a communication protocol. 61850 defines what is shown yellow here, an abstract model for the information exchange using different kinds of service capabilities. In addition to that, 61850 defines communication profiles using existing standard protocols like ISO 9506, which is the MMS protocol, ASIN 1, TCP, IP, and Ethernet. All the blue part is not really defined within 61850, it's only referred to in 61850. But 61850 definitely defines the methods for the information exchange in the abstract communication service interface, which is 61850 part 7.2. What you can see as well on this slide is that we have different communication methods. We have client-server communication, which is using the full seven-layer stack, but we also have Goose and sampled-value communication, real-time communication, which is directly mapped on an Ether type. But 61850 is more than just about communication, 61850 also defines a domain-specific object model. If you look at the example of the switchgear here, we have a couple of models we call a logical node. These are building blocks for the semantic object models that are defined in 61850. A few examples here, like the XCBR at the bottom, which is the model of the circuit breaker, XSWI for switches like disconnecting or earthing switches, current transformers, TCTR, voltage transformers, and last but not least, another example here, the gas density monitoring that we have in a gas-insulated switchgear, the SIMG. All these red blocks here are called logical nodes, and they provide all the data, all the information that we need to interface to our application, to the process. This is the interface to the process. But of course, in a substation automation protection system or in any automation system, we have more than just interface information, we have also software functions. The software functions also need some models, and for that we have more logical nodes, like the PDIS as a distance element for protection function, PTLC, time over current element, the measurement unit, MMXU, and the CSWI, which is the control functions for switches like circuit breakers or disconnecting switches. All these logical nodes here represent, in fact, information that you have produced in a software algorithm in your automation system. So what you can learn here from this first slide is 6150 has two aspects of data modeling. One is the models of the process interface. These are the process closed logical nodes, they are shown in red in my slides. The other ones is the models with the information from your software algorithms. These are the green logical nodes shown here, this is the software produced, the information produced by your software. When we talk about 6150, an important thing is that we need to think in functions, not in boxes. 6150 doesn't define how you have to structure your devices. 6150 defines functional elements, logical nodes for functional elements. As an example here, you see a typical model of a distance protection relay. We have the interface to the current transformers and voltage transformers at the bottom left. We have then multiple distance elements, three zones shown here. We have the scheme logic, if you have a distance scheme implemented, we have the trip logic, the PTRC, and at the end we have the interface to the breaker to trip, which is logical node XCBR. All these elements are defined in 6150 as logical nodes with their data objects, with their information. But 6150 doesn't say how you have to allocate them to devices. The first example is shown here, this typical conventional approach, like we have it today where all these functionalities are implemented in one device, the distance protection relay in that case. We have the interfaces to the process and we have the software functions with the protection algorithms. However, 6150 also would support in the future an approach like this here, which is more process-based approach, where we have a merging unit as an interface to the current and voltage transformers, transmitting sample values to a distance protection IED that is reduced now to software functions with the protection elements. Then again, we have an interface to an input-output unit that provides interface to the circuit breaker. So in that case now, the distance protection IED would not implement any physical inputs or outputs anymore. It's mainly doing software functionality, but the merging unit and the input-output unit to the circuit breakers are really doing the process interface. So such an allocation of the function to devices is possible with 6150. The third element of 6150 is, as we already have discussed, the system configuration language, which provides a set of files with configuration information. It has, when we start as a first step, we can produce a specification of our future system, single line diagram, with the functionality required. We can then produce a first output, a standardized file, which is called a system specification description. On the other side, we have from our devices that we would like to use, the IEDs, the intelligent electronic devices, protection relays, bay controllers. We have another standardized file, which describes the device capability. It's called IED capability description, or ICD file, device capability. That's where we find the data model of the device that we need to do our functionality. And that's where we also find a formal description about the communication capabilities, the engineering capabilities of the device. These two files are the initial input to the engineering process. We then can take an engineering tool, a system configuration tool, import these files, and do with the system configuration tool all our system configuration, instantiating devices, defining data flow, information exchange, configuring all the communication parameters. And at the end, the result that we get out of that device is what we call a substation configuration description. This is, again, a standardized file that includes all information required from a communication viewpoint to configure a substation automation system. So now, all the devices can import that file into their device-specific configuration tools, call that IED configuration tool, or ICT. They can do the local device engineering, and then, based on that, they can download the configuration into the device, and at the end, on the right-hand side, we have our configured system, our configured substation automation and protection system. This is also standardized as part of 61850. Then, 61850 has some specialties or additional services, capabilities. One of them is GOOS. GOOS basically is a communication service. GOOS, by the way, stands for generic object-oriented substation or system event. It's a communication service that allows to use messages to be exchanged between IEDs instead of wires. So typically, when you have protection or control functions implemented in a substation automation system, the devices have to exchange binary information. Traditionally, this is done with a bunch of wires, wired from one device to another, and one device output to a device input in the other device. With 61850 GOOS, this can now be replaced by a communication service. So GOOS basically is a communication service that allows a fast transmission of status information as a multicast over the communication network. In a similar way, like in the past, this binary information was transmitted using wires between the devices. GOOS is now providing a communication service that does this functionality. An example of using GOOS is shown in this next slide. This is a typical application where you have a reverse blocking functionality. So what we see here is a transformer bay. On the top, we have the high voltage or the higher voltage part. On the bottom, the lower voltage part with a low voltage busbar. Individual feeders going out, and it assumes that the feeders are radial feeders, so the energy flow is always going out to the feeders and cannot get back. The typical protection function here would be that you have your feeder relays that do the feeder protection on the low voltage side, each of them. But behind that, you have on the low voltage side of the transformer, the transformer protection device, which also implements a busbar protection for the low voltage, the 16 kilovolt busbar in this case. So basically, when you now have a fault on this busbar, this protection relay will see the fault in that direction, these feeder relays will not see the fault in this direction. On the other side, when we have a fault on one of these feeders, the relays will see the fault and the protection will see the fault. So to avoid that the protection trips, in the case we have a fault on the feeders, we have this reverse blocking signal that is typically sent back to this device. As soon as this relay here sees the fault on the feeder, it will activate that signal and the relay will block. However, if none of these relays does see a fault out here, it can assume the fault is here on the busbar and then this relay will trip immediately. So this is a scheme typically implemented in conventional substations and using wires. With 61850, we can now replace this wire with goose messages. There may also be another wire that initiates breaker failure. With 61850, all these signals can now be implemented as goose messages. So we would have, as an example, a goose message with the start information of the distance element, which would be used to block the busbar protection relay. And we would have a goose indication with the operate indication from the trip conditioning logical node, which is the physical trip to the breaker. And this would typically be used to initiate the breaker failure function. Another possibility of 61850 is the possibility to transmit sampled values. So here we have an example where we have intelligent devices, merging units, IEDs connected to the current and voltage transformers. And they are transmitting sampled values to the switch. And they are transmitting sampled values to the switch. And these sampled values are then distributed to bay controllers, line protection as they are needed. So instead of having analog signals between the CTs and VTs to these different devices on the left side, we now have communication possibilities that transmit the communication signals to the devices on the left side. So these have been a couple of features from 61850. One of the key features of 61850 is the semantic data model. In legacy protocols like 6875101 or DNP3, information objects are typically identified by a point number. And the data type, like single point status information, is typically identified by a decimal information or 16-bit integer measured values. A DNP3 example is shown on the top of the slide. Extra documentation like a device profile is required to know that, as an example, the binary input with number 6 represents the open contact of a circuit breaker. In another device, that same information might be available in a single point status with number 25. Or the open and closed contact could be combined to a double point status with number 315. With IC61850, the objects are defined with standardized names and data types. For our example, the standard defines that a data object with the name POS represents a switch position. The data attribute with the name STVAL contains the status value. So the value has a double point status in this case. And if the data object POS is within the logic node XCBR, it's the position of a circuit breaker. So with 61850, the information is not anymore identified through anonymous numbers. The information is now identified with names that have a well-defined semantic meaning. Taking the previous example, in DNP3, the object is globally identified through the object type, which is 1 in that case, the variation, and the point index, which was 6. So this would be 1, 2, 6 in the above example. In addition, of course, the ID needs to be identified through a unique address. A DNP3 master device would use this address information to read the value of the position. In IC61850, the object is identified through the hierarchical name. While the data object name POS and the data attribute name STVAL are unique within the scope of the logical node XCBR, there may be multiple circuit breakers and thus multiple XCBRs within the device. Therefore, a prefix and instance number are added. So our position would now have the name QA1 XCBR1 dot POS dot STVAL. To make it unique within the system, IC61850 is using, in addition, a logical device name rather than a physical ID name. So one ID may have multiple logical devices. With a logical device name AA1Q1CTRL, the unique identification will then be as shown. We will discuss later how exactly logical device names, LN prefix, and LN instance number are created. For the communication, when IC61850 is mapped to MMS, the name shown now is used with a little variation as addressing information. The object reference for MMS would be AA1Q1CTRL slash QA1XCBR1 dot POS dot STVAL. Basically, the dot is replaced with a dollar sign. In this slide, we have a first example of an IC61850 data model for a device. The device is a Bay controller as shown on the left side. The circuit breaker QA1 and the disconnectors QB1 and QC1 are controllable. For each of these switches, we find a logical node CSWI for the control, CILO for the interlocking function. The circuit breaker has a logical node XCBR. As process interface, the disconnectors use the logical node XSWI. The logical node MMXU is used for the measurements. In the example shown, we find individual values for phase voltage and current per phase. The values are available as floating point value. To summarize this first part of the tutorial, here again the key elements of 61850 in a different view. What do you need to do when you design a device that supports 61850? The major part of the information exchange and the communication protocols used are part of the stack that you have bought. The first step you have to do is to integrate the 61850 communication stack into your application design and to adopt the information transmission to the IC61850 communication services. Here you need to consider that IC61850 communication typically works event-oriented and that the status updates include the timestamp of the last change. The second step is to create the data model based on the information that is available in your device. The 61850 data model does not only provide the operational information. As we will learn in the next part of this tutorial, the data model is not only the data model, it includes as well configuration attributes and parameter settings. Finally, you need to consider as well the IC61850 engineering process with the system configuration language, the abbreviation is SCL, that is defined in part 6 of the standard. You need to consider how to map your existing engineering functionality into this process and you need to identify what extensions may be required to your existing ID configuration tool. In the next part of this tutorial, we will now have a closer look on the elements of the data model.


Video ID: 1E4u75WdpFA
In this video, we're going to discuss how you can manage your DNP3 device profiles and your IEC 61850 to DNP3 mapping files using DNP3 Forge. DNP3 Forge is a free tool to manage these functions, but also has a couple of add-ons to provide even more functionality, for example, scripting and the ability to export CSV files. So here I have DNP3 Forge, and the first thing I'm going to do is show how to create and edit a DNP3 XML device profile. To create a new file, we go to File, New File, and we can choose the device profile from January 2010 or from July 2012. Let's take the latest approved one, the July 2012, and it opens up a profile. Now within the tree here, I can click on any value and get more information over in the Properties pane. If I drill down to the lowest level, then I get a property grid where I can fill out the information. So in this case, I'm creating an outstation profile. I can specify the vendor name, the device name, hardware version, software version, and so forth. So let's go make a few changes. Maybe we want to change the data link layer address. This would be a really bad idea in practice, but I am going to fix my data link layer address at four. I'm going to never validate addresses. So I can continue on making changes like this. At each phase, notice as I make a change, and I click on the value to change it, I have the property grid over here where I can set the capabilities, I can set the current value. And down here, we have a description field that indicates the purpose of this field and indicates a reference to the Triangle Microworks source code library and the TriangleMicroworks.net source code library to show where this value is set in the library. So as you create your device profile, you get pointers of where to make the corresponding changes in the source code library. Let's go down and create a few points. You can come to, say, the binary points, right-click on data points, and add a data point. And I can fill in over here, point index, name, description. I can add another point, and so forth. Let's come down and create a few analog input points as well. So now let's take a look at how we can map between 61850 and DMP3. To do that, I'll open an ICD file. Let's say File, Open File, and load my ICD file. Mapping is easily accomplished by drag and drop, but that's easier if I can see both my ICD file and the device profile at the same time. So I can grab the ICD file and rearrange my display so that the DMP device profile and the ICD file are side by side. So now I can expand down into my ICD file and find the point I want to map. So for example, let's look at the phase A current. Since I am mapping this point, I want it to be required, so I will click this checkbox, and then I can just drag the point and drop it on my analog value. The Edit Mapping dialog pops up and lets me verify the mapping and shows me what rule is being used. So that mapped the value. I can also require the quality and time and map them as well. And of course, I would do the same thing to map the phase voltage, and I will put them on analog 1. As you can see, it's very easy to map points by drag and drop. I'm going to save this file, then I'll close it and show another way to create device mappings. So here I have the same ICD file I was working with before, and notice I still have checked the values that are required to map. I'm just going to go to Tools, Create New Device Profile by Mapping Selected IEC 61850 Attributes, and it creates the new mapped file. I go look at my points list, look at my analog points, I'll see that it's created analog 0 and analog 1, shows the points over here, point name, and fills in the description what point it was mapped from in 61850. Now I'm going to save this file, and move on to show some additional features of DMP3Forge. I'm going to go to the Tools menu and select the Validation and Test window. The test window demonstrates scripting capability of DMP3Forge. DMP3Forge allows you to customize it using Python scripts and includes some predefined scripts to do DMP3 level capability validation. So for example, if I want to take the device profile I just created and validate it against level 4, you see in red any errors for items that are not supported, in orange are places where the device generates responses that are not part of the subset, and then in blue it shows additional responses that are created that are not part of the subset, but that's okay because there is the capability to configure to only return responses that are within the subset. I can create my own scripts by clicking the green plus button and adding a new Python script here. I can also compare device profiles, so let me open up the first profile that I saved off. I can go to Tools, Compare DMP3 Device Profile Documents, going to Compare Map 1 to Map 2, and these yellow circles indicate area where there are differences. So if we drill down, remember I made changes in the data link layer, I had it fixed at 4, turned off address capabilities, so those differences are showing. If I want to set it back, I can match them up and then hit Apply. Now they match and notice the yellow circle went away. If we drill down and look at the points list, we see the primary difference in the points is that the automatically created file included a description describing where the points were mapped from. DMP3 Forge also has the ability to display the device profile in human-readable format. To do that, we go to Tools, View DMP3 Device Document, and it will run the XSLT and provide us a profile that looks like the old document-generated device profiles. This is very nice for providing a human-readable format, for example, to include in a request for proposal or in a response to a proposal to indicate the capabilities of your device. Another feature of DMP3 Forge is the ability to export CSV files. This is one of the optional features of Forge. To do that, I go Tools, Export Mappings, or Export Data Points. To a CSV file, I'm going to export all my points. I get the ability to define the format of the CSV file. When I say OK, it will create the export, and I'm going to save this off as an exported file. Now, I have a workspace in the test harness that's designed to read that file. So, if I launch my test harness... And I'm going to load this workspace, configure master from CSV file. Look in the data window. We see that no points are configured. I'm going to go to the command window and read the configuration from the CSV file. Browse to the file I just saved off. And now we see the data window updates, and I have the points that I created from that mapping. Notice that the description that was in the mapping file comes through and is shown in the description of the test harness. So, I hope you found that useful. Once again, DNP3 Forge is a free tool that's available to manage your DNP3 XML device profiles, as well as to manage DNP3-61850 mapping files. And then it has the optional components to add additional features through scripting, such as the ability to validate a file or the ability to compare two device profiles. And it has another optional feature to import and export CSV files. Please feel free to contact us if you have any additional questions. For sales questions, contact sales at trianglemicroworks.com. For technical questions, contact support at trianglemicroworks.com. Thank you, and we look forward to hearing from you.


Video ID: iQXmy7F7H2c
I want to talk a little bit about the substation configuration language. This is an XML-based language for configuring substations and substation devices. It's designed for the exchange of information between engineering tools. It includes descriptions of the application components, device models, logical nodes, communication infrastructure, and the relationship of all of that with the power system. This is a major innovation within 61850 that we think is going to have a really major economic impact in the overall substation automation world. This is already fostering a wide range of tool development that will significantly change the way we develop substations in the future.


Video ID: bYF-oHxxgbw
Now, let's take a look at the data link layer. The data link layer provides the following features, framing, control codes, optional data link confirmations, addressing, and error detection. Let's look at the data link layer header in detail. The data link header begins with the 0564 framing characters followed by the frame length, a control code, destination and source addresses, and CRC bytes that are added to the end of the data link header. The framing characters indicate the start of a new frame. Receiving devices monitor incoming data looking for the 0564 sequence as an indicator that a new frame has begun. The length indicator indicates the number of bytes in the frame, starting with the byte after the length character, the control field. The length does not include any CRC characters. The first bit of the data link control character indicates the direction of the message. The next bit indicates whether this message is from a primary or initiating station or if it is a secondary or responding station. As we'll see later, both master and out stations may be primary or secondary stations. When the direction bit is set, indicating the frame is being sent from a master, the next two bits can be used for frame sequencing. If the message contains confirmed data, the FCV, or frame count valid bit, is set, and the FCB, frame count bit, toggles with each frame. The last four bits contain the function code, which will be explained later. If the direction bit is 0, the FCV and FCB bits are replaced by an unused bit and a data flow control bit. The function codes vary depending on whether or not the primary bit is set. This table outlines function codes and frame types when the primary bit is set. This table outlines the function codes that are used when the primary bit is not set. DNP3 uses a balanced link layer. With a balanced link layer, all devices are considered equal. This means that some sort of collision avoidance is necessary. This can be accomplished by full duplex connection so that communications may flow in both directions simultaneously without conflict, or via media access control, such as Carrier Sense Multiple Access, CSMA, or Carrier Detect, CD, at the physical layer. This picture shows the difference between primary and secondary frames. In the first example, the master sends an application request message and requests link layer acknowledgment. In this message, the DIR bit is set since the message originated from the master, and the PRI bit will be set since this is a primary message. The outstation then sends an acknowledgment. The DIR bit will not be set in this message because it is from the outstation. The PRI bit will also not be set because this is a secondary message. Next, the outstation sends an application response message. Although this is a response at the application layer, at the link layer, the outstation is initiating the message. Therefore, the PRI bit will be set. The DRI bit is not set, however, since this message is from the outstation. Since the slave requested link layer confirmation, the master sends an acknowledgment. The acknowledgment is a link layer secondary message, so the PRI bit will not be set. However, the DIR bit will be set since this message is from the master. The DNP3 link layer contains both source and destination addresses. The provision of a source and destination address simplifies message routing in certain network topologies. For example, having both source and destination address supports peer-to-peer systems in which a device can change its role from outstation to master. A DNP link address is a device's logical address. A physical device is permitted to respond to multiple addresses. That is, it may contain multiple logical devices. Each device will appear to the master as a completely separate device. The destination and source address are always 16 bits and are transmitted least significant byte first. The application layer does not contain addresses. The CRC calculation is defined in the DNP3 Specification Volume 4 Data Link Layer. The CRC is transmitted least significant byte first and is included at the end of the link layer header, after each 16 bytes within a frame, and at the end of each frame. The next slide provides an exercise using the communication protocol test harness to analyze link layer messages. This is followed by a short quiz to help reinforce these concepts.


Video ID: nDiJttHNIjQ
This video shows the data monitor view for HAMR, the 61850 client component of the 61850 test suite. To enable the data monitor view we can click on the data monitor icon. The data monitor view will show any currently active control blocks and any pulled data. So for example if we enable a report control block that block will now be shown in the data monitor view. Likewise if we enable polling for a point that will also be shown in the data monitor view. The data monitor view is very useful for seeing real-time data being updated from many different sources. However you should be aware that it's possible to miss a data change in this view because by default the user interface is updated once a second. Also note that the model does not store a history but is simply a real-time view of the data that's being received.


Video ID: 23N_XMaeyDQ
In this video we'll look at how to view data with the test harness data window. In the test harness data window on the left side is a tree showing all of the devices, sessions, and data types that are configured in this test harness. Now in this test harness I have both a master and an outstation and they are communicating with each other. Within this view I can right-click on the channel and modify or delete it. I can add a session as well and right-click on a session and modify and delete it as well. So this is the same functionality that's available from the main test harness window where we can also modify or delete sessions. In addition within the data window I have the option to save the session or restore the session as an XML file, clear the database, and also work with simulation files. We'll cover simulation files in a separate video. Within the tree view the data window shows everything below this value in the tree. So for example when I click on the channel I can see all of the points that are configured in that channel. If I click on the session I only see the points for that session. In this particular example they're the same because I only have one session on the channel. If I click on a specific data type then I only see the points of that particular data type. In the master view the data window shows all of the points that have been reported by the outstation as well as their current value, the flags, and the time it was updated. In the outstation view we're actually looking at the outstation's database as it exists on the test harness. So within the outstation we can change the value of the points in the database and they will be reported up to the master. So if we right-click on a point we have the option to change the value or set the flags. For a binary point the only change we can make is toggling the value. It's currently off. If I toggle it it turns on. Toggle it again and it turns off. For double bit inputs I can set any of the double bit values. I can increment counters and I can change the value of analog inputs using either the up and down arrows or just typing in a new value. We can also modify the flags for each of these points. To do that we right-click and select set flags and we get a dialog that lets us set the various flags. This dialog is custom depending on the data type. So for example a binary input has a chatter flag whereas an analog input does not have chatter but does have over range and reference check. Now you may have noticed as I've right-clicked on these points that one of the options is to hide the data point. If we select that this point will no longer be shown. Note I have point number zero selected. Click hide and point zero is no longer shown. If we do that accidentally or we want to get the point back we can go to options show hidden data points. Now the points are shown. The hidden points are highlighted in this blue and we can unhide them by right-clicking and selecting unhide data points. Hiding data points is very useful for creating a custom view. For example maybe you want to create a view of only a specific set of points on a device. Perhaps they are the most critical points. So to do that we can select view create new view and name this view. I'll just call it important points. Now we can select that view and we can hide any points that we don't want to be visible in this view. So now this view doesn't have points one two and three but if we go back to the default view they're still visible there. Also in the data window there is a description field. We can put anything in the description field to describe these points and when we save the workspace these descriptions will be saved and if we reload the workspace they'll be reloaded. So this makes it very easy to put English descriptions on all of your points so that you don't have to keep remembering what's point zero and what's point one and so forth. We can also rearrange the view by dragging columns. The test harness also supports configuring the database from a CSV file. To demonstrate that I'm going to close this workspace and I'm going to open a sample workspace that we ship that reads the configuration from a CSV file. So if you notice we have no points in the slave now but I can load a sample script that reads a CSV file and when I run it I get a browser window. I can select the CSV file I want to read and then the database is populated with the points in the CSV file including the description field. So the data window provides us a great view into our devices. We can modify the device configuration. We can view the data in the database. For a master this will be the points reported by the outstation. On the outstation it's the points that will be reported to the master. We can modify the points in the database, modify the point values in the database, and we can also create custom views to show us select portions of the database.


Video ID: K_93JvaKeCs
The Communication Protocol Test Harness also has the ability to perform conformance tests. This video is going to demonstrate the optional DNP3 conformance test module. To start the module, we load the DNP3 slave tests, which brings up a test script window. From here, we can edit the configuration, and the configuration window has a set of tabs, starting with General Setup, one for Connection, and then a tab for each of the data types that are tested in the test procedure. On the Connection tab, I'm going to select Simulated Slave in the Test Harness. In this configuration, the conformance test will be run against a simulated slave that runs in the test harness. For testing your real device, you would actually want to connect to a device through a serial port, TCP IP, or UDP only. If we look on the Input tabs, there's an option to use an Omicron CMC156, 256, or 356 to generate the test values. This option can really aid with automated testing, as the test harness will send commands to the Omicron to set up test conditions, and then read the values and see if it got the expected results. If you don't have an Omicron to generate test conditions, the test harness allows you to use a custom TCL command to create a test condition, or to use DNP3 virtual terminal messages, or it will pop up a dialog box and instruct you what inputs to set up. Throughout the configuration window, the test harness uses pop-up help to describe each of the fields that you may need to configure. Once we're configured, we can save the configuration, and then next time we run the test, we can load that configuration, so we don't have to reconfigure it each time. Across the bottom, we have a series of buttons. The first allows us to connect to the device under test, and this one allows us to interrogate the device, and the purpose of this is just to confirm that we do indeed have our communications working. On the left side, we have a list of tests, and on the right side, we'll see the current state of execution as the test is run. We can select the test based on a prescribed set defined in the standard, for example, select level one or select level two, or we can select individual tests. For the purpose of this video, I'm going to select a few of the class data tests. I'll just do class zero and class one, and then we can click execute, and the test will begin. Notice on the left side, we see the current progress. We also see a check mark indicating when the test has been completed, and it's color-coded, green indicating the test passed, red would indicate the test failed. And on the right side, we see the current state of each of the tests, the step that needs to be provided, and what's happening in that test. All of this is also logged in the communication protocol test harness protocol analyzer log. So at the conclusion of your test, you can save out this log, and then you can review it to make sure you understand the test results, and you can submit it to the DNP3 users group for verification of your conformance. And now the tests I've run have completed, and I got this orange warning reminding me that I'm running against the simulated slave in the test harness. Of course, if you're testing a real device, you would not see this warning, and I get a final notice telling me that all tests are complete, and the number of tests that I ran, the number that passed, the number that received warnings, and the number that failed. As you can imagine, this module greatly simplifies the task of conformance testing your device by allowing you to automate major portions of the testing and giving you an indication of whether the task was successfully completed.


Video ID: ANz6Lxk5wnI
Now that we've seen the objectives of a SCADA communications protocol, let's take a high-level look at DNP. We'll begin with a general overview, including the history and present usage of DNP3. Next, we'll look at typical operational features of the protocol. DNP stands for Distributed Network Protocol. It was developed by GE, previously Harris or Westronix, and was based on the early parts of IEC 60870-5. After developing the protocol, Westronix turned it over to a users group. This was a rather unusual move, since most manufacturers at the time kept their protocols proprietary. It turned out to be a very smart move, however, because the open protocol gained wide acceptance throughout the world. Today, DNP is used worldwide, including North and South America, the United Kingdom, Australia, New Zealand, parts of Asia, and South Africa. It is well known for its use in the electric utilities industry, but it is also used in oil and gas, water, industrial, and other applications. DNP and IEC 60870-5-101 have been specified in IEEE P1379, Recommended Practice for Data Communications between Intelligent Electronic Devices and Remote Terminal Unit. Newton-Evans Research recently conducted a study of communication protocols. They found that the DNP-3 protocol is the most popular protocol in use in the North American electric utilities. One of the Newton-Evans surveys looked at the current and planned uses of protocols within the substation. As shown in the graph, DNP-3 is the leading protocol currently used within the substation. DNP-3, especially land-based implementations, is also the leading protocol for planned usage within the substation. Another survey looked at the current and planned use of protocols from the substation to the external host or network. Once again, DNP-3 is the leading protocol for this application. Furthermore, DNP-3, especially land-based DNP-3, is the leading protocol for planned usage in this application. The DNP-3 protocol is owned by the DNP-3 Users Group and is available to all members. Basic membership cost is only $300 per year. The DNP-3 Users Group currently has members from vendors, utilities, system integrators, and software developers. All of the committees that make up the DNP-3 User Group are staffed by volunteers. The DNP-3 Technical Committee is responsible for the managed evolution of the protocol. This includes providing clarifications to the specification as well as recommending enhancements. The Technical Committee is chaired by Andrew West of Invencis Process Systems Australia and the secretary is Alan Scott of NOSA Switchgear Pty Ltd. The committee meets via conference call twice per month and meets in person once a year. In addition to this, the committee interacts daily via a mail list. The Technical Committee leads the managed evolution of the protocol. If the protocol never changed, it would become stagnant and fail to meet people's needs. But if it changes too often, then it's too difficult for vendors to remain compliant, thus the need for managed evolution. The committee defines new features, updates the documentation and test procedures, and presents these to the Users Group for approval at the annual meeting each year. The committee clarifies existing documentation when different interpretations exist as well. This results in a controlled standard. It avoids multiple vendor-specific variations of the protocol. DNP3 documentation includes the specification, conformance test procedures, and technical bulletins and application notes. The DNP3 specification is provided in eight volumes. Volume 1, the introduction, provides a good overview of the protocol. The remaining volumes provide details on various aspects. Some volumes contain more than one part. For example, the data object library consists of three parts. All of the DNP3 documentation is available for free download from the DNP website by DNP User Group members. The DNP3 specification is very comprehensive. It would be impractical to try to implement every possibility described in the specification. Consequently, the DNP User Group has adopted four subset levels. The subset levels break the specification into more manageable pieces. Each subset level represents an interoperability agreement between devices of the specified subset. Each subset defines a specific set of functionality that must be supported for that subset. Each subset includes all of the previous subsets. Subset level 1 is designed for small IEDs. It uses simple polling, including class polls, which will be described later. Subset level 2 was designed for large IEDs or small RTUs. It adds object-type polls, extra data formats, and other useful features, such as counter-freeze operations. Subset level 3 is designed for very large devices. In subset level 3, the master can poll for any combination of objects and ranges. It allows for a larger combination of data formats and also allows for reassignment of classes. These features will be discussed later. Subset level 4 was approved at the Annual Users Group meeting in February 2007. It adds several new features, including an XML version of the device profile document. A level 4 device must provide an XML version of the device profile document containing both the capabilities of the device and its current settings. This file may be obtained by a means other than DNP file transfer. For example, it could be generated by a PC-based configuration software and delivered via CD-ROM or USB mass storage device. Level 4 also adds self-address reservation. With self-address reservation, if a device receives a message with a destination address of hex FFFC and the self-address feature is supported and enabled, it should respond normally with its own source address instead of FFFC. Subset level 4 also adds additional data formats, including object group 0, device attributes, double bit inputs, variations with time for frozen counters, frozen counter events, and analog input events, floating point variations for analog inputs and outputs, analog input reporting deadband, and event objects for binary and analog outputs. While the features of higher subset levels can be useful, normal DNP operations rely on subset level 1 functions. There are several ways for members to stay current with the DNP Users Group. The DNP Users Group holds an annual meeting each year at the Distributech Conference. This meeting is open to members and non-members alike. The meeting contains presentations about the status of the various committees and about the protocol itself. The technical committee presents its recommendations and upcoming areas of focus. Members can vote to adopt new specifications and technical bulletins at this meeting. The DNP Users Group website contains a What's New section on the front page. This area is an excellent source for keeping up with DNP-related news. The DNP Users Group website contains a forum which can be used by DNP User Group members to resolve questions or share information. The DNP Users Group also periodically publishes a newsletter and other information broadcast to DNP User Group members. In addition to the protocol specification documents, archived copies of presentations about DNP are available on the website as well. Now let's take a closer look at the protocol itself. In this section, we'll discuss data acquisition methods, data classes, and report by exception. In DNP3, static data refers to a point's current value. It is common for a Class 0 poll to return all points in the outstation. This is not required, however. It is acceptable to have points that are not returned by a Class 0 poll. In this case, these points can only be read by direct reads of the point. However, DNP requires that any points in Class 1, 2, or 3 must also be in Class 0. Data can be in Class 0 and not be in Class 1, 2, or 3, however. The following slides look at DNP data classes in more detail. In DNP, the term static data refers to a point's current value. A Class 0 poll returns the current value for all points in Class 0. As mentioned before, any points in Class 1, 2, or 3 must also be in Class 0. Data can be in Class 0 but not be in Class 1, 2, or 3. In DNP, events are associated with something of significance happening. This is most commonly a state change or a change in the data's value. However, it could also be because a value crossed a threshold or exceeds a dead band. It might also represent a snapshot taken at a particular time. DNP supports three event classes called Class 1, Class 2, and Class 3. The protocol specification does not assign any significance to these classes. Some installations report binary input data in Class 1 and analog input data in Class 2. However, data can be assigned to the classes in any manner that makes sense in the system. Classes may also be assigned and changed via the protocol. An event poll, that is a Class 1, 2, or 3 poll, only returns changes or events. DNP3 supports Report by Exception, or RBE. With RBE, the outstation only reports data changes. RBE can be used with polling or with unsolicited responses. With RBE, the outstation adds changes to an event buffer. It may add all changes, only the most recent change, and which type is added can be configured per data type. Data from the event buffer can be sent via unsolicited responses or in response to a class poll. The data remain in the event buffer until it is acknowledged by the master. Events, or data changes, can be reported either as a sequence of events or as only the last change. With sequence of events, all events are reported, and they are typically reported with timestamps. Digital inputs are typically reported in this method. With last change, only the most recent change is reported, and is typically reported with a variation without a timestamp. Analog inputs typically use the last change method. Here is an illustration of reporting events via the Report by Exception mechanism. First, an event, for example a binary input change or an analog change in excess of the dead band, occurs in the outstation. The outstation reports this change via a response to a class poll or via an unsolicited response. The master confirms receipt of the events by sending an application confirm. The outstation removes the reported events from the event buffer only after it receives the application confirm from the master. This guarantees delivery of the event to the master. Let's take a look at the difference between static data polling and event data polling using a digital input as an example. In this example, the master polls at a specified interval. Each poll time is indicated by the vertical black lines. At the start of our example, the input is low or zero. The input then transitions to a high state. Whether the master polls for static data or event data, the device will report the high state on the next poll. However, with DNP event data, the outstation could also report the time of the transition. Next, the input transitions low and then high again. With static polling, this transition is missed as the device reports the current state or high on the next poll. However, with event polling, the device reports both transitions. During the next poll interval, the input transitions low again. With static or event polling, this transition is reported. During the next poll interval, the input does not transition. With static polling, the current state, still low, is reported. No events are reported with event polling, however, because nothing has changed. During the next poll interval, the input transitions high, low, then high, and then low again. With static polling, only the current state, low, is reported. All of the transitions have been lost. With event polling, however, each of these transitions are reported. If capturing transitions are important, then either the static poll rate must be high enough to capture them, or event polling should be used. The beauty of report by exception and event polling is that all transitions are captured, with timestamps if desired, despite using a slower polling rate. This can potentially reduce bandwidth requirements. When using report by exception, it is important to report all consecutive samples for each data object in the same order it is read. This allows the sequence of event logs to be built. The master station updates its database in the order that data are received. With this methodology, the final value will be the most recent value. However, should the event buffer overflow, the master must reinitialize its database by reading all current values. DNP3 supports multiple methods of retrieving data. Master outstation networks typically use polled static or polled report by exception. In point-to-point networks, unsolicited report by exception and quiescent operation may be used. With the polled static data acquisition method, a class 0 or specific data request message is sent to each device, and the device returns the requested data. With polled report by exception, a class 1, 2, and or 3 poll request is sent to each device, and the device responds only with the events for the requested class. An occasional integrity class 1, 2, 3, 0 poll is used to ensure that the database remains in sync. With unsolicited report by exception, the outstation sends unsolicited responses containing changes or events. This allows the master station to be updated on events without the need to poll. However, it requires some type of collision avoidance mechanism if multiple outstations are sharing the same media. In addition to receiving unsolicited responses, the master station occasionally sends integrity class 1, 2, 3, 0 polls to verify that its database is up to date. With quiescent operation, the outstation only reports data via unsolicited responses. The master never polls the outstation. This method is most commonly used in point-to-point networks. It is useful, for example, in dial-up applications in which the outstation dials into the master to deliver its unsolicited responses. The DMP3 specification requires that any master or outstation device that supports unsolicited responses must also support the enable and disable unsolicited response requests. Furthermore, the specification states that when an outstation restarts, all points must be disabled from sending unsolicited responses. The outstation must still send its initial null response, that is, an unsolicited response with no data. However, it may not send unsolicited responses with data until it receives an enable unsolicited response request from the master. Note that the outstation may still be generating and storing events before it receives the enable unsolicited response request. It is just not allowed to send those events via unsolicited responses. DMP is based on a set of paradigms. To ensure maximum benefit and interoperability, these guidelines should be followed. First, use report by exception. The master should read an initial image of all field data using a static poll to retrieve the current state. All changes should then be reported using report by exception. DMP uses a separate event buffer for each data type. All data reported from these buffers must be in increasing time order. That is, the oldest is reported first, and events are reported in time order until the newest is reported. DMP treats binary input and double-bit binary inputs as a single data type for the purpose of reporting events. That is, single and double-bit inputs are reported together in time order. As shown earlier, DMP confirms reception of events, and events are removed from the event buffer when they are confirmed. Note that when reading static data, either via class 0 or reading specific data types, you should always read event data in the same request. The DMP3 specification requires the event data to be sent in time order, ahead of the static data. This procedure ensures that the database will remain in sync. If event data are read after the static data, the database can be updated with old information if the events occurred before the static data were read.


Video ID: PAhiqTv3FPE
Welcome to the Triangle Microworks IEC 60870-5 communication protocol training videos. This video is part two in the series. In part one, we provided an overview of the IEC 60870-5 protocol, talked about stack layers and the application layer. In this video, we're going to dive into more detail on the ASDU structure. All right, I've mentioned ASDU several times already. ASDU is an application service data unit, and it's a concept that's used at the application layer. The ASDU always includes the type of data which is contained, the number of data objects in that unit, and the addressing for those data objects, the cause of transmission, or basically why the data is being sent, and the data objects themselves. These ASDUs are defined in the standard, and they're built up with several different base types, so there are several standard ones that are defined. The spec also leaves open the possibility of creating custom ASDUs. Of course, for interoperability, that could be a limiting factor unless both sides recognize the same custom ASDUs. And finally, note that 101 and 104 do have a different set of standard ASDUs. They're basically the same, with the biggest difference being in the time tags, where the format for those are different. So if you look at how the ASDU is put together, there's always a data unit identifier. This identifies what type the ASDU is. Basically it describes the format of the data, what's included, and the basic types of data that are included in the message. It also has the VSQ, or Variable Structure Qualifier, which identifies how many objects are in the ASDU. You can have the cause of transmission, which is why the ASDU is being sent. There are several different cause of transmissions, and the cause is different depending on whether this is a control, or a pull, or a read. And then there's the common address for the ASDU. This can identify the address for the station, or the logical sector. And like I said before, 60870-5 allows you to have multiple sectors for a single outstation. Also this common address allows you to uniquely identify every piece of data in the entire system, because every sector has to have a different common address. So it's a way of basically taking the DNP3 point numbering scheme, but instead expanding that to the entire system, so that each point, not only do you have a point number, but you also have this common address, which uniquely identifies each sector. So that's the header with the data unit identifier in it. Then there's the information objects, of which there can be multiple, or there can be just a single information object. The structure of the information objects is based on the type identification. It will always include the information object address, and always include at least one information element, and it can include a time tag. There's several different ways these can be put together, just like there's several different ways an ASDU can be put together. So we have some examples in just a little bit to kind of clarify what different options are available. Okay, so now I'm going to go through the ASDU structure in a little bit more detail. The type ID is the first octet in the ASDU. There are up to 127 different ASDU types. They're basically grouped into different sections here. So you have the status and the measured values. These are basically all the different data types which can be included. You have commands for identifying what type of command request is being sent for the master, and you have the new security types, which are added for the secure authentication part of the standard. And then you also have commands for control, commands for monitoring, but these are things like interrogation and reading objects versus commands for control or setting outputs or binary outputs and set points. And then there's commands for measured values and file transfer related commands. So several different types of commands. Here are just a few examples. You have single point, with or without time, double point, with or without time. You have a single point command, a double point command, a read command, clock synchronization. If you compare this to DNP3, it's roughly similar to the different object groups and variations, except it's fixed. With DNP, the master can request a specific object group or data type and a specific variation of that data. With 60870-5, it's fixed. The master cannot request different variations other than to request a specific type identification, and then it gets the variation that's defined with that type. All right, moving on. The second octet in the ASTU is the variable structure qualifier. Basically this is telling you how many elements are in the ASTU. Now there is an option to have a sequence ASTU. And so if it's a sequence, it's basically the address is going to tell you what the starting element is, and then this number is going to tell you how many elements from that starting point you're going to include in the ASTU. If it's not sequenced, then the number here is really how many objects you have, and each one is going to have its own address. If you read the standard, it tells you ASTU by ASTU whether or not it supports sequenced data. And so it's kind of a complicated concept, but once you look at the ASTUs, it's very clear why this is in here and how the ASTU is structured. Moving on to the third octet is the cause of transmission. This is a very important concept because basically I would say it replaces several different concepts in DNP3. There are 47 different cause of transmissions. So let's skip up to look at some of the cause of transmissions. Basically cause of transmission identifies why the data are being sent. These are really for different types of requests and responses. So when an ASTU is sent, it will include the cause of transmission. So for example, if you're doing some sort of polling in which the outstation is sending to the master, you can identify if this data is a cyclic measurement that's being taken or if it's really event data that's being sent. Then we move on to controls where the cause of transmission can tell if this is an initial activation or if it's a confirmation that's being sent back for the original activation. Or if the activation is being terminated or the command is being terminated. If the master requests something that the outstation doesn't have, the outstation could say this is an unknown address or an unknown object. So there's really no equivalent to this in DNP3 except for maybe some of the IIN bits. The cause of transmission is an option for each ASTU, but the standard does define which cause of transmission can be used for each of the ASTUs. So for a particular ASTU, there may only be just a few causes which can actually be used. Once again, if you look through the standard, it's much clearer when you look at each particular ASTU as it clearly shows what's supported and what's not supported. So that's basically the cause which is included. There's also some other bits here so you can put the ASTU in the test mode or you can have a positive or negative confirmation for a request. There's also an option to include the address for the station which originated the request, but this isn't very commonly used. Alright, so the last part of the ASTU header is the common address. Like I was explaining before, each physical outstation can have multiple sectors and so there's several different ways you can do the addressing. If you have just one station in one sector, then your ASTU common address really is the station's address. If you want to set up multiple logical sectors for one outstation, then you can have a common address for each of those sectors. That allows you to expand your addressing space and it also allows you to segment different objects into different sectors. The spec really goes into a lot of details about the different topologies that this allows. I won't go into that because there's a lot of different complicated factors there, but the common address can be one or two octets, but that's fixed across the entire system. So if you need a large addressing space, you can make this two octets. There's also a concept of a broadcast address. So if you want to send a command to all of the stations at once, then you can set the address to FF if you're using one octet addressing or FFFF if you're using two octet addressing and then that request will be seen by all of the outstations and they all know what they need to process this command. The complete addressing for the system will use that common address that I was just talking about and the information object address or IOA. For example, if you had two different stations, one station could have just one sector and the other station could have multiple sectors. So each of these sectors has a common address and then each of the elements has an IOA or information object address. So between the combination of the common address and the IOA, you have a distinct address for every object in the system. That's different from DNP3 in which the address number is reused for each data type. So in DNP, the uniqueness of addressing comes from data type and index number, whereas in 60870-5, it's based on the common address and the IOA. The final piece in the ASDU is the information objects. There are three different ways you can put together the information objects as shown here. There's a single information element and that's basically when you have just a single information element which has just one address for that element and you can have a sequence of these included in the ASDU. So you could have multiple of these single information elements and that's this third type over here, or you can have a combination of information elements where they're a different format and they're multiple elements. This is to accommodate things like having a single point status with a timestamp. Those are technically different information element formats and then they're put together to form this information object. So it's kind of getting into the weeds of the terminology and the standard, but basically I wanted to outline how you can put together different elements into objects. The single information element could look like a single command or event. This combination of elements, like I said before, is like a data value with a quality or time included and then a sequence is when you have a whole series of measured values. So you can basically just say this is the starting address for this series and how many of them are going to be included. And when you look through the different ASDUs in the spec, it basically tells you which of these are supported for the different types. At the lowest level in this whole structure of information objects is what they call the information element. This is put together using just these four basic types. So you have a Boolean, an integer, a real number, or a bit string. It can also include a quality descriptor, which is usually just a few bits that are used for quality. It's important to point out that time is not included in this. There's several different information elements for time specifically, so time is not part of the single point status or part of an analog value. It's its own information element, which is put together with other information elements to create an object. I get to a couple of examples that I think will clarify this in just a second, but basically there's 40 different information elements which are specified in the standard. So just a few examples, there's a single point with quality. And you can have a quality descriptor, which has lots of different bits for setting quality, versus the single point with quality, which just uses a couple of bits. There's also a short floating point number, a binary counter, a single command, which we put together in an information element, or there's several different types of time which are defined. This is kind of equivalent to data type variations in DNP3, but once again there's fixed, and so the master cannot request a certain variation. It's just defined that way, and that's how it will all be used. A very simple example of an information element is the single point with quality. This basically has these five bits in it. So it has a single point status, a bit for blocking so that the previous value stays there. It has a bit for substitution so you can override the value. It has topical or not topical to tell whether the value has been updated or not. This would typically be used in, say, a data concentrator which is receiving data from downstream. And then there's a valid or invalid quality bit. So this is just one of the 40 different information elements. So as an example, let's put together an ASTU for a single point information item. So you include that single point with quality with a three octet binary time and the address for that object. That's the information object when you put those three things together. The data unit identifier will identify the type as being a type 2. The VSQ will tell you that it's not a sequence ASTU and it has, in this example, two octets. It could be one object or there could be up to 127 objects in this ASTU. It'll tell you the cause of transmission. If this was a cyclic measurement from the outstation, the outstation would identify this as a cause of transmission of one. And then it has a common address for the ASTUs and that's the address for the sector. In this example, we've got two of these information objects. Like I said, you could have up to 127 in one single ASTU. So here's another example, this time for a command. This is a single command and so basically it tells you the command state and then the qualifier for the command. The qualifier includes the operation for the command. So this is kind of like DNP3 where you have pulse on, latch on, or those types of modes. In 60870-5, you have short pulse, long pulse, or persistent. Persistent obviously it's going to set it and that's going to remain until it's changed by another command. Whereas the pulse, the short pulse and long pulse, pulse the output for a predefined period of time. Unlike DNP3 in which you can actually specify the length of a pulse in the command. In 60870-5, there's a configuration option for the length of a short pulse and a long pulse and you simply say short pulse or long pulse. It also has an indicator whether this is the select or the execute for the command. And that's basically all that's included in that single command element or SCO. So when you put together the ASDU for the single command, if we look at the information object, there's only one for a command and it includes the address for the object which is being controlled. Then it has the SCO or single command element included. And then in the data unit identifier, it'll have the type ID set to 45, which is the number for a single command. Also has a zero for the sequence because it's not a sequence measurement. And it says that there's only one object. When you're starting a command, the cause of transmission is activation or six. And then of course, it has the common address for this set of ASDUs. That's how you'd put it together for a single command. I think it's really helpful to look through the spec and look at all the different ASDU types to see how they're put together and what information elements are included. It also includes the bits and bytes of how those ASDUs operate. It's probably worth noting that you probably don't need to know all of this information in order to send a request or in order to process a request if you're using the Triangle Microworks library. But this is what's allowed at the protocol level and what you'll see if you're actually looking at the bits on the wire. Because there's a lot of information here, you only need to look at it if you're looking at the bits or if you want to understand why there's a limitation. I should also point out that the information object here tells you which point you're going to be operating on. In DNP, you have index numbers or point numbers. The point numbers in DNP start at zero and go to N-1 for each data type. But for IEC 60870, every point in the system has to have a unique information object address or a unique common address plus the information object address. So this means control points and monitor points don't have the same address. So it's important to note that in IEC 60870 is that every point has a unique address. Unlike DNP, which the address is unique per data type. Also in 60870-5, the IOAs don't start at zero like they do in DNP. And they can be whatever values you want, sequential or not sequential, it really doesn't matter. You can choose the values that you assign to these points. Also note that zero is generally illegal as a information object address. In other words, it's typically the starting address in DNP. So you don't have to have point one. You could start at whatever number you arbitrarily choose or whatever number makes sense for the numbering scheme in your system. Okay, I think that about does it for the ASDUs. In the next video, we're going to jump back into the application layer and look at that along with polling and controls.


Video ID: aIbQmXVVYdM
you


Video ID: U8BD2rnMAp0
In this video, we're going to demonstrate DNP3 Secure Authentication version 5 and user management via a DNP3 authority. So here we have the distributed test manager simulating a master talking to two different outstations, outstation 1 and outstation 2, and the DTM is also simulating those two outstations. Now let's bring up the DNP3 authority, and we see the DNP3 authority is connected to that master and knows about the two outstations, and we've defined three users, Dave, Jim, and Joe. Dave is defined as a viewer, Jim and Joe are both defined as operators. Now let's map Jim to outstation 1, and when we do, we see a series of messages beginning with the Secure Authentication user status change as the master sends the user status change message down to outstation 1. Let's assign Joe to outstation 2, and again we see the user status change sequence, but this time it's to outstation 2. And now let's assign Dave to both outstations, and again we see the user status change messages as Dave is assigned. So now Jim can operate outstation 1, Joe can operate outstation 2, and Dave can monitor both outstations but can't send any controls. So now let's see that in action. If we go to the DTM, go to outstation 1, and we're going to send a CROB command, we'll send the latch-on command, and we can choose one of the assigned users. Let's choose Jim since he's assigned to be able to send controls, he's an operator, say OK, and we see that the command succeeded. Now let's send another CROB command, but this time let's send it by Dave, remember Dave's a viewer, so let me clear the log, and then we'll send the message from Dave, or try to send the message from Dave, and we see that the command fails because of the challenge reply because Dave is a viewer and not allowed to send controls. And notice we have an error message now telling us that the CROB command failed. Let's clear that error and go take a look at outstation 2, and again we can send a CROB command, we'll send the latch-on, if we send it by Joe, the operator, the command succeeds. But if we send the CROB command from Dave, the viewer, the command fails. So now let's suppose that Jim didn't complete his proper training certification, and so he is downgraded from an operator to a viewer. Again we see the user status change messages, updating that change in his authority. And let's go ahead and assign Joe to outstation 1 so that someone can control this outstation. And again we see those user status change messages. So now if we try to send the CROB command here on outstation 1, if we send it from user Jim, who's been downgraded to a viewer, it fails. But we do have the option now to send the message from Joe because he's been added as a user and an operator to this outstation. And here we see the command passed. So now let's say Jim regains his certification, we'll make him an operator again, but let's suppose Joe leaves the company. So we're going to remove Joe, and we can even remove Joe from the list of users. And now when we go to execute a command, Joe is no longer in the user list, but Jim once again is able to execute the command. So this is a very simplified example, but it does show some of the power of being able to control users and user permissions via a DNP3 authority with DNP3 SAV5. Note that all of the user updates were done securely without having to drive out to the substation or have direct access to the devices.


Video ID: HXTMrkQQRW8
Welcome to the SCADA Data Gateway Quick Start video. This short video provides an overview of features and uses of the Triangle Microworks SCADA Data Gateway. This overview describes the most common uses of the SCADA Data Gateway, mapping a master component to a slave component. One common application uses a protocol master component to collect data from outstations and an OPC server to serve this data to an OPC client-based HMI system. Another common application uses an OPC client component to retrieve data from an OPC server and a protocol outstation component to serve this data to the SCADA master. You are not limited to these combinations. For example, the SCADA Data Gateway can map between any protocol master and any protocol outstation components. The techniques for configuring this combination are the same as those shown in this presentation. Master-to-master and slave-to-slave mapping is also supported. Please refer to the SCADA Data Gateway Implementer's Guide for more details on these options.


Video ID: E9aoJoIC8t0
This diagram shows the organization of the ACSI models and their scope. The server basically represents a communications capability at some address, which you can, down below it shows the types of services you do directly with a server, which is to be able to associate or make a connection with it, to be able to do time synchronization, and to be able to do file transfers. These abstract services operate at the server level. Within a server are multiple logical devices. These may or may not be different physical devices behind the server, depending on how you organize as a gateway or whatever. Within each logical device there is a set of logical nodes, and at the logical node level you have a set of control capabilities, substitution, settings, parameter settings, which operate on a set of data objects within the logical node. These data objects within the logical node can be organized into data sets. These data sets can be arbitrary types and organization of the data, but they define a group of data objects for the purposes of the reporting and logging services, goose, the generic object-oriented substation event, and sampled values.


Video ID: X63ccZnAVTg
Good morning, everyone. My name is Beth Capellas, and I'm the sales and support manager here at Triangle Microworks. Welcome to our latest webinar series, IEC 61850, Substation Design and Testing. This is a two-part series, and today's webinar is Part 1, IEC 61850, Substation Design Workshop. Part 2 will be hosted on September 21st at 10 a.m., and is titled, Tester Design, Simulating and Validating an IEC 61850 Substation Design. If you haven't already, please sign up for Part 2 at our website, trianglemicroworks.com, under the tab Training, Free Webinar. Before we get started, I'd like to go over some housekeeping items while everyone is still joining. The webinar is scheduled to be one hour long. We have a lot of material. We have a lot of material to cover, but we will try to take at least two breaks to answer questions. The audience is on mute, so please submit your questions online using the webinar toolbar under the section titled Questions. You can submit questions as soon as you have them. Please don't wait until the end. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly. We're also happy to stay on after the webinar is over and answer as many questions as we can. With that, I'd like to introduce the folks that are behind the scenes answering questions. Today we have with us Jim Coats, President of Triangle Microworks, Andy Ruder from Helinks, and Christoph Brunner from IT4Power, who is the President of IT4Power. Next, I'd like to introduce our speakers. First, we have Jörg Ruder from Helinks. He is the CEO and founder of Helinks. Helinks is a Swiss company focusing on the development of SAS engineering tools. With a background of 30 years of experience in software development, process, and power automation, Jörg's work is driven by the vision of a device-independent IEC 62050-based SAS engineering process and the appropriate tools to support it. He's also an active member of the IEC 62050 standardization body, also known as IEC TC57 Working Group 10. Next, we have with us Jackson Moore, Application Engineer at Triangle Microworks. Jackson received Bachelor's of Science degrees in both electrical engineering and computer engineering from North Carolina State University and has a background in power system engineering. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer, where he designed and developed load management control systems for multi-source microgrids ranging from 1 megawatt to 30 megawatts. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges our clients face today. So, now that you know everyone, I'd like to give you a little bit of background on Triangle Microworks. We were established in 1994 and have over 28 years of communication protocol experience. We are headquartered in Raleigh, North Carolina, and have customers in over 70 countries. We work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs and development time. Licensing a source code library also includes support for your implementation in any issues that may arise in the field. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They are designed to troubleshoot, simulate, and automate testing for both a single device or complete substation. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. This is available as software only or pre-installed on hardware. We are heavily involved in the standard committees that support SCADA protocols, like UCA Group for 60950 and DNP User and Technical Committees. Through our participation, we help define the protocols, stay up to date on the changes taking place, and ultimately provide feedback based on our customers' needs as the protocols continue to evolve. So now that you know a little bit more about Triangle, I'd like to go ahead and hand it over to Jörg to kick off the webinar. Jörg, I'll go ahead and make you presenter. Okay, Jörg, you have the, you have the floor. Jörg, we can see your screen, but we're not hearing you. Jörg, we can see your screen, but we're not, I don't know if you have your own mute button on. Hello? Yep. Now we can hear you. Can you hear me now? Yep. Perfect. Okay. Okay. So sorry for this. So, Beth, thanks for the introduction. Welcome to all the attendees. I'm glad to be here today to talk about the 6150 top-down engineering process and introduce the presentation of Jackson. The 6150 standard defines two different kinds of tools, the IED configuration tool, and the system configuration tool. The IED configuration tool, abbreviated ICT, is manufacturer-specific and it serves to configure the 6150 devices. The system configuration tool serves to specify and configure the 6150 system, including the communication configuration for the protocols such as GOOs, sampled values, and MMS reporting. If the ICT is manufacturer- and device-specific, the standard requires a well-defined behavior and interfaces to cooperate in the standardized engineering workflow. The ICT and the SCT exchange data using XML files, which are based on the SCL schema definition. SCL stands for system configuration language. The SCT can be split into major functionalities, system specification, and system integration. ALINX is a manufacturer-independent system tool that integrates partly also ICT functionalities. In the 6150 engineering workflow, there are two main participants, the SCT and the ICT. At the beginning of a project, the system specification tool is used to create the specification of the system. This specification is documented in the so-called system specification description, abbreviated SSD file. The system configuration tool reads the SSD file in order to know what it has to do. In addition, the configuration tool needs to know the capabilities of the IEDs that are involved in the project in order to produce a proper configuration file for the IEDs. These capabilities are provided by the IED configuration tool. The IED configuration tool exports so-called ICD files. ICD stands for IED capability description. The system configuration tool imports the ICD files and creates the IED in the project. Based on the specification, the SCT then configures the communication protocols for all the devices. This means it builds the publishing data structures and the subscribing data structures for all devices. This information is then exported by the system configuration tool as a so-called system configuration description. The SCD file is imported by the participating IED tools. Based on this information, these tools can then build the device configuration files and finally load the devices. In terms of Edition 1 of the standard, this process was very hard to execute because the interoperability of the IED tools and the system tools was not yet given. That is why up to today, many 6950 projects are still done in a bottom-up way and even without a system tool. Utilities prepare a non-6950 based non-formal specification. The engineers are designing the systems also with non-6950 tools using generic tools like Excel or Visual Basic. They perform all the design and frameworks such as defining data sets, contour blocks with those tools. Only when this work is done, they put the IEDs on the table, take the IED tools and configure the relay each one by one, implementing publishing and subscribing between the pairs of relays. If they are lucky, they can copy and paste some files or import and export the files for publishing and subscribing. This process works well for smaller systems and in a lab environment. But when the systems are larger or when it comes to maintenance, then the lack of a consistent system view really creates a lot of issues and problems. That is why the utilities today are pushing. Here we see the same project phases, but the specification and design is already done using the 6950 data model and the entire process is supported based on 6950 tools. The usage of STL files allows the creation of standardized template libraries. The advantage of these libraries is that they are very efficient and allow an error-free engineering process or nearly error-free. They ensure standardized designs even when the projects are done by different teams. This reduces the maintenance efforts. However, there is no free lunch. Creating and maintaining the templates is a challenge for utilities and the system integrators. Today, in this webinar, we are going into demonstration. We have a complete substation configuration file, so an STD file, based on predefined templates. It is extremely efficient and that is why we can perform this in the timeframe of this webinar. The templates we are using are made for this example. Our customers develop their own templates that reflect their requirements and protection and automation schemes. What we are doing in the demo, we will cover three steps of the 6950 process. First, we create a system specification, that is the yellow arrow. From the system specification tool, we provide the specification to the system integration part. Second, we add the IEDs to the tool and we connect them to the network. Third, we automatically configure for the complete substation, the data sets control blocks and build the STD file, that is the red arrow. This slide shows how we create the content of the STD file in four steps. The four blue boxes are the four sections that are contained in the STD file. In the first step, we create the substation and the data exchange. We start from the bay and application templates on the left and fill the left section of the STD file, the so-called process section, with the substation structure. This includes functions and specification logical nodes as well as the data exchange between the logical nodes. The lower left part contains the data types for the specification and later also the data types of the IEDs. In the second step, we add IEDs to our project and connect them to the networks. With this, we are filling the right part of the STD file. The third step creates the link between the electrical system on the left and the devices on the right. This is done by associating logical nodes from the specification to the logical nodes inside the IED. After this step, the STD file contains the information how the data points of the electrical system are related and connected to the IEDs, as well as the data points which are exchanged via the protocols between the IEDs. The fourth and final step creates the communication configuration inside the IEDs. The red part is based on the data exchange from the specification. This is represented here in the dotted orange lines. Our demo substation that we will build now together with you is a fully digital one and consists of four feeder bays and two transformer bays. Each bay has four IEDs from different vendors and on station level we have SCADA system in the gateway. We use a process bus interface to connect the CTs, VTs and switches with sampled values. The gateways and SCADA are connected to a station bus. We built this substation based on two templates, a line feeder template and a transformer template. We also use data exchange patterns to create the different applications. There we do the MMS reporting to the SCADA in the gateway. We configure the sampled values for all the bays on the process bus. The switch control over the process bus. These are both applications. The trip over the process bus also goes and we implement a breakout failure scheme for the line and for the transformer bays. This is goose messages over the process and the station bus. Okay, so that is what you will see right now and now I would like to hand over to, yeah, back to Beth for some questions and then Jackson will start with the demo. Thank you for listening. Great. Thanks, Jorg. We don't have any questions at this time. So, Jackson, I'm going to go ahead and make you presenter and we can get right into the demo. All right. Thank you, Beth. All right. Thank you, Beth. So, as Jorg mentioned, today I'll be using Helix SDS in order to do a demonstration. And that demonstration is going to follow the four steps that you see over here on the right-hand side. So, among other things, Helix SDS can be used to create both an SSD or a system specification description as well as a full SCD or system configuration description. And because these are such important terms for what I'm going to be demonstrating today, I do want to take a moment to reiterate their definitions. As implied by the name, an SSD is used to specify the functionality and layout of a substation. An SSD would typically be created by utility as part of their substation design phase and then might be provided to a system integrator as a blueprint of sorts. However, an SCD is created later in the engineering process. And while it typically does contain the layout and functions which were defined in the SSD, it crucially also contains the configuration information necessary for the 61850 communications, which actually allow our substation function. So today, as we can see here in step four, my goal is to create a full SCD. However, in the process, I will be creating a specification along the way. This could be exported from Helix as an SSD or system specification description. And one final note before I begin, as Jorg mentioned, in order to expedite today's demo, I will be utilizing a series of bay templates and application templates, which I have made ahead of time. These templates are reusable building blocks, which a substation designer can use to rapidly complete a design. The ability to utilize this kind of Lego-like approach is actually one of the biggest selling points in my eyes to both the 61850 engineering process in general, as well as Helix specifically. The creation of these templates is really outside the scope of today's demo, but should you be interested in learning more about how you can create your own templates in a future webinar, please let us know in the survey at the end. So with that, I'm going to dive into the tool, and we'll begin with step one, creating the substation and data exchange. So what you see up on my screen now is the Helix dashboard. And this is where we see the different tools within Helix that we're going to be using today. Specifically today, we'll be starting with the single line tool, where I will draw out my single line and specify which functions I want my substation to perform. Then we'll move over into our application tool, where I will specify the signal exchanges needed for my specific functions to actually work. Next, we'll be taking a look at the system diagram, where I will bring in IEDs and define their communication parameters. And then we'll take a look over in the communications, where I can see the goose reports and sample value messages that I've configured. And then finally, we'll be exporting our SCD using the export SCD button, as well as exporting our human readable documentation using the documentation button. So let's go ahead and jump into our single line. So this is where I'm going to be building the single line diagram for my substation and define the functions within it. The information contained in this diagram will be used to generate the substation section of our SSD. And as I mentioned, that will be carried forward on into our SCD once we associate our specification with instantiated IEDs. So as you can see, ahead of time, I've gone ahead and created a substation named Raleigh, a voltage level E, which is 110 kV, a bus bar down here in the middle named BB, as well as two transformers, T1 and T2. So now it's time to begin creating the different bays in my substation. As I mentioned a moment ago, today I'll be using a library of templates that I've made ahead of time. In this case, these templates correspond to two typical bays that a utility might repeatedly use across multiple substations, a line feeder bay and a transformer bay. And as you can see, these are over here on the right hand side in my palette. So I did make these templates ahead of time. And I do want to reiterate that designing these bays is a step that you would need to complete ahead of time. And then they will be specific to your substation design. They might not look like mine. So when I go ahead and select a line feeder bay and drag it over here into my substation on the right, I can just click to instantiate it. I'll go ahead and give it a name, 01. Oops. Did I select my voltage level? And we can see the different functions that are contained within this bay. I do want to go ahead and add all of these and just say OK. And I can see that my bay has been added. So a bay, or at least a template, is more than just a collection of conducting equipment that's connected like you see here. There's actually a lot more to it. So I'm going to go ahead and drill down into this bay and we can see a little bit about what's inside. So a template is made up of functions represented here by these white boxes. So, for example, the distance function up here in the top right. These functions are used as a mechanism for grouping 61850 L nodes, seen here by the green boxes, which are working towards a common purpose. So in this case, our distance protection function has a PDIS logical node or L node within it, several others as well. And these are all working together towards the common goal of doing the distance protection. A function can be associated with a piece of conducting equipment, which we see down here. Our voltage samples are linked to a PT or BT. And optionally, these functions can be assigned or grouped using a virtual IED. So the inclusion of virtual IEDs is not necessary to a template, but it will make things a little bit easier down the line, as we'll talk about a little bit later, when I want to actually associate these virtual IEDs with instantiated IEDs. So one thing that I do want to point out is that data exchange is not defined here. These functions really have no concept of data exchange. We do define our inputs that a function will need in order to function, as well as its outputs. But we don't define where this data will come from or how it will get there. And this is done using an application. So to get to our application, I'm going to come back up to our dashboard and open up the applications tool. So as I mentioned, in order to specify communications between our functions, we must create an application. And over here, you can see several that I've already created. An application defines signal connections between our specified functions. Now, in the traditional 61850 engineering process, an application would be specific to a particular part of a particular substation. But one of the key features of Helinks is it allows us to turn a specific application into a more generalized application template. And that's actually what I've done here. An application template is a generalized pattern for data exchange between functions. That's a really important bit here, so I do want to repeat it. An application template is a generalized pattern for data exchange between functions. And that can be applied repeatedly and automatically as new bays are added. And that really does save a tremendous amount of time. So let's go ahead and drill down into one of these. Let's take a look at sample measurements. So on our left-hand side, we see our current samples function. And currently, this is just defined as a signal. Once we instantiate IEDs and do a couple more steps that I'll show here at the end, this will turn into sample value streams. But currently, this is just an undefined signal. And it's passing from my current samples function on the left. This one is being subscribed to by both my measurements function on the right as well as my distance function right below it. And that's going to be my distance protection. Whereas my voltage function over here, that's also being passed up to my measurements. And measurements might be converted into an RMS value and eventually passed up to SCADA via MMS reports. Whereas distance, like I said, is our distance protection. And voltage is passing to those two. Whereas if we follow the line from our current samples, I can see that is passed down to our overcurrent protection, but our voltage is not. And in this case, we're actually using overcurrent as a backup protection. So you'll notice that I didn't have to draw any of these lines. They were already here when I opened up this application template. And that's because I used a template. In the traditional 61850 engineering process method, I would need to add an instance of each function. I would have a library that I could refer to that would correspond with what we saw moments ago in my bay template. So I would add each one of those. And I would begin drawing these lines manually in order to create this association of signal connection. But because I'm using an application template, I've defined a set of rules that allow this to happen automatically. So in a moment, as I add more bays, my application template will be applied again in each new bay. And we'll see this diagram grow. So for those of you that are interested in emerging areas of the 61850 standard, these application templates are analogous to the upcoming 7-6 edition and the basic application profiles described therein. Now, before we go ahead and move back and I start instantiating the rest of my bays, I do want to just take a look at maybe one more application scheme just to give you an idea. So let's take a look at breaker failure, for example. So currently, keep in mind, I have just one bay added. So there's not a whole lot going on in my breaker failure scheme. Typically, I'm going to open up breakers in other bays when I have a failure. But currently, we see a trip signal being passed over here from my trip function over to my breaker failure function and then an operate internal kind of a backup trip being passed over here to my XCDR. And whereas the lines we saw before in sampled measurements will eventually be turned into sample value messages, these will likely be turned into goose messages once we complete our project. So now it's time to head back over to our single line diagram. Go ahead and close out my tabs over here on the right. And I'm going to go ahead and begin instantiating the remainder of my bays. So I'll start out by adding a transformer bay. We'll give this the name 02. And these are the functions that are going to be within my transformer bay. Many of them are the same. Some of them are unique from what we saw over in my feeder bays. I'll add two more feeder bays, 03. And please, if you guys have any questions, we will be pausing in just a moment. So now would be a good time to send those in. I'll add a 04. One more transformer bay, 05. And then finally, one last line feeder down here at the bottom. Now, you'll notice currently these different bays are kind of floating in space. So I am going to move them around and add in some electrical connections to actually connect them to my bus bar and to my transformers. And I'll do that using the electrical connection tool up here at the top. And just connect each one of these bays back up to my bus bar. All right. At this point, we've really completed our substation specification. If we come back over to our application area that we saw before, I can come back into my sampled measurements application and see what my template has applied. And you'll notice that there's a lot more going on here. So whereas previously I just instantiated a single bay and we had our current and voltage samples being passed over to my measurement distance and protection function, now as I scroll down, and I know this is probably too small for you to read, but you can see that each bay that I've added, the template has been applied, and I've gone ahead and established those new signal connections. Again, the fact that I'm using a template, which is a pattern that can be applied to my functions in order to establish these connections, the fact that I'm using that application template means that I didn't have to draw each one of these individually. And it's a little bit of work up front to create that application template, but I think you can see how it saves a lot of time down the line. If we come over to our application schemes and take a look at our breaker failure, for example, we can see that in addition to our trip signal that we looked at last time and our operate internal, we now have an operate external. And this is not being passed to the breakers in our own bay, but this is being passed to the other neighboring bays, so that if the breaker in bay one fails, I can open the adjacent bay's breakers. Just to give you an idea of how these applications are defined and how something like this operate external could be coded in and intelligently know how to be passed to other bays, I'll take a look back up at the application schemes. We'll go over to the configurations and scroll down, and we'll see that there are rules that you can define. So as an example, my breaker failure bay must be equal, whereas for the internal trip, whereas my external trip, the bays must not be equal, so it can only pass to external bays. So with that, we've really completed our specification, and if we wanted to at this stage, I could come back over to my dashboard and export this as an SSD. As I mentioned, this is something that could be handed over to a system integrator or even multiple system integrators as part of a bidding process. It could also export an ISD, an IED specification description, because I did specify virtual IEDs in this case, and that could be passed over to an IED vendor. I now know exactly what functions I want my IED to perform. I've specified that, and I could pass that out to an IED vendor and say, hey, here's a machine-readable, standard way to describe the functionality that I want. Can your IED do this? And that might end up being part of a frame agreement that's negotiated between a utility and an IED vendor or manufacturer. So we're about to turn it back over to Beth for questions, but before we do, I just want to come back to our diagram that we started with and point out that we've now completed step one, create the substation and data exchange, and when we come back from questions, we'll move into steps two, three, and four, instantiating our IEDs, mapping the logical nodes, and then finally configuring our communications and exporting our documentation. So with that, Beth, I'm going to turn it back over to you for Q&A. Great. Thanks, Jackson. Yes, we have gotten a number of questions in and certainly keep those coming. Answers are being provided online, but let me go ahead and go over a couple of them for the audience. So the first question, this is for Helinks. Can any type of bay be created in the prior, for example, a motor feeder bay? So I don't know if someone from Helinks can answer that. Yeah, you're completely free to define and design your templates. You define the bay type, you add the conducting equipment, you add the functions, you define the data points, the logical nodes, and once you have the static definition, then you can start to design the application. Great. If you're happy with the result, you put it in the library and you have a new element. Okay, thanks, Jorg. Next question, this is also for Helinks. No one in North America knows or uses Q0 or Q1 standard for equipment naming. Is it possible to use different equipment names and standards? You are free to name all the elements in the rules that the 60 by 60 standard allows. So there is no specific restriction for naming a bay or naming the conducting equipment. Okay. Next question, and I'll open this up to any of the speakers. How do you suggest verifying or testing templates once you have created them? So we'll cover that a little bit in more detail when we get into our second part to this webinar series. But one option would be to, once you create your NSSD, or I'm sorry, really, an SCD and have instantiated it, you could export that out and bring it into a tool that we'll be showing next time, DTM or distributed test manager, which allows you to run a series of tests to validate the functionality. Not necessarily performance because you won't have your real IDs. There are options to put hardware in the loop and have your real IDs present, but you can simulate all of your IDs using DTM. Like I said, we'll go into that a little bit more detail in part two, which is September 21st. Great, thanks, Jackson. Next question for Helix. When you edit a bay, can you add more function at the same time or have it predefined? Yeah, you can. Once the bay is instantiated from the template, you can modify the instance at your guide. So you can add functions. You can add signals. You can modify names. At a certain point, you can decide that this modified will become a new bay and you put this new bay in the library. Okay. Next question. Is there any rhyme or reason to creating the function groups for LNs or is it just what makes sense to the individual? You should be aware that what we are demonstrating here is design decisions that we took and we thought reasonable with, let's say, our limited experience. At the end, it is the knowledge and the responsibility of the design engineer to define his functional structure and allocate the logic node inside the functions and take the proper signals. You're free to do this. You can implement your design ideas. Okay, thanks, Jorg. So we do still have some more questions coming in, but Jackson, I'm going to go ahead and cut back over to you to finish the demo. We will continue to answer those questions online and then take an additional break. And again, at the end, to review more questions. Excellent. All right. So before I dive back in, I'm going to show this diagram again. So as I mentioned, we did create our substation section using the single line tool, and then we saw how signals were automatically created using our application tool or technically our application template. And now we're going to move into instantiating IDs and connecting them to the network. So for that, I'll be coming into our system diagram tool. So the purpose of the system diagram tool is really twofold. One, we're going to use it to load our ICDs, which define the IED capability descriptions of the capabilities of the IEDs that we want to add. And then we're going to be instantiating those ICDs multiple times. So keep in mind an ICD is generic. And if you have multiple instances of that device, you can instantiate it multiple times. So that's part of what happens here. The other part is configuring our communication parameters. And so that's going to be IP addresses, subnets, defining which access point on a given IED is connected to which network, maybe the station bus or the process bus. And so you can see I've actually done a lot of the work ahead of time. If I zoom out a little bit, you can see these different columns correspond to the different bays within my substation. And then I have a station bus network up here at the top, as well as the process bus down here at the bottom. And then I also have a HMI and gateway up here at the top as well. But I did leave a gap that will need to be filled, which is over here in Bay 3, just to give you an idea of what this process looks like. So I have loaded in my ICDs ahead of time. And they're broken up over here in the palette on the right by manufacturer. So in Bay 3, I'm planning to add an AVB device, specifically a REDD615. I already have the ICD for this loaded into the tool. So I'll go ahead and select that from the palette and then come into the system diagram and just select where I want to add that. In just a moment, we'll see it has been added. All right, process completed successfully. I'll go ahead and give it a name, E for my voltage level, E03 for the bay, and then BPU because I want this to perform the role of a bay protection unit. I'll go ahead and zoom a little bit in on this bay just to give us a better look. Now I can come up to my network drawing tools up here in my palette, and I'll click Connect to Station Bus, come back to my ID, and click and drag up to my switch in the station bus. I'll select access point one for that connection, and then scroll down and go ahead and add a connection to my process bus as well. And I can use the default IP addresses that are just assigned for now. But if I wanted to, this is where I could change these. And keep in mind, whereas what we did previously using the single line tool, that's going to ultimately end up in the substation section of our SED. What we do here is going to populate both the communication section of our SED as well as the IED section. So, you know, these ICDs contain data models. That's all going down into the IED section. And then the communication parameters and networks that I defined will be in the communication section. So if I want, I can hide these IP addresses just to make it a little bit cleaner. And we really now completed this step of our process. We've instantiated all of our IEDs. But we haven't really merged the two pieces of work that we've done. We've got our substation section, which was, you know, defined before the break, and now we've got our IED and communication section. But they're really still kind of separate. In order to merge the two, I need to come up here to model operations and select implement IEDs. And I do want to kind of show you another picture that is from your presentation that kind of captures what's happening here. So over here we have the creation of the SEL communication elements depicted. And what's happening here is that our IED and logical device and logical nodes, which are defined in my IED section, are now being combined with my logical node types that are defined as part of my data templates, as well as my L nodes that are defined over here in my substation section. And to kind of illustrate this a little bit further, I'm going to come back over to my single line diagram and open up a couple of the bays just so you can see what's happening here. So just to drive this home, I'm actually going to open up two bays so you can see that this is happening everywhere at the same time. So I'll open these up side by side. Oops. Well, one on top of the other. And let me just pick the correct bay here. All right. So a little bit about what's happening here. You can see currently I have these chain links that are broken, and these correspond to each L node in my substation section in my different functions. And then I have these exclamation points in a yellow triangle. So once I click implement IEDs, what's going to happen is these virtual IEDs are going to be associated with my actual IEDs. And these are based on a set of rules that I've predefined. And then my L nodes defined here in my functions are going to be associated with a logical node on one of the actual IEDs. So we'll see these chains change to a linked state, and we'll see these exclamation points inside of the yellow triangle change to a green checkmark once I go ahead and click this button. So I'm going to go ahead and implement IEDs. And sure enough, we see that it's worked. We've now linked the L nodes in my substation section to the logical nodes in my IED section. So that's actually, if we come back over to our diagram, we've now completed one, two, and three. So that was map logical nodes in data objects and attributes. And now we're ready to move on to step four, which is configuring our MMS, GOOSE, and sampled values. So in order to do that, I'm going to come here and just simplify it down a little bit, come back up to my dashboard. And first, I want to show you that these communications don't already exist. To do that, I'm going to come into my communications tool. And we'll see that this is going to be filled out in just a moment once I go ahead and click the implement communications button. So coming back up here to model operations again, I'm going to click synchronize SEL comms. So I can actually just use the default settings here. But what's happening is I want to configure my communications. I want to configure all report, GOOSE, and sampled values. And I want to configure both the publisher and subscriber for each of those, as well as configure all IDs, not just a selected ID. So I'll go ahead and hit proceed. It's going to ask me if I just want to continue. I can go ahead and say yes. And now I can come back into my communication editor in the dashboard, and we can take a look at the work that was done for us. So basically what's happening is in the application section earlier, we saw those lines flowing between our different functions. But at that time, they were just signals of an unspecified type. Now, using the set of rules defined in my templates, the sample measurements have been turned into sampled value streams. The GOOSE message is necessary for my break or failure. That's going to turn into, in most cases, a GOOSE message. If it's internal to a single device, it will simply be documented in the application section but not turn into a GOOSE. But now I can see that here in my application tool. So if I come into, for example, my report section, I can find an IED, and I can see the different reports that are contained in the both logical device and in the specific logical node. For example, unbuffered report control block A. And if I want to see some more information about it, I can expand it out and see some of the optional fields, trigger options, what will cause this report to be published, so on and so forth. If I come into my datasets tab, I see my GOOSE control blocks, denoted here by a G at the beginning, and then sampled value datasets. And if I look at this next device down, this is actually going to be a buffered report. And if I expand it out, I can see the information that's going to be contained in this report. In this case, we have an overcurrent protection logical node, and we have the PTOC op and start signals within that. Now, if I want kind of another view of all the GOOSE communications that are happening in my system, I can actually come down to the GOOSE matrix that we see here at the bottom. I'm going to go ahead and do an expand all. So on the left-hand side, again, we see our IDs and the different logical devices. Moving over one column, we can tell which GOOSE control block is actually being represented by this row. And it's called GOOSE matrix because we can now see every GOOSE message defined in our system, and we can see both who the publisher is and who the subscribers are. This is really the matrix section over here on the right, where we see a percent sign. It indicates that it's a publisher for a given stream. And then if we move across the row, you can see who's subscribed to that GOOSE stream with the word yes. So we've really now completed our engineering process. We have, if we go back over to our slide one more time, we've created our substation section, instantiated our IEDs. We've mapped or merged our instantiated IEDs with our substation section, merging our logical nodes, data objects, and attributes to our L nodes from our substation section. We've configured our GOOSE, MMS, and sampled value messages. And it's now time to export our documentation in the form of an SCD as well as our human-readable PDF. So to do that, I'm going to come back over to my dashboard. And then first, we'll start out with the SCD. I can just click the Export SCD button. In this case, I only want to export the SCD. There's a host of other information I can do, including some proprietary SCDs that are necessary for specific manufacturers. And I can just hit OK. And if we come down, once that finishes generating into our Project Explorer, you see some errors. We can take a look at those later. And open up the Out section. We see an SCD was created with today's date. I can right-click and open that with the SEL XML editor. And we see a very large XML document, which is our SCD. If I scroll all the way to the bottom just to see how many lines we have, this is only over 318,000 lines that we've created just now by doing the steps. At the beginning, we had no SCD. Now, we've created this. So, it really is impressive what you can do in a short amount of time when you leverage templates within Helinks. So, I mean, the good news is you don't need to understand this in order to use Helinks. This is really just the machine-readable documentation that the tool is creating for us. If we want to just take a peek, go ahead and jump down to maybe an Input section. So, this is going to be, in this case, this is inputs for a specific IED. This is actually our gateway. We did configure this gateway to subscribe to MMS reports. And the tool did go ahead and create XREFs so that our gateway would know specifically which reports it should be subscribed to. And that's in the form of these XREFs that we see here. That's just one of many, many pieces of information that is contained within this SCD file. So, in order to get a more human-readable format, Helinks also includes the ability to export a PDF. That includes some of this documentation. So, I'll go ahead and hit the Documentation tab. And I filled in some default information. We'll just go ahead and hit OK. And this will take a moment to generate. But in a moment, it'll output a PDF that I can walk you through that will show a lot of the same information that's contained in the SCD, but in an easier-to-read format. I'll go ahead and drag that over and scroll up to the beginning. So, this is the project report for my substation named Raleigh. And we'll go ahead and point out a couple pieces that are contained within it. I like to start with the single line. So, this is the diagram that we drew at the beginning of our presentation that contains our different bays. Generally, I would say the first half of this document corresponds with the substation section or the specification that we did, whereas the second half of the document contains the information specific to the IEDs that I've instantiated. So, scrolling down, we can see the different application groups that I've implemented, different data sets, all contained within this document. And this could be passed on to substation engineers. This is part of the packet that's usually handed over to a customer at the end of a project, if you're a system integrator who's designing a substation for them. So, it really becomes a nice reference document that you can use to know how your substation is actually performing. So, with that, we've really come to the end of our presentation here with this 141-page PDF that we've generated as part of our engineering process. And I do hope that you guys have learned a lot about how the engineering process can work when you're leveraging some of these advanced features here in Helinks. With that, I'm going to turn it back over to Beth for questions and answers. Okay, great. Thanks, Jackson. So, what I'd like to do, we do have some time left for more questions and answers, and they've continued to come in. I would like to just wrap up a couple of final points, and then we can spend the rest of the time focused on questions. So, I'm going to go ahead and take the screen back from you there, Jackson. So, just a couple of final thoughts. When you do log off of the webinar after we finish our next Q&A session, you will be presented with a survey. It is only five questions, takes under two minutes to complete, and we really appreciate your feedback. You do also have an option to receive a certificate for one PDH, professional development hour, if you do need that. We can provide that upon request. The recording for this webinar will be posted on our website early next week. So, at any time you can see a recording, we will also, anyone that answers the survey, we will also follow up with a link to the recording as well as the presentation. If you do go to our website to view the recording, there is also an option there to sign up for our newsletter. It keeps you up to date on the next webinars that are coming out, on any training events, and any product releases. And of course, as always, if you have any questions or comments, we'd love to hear from you. Just email sales at trianglemicroworks.com. So, with that, as I mentioned before, the questions are still coming in hot and heavy, so I'd like to just start to dive into the list of questions. So, to start off with the first one, and most of these will be for Helinks, so anyone from Helinks can feel free to jump in to answer. Could you model a PRP network here, or does it not matter for the purposes of this tool? The SCD file can contain information about redundancy. With every connected access point, you can select the redundancy protocol and also other parameters. But this is, for the communication configuration, pure information. It is really not necessary if you want to create a SCD file. Okay, next question. How have you verified the IEDs selected for implementation to comply with the SSD? That has already been done during the template creation. So, the templates, they can be teached to work with different kinds of relays. The templates can export relay specification, so then they know the relays and can match. But the templates can also be teached to work with out-of-the-shelf relays by remembering how these mappings are done in a quite sophisticated way. So, if you teach the templates the relays, then later, when you integrate, you won't have any problems. The templates just know them. That is why we could make the logical node association with one single click. Okay, next question. What is the base for the specific IED selection? Yeah, that's related to the same. The base is the knowledge, the experience of the protection and automation engineer. So, at the end, when he defines his substation, he has to bring in his knowledge, select the proper IEDs and teach the base to work with these IEDs. Naturally, we have somewhere a button for automatic mapping and he can drop in some IEDs and we try to make an automatic mapping. But at the end, everyone needs to look in detail on which logical node he's using for which function and what the detailed parameters are. So, we do not replace the engineer's knowledge. We just capture the knowledge and make it repeatable. Okay, next question. This one is specific to GUS configuration. How do I control how datasets are created? I may want to publish only one GUS dataset from a device or I may wish to split them into different datasets. Okay, there is a concept of channels, which we didn't show here. And these channels allow you to group your data exchange and by this influence exactly how the datasets are created and in which way the control blocks are used and configured. Okay, next question. Several IEDs are programmable devices, thus their models, ICDs, actually are built by reflecting what you specify in the SSD. But there might be differences like organization of data. DOs are the same or different LNs. Measured values in MXXU, IO signals in GGIO. Okay, basically these are two questions. One is the usage of off-the-shelf ICDs against specifically configured ones. So, you can have both cases with the SDS. We can specify the IEDs and the IEDs can be configured according to specifications and then they fit or you take them off the shelf. But even if they are specifically configured, there may be differences as is stated in the data objects or maybe in the capabilities of the logical nodes. In this case, the mapping of the logical nodes and of the data can be individually tweaked so that, for instance, we can use a data object differently than it is originally specified and the system recognizes this and validates this. Okay, next question. Does one absolutely have to build templates, say not creating them but doing the work in order to get a feel with Helix and then moving towards working with templates? Templates usage is optional. We recommend first not use Helix at all. Play with the vendor tools. Learn 61850. Understand SCL. Then maybe take the system tool Helix, make a small project and once you feel familiar with it, then you think about templates. It's not a process that you can just start from scratch. Okay, next question. Is there still some parts of Helix that are vendor-specific? If some devices are not supported yet, what are the limitations? There are vendor-specific parts that we encapsulate in so-called vendor packages. In the Edition 1 and also in Edition 2, still these were necessary to get things up and running. Since Edition 2.1, this is not necessary anymore, but still we have some vendor packages, especially to support better subscribing or configuration of subscribing for some relay types. These vendor packages are always on the move because things are changing and we are updating them with the feedback from projects and customers. Okay, next question. This seems to be a point of clarification. Is STS system integrator basically a replacement of vendor-specific IED configurators like SCL Architect? Absolutely not. As I have said in the presentation, we have the IED configuration tool and we have the system configuration tool. The tool does not replace the IED configuration tools. Okay, next question. Next question. Can you elaborate more on the configuration of goose messages? For example, for relay tripping, what data attributes do you need? Similarly, for other functions as well? Just to be very short on this, if you look at the definition of the logical nodes, there we have this famous logical node groups and we have the P group with the protection relays. Every protection logical node has data objects, most of them data objects for the start of the protection event and for the trip. That are typically the candidates that we are using, the data that we send via goose to trip the breaker. Okay, and this might be one of our last questions. Can you integrate an IED configuration tool from an IED manufacturer inside your tool? We have done this, especially for some gateway manufacturers as a special functionality and as an OEM or, yeah, it's possible. It's just a matter to discuss how it can be integrated, but we have provided the interfaces for this. Okay, well, that does bring us to the end of our questions and the webinar overall. Again, I want to thank everyone for spending some time with us and participating in the webinar and submitting all your very good questions. I ask you just to take two minutes to fill out the survey. Once you log off, you'll be presented with that and we'd love to get your feedback. And, of course, from there, as I mentioned, we'll send you a recording of the webinar as well as a copy of the presentation. So thank you again, everyone. Take care. Thank you.


Video ID: aT2pnB26twA
In this video, we'll demonstrate adding a master DNP TCP channel. To begin, right-click on Gateway and select Add TCP Channel. Select the session protocol as MDNP, that's master DNP. We'll take the default IP address of 127.0.0.1, the loopback address. Masters are typically clients. We'll stay with the default DNP port address, port number of 20,000. And we need to enter an alias name. I'll just call it MDNP. Click OK and the session editor opens. Again, we'll take the defaults for the remote and local link address for this example. We'll take the default link confirm mode of Never, which works in most applications, and the default absolute response timeout of 30 seconds. We're also going to take the default auto request mode. This works in most applications. It's pretty unusual. You need to change it. But if you do, click on this little box and the auto request mask editor opens. Then click OK and the Add Data Type dialog opens. Select the data types you want from the drop-down menu. We'll just add some binary inputs. If you want to add additional data types, just expand it back out. Click on the session and then right-click. Select Add Data Type and we can add additional data types from here.


Video ID: 3ZpsD4blvK8
This video will show how our Navigator, TestSuite Pro, and DTM tools are used in the IEC 61850 engineering process. SCL Navigator is a Windows application for visualizing, verifying, and repairing IEC 61850 system configuration language files. It can also be used by IED vendors to create ICD files for their devices. SCL Navigator contains a 61850 aware editor that provides a structured environment for compliant SCL file editing. It can be used each time a new SCL file is generated to inspect the new file for changes, check for errors, and repair any issues that may prevent the next tool from processing the file. The Navigator icon shows each time it can be used in the 61850 engineering process. The conceptual phase is an iterative process where the system configuration tool creates an SCD file which DTM uses to simulate all of the IEDs in the substation and create automated tests to confirm individual device behavior. TestSuite Pro is a diagnostic tool that uses this SCD file to monitor, troubleshoot, and confirm the behavior of the new substation design simulated in DTM. Workspaces created in both DTM and TestSuite Pro during the conceptual phase can be reused over the life of the substation to evaluate changes to the substation design or the specific equipment installed. At factory acceptance testing, some of the simulated IEDs in DTM are replaced by actual IEDs in the lab. These IEDs have been programmed by their corresponding IED configuration tool with the same SCD file used by DTM to simulate the missing IEDs. The TestSuite Pro and DTM workspaces created in the conceptual phase are now used to monitor, troubleshoot, and confirm the behavior of the newly added IEDs. Automated tests created in DTM for a template substation can be used to validate each new SCD file and behavior of the corresponding IEDs prior to commissioning each new substation. During commissioning and ongoing maintenance in the substation, the TestSuite Pro workspace created with DTM during the conceptual and factory acceptance phases is used to validate the behavior of the substation. Once the substation is operational, the conceptual and factory acceptance testing phases can be reused during the life of the substation to evaluate changes to the substation design or the specific equipment installed without taking the substation out of service. The DTM workspace may also feed a second SCADA system used for operator training to simulate catastrophic conditions. For more information about our products or general questions about 61850, please contact us using one of the methods in the description below. Thank you. www.dct.com.au www.dct.com.au 


Video ID: ckDIe5PYyAw
Let�s take a closer look at the transport function. The transport function provides the information required to reassemble the link layer frames into application layer fragments. The transport function contains an FIR, or first bit, an FIN, or final bit, and a sequence number. The FIR bit is set in the first frame of a fragment. The FIN bit is set in the final frame of a fragment. For single frame messages, the FIR and FIN bits will both be set. For multi-segment messages, the FIR bit will be set in the first frame, and the FIN bit will be set in the last frame. Intermediate frames will have neither the FIR nor the FIN bits set.


Video ID: KPcLZlUurp4
This video demonstrates using the Distributed Test Manager, or DTM, to verify point mappings. This video is the fourth in this series. Please view video number 1, Demonstration Overview, and video number 2, Demo Components, for a complete overview of the demo, as well as a description of each of the components and their roles. In this video, a DTM script validates the data mapping in a gateway device. We're using the SCADA data gateway as the gateway device to map between IEC 61850 and DNP3. A DTM script changes each mapped data point in a simulated IEC 61850 server, and verifies that only the corresponding point defined in an Excel spreadsheet changes in the DNP3 master. This script may be modified to work for any protocol supported by DTM, and could even be used to document the point mapping if one does not already exist. Note that any errors found may be due to mapping errors or documentation errors. The Excel spreadsheet contains the following information. Source and destination device paths, data type, source data point path, and destination data point path. The end of list designation marks the end of the mappings. This technique allows an unlimited number of mappings to be tested. The file will be updated by the script to indicate the test results. Now let's exit the Excel file, and run the script. As we do, we'll notice the output shows the test mappings described in the Excel file. Now let's load the Excel file. Notice the mappings we viewed in the output match the mappings in the Excel file. Also, notice that the test results have been updated with the results of the test. The test found two errors. If we look at the mappings with errors, we see that the first error is due to an incorrect mapping. The Excel file indicated that phase A current magnitude should be mapped to DNP3 analog 0.1, but it is actually mapped to DNP3 analog 0.0. This is probably a typo in the documentation, since the previous row maps phase A current magnitude to DNP3 analog 0.0 also. The second error shows a missing mapping. The Excel file indicates that phase B current magnitude is mapped to DNP3 analog 0.1, but changing the current did not affect this point. Additionally, no other points changed, indicating that the phase B current magnitude is not mapped to any DNP3 point. Now let's look at the script that drives this test. The script is composed of two lines of code. The script is composed of three parts. First is some initialization code. Second is a test loop. In this loop, we change the IEC 61850 server data based on the data type and verify the correct data point changed on the DNP3 master. Finally, we see the callback function. This function is called whenever a data point change is received. Notice the print statements in the test loop and the callback functions. These statements print to the output window in the test script. Having the correct point mappings is crucial to the proper operation of your system. This script provides the basis for point map verification and documentation. You can easily add more points to the test loop, You could also tweak this script to include other protocols. And with relatively minor changes, you could even use this technique to develop your point mapping documentation.


Video ID: l7-gMcoLGtE
Because traditional SCADA protocols were defined to move data with a minimum amount of overhead, the meaning or semantics of data as it's spread through the system has to be maintained in different ways. So typically you'd have configuration of RTUs and IEDs, configuration of databases, and configuration of applications that all have to be done to make sure that the right data is getting to the right place. So instead of moving phase A voltage from some particular line, you're moving point number 3850, and someone's got to figure out all along the system whether that point, as those point maps occur, that the right data is flowing in the right places. This validation is costly and time consuming and requires a lot of staff time to be able to maintain this.


Video ID: Lbl_Xhp1-8o
This video is a quick tutorial on how to migrate from an older version of our SCADA data gateway, version 4.2 or older, to our newly designed versions, which is anything 5.1 or greater. Before I show you the step-by-step instructions on how to migrate, I'd like to talk to you briefly about why you should upgrade. The new SCADA data gateway is built on an extremely reliable platform, which now includes support for both Windows and Linux machines. This version of our SCADA data gateway has been redesigned to incorporate a web-based user interface. The user interface has a familiar look and feel from our previous versions, so the learning curve is minimal. But the browser-based interface allows for remote configuration and monitoring of gateway operations. New features that you'll find in this version include user access management to give individual users access to the tools that they need based on their role, audit logging for user access control to review historical data on actions performed by each of the users, workspace support for quickly swapping between different configurations of your gateway, protocol and system logging with added filters to quickly locate event data, the ability to drag and drop multiple points for more efficient mapping, an added special view to quickly see the performance metrics and the health of the system, and finally the new ability to search and filter on points lists from the web interface to display all the data that you're most interested in. So now that we've talked about that, let's move on now to actually migrate. If you had our old gateway, it should look something like this. The first thing that you'll want to do to migrate is to save your old workspace and configuration files and close the application. So you can do File, Save. And then after saving, you can go ahead and close down the SCADA Data Gateway application. There's no reason to save again. Then what you'll need to do is run the installer for your SCADA Data Gateway 5.2 or higher. You'll want to run it as administrator. Now this screen is very important, as you can tell by the large lettering. You'll need to have credentials to log into your SCADA Data Gateway and set it up. That information is displayed here. Afterward, click I understand, then next. Now we're going to go through the normal SCADA Data Gateway install process. For time's sake, I'm going to go ahead and speed this section up a little bit. So this next space is how we actually do our workspace migration. This will set up your first workspace from your default configuration from your previous SCADA Data Gateway. So this is the tool you'll be working with. The first thing you'll need to do is select your INI. So you can find your INI file at the normal install place, or if you pick somewhere specific, you can navigate there. Normally, it is in Program Data, Triangle Microworks, SCADA Data Gateway, and it is called tmwgateway.ini. You can give it your workspace a new name if you don't want to just use your INI's name. Then you can go ahead and migrate your workspace if the defaults for the workspace configuration location is to your liking. So a lot of stuff will output here, and you can navigate through to look at your errors and your warnings and any of your other logging information. Most importantly, you want to migration done at the end and verify that there are zero errors. You can also copy this out to a clipboard or use the workspace as your default for your first startup of your new gateway. From here, you just hit the X or hit OK, and then click Finish. From there, a simple restart should get you ready to go. you


Video ID: YsRb6glzKJo
This video will be explaining how our tool Distributed Test Manager, or DTM, can be used to automatically test our SCADA data gateway. It will be broken up into two parts. The first part will be on setting up the test environment for our SCADA data gateway. And the second part will be on how to set up the automated tests within DTM. But before we do that, let's take a look at this diagram. This diagram is an overview of the environment we will create and the methods behind how we'll be testing our gateway. In the middle, we can see the device under test, the SCADA data gateway. In a normal environment, the SCADA data gateway could be communicating with IEDs, seen at the bottom of the diagram, and with a SCADA master, as shown at the top of the diagram. This will allow for data to be transmitted from the IEDs up to the SCADA master through the gateway. You will be recreating this environment within DTM by simulating the IEDs and a SCADA master. This will allow DTM to set up the initial conditions that each test may require and validate how the system responds. For example, if a control is issued from a SCADA master simulated within DTM and you know what the expected results should be within the IEDs, DTM can watch for those changes and pass or fail the test accordingly. You can expand this test procedure to include all required controls and points, fully testing your SCADA data gateway's functionality. If all the tests pass, you can be confident that your SCADA data gateway was properly configured and if a test fails, you'll know you need to investigate your configuration for these points more closely. So now that we've looked at the diagram and talked about the overview, the first step we need to take to automate our SCADA data gateway is to set up our testing environment. For the sake of this video, I'll only be simulating two IEDs and one SCADA master for our gateway to communicate with, but of course this could be expanded to include many more devices. Based on the design for my system, I have pulled out the relevant device configuration information and put it into an Excel spreadsheet. At the top, you can see my SCADA data gateway IP addresses along with any of the remote IP addresses for any of the devices it needs to communicate with. This information will be required for setting up my test environment. So let's go ahead and get started. You'll see over here on the left is our DTM or Distributed Test Manager tool. On the far left side of the application, you'll see this workspace tab. This workspace tab is where you'll add all of your devices, all of your scripts, any of your folders, any of your test managers, or basically anything that you need for your DTM to run properly. We're going to go ahead and create the devices that we talked about over here in our Excel spreadsheet. To do that, you go to the new workspace and right-click, Manage, Add, and then from here you have all of the options of things you can add. I always like to start with folders as it keeps things a little more organized. I'll name it IEDs and hit OK. Now instead of right-clicking on the new workspace tab, you can right-click on this IEDs folder and follow the same process. So the first device that we'll be looking at is this bay controller device. That is a 61850 device. So I'll go down to IEC 61850 and I know it is an IED or a server that we'll be communicating with. So I'll click Server. Luckily for us, the configuration of a 61850 server is done completely through this wizard and a lot of the information is stored within its SCL file. That can be your ICD, IID, CID, SCD, any of these type of files will work. So you can just go down to SCL file and click this paper and pencil. From here you can click Import Configuration and navigate to your SCL file, but I'll be using a sample configuration file. I'm going to be choosing the breaker.ICD as I know that's what's going to match my SCADA data gateway. This pop-up here is letting us know that we can actually monitor the breaker.ICD, meaning if you're still going through your engineering process and changes are still happening on top of your ICD file, you can go ahead and monitor it. That way the next time you load this workspace up, it'll automatically update to your newest version. I know that this configuration file is not changing as it's one of our samples, so I'm going to hit No. From there you just click OK and it will bring in all of the information needed to simulate this device. Now I know that my SCADA data gateway is looking at a specific IP address, so I'll go ahead and include that. As you can see, our SCADA data gateway is communicating on 10.10.10.106, and it's expecting the bay controller to be at 10.10.10.102. So I can just simply come here and type it, or you have a drop-down of all the IP addresses listed on your computer. If you happen to have an IP address that isn't necessarily listed on your computer, instead of being green, it'll turn red. From there you can click Assign IP Addresses and go ahead and add that IP address to your computer to allow for proper configuration of your device. I'll move back to what my gateway is expecting, and from here I'll just click OK. Now that my device has finished its initial setup, you can see that the Wi-Fi symbol or the connection indicator here has turned green, indicating that it has connected up to a client. That would be my SCADA data gateway. So that is how you'll set up a 61850 server. You can then go to that server, right-click, Show Model, and have access to the whole model, all of the logical nodes, the logical devices, all the way down to the data attributes. For example, I could follow this XCBR all the way down to its position, and the positions STVAL. So now let's take a look at how you would configure a 101 IED. So again, we'll go back to the IEDs folder, right-click, Manage, Add, 101, and I'm going to do a Control Station. From here we're given a wizard again. You can give it a specific name. So for example, I could say 101 IED, and tell it which computer to be hosted on, which will obviously be the one where I'm going to be simulating the environment. In the Channel tab, this is where you have access to things like TCP IP information. That includes your local address and the remote address. I have that information again over here on the right. So for the local address, I will have 10.10.10.110, and the remote address that I'll be connecting up to is 10.10.10.112. Of course we'll be keeping the port as whatever's default for the protocol. You can then go through the rest of the wizard, such as session information, if you've got any changes specific there, sector information, and any of the other information that might be considered more advanced and not used most of the time can be found in this Advanced tab. Things like delays and buffer sizes. Of course you can go back to the Database tab, and this will allow you to properly configure up what the database for your 101 device should be. So for example, we just have a default database here with several pieces of information, several different data points for each of the types that we can support. I'll go ahead and click OK. And now that has properly set up my 101 device and my 61850. So for the sake of this video, I have already simulated all of the devices that will be the IEDs that my SCADA data gateway will connect to. Now let's go ahead and add the SCADA master. Following the same process as before, I'll right-click, Manage, Add. I'll do another folder, SCADA Master. Then on the SCADA Master folder, I'll right-click, Manage, Add, DNP3. And for the master, I'll obviously be creating a master. This will be hooking up to the database that our SCADA data gateway is concentrating all of the values to. Again, I'll give this a name, so I'll name this SCADA Master, and I'll go to the channel. What you might notice is the configuration of our 101 and DNP3 look very similar. This is an intentional design decision that we made. So that way, once you learn how to configure one device, you'll already know how to configure any type of device. So following the same process as before, I'll go ahead and add the local IP address. That'll be on a different net, so 151.100.100.51. And the remote address is 151.100.100.50. Just like before, you can go to the Session or the Advanced tab, or you could go to the database and make any changes. Since I'll be pulling the database in from my SCADA data gateway, I don't have to properly configure it here. So from here, I can just click OK. And just like before, we'll see it turn green and everything is now connected. I can then go to this Master and right-click, do Commands, and do an Integrity Data Pull. This will allow me to gather all of the information off my SCADA data gateway. So you can do this on Connect. So anytime your device connects up to your SCADA data gateway, it'll perform an Integrity Data Pull. Periodically, you could do it, say, once every 24 hours. Or you could just execute it once, which is what I'll do. So now, if I go and do Show Points on my database, we'll have access to all of the points within my SCADA data gateway. You can also go back to the node that was created for this Integrity Data Pull and execute it again. Or if you'd like to add a different control, you can right-click, Commands, and select any of the controls provided here. I'm going to set up an unsolicited message. I'm going to enable them. This will allow me to get any of the changes that happen within my SCADA data gateway automatically reporting straight up to my SCADA master. So I'll do this on Connect. So anytime it connects, I'll also execute it now and hit OK. And that's all you need to do to configure your devices to set up the proper testing environment for your SCADA data gateway. So now that we have the environment set up to test our SCADA data gateway, let's go ahead and start adding automated tests. You can do this by going to the new workspace, right-click, Manage, Add, but this time we'll be adding a test manager, specifically the gateway tests. We'll give it a name and hit OK. From here you'll see a new node has been added to our DTM workspace, and we can interact with that now. Now this is just the framework for our test. We'll be adding the test inside of this. To do that, you right-click and then do Show Test Action Node. From here we'll need to let DTM know which points need to be changed and which points should be monitored for that change. So you can do that by clicking Manage Tests, clicking here to add a new item. This item will be a test, and I will be doing a monitor data point test. Now it's important to explain what's going on here. We have two tests by default within DTM's test manager. We have the monitor data point test and the control data point test. The monitor data point test will cause a change within the IED. That value will then be reported or sent somehow via unsolicited messages or polling or reads or whatnot up to our gateway. Gateway will then put that into our DNP3 database within the gateway and then send that up to our SCADA master here in DTM. Since we know what point changed and we know what point should change in the SCADA master, we can verify the data properly got through. The control data point test is the opposite direction. It would be the SCADA master within DTM issuing control down to our gateway, the gateway, the SCADA data gateway, then deciding where that control needs to go within our IEDs and issuing that control down. So for the sake of this video, we're going to do a monitor data point test. We'll give it a name. Now this name is going to be based off what test we'll be running and I can see the mappings over here for our SCADA data gateway in the spreadsheet. So I will name this P101. Now I need to tell DTM which point will be changing. Well this would be our 101 point, T1P101. So I'll go back over to our IEDs, I'll show their database, and then I'll grab that point 101 and just drag it into this dialog box. I also need to tell DTM which point to monitor. That would be our DNP SCADA point, our T1P1. So I can go back over to our SCADA, show points, and then grab our point type one, data point one, and drag that over. Technically from here I could just hit okay and DTM can infer all the rest of the information. But you can also go in here and fill it out yourself. For example, if you have initial values, if you want to set it to false and then set it to true for like a boolean, or if it is an analog value set it to zero and then a hundred, you can force those values. Otherwise DTM is smart enough to go look at its current value and just toggle that or increment it or whatnot. You can set up the expected value or an except delta for that expected value. For example, if you have an analog value and you're sending 2.5, maybe you know your SCADA data gateway will round that up to three. So you might want to put a delta of 0.5 or one, whatever is within your testing parameters. You can also change the qualities so you can verify things like quality being passed through your gateway, or change the wait time for your gateway. So for this video, I'm going to go ahead and just put 10 seconds in here. So a thousand milliseconds or 10,000 milliseconds for the wait time and hit OK. I'll go back to our test manager and we'll see a test has been added here. From here, I can just double click on the test. It'll go out. It'll change the value in the IED. The value will then be passed through the gateway and reported up to my DNP3 master via the unsolicited messages that I enabled in the last part of the video. Of course, now you can repeat this process by going back into your tests, adding a new monitor point, going to your device. The next one would be 102. And then we would need to do the SCADA master as well, which is 0.2. And you could repeat this process for all of your points. Of course, this might take a little while to go in and manually add all of this stuff. So we've added the ability to kind of multi-add all of these points. Luckily for me, the mapping process here is very logical. It just increases by one each time. So I can go in and use the powers of Excel in order to kind of mass produce my tests. So I can export this Excel file, and I will save this as a CSV file. And then I can go ahead and open that CSV file. So I can do that by doing import here, right click, and then I will do open with Excel. This will bring up the CSV file generated through DTM. As you can tell, each of the columns here are labeled with what value should be inside of them. And you can customize this as much as you want. But what I can do is just grab these values here and drag down. And what you'll see is these points are then actually being incremented for me. So I know that my test will go all the way through 0.109. Currently I've gone through 107. So if I just go two more. I can then save this off, close it. And then I'm already sitting here at the import CSV file. And we'll see that all of the tests that I made through the CSV file are now populated within here. So now if I just click OK, and go back over to tests, we'll see those tests are there now. And I instead of individually clicking on each one, I can just click run all, and it will go through and change each of the individual points on the 101 device in my IED. The value will then go through my gateway and report up to the SCADA master. Now if any of these tests fail, I would be able to go and look at the specific point that failed and check the configuration and the mapping between the 101 point and DMP3. Of course you can expand this in all sorts of ways. You can add the analog values as you see below here. We can add in the values that are supposed to be mapped over from the bay controller device or we can make our test a little more complicated and have more pass fail criteria. So I'll go ahead and click stop. We'll see a failure there because I interrupted it. And I'm going to show you how you could set up more advanced tests. So in order to do that, I'm going to go over here and shut off the unsolicited message. So I can right click and then do manage edit and do disable and hit execute. That should disable the unsolicited messages. And the reason I'm going to be doing that is because the more complex tests I'm going to be using are going to dictate which way that our SCADA master expects to receive the data. So I can do that by coming into the test manager, clicking manage tests, and then selecting new test actions. But I'm going to create a new test manager to do this so that way we can keep everything organized. I'm going to hit cancel, close this out, and I'm going to create a new test manager. I'll name this test advanced, hit okay. Now I have two different test managers that I can run. And from here I can just right click, show, do manage tests, and now I can select the more complex actions. So if I come down here, I can turn off this protocol independent. That's where the control data points and the monitor data points are. And I can enable the DNP3 specific actions. That can be analog inputs and outputs, binary inputs and binary outputs. We'll just take a look at the binary inputs. So I can click this and hit okay. And from here I can go ahead and add more tests. So just like previously, I can click to add a test, then I can select the actions. But instead of having control or monitored point tests, we now have read binary input class, read binary input index, or unsolicited binary. So we can now have a little bit more specifics for how we're receiving the data and how we're supplying data. So if I just do a read binary index, the first part will look very similar. Then I can go and grab the T1P100, put that in here, and then I can go ahead and grab what that maps to, which is the T1P1. And all of this looks the same. Of course, you can even name it, you know, this is going to be P100 by read. Now we can get to some of the other columns that we hadn't seen before. Obviously, we've seen initial value, set value, expected value, read delays, you can set up different variations, of course, the qualities here as well. But then we can repeat the same tests by adding, say, a test where we're going to read it by its class data. So now this is P101, put 100, and we can use the same values. So we'll grab this guy as the SCADA master, the same point here. And in this situation, we'll then be getting it by its class data. So we can set it up for a specific class data to be read. All of the information has been changed. You can see some of these are now grayed out, so there's no variation when you're doing it by class. But you do need to supply what class this could be in. So you could do class 1, 2, or 3. You can even get some negative tests in here where you know a point shouldn't be part of class 1 or 3. You can actually run this test three times and verify the value does, in fact, change when you change 2, or it fails when you do class 1 or 2, or 1 or 3. And of course, we can add the next test, which is what we already did from the other one because I enabled unsolicited. We can set up unsolicited. This could be 101 by unsolicited. And just like previously, we can bring these same points over. And then of course, as you would expect, class has now been grayed out because you don't need class data when you're doing unsolicited when you enable the unsolicited. So now if I hit OK and go to our test advanced, we can come here and do read all tests. And now we can run the same test three different ways, validating three different behaviors from our SCADA data gateway. Now you'll see how one has failed here, and that's because I didn't actually supply the class for this point. So I know I can go in here and investigate either why my test failed on DTM side, or I can go into my SCADA data gateway if I did provide the parameter and see why it failed. So these are the different ways that you can set up automated tests within DTM. First you'll need to go ahead and set up the test environment with your IEDs or your SCADA master. You'll need to have access to the IPA information so you can simulate those devices. Then after that, you need to have the information for the point mapping, such as which IEDs point changes will affect the SCADA points on the end. You can also run these tests backwards by doing the control tests. I hope this has helped you figure out how to use the test manager actions and to automate testing of your SCADA data gateway. And of course, if you have any questions, feel free to reach out to us at support at trianglemicroworks.com. Thank you.


Video ID: NlNobKIoUzw
Do you need to add support for an industry standard protocol into your device? By using our software libraries, you'll be able to reduce the amount of development effort required to support the protocols, which lets you focus on developing company proprietary or market differentiation features. Why spend effort developing to a standard that's required to play the game, when you can spend that effort developing features that set your product apart from the pack? In addition, our libraries are widely accepted and deployed around the world. Using our libraries reduces compatibility issues, lowering software development risks of your development. All of our libraries are available in NC Standard C. Some protocols are also available in C++ and as .NET protocol components. Our NCC and C++ libraries are designed to be processor and operating system independent. While we provide target implementations for Windows and Linux, they are easily adaptable to any operating system, including informal, device-specific operating systems that are often found on small embedded systems. Speaking of embedded devices, our libraries are designed to have a small footprint and operate efficiently, making them ideal for these applications. At the same time, they're very scalable, so they work well in large computer-hosted systems as well. We were the first source code library to market with DNP3 Secure Authentication Version 5, or SAV5. We're also the first library to offer full support for IEC 61850 Edition 2. Our library supports running version 1 and version 2 on different connections in the same device, which simplifies your upgrade path. For more information, please visit the source code library section of our website at www.trianglemicroworks.com slash products slash source dash code dash libraries.


Video ID: UgZfvcOlnlo
In this video, we'll be going over how to install, configure, and license DTM. To download DTM, you'll need to navigate to our website, www.trianglemicroworks.com. From here, you'll need to log in or register an account with us. After that, you'll be taken to the Products page. You can download any product you own, or download evaluations for any of our other products. Click on Distributive Test Manager, and your download will begin. During the install, you'll see this window. It is used to configure DTM's service, network, and licensing. If you've already installed DTM, and you need to reconfigure it, you can always access this window by opening DTM Administrator, clicking Tools, then Configure DTM Services. It can also be launched directly as DTM Configuration Tool. DTM can be used to distribute large, simulated systems across multiple computers. This is what's configured on the first tab. The first radio button will configure your PC as a DTM Master. A DTM Master is the licensed DTM computer that allows all other machines in the network to communicate with it. The second radio button will configure your PC as a DTM Slave. DTM Slaves do not need to be licensed, but must be connected to a DTM Master. These machines are used as extra resources for your simulations. DTM Slaves are not required, but for very large system tests or network traffic tests, they can be very helpful. When configuring a DTM Slave, you must provide the IP address of the DTM Master. You can also launch the License Manager from here, but we'll get back to that later. The second tab is Adapters. The first drop-down box will select a DTM Adapter. This adapter will be used to communicate DTM-specific data like communications between the DTM Master and Slaves. This can be helpful when trying to separate traffic, for example, DTM traffic from corporate network traffic, or DTM traffic from protocol communications. The second drop-down box is 61850 Specific. This will allow you to select a default goose adapter, which is used for broadcasting goose messages across a network. The last tab I'll be talking about today is the Service tab. This tab allows you to decide whether you want to run DTM services as Windows Console applications, or in the background, as Windows Services. Console applications allow you to monitor background processes, like DT Manager and DTControl, while Windows Services will hide these away. You can start and stop DTM services from here as well. This is how I generally launch my DTM services on my Slave PCs. The last thing we'll be talking about in this video is licensing DTM. If you don't have a license, don't worry, we'll automatically provide you a temporary license when you install, so you may trial our software. If you do have a license, you can launch the licensing window during the install by clicking Manage License from the Service Configuration window. You can also access licensing from DTM Administrator by going to Help, Manage Licensing. You will be prompted that starting up the License Manager could cause conflicts with any currently running DTM processes. Make sure to save any work that you may have done before updating your license. From here, you'll be able to click Activate Product Key, and Activate Online. Now, just enter your key, mark it as new or an update to an existing key, and you're good to go. If your PC is not connected to the internet, please contact us directly and we'll walk you through the process for offline licensing. If your license key is already active, you can go back to the License Manager and see details of your license. Important items to keep in mind are maintenance plan expiration date, number of connections or devices, and any protocol that you're licensed for. For more information on this or any other questions about DTM, please contact us using the information in the description below. Thank you.


Video ID: HKUdSsmbKJ0
My name is Beth Capellas, and I'm the marketing manager here at Triangle Microworks. Again, I just want to thank everyone for joining us today. Today's topic will be learning on how to automate your SCADA testing with your gateway or your data concentrator. I do just have a couple of housekeeping items I'd like to go over before we get started. We are expecting a large number of folks to join, and I see quite a few of them have already joined. So everyone is currently on mute, and it sounds like now everyone can hear us. The big thing is we definitely want you guys to ask questions throughout the whole webinar. However, what we'd like you to do is go ahead and submit that question via the GoToWebinar menu. Those questions will be queued up on my end here. We will take a couple of breaks throughout the webinar to answer some of those questions. And anything that we are not able to get to, we will respond via email after the webinar. So just as you have a question, go ahead and submit it. It is being seen. And like I said, we'll take a break. We'll answer some. Hopefully get to them all, but if not, we'll get you the answer one way or the other. I also want to mention the webinar is being recorded. That recording will be emailed to all the attendees after the webinar. We will also be posting the recording on our website. So if you go to trianglemicroworks.com and go to the Training tab, you will be able to find the recording there, as well as our upcoming webinars and any other training courses that we will be hosting. With that, I'd like to introduce our presenter, Matt Green. Matt is our Senior Applications Engineer here at Triangle Microworks. He's been with us for about six years, and he specializes in QA testing with our simulation tools in Gateway. So I will hand it over to Matt. Thank you for that introduction, Beth. So I guess I'm going to start with a few slides from a PowerPoint, give a little introduction of what we're going to be going over today, an introduction of who we are. Then I'm going to go to a live demonstration of our testing tool. And then at the very end, we're going to circle back to some PowerPoints and extra slides of what we went over and a few other things we can do. And then, as Beth mentioned, we are going to have some break points midway through and toward the end where we're going to answer some of the audience's questions. So we may take some breaks for that. So let's just start with who we are. We are Triangle Microworks. We were established in 1994 in Raleigh, North Carolina. We've been in the SCADA protocol industry for over 25 years, and we have products that we supply to the entire spectrum of the utilities industry, from equipment manufacturers to system integrators to electrical utilities. So some of the products that we sell are the source code libraries that I was talking about in the previous slide. We sell our SCADA data gateway, which will be the device under test for this demo, and several other testing tools. What you'll see on the screen right now is a few ways that we have displayed logic sequences that may happen inside your system. The bottom one here is our logic analyzer, at which you can see the timings of when certain values go high or low and see them in coordination with each other. What you're seeing here on the right is our signal flow diagram, which allows you to see how systems propagate around the system and interact with each other. And these are just some of the displays that we have to help you show what is happening in your system. So this is going to be our device under test today. This is going to be our SCADA data gateway. This is our own gateway tool. It's very similar to an SMP, an RTAC, any sort of data concentrator you have, or potentially a gateway. It does cross-protocol mapping, data concentrating, and other SCADA functions. So the test tool that we're going to be using is Distributed Test Manager, or DTM. Essentially what DTM is is it is an application that can simulate large systems of devices, and we're going to use this to simulate the downstream and the upstream devices for our data concentrator. So this is going to be kind of a quick overview of what we're going to be simulating. Everything you see here in green is going to be simulated in DTM. That's all of the 61850 IEDs downstream, which is going to communicate via reports up to our SCADA data gateway. The SCADA data gateway is then going to map those 61850 data attributes to an outstation inside of the gateway as well. That will report upward using unsolicited messages to our simulated in DTM DNP3 master. And since we're going to have both sides of the communication, where the data began and what it changed to, and then at the end what it was eventually mapped and reported up to, we can then test that the values went through our gateway device properly in a specific amount of latency. DTM will also need to have knowledge of the mapping information, which we will supply to our test. And that's pretty much what I'm going to be going over today on how to set up in DTM. So let's just jump straight over to DTM. So this is our distributed test manager administrator application. This is how you're going to interact with the DTM network. And right now you're just going to see our getting started page. This is the very first thing you'll see when you load up DTM. And I'm going to go ahead and start one of the workspaces I had started with earlier. So the only thing you're going to see here in the workspace is a generate mapping JavaScript, which we won't be using today, and also a folder called test that has our status tests inside of them, which I'll be getting to a little bit later in the demonstration. But I just want you to know that all of this stuff is included in the sample workspaces that we provide. So when you install DTM, you'll automatically have this in something we call a SCADA gateway testing workspace. So I just pulled that test directly out and put it into this empty workspace. So we've actually already configured our SCADA data gateway before the webinar started, and it is plugged directly into my computer via Ethernet. So all of the devices have been configured as they would in your actual system deployed. So what we're going to need to do now is we're going to go ahead and simulate the downstream and upstream devices. We need to go ahead and create those. So for organization purposes, I'm going to go and create a folder. And I'm going to name it downstream. Then I'm going to create an upstream folder. Upstream data. So currently these folders aren't really doing anything. They're just for organization purposes. You could name them whatever you want, but I've decided to name it based on the diagram we have here. So now I would like to create our entire downstream substation. So I could create these devices individually, or I could use our import substation tool. If I use our import substation tool, I can do it all from one interface. That uses a SCD file. So I'll go to file, import substation. This window here is basically just asking us what configuration file are we going to use. So I'll click import configuration and navigate to my SCD file. And then I'll hit okay. So this is bringing up our import substation configuration window. There's three different ways that we have set by default. The very first one, which is the one we'll be using today, is the SCL file restrict mode. And essentially what this is going to do is it's going to look inside of the SCL file, and it's going to find all the IP addresses, and it's going to find all the configuration information for the IEDs inside. And it's going to limit it directly to that. This is exactly how you'd want to test it in the field, because those are the configuration options you're going to select. So the next thing I'm going to do – well, actually, let's go back a second. The second option you can do for importing a substation is user defined. Essentially this is going to let you choose any IP address, any subnet mask, any configuration, and just be able to change it. That way you can make it fit your testing environment. And the very last import substation option you can choose is loopback addressing. And essentially for loopback addressing, it just sets everything to a 127.0.0 IP address, which every computer will have, and it will just feed any of the traffic back into your computer. That way you don't have to worry about IP conflicts, or you don't have to worry about any sort of configuration problems. But since I'm going to be configuring this like it would be out in an actual system, I'm going to leave it at SCL file restrict. So now let's look at our individual devices. So what you'll see here is that some of our IP addresses are green, while others are red. So if it's green, that means that your computer already has access to that IP address on your specified network adapter. So because 127.0.0 is accessible to any network adapter, I automatically have those, but I don't have any of the 10.10.10 subnets. So I'm going to go ahead and choose which computer I want to simulate these devices on by multi-selecting the host column, and then selecting a new host on mgreenvc, and I'll apply that. So this essentially is just saying this is where I want my devices to be simulated. So I could set up a different scenario where I have multiple machines in my DTM network and want to simulate my devices across the network. And then what I'll do is select all the IP addresses I do not have, all of the red ones, and then just click Assign IP Addresses. What this will allow DTM to do is reassign IP addresses to specific network adapters in your DTM network. That way you don't have to go in through the built-in Windows tools or the command line. I'll click Assign IP Addresses. From here you'll see a similar window. You'll see all of the devices that I'm planning on creating and adding IP addresses for, the IP address and its mask bits, and then the network adapter that it's going to be set to. I could come in and individually change these to different network adapters if I had multiple of them on my computer, but I want everything to go to this one. So then I'll click Assign. After clicking Assign, you get this pop-up, and essentially it is one of your last warnings to say, hey, you are configuring your network adapter. We've already done a ping test and tried to check to make sure that adapter IP address is free and you're not causing any IP conflicts, but we want you to make sure that you know what you're doing and you're not going to cause any sort of conflicts on your either testing environment or corporate environment. So because I'm confident in this, I can click Yes. So what we see now is that all of the red IP addresses have now turned green because my computer is now configured to have that IP address on its network adapter. So I'll go over here to these IEDs. These are the ones I'd like to pull in, and then I'm just going to drag this to my substation folder, and then I'm going to close the import substation. So what you'll see is there is a little connection indicator next to the folder and next to each of the channels. When it's green, that means that the device has connected to the client in our SCADA data gateway, and eventually what you'll see is they'll all turn green. A few of them were orange because they weren't quite connected yet. And once they all turn green, I can actually just hide away this folder entirely, and now I can see from this green connection indicator that all of my devices inside this folder are connected. So everything's good to go. The next thing I'm going to do is I'm going to create our upstream device here, the DNP master. So I'll right-click and do this as an individual device since I'm not doing an entire substation, manage, add, DNP master. And from here we'll get our all-in-one configuration. You can configure the device, which essentially is where it's located and what its name is, the channel, which has all of its TCP connections, and then you can drill down into the other configuration options. But for today I'm going to do it as default. Though my SCADA data gateway has a different remote address for my database, the outstation. I'm going to go ahead and set that, 10.10.10.7, and increase the port to the proper number. So now we'll see that my MDNP device has been created and connected. The only thing we need to do from here is enable unsolicited messages. So to do commands, you right-click on the device you would like, go to commands, and then I am interested in enabling unsolicited messages. I'm going to hit okay. And that will create an enable unsolicited message node. Now I can right-click on this and execute the commands. And at this point we have now created all of the downstream devices in our substation. The gateway was already configured before the webinar started, and now we've got our master upstream set up. So at this point we're going to stop and take some questions and see if there's anything that we should answer. Beth? We did have one question come in. Do you have to configure devices each time you want to test? I guess it depends on what test means. For each individual test, you will need to build up the different configurations for your devices. That being said, now that I've made this workspace, I can just go and save that workspace off. And then if I want to repeat this same test, I'll already have all the device configurations set up for all the devices for my downstream and upstream. Obviously, if the configuration in my SCADA data gateway changed, I may have to go back and tweak some of those fields. But if you want to rerun this test, say you make some changes and you want to make sure you haven't regressed or anything, you can still use the same workspace and the same tests over again. Okay, I've got another question that came in. Can a master be set up to pull the data gateway? Yes. So if we've got our SCADA master here, I enabled unsolicited messages just so it would happen automatically. If I went to a different command, I could do an integrity data pull or report by exception pull. And when you go into these, you can actually set it up to do it periodically. So you can either do it manually if you click the execute button or right click on it and say execute, or you can set up this periodic playback here and decide if you want to do it, say, once every hour, a few minutes down to the seconds, even the milliseconds. And that will go ahead and pull either the report by exception or just do the integrity everything. And there's actually several commands that you can actually set up inside of a master. Okay, I've got another question here. Are those IEDs, are the IEDs, physical IEDs running in the network or simulated IEDs? So all of the devices I created in the downstream substation folder, those are simulated devices that DTM is simulating. That being said, once they're instanced in DTM, there's no smoke and mirrors going on where DTM is kind of talking to itself and doing anything. It's physically, I mean, it's simulating the devices out there. So if you bring up Wireshark or if you bring up an actual physical device and plug it into this machine as a 61850 client, they're going to communicate as if it was a physical device on your network, but it is simulated in DTM. And that way we have control over it. We can change data and work with it downstream so that we know all of the conditions to our tests. Okay, I've got lots of questions coming in. We will try to get to a couple more of these and then might have to move on. Can I send goose from downstream IEDs? Yes, absolutely. So our 61850 stack, which is our source code library, is also used inside of DTM. It supports goose messaging. It supports, obviously, reports, as we're doing in this, and it also supports sample values. And it supports all of the features of 61850. So you should be able to do that as well. As a matter of fact, the devices I've instanced today actually do goose, and if I had given them a goose adapter, which is selectable, similar to how the IP address adapter was selected, I would be, anytime I changed any data related to a goose control block, it would send out a goose message. And they also subscribe to goose as well. I'm going to try to take two more. Two more. How about serial communications from gateway to SCADA? Yes. So I configured this device here with an IP address and port number. There is a radio button that you could click to change it to a serial connection as well. So we do have that option provided in DTM. Again, I will try to come back to some of these questions. Is there any single line diagrams that could show the connection of IEDs in a substation or network? Yes. DTM can provide a single line diagram. It's something that we currently build up manually, and we may be working on automating that in the future. And I'll actually circle back to that question a little bit later after we finish the demonstration of automating the gateway testing. But, yes, there is a way to do single line diagrams. That's through our insight displays, which can be used to graphically show off data in your system, show off single line diagrams, front panels, anything along those lines. Okay. Okay. I see lots of questions coming in. Keep them coming. We will get answers to them. I'd say at this point go ahead to Section 2. Okay. And they'll loop back to questions. So I'm going to go ahead and close these folders and go down to this test folder. And I'm going to open up the test manager. So this is one of the newest features in DTM. It was released, I think, two patches ago. This is our test manager. And essentially what it does is it's a way you can automate the tests in your network. So let's just take a look at one of them currently. Essentially what's happening here in the description, you can see, is that there is a path to a 61850 data attribute, and that's the source. And the destination has a path to an MD&P database, which is a T1P0. So essentially what I've provided with a file lets DTM know that this downstream point should eventually map to this upstream point. So when I run this test, it will change it downstream. It will go to gateway via reporting. It will map over to D&P and then unsolicited message back to DTM. Once it sees that these two are equal and it's the proper value that's changing at the right time, it will also calculate the latency, how long it took to get from the bottom to the top. And that will allow you to determine if you're within your criteria. We'll click on that. So now I've run the first test. Let's get a little bit more real estate down here. So I've run the first test. The description didn't change, but the log did change. So essentially what you'll see here is it's changing the downstream device from false to true. And then eventually it gets back the SCADA point that changed, the upstream, and that was T1P0, like we expected. And then so the value is that the test passed, and the latency was about 250 milliseconds. So essentially this whole loop from the bottom to the top took 250 milliseconds, and the value changed. It changed to the right value, the right quality. And anything like those can be tested as well. So I've run one test, but I can also run multiple tests. But let me show you something else before I do that. I'm going to go ahead and show the database of the SCADA master, which essentially what this is going to do is it's going to show the point list. And I'll just get rid of some of the extra sections, say real estate. So the first test, as you can see, was triggering on T1P0. So you'll see that this value is now on. It's received an unsolicited message from the gateway and now has gone on. So when I run the second one, we should see the second point change. And sure enough, in our database it did. It took 100 milliseconds to go from the 6150 up to the NP. And you can continue down this list and see your database change as you're receiving information. Now this wouldn't be super exciting if every single test passed every time. So we've properly configured the next one to fail. So if I run this, it'll time out, and you'll see that the value, the test failed, and that the value was not properly received. Now we've configured that to fail, but once you see one of these Xs, you can decide what you want to do with it. You need to go investigate either your simulated devices in DTM, perhaps something wasn't configured properly there, or maybe even look into your SCADA system and figure out what's configured on your device under test. So now I've run four tests individually. That obviously is a manual process you can run. You can also just multi-select several devices and say just run the selected tests. And it will go through and run them. And, of course, there's a run all tests for when you want to click the button, run your tests, and come back to your results. So we've already run the test, and we've already got all of our results. What you can do now is you can actually export this result. You can export this to XML, CSV, HTML, or text. And that way, when you have these results printed out, you've got a log of what happened. So if you need to rerun the tests, you can just reset the tests. You've already got all your results saved off, and you can rerun the test. One more thing I wanted to show off was our protocol analyzer. So there was a question earlier today about if there were physical IDs sitting on the network or if traffic was being created. So I'd like to show that off. So I'll just right-click here, show protocol analyzer. And then I'm going to put it over here. So essentially, what you're seeing here is all of the traffic that has gone along the DNP object. So any of the tests that I've run so far have actually already gone through the protocol analyzer, and you can see all of the traffic that has gone along the DNP object. So any of the tests that I've run so far have actually already gone through the protocol analyzer, and you can see that the values are being sent via unsolicited message. So the protocol analyzer is a live constant thing, and you'll see values as you change things. So this is the end of the test manager section that I was going to show off. We're going to go ahead and start asking some questions about BTM, about the test manager, or potentially some of the ones that we've missed before. Okay. I've got a couple other questions here. Can you show how the status test script is built? Could this be used for 6.1 and 5.2? Can you show how the status test script is built? Could this be used for 6.1 and 5.2 clients and servers? Sorry. Can you repeat that again? Sure. Can you show how the status test script is built? Could this be used for 6.1 and 5.2 clients and servers? Yes. I can go into that a little bit. There's some JavaScript behind the scenes that's going on and a CSV file that's going on behind the scenes, so I don't want to dive too much into detail for it just because it's a little past. That being said, that's absolutely something that would be a good thing to message us about or have a private kind of one-on-one webinar for, and that's something that we can go over and configure specifically for your test case. But let me bring BTM back up. I was moving back over to the PowerPoint, but since we're talking specifically about that, I can bring that up and show you what that looks like. So let's go ahead and open that back. Let me go back to BTM. All right. So we've got this edit test script button here, and when I click that, that will pop up our IED. And essentially this is our JavaScript IED. This is where you can come in and do all sorts of our JavaScript. You can have these directly associated with the test manager nodes, or you can have them free-floating in your workspace as well. So this is, let's say, 170 or so lines that runs all of this, and essentially what happens is we have a CSV file, and I think I'll pull that up so I can show you that as well. So we have this CSV file set up, and essentially this is our mapping. It is showing us which device we have here. And let me make this a little larger so you guys can see it. We've got our device here. We've got the pathway to the point, what type of point it is, and where that should be mapped to. So that's the T1P0. Once you've formed this Excel sheet, this CSV file, DTM will then cycle through it and read each point location and where it maps to. And you can just take this template and fill in your information for it. And, again, if you're not using DNP upstream or maybe it's 6150 through and through or anything along those lines, we can help provide a template for that as well. So from there, essentially we just go and we loop through that entire file. We grab the name of the device, the path of the device, exactly what I was saying before, the type of value it is, and we essentially strip that information off. And then in the main meat of our test manager, we're taking that path provided by the CSV file, changing that value, and then waiting to see if the path of the DNP device point changes. And so that's kind of what the script is. That's kind of the quick overview of what the script is doing. Any more? A whole lot more. Sorry, a bunch more came in. Someone had asked the question about can real devices be used with simulated devices. Yeah, absolutely. That's another thing I was going to go over once we get back to the PowerPoint, but it's good you guys are getting ahead of me a little bit. Yeah, so that's another one of the use cases we have for DTM. We'll simulate, like, say, this entire substation, right? And then we'll test it, make sure that the substation is doing what we expect, and then we'll go through and we'll just right-click and disable that specific channel, right? And once we disable that single device, we can take in a real physical device, plug it into the slot that that simulated device was supposed to be occupying, and then retest our network. Once we retest our network, we can tell, yes, everything's still functioning, or, no, something's wrong now. It probably is the device that we just added in. And then we can slowly kind of integrate devices in one at a time until we have our full substation and it should still be working properly. Then there may be another part to that question, which I want to address just in case I misunderstood it. If you have a real device and you want to run it through the test manager, we won't have this downstream, so we won't know what the value has changed to. But we could easily modify the test script in order for a user to enter the value. So if your physical device downstream changed and you want to see that mapped to the simulated SCADA master up top, we could simply just prompt you and say, what does the value change it to? You put in 100 or true, whatever it is. And then our test manager can take that value, compare it to what they got upstream, and then you can also test your gateway that way. Another question. Can upstream and downstream IEDs be any protocol or only 61850 to DNP? So this webinar was set up with 61850 and DNP, and so is our sample workspace. But that's mostly because those are some of the most highly sought after and highly asked about protocols that we have. DTM, and I'll just show you, supports DNP3, ICCP, 6870-101 and 104, 61850, and Modbus. So those are all the protocols that are currently built into DTM, and we can do those upstream, downstream. You could have the same protocol at the bottom as you have at the top. You can map however you want. The test manager can be set up any which way. It's all just JavaScript, and as long as you can edit it or you ask us to edit it, we should be able to do any protocol to any protocol as long as it's inside of DTM. Can DTM be installed in a virtual machine? Yes, DTM can be installed in a virtual machine. As a matter of fact, the majority of the tests I run are with my personal laptop or my work laptop and my virtual machines. That being said, with virtual machines, you don't get a trial. So when you download DTM to a physical machine, you get a certain day trial, and that's just something where you can use and play with DTM for a while. If you download it to a VM, you have to license it. That being said, if you want to have multiple computers in your DTM network, you could download the trial onto your physical machine, and then you've got, say, 21 days, and then you could use VMs to host your devices across your network, and you wouldn't necessarily need a license for each one. Okay, I see this question coming in from a lot of different people, so it's the same question that will hopefully answer a bunch. And I have an answer here if you need it. How many IEDs can you simulate at the same time? So the answer to that, I know we've messaged some of you privately, but depending on the size of the model and what you're changing in the hardware, we should be able to handle up to 1,000 devices. So I've seen that question come in from multiple people, so hopefully that answers a bunch of folks' questions. Yeah, absolutely. So I've run several load tests on DTM, and as Beth said, it does kind of depend. But for my situations, I've got a 5- or 6-year-old laptop at this point. It's got a 2.4 gigahertz processor in it, Intel, probably like i5 or something like that. I've got 16 gigs of RAM, and I can simulate hundreds and hundreds of devices. Another thing to kind of look out for is how quickly you're changing data on all of these devices, and are you changing every bit of data or a small bit of data. So I would say hundreds of devices to thousands of devices for sure. And then it depends use case to use case. And, again, that is something we'll absolutely be willing to test here in-house for you. So if you say, I need to do X, Y, and Z, we'll simulate it here in-house and say DTM can support that, or DTM can support more than that if you need it, or if you change your parameters to this, you should be able to do it. Of course, that's also dependent on what is in your computer and how many computers you distribute across. Some of the extremely intensive loads that we have, if you can distribute across, say, two laptops or two virtual machines similar to my specs, you should be able to get a vastly higher return on your devices. Okay, I think we're going to do one more question and then we'll go ahead and do the last section of the demo. How much time does it take to build up testing in DTM, let's say for 100 IEDs and one SCADA? That's a good question. So I've actually started from basically scratch here. Now I only pulled in nine devices from my SED file for the downstream substation, but you could scale that absolutely. If you have an SED file with all the devices you need to pull in, I mean a lot of it was just multi-selecting and configuring and then multi-selecting and dragging and dropping. And then creating one device upstream or multiple protocols upstream, you would just go through kind of the same manual process as I did before. So if you have an SED file that has all the devices configured already in it, it's relatively quick. It's about as fast as I did it today. But if you need to, say, have duplicates of the same one or you don't have a full SED file and you need to do it individually, it's still not difficult. You can actually go in and configure one device to be created and then just type in a number and create, say, 100 of that exact same one with slight variations on IP addresses or port numbers just to prevent conflicts. So it's relatively straightforward. We would need to update the test script a little bit behind the status tests, but it may actually just be as easy as changing the CSV file. So I'd say for this type of workspace, it's not actually too difficult. You can get it done probably in a day or so. Okay. With that, I would say let's go to Section 3 of the demo. We want to make sure we get through all the material. And then if there's time in the end, we'll continue to answer questions. So I'm going to go back over to my PowerPoint. So we're going to talk about some other things that DTM can do or some variations on the automated testing of gateways and data concentrators. So this is the scenario in which you are just trying to validate your SCADA or HMI datas. So instead of, say, mapping up to a protocol and sending that over to a master, as you'd see before, we're able to just strictly look at the 6250 clients inside your SCADA master. And so because we don't have the upstream device, we can't do the full loopback automatically. But what we can do is we can look at your SOE, your sequence of event logs, and compare them to what DTM did. Once we get your sequence of events, we can compare that, and then we can validate that you got it. And if we have timestamps, see exactly what your latency looks like. So you can still automate this testing of your device if you're simulating something without an upstream device by doing a sequence of events. You could also, if you didn't have the sequence of events logs or you wanted to do it more visually, you could plug your SCADA master into, say, a monitor or some sort of display. And then from there, we could basically poll the user and say, hey, we've changed these values. You should see this configuration of data on your display, and then do you see it? And then they can go through and say pass, fail, skip, anything along those type of lines. And as I mentioned before, it's absolutely configurable. So if you need a specific type of test that's a variation of something we've shown off today, it would be relatively straightforward to kind of create that for you. Another test scenario that's similar to what we've looked at today, which a few customers have asked about we decided to kind of include, is load testing. So let's say you've already done the test that we did today. All of your points are going through properly. Everything's looking great. What's the throughput, right? How much data can I actually change on my device under test before it starts slowing down or worse, failing? And you need to figure out if you're within the acceptable range for what the device respects for. So what we can do is we can have, say, hundreds of devices, and each of them, instead of changing the data one bit at a time and going through and checking to make sure it maps perfectly, what you can do is you can change all the data on all the devices at a certain rate. And you can crank that rate up or add more devices or add more points, anything along those lines. And all of them happen simultaneously. So that load is all being crammed through your gateway or your device under test and pushed up to your SCADA master. And so that's how you can really test where the breakpoint is and if it's within an acceptable range. And we can actually combine the test I was just talking about with the throughput with the point change test that we did earlier. So what you can do is you can select a list of points and data attributes in your devices downstream to change and leave out several key data points. And then what we can do is we can put that load, we can crank it up to a certain acceptable range that you need, and then go through the test manager and change points individually. So all of this is simultaneously changing data and pushing it through while individual test bits are also going through. And then you can measure your latency of how long it took for your point to get through under the excessive load. So there's several different variations of these tests that you can run. And then someone mentioned this earlier as well. So I think it's really good that we decided to include it. This is our substation behavior validation. Essentially what we can do is I've taken those same IEDs and I've put them in a more folder kind of layout to organize it a little bit better. And what you can see is here, hopefully, is we've got these logical node scripts here. So all of the logical nodes that are needed for our substation are written up in JavaScript here. And that's essentially just the behavior that that logical node should perform. And what we can do is we can reference this library of logical node behavior scripts on our individual devices. So I've got this line 1 Bay controller here, and it's got all of its logical node behavior underneath it. And this is not a new JavaScript. This is just pointing up to our library. And each of these scripts is then active. And how that helps us is we can now simulate the behavior of our devices. Each of our 9 devices or 12 devices or whatnot all have their behavior simulated. Then you can drag individual data attributes or individual values over to this single line diagram and interact with the diagram. So I could double click on this breaker to open and close it. I could click this PDIF button, and that would cause a trigger, a fault on the line, and the breaker would open. But because the breaker is open, that would send out the goose signal. The goose signal would be received. So you would be able to see how signals propagate through your system and if it would respond properly. And then as someone asked earlier today, we've got the partial simulation device integration. And essentially what that is is you've got this whole substation. You've got it set up. Your recloser events are working. Your breaker failures are working. Everything's going great. You go in. You disable one of the breakers. You put a physical breaker in its spot, configure it properly for its location, and then you run the exact same test again. Did it open when it was supposed to? Did it fail when it was supposed to? Are you getting the proper responses that you expected? And you can then take this device by device and build up your substation or your test environment and make sure that all of the tests that you're running for are still properly working. So we've got one more slide here. It's the final thoughts, just kind of recanting or re-saying what Beth said earlier. And then we're going to kind of open up just for more questions toward the end. So we've got the webinar that's going to be recorded. It should be edited, and it will be posted to our YouTube channel. It's going to be also referenced in our website, trianglemicroworks.com. Underneath, I think you said training or product training? We have a whole training tab where you can see what courses we're training at different conferences. We offer some free courses, full-day 6150 training courses, and we have a whole tab there on free upcoming webinars. We also have in this GoToWebinar window some of our handouts. We've got a brochure that kind of goes over some of the stuff we talked about today and the diagram that I've been referring to. And as I've said I think two or three times earlier when we've had some kind of questions, if you're interested in this, if you've got questions about this, comments about this, if there's a variation that you want to set up for your environment and you want support with that, just contact us at sales at trianglemicroworks.com. We'll walk you through it. We'll help you learn. We'll make sure you've got the tools and resources equipped to learn and be successful. And we love to see what people are really doing with our software and how we can really solve their problems and make things easier and make our products better. So just feel free to reach out to us. We've got a really good sales and support team, and we're looking forward to hearing back from people. I will say we do have quite a few more questions here, so we can kind of back to the questions. What I would also mention, just to add on to Matt, is you can download a free evaluation license to DTM as well as our gateway and any of our testing tools at our website as well. So if you want to go and play around with it for a little bit. And, again, as Matt mentioned, we can certainly walk you through the tool. To kind of flip back to some questions, we've got some good ones here, particularly on JavaScript. Does DTM include default scripts, or does the user have to program their own scripts? Both. So we ship several sample workspaces, ranging across all the protocols that I specified earlier, and each of them has built-in sample scripts inside of it. And the sample scripts will say things like, how do I change data? And how do I set up commands? How do I create the devices? Anything along those type of lines will be seen in the sample scripts in those sample workspaces. Then we have larger sample workspaces, similar to the one I showed off today, that has this test manager set up and has the test scripts behind the test manager, and you can go and see exactly how we thought about that. That being said, it is JavaScript, and you can do whatever you want with it. So you can start with a blank JavaScript, program it up how you want, do behavior scripts, do tests, do data changes, do commands, anything along those lines. And, as I mentioned before, those are the type of things that we really like helping people out with. We like being able to see what they're doing, what they need, and what ways we can provide resources to them. And when you bring up the JavaScript editor, there's a section at the bottom labeled Help, and that will let you know of all of the functions built into DTM. So, of course, there's a whole library of resources on the Internet about JavaScript, but we've got specific libraries of DTM-specific functions built right into the editor. So that is how I'd recommend going and getting started with using JavaScript. Okay, I've got another JavaScript question here. Are the scripts for the logical nodes coming from a library, or are they created by the user? So currently we have an internal library used for our logical nodes. That does come with a sample workspace, though, when you install DTM. So you can go in and look at them and make edits to them, see kind of the process we went through in making them. We're building that library up as time goes, and we're going to have some kind of exciting features to make announcements about that coming forward. But, yeah, those are provided in the sample workspace, and feel free to take them and play with them. And if you're missing certain logical nodes that are super critical for your situation, shoot us an e-mail. Another question. How did the mapping between 61850 points to DMP, how is it done in the demo? Do you have a mapping window? So the mapping wasn't actually done in DTM. The mapping was done in our device under test, which was our SCADA data gateway. Our SCADA data gateway is another application we sell here in-house. It will map protocols. It does data concentrating. It does all the SCADA features that you need, and it is actually normally installed inside of substations. Of course, you could use it for internal testing reasons as well, but the actual mapping was done in the gateway. DTM knew what was being mapped to what point was being mapped to what point because of the CSV file that I provided, which is just a template you can fill out. On each row, you fill out a 61850 or whatever downstream path you need to the point, and then whatever upstream point you need, and then the test manager will go ahead and make sure that those are equal after it receives the point change event. Can you run the test with several SCADAS at the same time? So yes and no. Yes, you can configure the test manager to do whatever you want. You can configure it to test multiple SCADAS upstream, but what we do internally, and we actually have done that before several times, is you can actually create multiple test manager nodes, and that's something that I didn't really go into during the demonstration, but I think that's a really good question. You can have multiple test manager nodes. So the very first one, if you have your upstream SCADAS DNP, you would have DNP status tests, and then say you have a 104 upstream, you would have a 6870104 status test, and that way you can organize them, and that way you don't have to worry about having all of your results kind of intermixed with each other, and you can have as many JavaScripts, as many test nodes, as many devices as you can build up into the workspace. Can you configure sequences or events or groups of variables with predefined values in the test, or do you need to test variable by variable? So for the test manager, we change one at a time just to see the point-to-point mapping is correct inside of our gateway, but absolutely you can do sequence of events, and you can do different states for all of your devices. We have a feature called a CSV playback, which essentially is just a CSV file, as I showed you before, and on one side there are the data points or the points list that you're interested in, and then across the top is what value they'll be. So you can either play them back state by state to make sure that your environment changes to the exact sequence of events you want, and make sure that your devices respond accordingly to them, and all of the devices in the column will all activate at the same time. So once you trigger your first state, all the devices and all the points will change to that, and once you trigger your second state, all those points and devices will change to whatever's in the second column. So you can trigger your entire environments at the same time with a CSV playback, and you can either do that automatically, say every second change state, or manually, or you can even record it, or you can even put times inside of the CSV file and play it back each state according to the time that you have listed inside of the CSV file, and every single protocol that we support has a sample workspace with a CSV playback in it, and the example of what the template should look like. Can DTM be in a routed network, or is this only on a local network? I think it should work on a routing network. We have several situations where we have the devices set up to be routable through all sorts of network configurations. It can work through any environment that we've put it in so far it's responded properly with, and I am getting some head nods from a few of my coworkers, and the answer is yes. I was trying to think of all the fringe edge cases where it could be wrong, but I think that should absolutely work in your environment. Can you simulate a signal from a recloser on, off, on? Yeah, absolutely. You can do that manually by clicking on the individual value and change that. You can set it up in a script to wait X seconds, change it, change in half a second after that. You can do it via CSV file. Like we mentioned before, there are so many different ways you could do that. I could keep you here for another hour. Is it possible to import any IEC 62050 project files such as an SCD, ICD into DTM? Yes. I showed off today that you can use our import substation and import the entire substation in one go. If you have individual CIDs, ICDs, all the different extensions there, you should be able to load them up into DTM. That being said, if the source code inside the configuration file is messed up or botched in some way, we won't automatically repair that inside of DTM. That's what you would use our navigator tool to do. But I'd say 80 percent of the files that we get, we can simulate right off the bat. And the ones that we cannot, it's because there's something wrong in the configuration file itself. In this one, I think I can answer it. Is the trial version full functioning? And yes, if you go to our website and download a trial version, you will get it's fully functioning DTM with all the protocols, DMP, 62050, ICCP, 6870, and Modbus. It will allow you to simulate up to 25 connections with the trial license. What system configurator do you use to create the SCD file? It really is agnostic to our application. As long as you provide us with the SCD file, we should be able to simulate it. We work with Helinks a lot, and Helinks can help create some of these configuration devices for us. That being said, several of our customers use proprietary stuff they have. Several of them use Helinks. There's all sorts of different applications that you can use to create this. But as long as it's properly configured and we get a good file, we can host it. What operating system is required? DTM is a Windows-based application, so it does need Windows, and it pretty much runs on every form of Windows I can think of. We recently stopped supporting XP and Windows 7 because of the security risks. We don't really thoroughly test anymore. That being said, every time I've run it on Windows 7 and higher, it's absolutely fine. That includes the Windows servers, Windows 10, of course, Windows 8, any of these types of operating systems. Is it possible to test control direction from simulated SCADA to IEDs included status changes? Yes, absolutely. Not only does it support upstream values that change, you can also send commands from the DTM, and then if that should be mapped through your gateway, you should also see them come down to your simulated substation like I had. That being said, the example that we have in the sample workspace does not do controls just right out of the box, but that's absolutely something you could do. So we are running into time. We can finish answering some of these questions if some folks do want to stay on, but we have come to the hour of the time for the webinar as well. We have covered all the material that we have to cover, and I think we've actually answered about 90% if not 95% of these questions. In case we missed any of the questions, I know Beth and I have said this a few times, in case we've missed anything or you didn't know how to type it in or whatever, send us the question. We'll either respond offline or we'll send you an e-mail or anything like that, so we don't want to leave anybody with questions, but I think we've got a few more. I think one just came in. Is it possible to test secure authentication version of DMK? Yes. So we have secure authentication on several of our protocols, and you can license and test that as well. Now, secure authentication is not available in the trial license, but it is on the purchase license. Okay. I think that's all the questions. Awesome. Thank you, everyone, for joining. You will get a short survey when you end this webinar. We do appreciate your feedback, and I will be in touch with a recording of the webinar, and please visit our website, check out our upcoming free webinars and training courses, and on our home page you can sign up for our newsletter, which will keep you in touch with the different offerings that we have as far as training and webinars are concerned. Thanks, everyone. Thank you.


Video ID: ByT_jM4w0AU
Hello, everyone. Welcome to today's webinar, What is a Gateway in the SCADA World? My name is Beth Kapelis, and I'm the Marketing Manager here at Triangle Microworks. Before we get started, I'd like to go over some housekeeping items and some introductions. The webinar is scheduled to be one hour long. We will be stopping for questions a couple of times throughout the webinar, and we'll stay on afterwards to answer as many questions as possible. Please submit your questions as soon as you have them. Don't feel that you have to wait until break or until the end. You can submit them online using the webinar menu under the section titled Questions. We have a team of experts ready to answer questions, and we'll respond directly to you, not the entire audience. So to be clear, the questions are not seen by the entire audience. They will only be seen by the folks answering the questions. We will stop a couple of times throughout the webinar to read and answer questions, and as I mentioned, we will stay on afterwards for as many questions as possible. Next, I'd like to introduce our main speaker, Matt Green. Matt is a Senior Applications Engineer here at Triangle Microworks. He's been with us for about six years, and he specializes in QA testing with all of our simulation tools in Gateway. Matt is also one of our lead product trainers and presenters for webinars and customer demos. Before we get started with today's topic, I'd like to just give a quick overview of Triangle Microworks. We were established in 1994 and are headquartered in Raleigh, North Carolina. We have customers in over 70 countries and work mostly with equipment manufacturers, electric utilities, and system integrators. The products that we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs and their development time. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They are designed to troubleshoot, stimulate, and automate testing. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with all the standard protocols as well as OPC. The gateway is one of the main tools that you will be seeing here today. So now that you know a little bit more about Triangle, I would like to go ahead and hand it over to Matt to kick off the topic we are here to learn more about. Thanks, Beth. So, we are going to start out today talking a little bit about what a gateway is and what it isn't. Most of you have probably heard of routers before, so we are going to make a kind of comparison to start there. So how is a gateway different than a router? A gateway is a single point of access to computers outside of your network. You can think of it like a door. While a router determines the shortest possible path data can travel from computer A to computer B. This is like a hallway or staircase. Gateways also have other functionality that a standard router would not. The next slide is on that functionality. So gateways tend to have these functionalities. They can work as a data concentrator. They can work as a protocol conversion, a local HMI or alarms, a historian, or have security features for both users and data. Data concentration is used when you want to gather data from multiple sources and combine them into one database for a top level SCADA device or HMI. This allows you to see all of the important bits of information at a glance from a single location. Protocol conversion or protocol translation is used when you have a system that needs to communicate across multiple protocols. This is commonly used when a substation is communicating with one protocol, but the transmission may use a different protocol. Another common usage of this is when a system is being modernized or upgraded to newer technology, but legacy hardware is still required during normal operation. Gateways can also act as a local HMI if it has a display available for in-substation usage. This display provides information on the health and status of the system and to provide alarms or events for conditions that may or may not be monitored within the control center. The historian functionality can be used to store and log all of the data in a system. This allows for review of historical data, which is especially helpful during testing or during catastrophic events. Some historians even have a reporting capability. Security features are also included in many gateways. This includes data-specific security, such as secure authentication, or user-based protection like role-based access control. So let's take a look at a very simple system. This is our simple data concentrator system. In this situation, we have several DNP outstations in our downstream substation, and we also have a DNP control station upstream. In this situation, what we're going to do is we're going to gather some or all the data points inside of these DNP outstations, and we're going to combine them into a single gateway database. This database is then exposed up to the control system, so that way the DNP control station can actually see all the variables and data stored within all the outstations beneath. So how exactly does this happen? Well, gateways typically have multiple network adapters, so they can access multiple different networks. For each of the connections to an outstation or server, you'll need to have an internal server database or internal master database. And then for each control station or master connection, you'll need to have an internal database as well for an outstation or server. After that, you can use the mapping logic here in the middle to map the points that you're interested in from the masters to the database. Once that happens, your database will be filled with the information from the external devices and available for your control station. So this is a real-world example that we deal with frequently. We have a few customers who have big wind farms, and on these wind farms they have multiple windmills. This can be up to hundreds of windmills, and each of these windmills is a replicate of each other and are performing the same operation. Because of this, the devices that are monitoring this are all storing the data in a very similar manner. So it's very easy for you to be able to combine and grab the information out of that, combine it into a gateway, and then that gateway can then push that information up to a historian, an alarm viewer, or the control station. These types of systems could easily have more than 200,000 points. So now let's take a look at our simple data concentrator example again. But this time, instead of having three DNP devices, we have a Modbus device in the system down here. A second gateway is added in between the Modbus device and the data concentrator. This one acts as a data converter and provides the DNP3 interface to a legacy Modbus device. Some gateways can actually perform both of these functions at the same time, and you wouldn't actually need this second gateway as a protocol converter. If your upstream control center is using a different protocol than the local substation, some gateways can act as both a data concentrator and a converter at the same time. This allows for the important bits of your substation to be sent up to the control center. In this specific situation, we have a DNP3 outstation, a 61850 server, and a 104 controlled station, all being mapped, translated to an ICCP database within the gateway, and then that is exposed up to the ICCP control center. One thing we haven't talked about yet is controls. Controls go in the opposite direction of what we've been talking about today. In this situation, the control center will send the control down to the gateway, and it's expected that the gateway can then map that control to its respective master and send that control down to the outstation that needs to receive it. Controls that are sent from the control station can happen automatically with a direct operate, such as reconfiguring a parameter in a device, but higher risk controls, such as opening a breaker, may require a select before operating the point. This is just an extra security step that we put in between the operate. It essentially allows the device to know I'm going to be performing this higher risk control, and then you have a certain time to actually issue that control. That way someone doesn't misclick something or send the wrong control by accident from the control station. Controls do need to be mapped to similar controls and other protocols when working as a data converter. So now let's talk a little bit about the data and its layout itself. Configuring a gateway does require knowledge of the SCADA protocols you'll be working with. Protocols can fall into two different categories. You've got point-indexed or tag-based protocols, or model-based. So tag-indexed or tag-based protocols, this is how it's kind of set up. A point is a uniquely identifiable physical or logical entity addressed by an index or a tag. It contains a value for inputs like analogs, binaries, or counters, or outputs like analogs and binaries. Protocols that support index or tag base points are DMP3, 6870-101 and 104, Modbus, and OPC. Model-based protocols are a little bit different. The semantic model provides paths based on standardized names for objects and attributes that allows for the system to communicate not only the values of the data, but also the meaning of the data and the relationships between various other data points. Because of this, because of these protocol standardizations, you can quickly recognize data and its meaning across devices and systems. Protocols that support model structures are IEC 6870-6, otherwise known as TASI-2 or ICCP, and IEC 61850. One thing we haven't talked about, which is also very important, is quality and time. That's what these next two slides are going to be going over. Let's start with quality. Both model and index or tag-based protocols provide a quality indicator with static or event data. These quality indicators give the user the status of the data being received. Some quality indicators are valid or invalid, communications lost, or bad data, or even that the data you're receiving is in test. Typically, index protocols provide the quality as a flag within the point received, while models provide it as a separate point in the same grouping you're receiving. When mapping quality between different protocols, some details might be lost because qualities are not standardized between protocols. Timestamps are a little different. Timestamps are only included with event data. These timestamps give a user an understanding of when the events occurred, and maybe more importantly the order in which they occurred. Both model and index-based protocols have separate timestamp attributes that are provided with event data. Different protocols have different requirements on timestamps, and accurately converting between the two is part of the gateway's job. Gateway should propagate the received timestamps if available, but one exception to this could be think of a situation where you have a Modbus device and that information is being converted to DNP. Modbus doesn't have a concept of timestamps with its point, so the best the gateway can do in this situation is fill the time-space slot when reporting up to the DNP with the time that it received the data from the Modbus device rather than the timestamp from the Modbus device which wasn't available. So now let's talk about data transfer. We have grouped different methods of data transfers into three different groups. While these methods may not be 100% identical protocol to protocol, we can draw some comparisons between them. Polling or reading is when a client requests the current state of data. This can be done either once or periodically. Reads are performed on an entire database, generally after the device has been restarted or there's been a loss of communication, to make sure that you can just quickly get back on track and make sure that the client or master's database is up to date again. This method requires the most bandwidth of your system to keep data accurate due to the large data that's being transferred. Hit polling or report by exception is when the client or master requests only the data that's changed since the last poll, instead of all the data that it already has. This can be done once or periodically as well. This requires less bandwidth of your system to keep data accurate because of the smaller set of data that is being transferred. The last one we'll be looking at today is unsolicited messaging, reporting, and subscribing. This is when the client or master requests that the server sends changes as they happen and not wait for any sort of polling period. These can generally be enabled or disabled. This is the fastest and most effective way to keep your client or master's database accurate while limiting bandwidth. I'm going to stop here now and pass the mic back over to Beth and see if we have any questions that should be answered before moving on to the live demo. We did actually get several questions that came in. Let me go ahead and read off some of those. Which protocols are typically used in a substation environment? We've mentioned a few of them so far. You're likely to see things like 61850, DMP3, Modbus, and some of the older systems. Generally there can be a mix of protocols as well like we've described a little bit. There's 101 and 104 as well, and TASI-2 and ICCP are the ones that we've pointed out today and talked about. Don't let me forget OPC. We've got OPC, OPC-UA, classic, alarms and events, all the different flavors of OPC. Okay, next question. How does a gateway deal with a system disturbance event like a trip? Generally when a lot of events are all happening at the same time, there's a lot of buffering that goes on. You don't want to overload the bandwidth and have packets get lost or things along those lines. You want to generally buffer up all of these events and send them out when necessary and when things cool down a little bit. If a lot of events are all happening at the same time, you want to look into 61850 supports buffered report control blocks. Next question. Who writes the mapping logic? I assume that's referring back to one of the slides I had earlier with the mapping logic. The gateways generally contain that themselves. Like our SCADA data gateway, we have engineers who work on the best, most efficient way to map from protocol to protocol. Because everything between protocols like quality and controls aren't 100% standardized, there is some trying to find what's the comparable thing in between the two. Our engineers write that up for our gateway. I'm sure other gateways work in a similar manner. It's not something you, the user, is expected to do live. Next question. Does your gateway support two-way controls between two control centers? One supports ICCP and another OPC? I will open that up to our experts. I would start with saying yes, but I want to make sure I'm not misunderstanding the question a little bit. We support controls from multiple different locations, but I want to make sure I'm understanding the question properly. They have answered it, actually. You are correct. Yes, it supports mapping OPC protocol to ICCP protocol. In our next question, do all gateways support all the functionality? I can't speak for all gateways, but I know that a lot of the gateways have these built-in functionalities for them. For example, our SCADA gateway can do protocol mapping, protocol translation, data concentrating. I can't say that every gateway contains all of these functionalities. You need to look in when considering which gateway you want for your system, but some of them are more common than others. You have to figure out which functionality the one you are interested in purchasing contains. If that will meet your needs. Next question, can proprietary protocols be mapped, for example, to DMP? We don't typically support that as far as I know. Generally you can route that type of thing through. I think OPC is a potential way that you can do that, but if we have a better answer from one of our engineers, feel free to fill in some information there. That is consistent with how they answered online. Do I have to map qualities? Half is a strong word. You don't have to map qualities, but if you don't have the quality of the data that you are getting, you have no idea what you are looking at. If you are getting quality that is considered bad, maybe something has gone wrong with your device and you think a breaker is open when it is closed or vice versa, you can have all sorts of logic that is based off of that bad data that will have really bad things happen in your system. Do you have to know? Should you? Absolutely. Without that, you have no idea the data you are dealing with has any meaning whatsoever. Next question, this might be a good one for the panel. How do we secure a SCADA data gateway? I can open that up to the panel or it was answered online. I can also read the answer. Typically using TLS and MMS certificate-based security. I would also say that later on in the presentation, I will be going over some of the security features that are built in with our specific gateway. I did touch on a little earlier that there are user access control that we have built in and that other gateways also do and then there are also things like secure authentication and TLS and things like you were reading off there. I think I'm going to ask, I'm going to do one more question and then we probably should wrap back to the presentation and the demos. This is a little bit of a broader question. How do you test a gateway? Absolutely. I think we actually did a whole webinar on how we test our gateway. We have a tool that is set up to allow you to test basically all of the inputs and outputs. That is effectively what you want to do. You want to isolate your gateway in a situation and simulate or stimulate all of the inputs to your gateway and then you can verify the outputs from your gateway. Whether that is connecting it up to your actual control center and verifying on screen that you are getting the data that you expected or you can do something like we did during our webinar when we did do this where we had everything built into our simulator tool both all of the downstream devices and the upstream devices and we had this constant loop going on where we would change a point and verify that it changed in the control center and then repeat that process as all points are changed. Effectively you want to isolate it and simulate the inputs and verify the outputs. Are there any more questions or should we move back to the presentation? Sorry, I was on mute the whole time. We do have a number of questions still continuing to come in but I would suggest going back to the presentation at this point. Our panel experts are working online to answer those questions so keep them coming and then we can loop back at the next break to answer some more of those questions live on the webinar. Okay, that sounds good. We have a few more Q&A sections we will stop at so we will have plenty of time. We will move over into the live demonstration of our specific SCADA data gateway. We will build up the first workspace that we will be looking at the first configuration of the gateway and show exactly how that is done. I have a few diagrams to show exactly what we will be building ahead of time and then we can go through and build that up. I will jump back over here. This is the first example we will build up and this is going to be with polling. We will have a control center of a DMP 3 master and our downstream device is going to be this Modbus server. Inside of the gateway what we will need to build is a Modbus client in order to connect up to the server. We will have to execute polling in order to pull the values across and we will do that at every 15 seconds. We will hopefully see a response about every 15 seconds. From there we will have the information inside of our client. What we will need then is a DMP 3 outstation database that our control center can interact with. We will go ahead and generate that database. Then we will need to create the mapping from our Modbus client into our outstation database and from there our control center will be able to get the values across. Let's take a look at how we will do that. In order to do that I need to introduce the two applications I will be working on today. The first application I will be working on today is our DTM tool. Our DTM tool is our distributive test manager tool. It is our system simulator we use for load tests and protection tests. It is generally our server simulator tool but it can simulate both clients, masters, servers, outstations and anything along those lines. In this situation, as mentioned in one of the questions, I will be simulating both the IEDs or the downstream devices and the SCADA or the control center. Based on the diagram we looked at, I have my DMP 3 control station and I have my server Modbus device. That is what I will be doing for all of the demos we will be looking at today. I will move this back off the screen and I will put gateway in the middle. This is our SCADA gateway tool. This is a web client for our SCADA gateway tool. I'm not running the gateway on my laptop right now. It is off running on a Linux machine right now. Since we will be looking at four different examples, I have four different web clients up here. Each of these web clients is connected to a different SCADA gateway and they are all running on Linux. That being said, we do support Windows as well. Let's take a look at the GUI since that is what I will be interfacing with. On the left, we have this workspace view. This is where we will be adding databases and our controls, polling, event polls, reports, things along those lines. Everything will be created and added on the left side. On the middle area, you will be able to see the details of what is going on whatever you have clicked and selected. Let's get started. The first thing we want to do is get my Modbus servers data into the SCADA gateway. To do that, we will need to create a master Modbus. You right click here and we will do an add TCP channel master. From here, you will pop up a configuration window. While I know most of this stuff fairly well, people looking at gateway for the first time may not. We have a built-in function here, this little help. If you click this, it will give you a pop out here on a description on what each of the configuration options we are setting up here will do. I will not leave these up, but it is important to know that if you are working with the gateway for the first time or maybe a new protocol you are not familiar with, you can get quick information just like that. Now let's go ahead and fill out the configuration. The first thing I will do is select a Modbus master, and then I will give it its IP address. I know that my server is being hosted on DTM, which is on a totally different computer, so I will give it that IP address here. The next thing I will do, and I could leave it at the local IP address at 0.0.0.0, but what I will do is give it its local IP address as well. You will also notice that we have a port number here at 502. That is the standard default port for Modbus. We try to fill in the defaults whenever possible to allow configuration as simple as possible, but we know that you may be on a test environment, you may be working with gateway for the first time and you want to play around with this. You can change the port number, but when at all possible, you want to try to leave the port number to be what the protocol expects. From here all we have to do is give it a name, master Modbus. The second thing that will pop up is going to be configuration options specific to the protocol you chose. Since I picked a Modbus device, it will give me a little bit more information and configuration here. Again, you can always click this question mark to get more information on it. In DTM, I set up the configuration just as everything defaults, so I don't have to change any of this, so I will hit OK. The last thing that will pop up is we need to give this master a database. Right now, we have the protocol up and running. It's already connected up to DTM, so everything is happy there, but it doesn't have any access to any data yet. For Modbus, there are four different types of data. We've got coil, discrete input registers, input registers, and holding registers. In this situation, I'm just going to go ahead and add a coil, but I will show you how to add more data types later, so I'll just hit OK. What you'll see over in the workspace area now is we have added this master Modbus, and you can see the protocol there in case you just didn't name it that. You can name it whatever you want. We've got this L1, and then we've got the T0, which is the coil data type. If we want to add more data types into this master Modbus, you just go to L1, and you right-click, and you can say Add Data Type. From here, you've got this window again. You can select the other data type that you want to add. You can do an input register and hit OK. If I go into this coil, this T0 coil, you'll see that there's actually no data in here yet, and that is because I haven't added the points that I'm interested in. To do that, like we did before in L1, you go to the T0, you right-click, and you say Add MDO. Let's talk about MDOs and SDOs a little bit. An MDO stands for a master data object, and this is just a data point that your gateway is going to have inside of it when you configure it. When I add this MDO, I will then have access to the data inside of my Modbus server. An SDO is a server data object, and it's exactly the same idea. That's the database inside of your gateway that is going to be exposed up to your control center or whatever your upstream device is. The MDO and SDOs can be related, and that's how we do our mapping, and I will be showing you that soon. I'm going to go ahead and add this MDO. We've got this information object addressing. That's just the index or tag that we were talking about on the previous slides. For default, it just went ahead and put one in here. You can actually name this or add a description or change the options of this different object, but I'm just going to go ahead and hit OK and pull over this index one coil. What we'll see is we've got a name here. It's the master Modbus. That's the device it's in. We've got L1, and then we've got T0, which is the coil, and P1. That's the index. You'll also see that that value is off. If I go over to DTM and I change that value, you actually won't see it update here automatically, and that's because we haven't told it how to transfer data yet. Let's talk about how you can get these different data transfer types that we talked about in the previous slide. You go back to L1, and this time I'll click on it, and we'll see all the information inside of the L1. In this situation, we have a bunch of action masks. This is essentially what is the gateway going to do. Let's just click this little pencil icon here to edit it, and you'll see a bunch of stuff filled out here. For a Modbus, you can essentially perform these reads or polls like we were talking about before. We've got the coils, the holding registers, discrete, input registers, everything along those lines are here. You can click this dropdown because so far it's configured not to do anything. You can click this dropdown. You can say you want to read all the points, or you can read a range. You could do something like starting index of zero, ending index of, say, two. At the very bottom here, you'll see that there is a Modbus section action period. What that means is this is how frequently we're going to be doing this. Generally, with polling, you're gathering a large set of data, even if it is the data that you already have within your database, and that can take up a lot of bandwidth. Generally, your polling rates are going to be kind of slow. I've seen them as much as once an hour type thing where it reads everything once an hour and it relies on other data methods in order to get data more quickly but use up less bandwidth. For this demo, because I don't want to sit around for an hour waiting for this, I'm just going to set it to 15 seconds and then hit OK. Now when I go back to my coil, my data type is still in here, and what we'll see is if I go ahead and change the data in my DTM, after kind of that 15-second polling period, we should see the value inside of our gateway update. We'll just give it a little bit more time, and sure enough, it did. Now I'm able to change the value inside of my Modbus server, and we'll have that information inside of the gateway. Now, it doesn't have to be simulated like we're doing in DTM. This could actually be a physical device, a simulated device, anything along those lines as long as it's communicating across this Modbus protocol. Now we have access to roughly 15 seconds of difference there, real-time information. How do we get this into the database that our control center, which is DNP3, is going to be able to monitor? Well, we're going to follow the same process. We're going to right-click here, and then we're going to add a TCP channel. We're going to choose this outstation. Again, it's going to look very similar. You're going to click this dropdown, and we're going to select a DNP3 outstation. Following the same logic, I'm going to tell it where its control station is going to be, and I'm going to give it its own IP address as well, and I'll name this DNP3 outstation. Hit OK. You will see this looks a little bit different, and that's because we have chose a DNP3 device this time rather than a Modbus. That being said, we still filled it with the defaults, and since my control center is also using the defaults, I don't have to swap anything here. I'll just hit OK. Following the same logic as before, we need to give it a data type. These are obviously going to be different as well because we are in DNP3, but the comparable data type to the coil would be your binary input. I'll hit OK. Now we have our device created here, and we've got our binary inputs. Mapping is actually relatively simple. You just go to the point that you'd like to map, and you just drag and drop it from the coil T0 to the binary input T1. At that point, you'll get a pop-up. It's going to tell you exactly where you're going to put it. To keep things the same, I'm actually going to change my indexing to 1, so that way 0.1 inside my Modbus is 0.1 inside of my DNP database. This could be however you want to organize it. It doesn't necessarily have to be this way. If your downstream devices don't exactly line up with your control center, you can wire this however you want, but for me, it makes most logical sense just to keep these indexes the same. If I hit OK here. What you'll see now is I now have a DNP3 outstation point. It is a T1P1, which T1 is just a binary input. You can even see right next to it that we are mapping it from the Modbus master from its T0P1. If I happen to make a mistake today, and this wasn't the point that I wanted to map, or I needed to remove a mapping or something along those lines, you can always click this button here, and it will delete any sort of mapping and allow you to start back over. That's the first demo I wanted to show off today and build up a workspace from scratch. The next few demos that we are going to look at, I've already built up the workspace, so we don't have to sit through me creating a bunch of devices on the flow. Let's jump back over to our PowerPoint for a second, and let's take a look at the next situation we are going to look at. This is the next system we are going to take a look at. We have our controlling station as a 104 device, and then we have our simulated DNP3 outstation again in DTM as our IEDs in the network. This time we are going to be using event polling, so I can show you how to configure that. Because event polling is going to have much less data in it, we can actually decrease that polling period to only every five seconds, rather than the 15 seconds we are looking at. After that, you will be getting your responses as the events are changing, as data is changing, that mapping will move over, and then we will have our responses up to our control station. Again, we will have this mapping here, and we will need to have a DNP3 master and a 104 database. Let's take a look at that. This is the next example we will be looking at. I have this master here, this DNP3 master, and what you will see here is because I was able to pre-make this workspace, I have a few more different data types. If you click inside, you will see I actually have five points being mapped instead of just that one. This was actually really easy to do. You can map multiple points all at once by dragging and dropping. You can drag and drop individually, but you can also multi-select. Once you have these multi-select, you can drag the whole group and drop them. Then it will give you this kind of configuration window. It will take all of the points you drag and drop, and it will fill out a text box here with all of the addresses you are mapping this to. This is fully editable, so you can go in and change these numbers however you want. This is how I was able to map over five or 10 points at once. If you happen to have hundreds or even thousands of points you want to map at once, I would not recommend doing it this way, as it still could be a long process, but we do have exportable configuration options in our SCADA data gateway. What that will allow you to do is take a CSV file or something over to Excel, use the built-in functions in Excel to change that CSV file, upload it back into our SCADA data gateway, and then all of those mapping points will have been done automatically. So now let's take a look at how we are getting this data across. So I will go to this L4 here, and we will take a look again at the action mask. So in this situation, I have a little drop-down, and you can see we are reading class 1, class 2, and class 3 change events. This is effectively doing the event monitoring and polling for events, rather than just polling for everything during a set time. Again, this is done at a five second period, rather than on the 15 second we were looking at before. So that is how you can do event mapping, and again I can go in and change the values, and they would swap out through our system and go back up to my control center. The next thing we are going to look at is reporting, and this is going to be a little bit different because we are going to be looking at 61850, so let me pull up my diagram. In this situation, we are going to have a 61850 client inside of our gateway. This client has already enabled reports, which I will show you how to do, and what that will do is allow this server to automatically, anytime the values change, send a report up to our 61850 client. That value is then going to be mapped to a DNP3 outstation database, which is going to be accessible by our DNP3 controllable station. So I will show you how that is set up. So effectively what we have here is 61850 is our first model-based protocol that we have been looking at. To show off what that looks like, I am going to pull back over DTM for a second and show it off. In this situation, we have a bay controller device, and inside of it, it has several different logical nodes. These logical nodes are the grouping we were talking about before. If I open up this XCBR, which I know is a breaker, we will have several different data objects inside of it, but the one we are going to be looking at today is the position object. This is the position or state of the breaker. We have on, we have off, we have intermediate, and we have bad. We also have the quality and time associated with this position, and any other attributes that may relate to this. As you can tell, this is not a flat table view like what we have been looking at today. So this data is actually all stored together in what 61850 calls data sets. What a data set is, it is a group of data that you find important, the stuff you want to be sent up via reporting. Some values can be read, just like other protocols, but if there is a value you are interested in, you put that into a data set, and then you put that data set into what is called a report control block, and when you enable reporting, you enable the report control block, all of that data is packaged together and sent up as part of the report. So let's look at how we do that in our SCADA data gateway. So I've got our 61850 client here, and what I'm going to do is add a report control block. So I just right click here, and then we just simply have add 61850 report control block. When you add these, it automatically enables those reportings on your server, so it goes ahead and sets up the ability for it to auto send those messages, and we've sent, we've done that with two of them so far. We've got our statuses, and that's going to contain the position that we were talking about before, and as you can see, it was off like it was in DTM, but it also has another bit of information in here that is our CSWI, that's a controllable switch, and its position as well. But it's not just binaries, you can also do measurement values. So in this situation, we have a phase voltage, and its magnitude is 874. I obviously can go into DTM and change that value, and what we'll see is that this value should come along a lot faster. We don't have to wait that 15 seconds or that 5 seconds for it to come across, because as soon as I changed it within DTM, it went ahead and published that, or pushed that report out to our 61850 client, and of course, like before, that value is also carried over because of the mapping to our analog value here and reported up to the control system, the control station in my DTM as well. I did want to point out one other thing while we were looking at the gateway tool. When I'm inside of my DNP3 outstation, you'll see that it has a mapping, and it's telling you where that value is coming from. What you may have noticed in other situations is if I went to our client, we're actually seeing multiple things. This is saying that the arrow has shifted the other way. This is saying that this XCBR position is mapped not only to the DNP point that I just showed off, but it's also being mapped to our OPC UA server, and that's something that if you're licensed for, will happen automatically for you. Anytime you add any points, any data to your SCADA data gateway, it's automatically going to be mapped over to this OPC UA server, and if I click on it, you'll actually see all of the data within it, all displayed out in this table view. This is really nice because that means if you bring up an OPC UA client and connect it up to your gateway, you should have access to all of the information inside of your gateway automatically without needing to configure it or do any CSV file INI changes, without needing to do any mapping. We'll just handle that all for you. The last thing that we're going to take a look at today is controls. We talked about this a little bit on the previous slides, so let me show off what that looks like in the gateway. This is what we're going to be creating. We have a controlling station that's a 104. It's going to issue a control down to the gateway. It's the gateway's job to then get that control and send it out to a DNP3 outstation that's out there. To do this, like we've done with mapping when the data was moving upstream, we're going to go ahead and put a 104 controlled station inside of the gateway's database, and then we're going to push that down to a DNP3 master. In order to do that, we have this configuration set up. We have our DNP3 master, and we have our 104 database. In this situation, we're going to take a look at controls. I'm going to pull back over DTM and show off how that looks. This time, I'm not going to be dealing with the IEDs or the downstream devices and changing values inside their database to propagate the event. I'm actually going to issue a control from the controlling station. To do that, you right-click on the master that you want to do, and you go ahead and do commands, and you can set up any of the commands that you would want to issue down. For this, we're doing an operation on a single point. As mentioned, we do have the protection of the select before operator, the select before execute, but if this was something that wasn't so risky, say it wasn't a breaker position or something, we could just do the direct operator, the direct execute, but we have it configured to do the select before operate, and then we've got the index that we're going to be working on is 2006. I'm going to go ahead and run these controls off in DTM and watch for the value to change within our gateway. Eventually, what we want inside of our gateway is our DNP3 master to issue the same output or the same control as the one being sent from the SCADA system. We'll monitor this point here, and we'll see that it's at latch off. If I send the latch on control, you'll see that that control came through instantly and was pushed back to my DTM downstream servers. From this point, we're going to bounce back to the PowerPoint, and we're going to take another break for questions, as I'm sure the live demo has probably generated a few. Yeah, we've got a lot of good questions coming in, so I'm going to try to focus on a couple that – I'm seeing a lot of questions come in about adding points, so I think this one question will knock out a couple of those. Can we add points in the gateway database in bulk, for example, via a CSV file? Yeah, so there's a few ways that you can add the data points into the gateway. One is we literally use a CSV file inside of our configuration. You can go into the back end of the gateway tool and export that CSV file. From there, you can just add a bunch of rows, add the points in, and then re-import that, and while you're there, you can also go ahead and automatically do the mapping. We do recommend maybe adding one or two of each point type first. That way, you see the format that it should look like. Maybe even do the mapping for one or two points first, and once you get that mapping done and exported, you'll see how it will be laid out. You can then kind of mass-produce effectively the lines using all the power that Excel has. Re-import that, and you'll have all of those points ready and defined. Great. Next question. Is there a way to create simulations in DTM such that you can specify a certain set of points to change state when choosing to run a simulation? For example, run a fault event which trips a breaker to lock out and cause a loss of voltage for every device downstream from the breaker. Yeah, we actually have kind of a sample workspace that comes along when you install DTM that has some of these kind of more behavior-approached simulations. What I'm showing off today in DTM, I'm trying to keep very simple as it's not the thing I have on the screen most of the time, but yeah, there is a whole scripting engine and default behaviors that we have included with some of our devices that allow you to do more complex simulations like that in DTM. Okay. Next question, and Matt, feel free to hand any of these over to the panel. What is the maximum number of points that your gateway has been tested with? I don't know the exact number of the maximum number of points that our gateway has been tested with, and I may hand that part off later, but I will tell you, and we'll actually touch on this in the last few slides, that our gateway is a very powerful application, and it's really dependent on the CPU and the RAM and just all the hardware specs where the gateway is installed on. Does anybody in our panel have any hard numbers or recent load tests or anything that we've done? So what I can do is read off the answer that we have here that has previously been answered by the panel. We have tested up to around 800,000 points. We typically have between 2,000 to 3,000 and 10,000 points in a single application, and we have production customers using around 100,000 points. Okay. Well, that's good, because I think I would have said closer to 400,000, so we were actually above the number I was going to give. Next question, how do we force a value in gateway? So you can set up an internal MDO, and effectively what this is is this is a master data object that isn't connected to a server or an outstation anywhere, and you can actually change that value inside the gateway GUI like I've been showing you, and if that's mapped to anything, that value can go out as well. Okay. Next question. Does OPC auto-map points support control to another OPC server? So we currently automatically add a OPC UA server, and those get automatically mapped, and correct me if I'm wrong, anybody on air, but you can also add an OPC client and I think issue commands or controls from the client within the gateway. Okay. Next question. Is it necessary to write the remote IP for a DMP3 outstation? I was being very meticulous today when I was setting up the one workspace. I wanted to show what you could do, not necessarily what you have to do every single time. Obviously you're going to need to have a TCP IP if that's what you're using connection to your device, and if you have firewalls or subnet differences that are causing issues, you might need to go in there and change that, but you can easily leave that at like 0.0.0.0 like I was saying. That will effectively just say, hey, gateway is running on this computer. If you can reach your partner, your client, your server on any of these adapters, just choose the one that works. So you don't have to, but I was trying to be very meticulous today just to show off what we can do and have that closed loop there. Okay. Next question. Do you have any way to get a timestamp from Modbus? I think I mentioned this offhandedly during one of the slides. Modbus doesn't innately have a timestamp that it sends across, so the most accurate thing you can do is use the timestamp that the gateway gets. So if the gateway gets a certain set of data at, say, a specific time, it'll put that time that the gateway received the data, which isn't necessarily the time that the event occurred as the timestamp, but it's the closest and best thing you can do with a protocol that doesn't necessarily support timestamping. And I think this is a related question, but you might maybe have partially answered it. How is the timestamp assigned when transferring a data point from Modbus server to DMP outstation? Yeah, I think that answer kind of answered it, but just real quickly one more time. The idea is Modbus doesn't send a timestamp, so you don't have access to a timestamp. So when the gateway receives the data, it says, this is the closest thing I've got, and then when mapped over, it'll use that as the timestamp. Okay. Next question. How are the ACK commands propagated back to the control station? So when the controls are sent from the control station down to the gateway, it's going to try to propagate that immediately down to the server that it's mapped to. And when the server downstream has a response, that'll go back up to the gateway, and that'll get kind of mapped back into the database held within the gateway and sent back up to the control center. Okay. Next question. Does the gateway support CID file imports for 62050 servers? Yeah, we support ICDs, CIDs, SCDs, anything along those lines. We should be able to support and import into our gateway. Okay. I think at that point, questions are still coming in, so they're continuing to be answered online. So let's go ahead. We only have a couple minutes left. Make sure we get through everything else we need to get through, and then we can go back to questions. Sounds good. So I'm going to resume. There are a few more features inside of our SCADA data gateway that we didn't go over in the demo. I kind of wanted to touch on those very quickly before we move on. The first is equations. So equations are a way that we can combine or modify data within the gateway based on mathematical or logical expressions. This can actually be really powerful because you can use data points gathered in the gateway databases that are from multiple different devices across your entire system to allow you to create points for more complex applications. So you can check, you know, if this breaker's open and this breaker's open, then we have this type of situation. And these points can also be propagated up. The next thing that we can talk about is the channel indicators and our system health. So one thing I didn't look at is a few of our, we have a few built-in views into the data into our SCADA data gateway. This view group pulls together all the important health, health warning data from across all the systems in your gateway to give you an overview of the entire health status of your system. And again, all of this data can be provided up to your control center for remote monitoring through the SCADA protocol. Someone asked earlier how we deal with security. This is one area of security we didn't talk too much about the demo. We have the SCADA data gateway supports role-based access control with user management. And with user management, you can create different users with different roles, allowing each person to have different levels of access to the system. So in the screenshot below, we only have two, but you can create plenty. The admin essentially can do whatever. It's a super user. Anything the gateway allows you to do, obviously, the admin should be able to do, whereas you may have someone who should just be a viewer. Their job is to make sure things are going well and alert someone if something poor is going on. So we have just user name, viewer user, just something random, and we have it as a role of users. And we have a bunch of built-in roles that you can set up and edit and add and things along those lines. And each of these will have a password that they can use right off the bat to verify their own identity. It's also worth noting that today we've looked at very small systems. A lot of our diagrams have been three or four devices being concentrated up to a single control center. All of our demonstrations today have been one device to one device connection. It's worth noting that we have a flexible license here. You can use tens or hundreds of points, kind of like what we've shown off today, or you could use up to hundreds of thousands of points, as we've answered in some of our Q&A questions. Again, just another kind of scalable thing, you can use two devices like we've done today, or even up to hundreds of devices that the gateway can support. We support this large range, and it's all really dependent on what the hardware that SCADA Data Gateway is installed on. And I think, as Beth mentioned a little bit earlier today, and maybe I have as well, our SCADA Data Gateway does run on both Linux and Windows platforms. I think that answers all, I think that is the end of our slides here. I'm going to pass it back over to Beth for just some final thoughts, and then we'll get back into the Q&A. We are at our hour, so I want to thank everyone for joining us today, and hopefully you've learned a little bit more about how gateways work, and how our SCADA Data Gateway can support your needs. The webinar will be posted on our website, if you'd like to watch a recording of that. It's published on our YouTube channel. You will get a survey after you sign off of the webinar today. If you answer that, I believe you responded with a link directly to the recording. You can download an evaluation license to the gateway directly from our website as well. And of course, if you have any questions, please email sales at trianglemicroworks.com.


Video ID: EmbZZp8pEeg
Welcome to the Getting Started with IEC 61850 TestSuite Pro video. When we first bring up TestSuite Pro you can see that we have a tool selection down the left side and in the center we have information about that tool. We also have a ribbon bar across the top with specific commands related to that tool. So as I select on the different tool selections we'll notice that center section in the ribbon bar update for the appropriate tool selection. Now the easiest way to get started with TestSuite Pro is to use the sample servers. The example servers are a separate application that simulate five different IEDs each configured as a bay controller. This application gives us a visual representation into the IEDs and allows us to control things for example tripping the breaker, adjusting the reported voltage, adjusting the reported current. We can even create a trip condition by dragging the current up above the trip threshold. So now we have these five IEDs running we want to connect a client to them. The easy way to do that is to use the TMW example workspace that ships with TestSuite Pro. Here we can see that the workspace is configured by an SCD file which is stored in program files. As we hover over the workspace description we can see the path for the SCD file. So if we load this file we'll load the SCD file, a custom display, and some additional configuration information. Now if we look at the workspace we can see each of the IEDs that we're connected to, the IP address of the IED. In this case it's always the loopback address because the IEDs are located on the same PC. We see a connected status and if we scroll over we can see additional information such as reports and goose subscriptions that we're subscribed to. Now let's take a look at DataMiner. DataMiner is used to find data from the IEDs that you're looking for for your test. There are several preset filters we can use. So for example we can use a measurement filter which filters based on the name and logical node class so that we can see the three-phase voltage and current. We can also use a protection filter which filters based on ACT which is a common data class used for protection and we can even filter on reported data. So this filter shows us the data being reported by the IED. Now it's also easy enough to create our own filters. We can just click on the filter for each column and select whatever filter information we want. And we can also group data simply by dragging a column and dragging the header up here. So now we're grouped by IED name. Next let's take a look at the Advanced Client. The Advanced Client lets us choose an IED up here so we can choose which of the Bay Controllers we want to look at. And for that IED it shows us the data model. So this is the traditional data model that we're used to looking at a lot in IEC 61850 and we can drill down and see specific information from this IED. Now if you're familiar with our Hammer tool, this is basically the display that was in Hammer. So when we combined Hammer and Anvil into a single tool, the Test Suite Pro, the Hammer display became the Advanced Client. There are several other tools here that we won't go into for this video but if you ever want information about them you can either click the Help icon or just go to File, Help. This will launch a browser that points to additional help information. So within here we can search for information. For example, let's look for help on Goose and look at info on Goose Events for example. We can also use the high-level categories to get us additional information. So for example, looking at Tools and Displays, Overview of Tools, we see each of these tools described here and we can click Learn More to get very specific information about the tool. So I'm just going to close the browser and return back to Test Suite Pro. Over on the right side is the display area. So this workspace includes several custom displays, starting with a high-level summary that shows all of the IEDs and then more detailed displays for each of the specific IEDs. Now it's very easy to create and modify these displays. For example, just go to Data Miner, find the value we want to display, and drag it onto the display. Now within this display, if we go back to our IEDs and the sample server, notice as we make changes, for example tripping the breaker, the display automatically updates not only the breaker condition but related information such as the interlock information. We can also look at the Goose Event Viewer tab and see the Goose events that are being reported. And we can look at reports. If we click on any specific report, then we'll see the report information update over here on the right. As we click on various reports on the left side, the information sent in that report is updated in the right. So this has been a very high-level overview of the Test Suite Pro. For additional information, you can use the help files that we showed in this video. You can also watch for additional videos from Triangle Microworks regarding this product. And if you have any specific questions, please be sure to contact us at support at trianglemicroworks.com.


Video ID: Ao0WBpzbJfw
Now, let's look at how the XML device profile can be used in the IEC 61850 engineering process. What is an IEC 61850 SCL? SCL stands for Substation Configuration Language. It is an XML-based language for configuring substations and substation devices. It was designed for exchange of information between engineering tools and includes descriptions of device models, communication infrastructure, and relationships within the power system. How can DNP3 XML fit into the IEC 61850 Substation Configuration Language, or SCL, engineering process? By combining generic IED files with substation templates and instantiating to obtain a central database for a specific substation. We can then split off a file to configure each IED or its PC-based configuration tool. The DNP Technical Committee is considering a proposal for a standard DNP private field in the ICD file. This picture shows the Triangle Microworks ICD editor, which includes a free component to create and edit DNP3 XML device profiles, shown on the right. On the left, we see an IEC 61850 ICD file. According to the proposal, the entire DNP3 XML device profile would be linked or embedded within a private field in the ICD file. This is the IEC 61850 Substation Configuration Language, or SCL, engineering process shown previously. Tools that understand DNP3 XML and IEC 60870-5 private tags can use or update the information in the DNP3 XML device profile. Others are required to pass it through unchanged. The Triangle Microworks ICD editor, or other system configuration tool, adds DNP3 data mapping information to the SCD file. Inside the substation, the IED configuration tool splits the contents out of the SCD into IED-specific CID files and uploads them to the IEDs. Note that the gateway IED will contain a private field containing a DNP3 XML device profile, with the mapping from IEC 61850 object models to either the value, timestamp, and quality of the DNP3 data points, or constants, for configuration parameters. Outside the substation, the SCD file can be used as an input for a control center-specific engineering tool to translate the data communicated from the IED via DNP3 back to an IEC 61850 object model. In summary, the DNP3 XML device profile gives you everything the old paper document had, plus it's machine-readable, so it enables creation of new DNP configuration tools. It can be used to create a DNP master that organizes data to user or other applications with IEC 61850 object models, and it allows DNP to participate in the IEC 61850 substation engineering process. For more information, please visit the Triangle Microworks website at www.trianglemicroworks.com. Our website contains reference material and training videos, as well as product information and downloadable evaluation versions of our products. You may also download the ICD Editor from our website. The ICD Editor contains a free component for creating and editing DNP3 XML device profiles. The download also includes a full-featured 21-day evaluation version of the other components, including the ability to create or edit IEC 61850 ICD files and IEC 61850 test server and test client. Additional information may also be obtained from the DNP Users Group, the IEC, or the ICA International Users Group. You may also want to check out the SCADA mailing list for exchange of SCADA information. For a more in-depth look at the DNP3 or IEC 61850 protocols, consider taking our cost-effective web-based training courses. For information on these courses, please visit our website at www.trianglemicroworks.com. You may also contact us by email at sales at trianglemicroworks.com, or by phone at country code 1, area code 919-870-5101.


Video ID: uHL78_oeqoQ
The 61850 communications architecture is defined in terms of two buses. These are logical buses, they may in fact share the same media and infrastructure. The station bus is used for high-level communication between IEDs and back to the substation host, which would be the pathway back to the control center. The process bus is used for streaming data between things like, from things like current transducers and voltage transducers. These samples can then be shared by the various IEDs in the substation. Three protocols are defined for use on the station bus. The first is a TCP IP stack containing some application layer things that we'll talk about shortly. The second is the goose generic object oriented substation event and generic status substation event, which are link layer protocols used to do things like protection tripping and that sort of thing across the station bus. The third protocol is the simple network time protocol that's a standard IETF. It comes with most of the TCP IP stacks, which is used to distribute time across the station bus. Three protocols are also defined for use on the process bus. The sampled values protocol is a specialized link layer protocol for use in distributing the digital samples across the LAN. The goose GSSE is again in the same link layer protocol used on the station bus for substation events and time sync can also be used on the process bus as well. These definitions all occur in a document 61850 Part 9-2 that defines the use of these. The 61850 documents are organized around a reference model. The reference model includes the application process, which has functions and object models that are defined in terms of a set of abstract communication services. These abstract communication services are intended to isolate the object model definitions from whatever is being used to convey the data underneath the communication scheme. Specific communication service mapping, SCSM, is the term used for the document that describes the mapping of those abstract communication services onto some kind of a communication stack. The first one developed was Part 8-1, which defines the mapping of the abstract services to MMS, manufacturing message specification, which runs over the TCP IP stack. Other SCSM's are in development, such as the mapping of the services to SOAP and web services. We expect that there will be more. The intent of all of this definition was to be able to do what we call future proofing. The idea is that in the future there's going to be different communication schemes developed and we want to be able to preserve the modeling above and just simply map the services onto different communication schemes. The same layered architecture as defined in the reference model is used in the way we build real devices. This diagram shows the triangle server architecture. Device applications, through a set of interface routines, interact with the core 61850 library. This package builds the object models directly from the substation communication language file, which defines the IED capabilities. These models are then mapped onto various services in the protocol stacks. Down the left we have the manufacturing message specification, which is ISO 9506, which makes use of abstract syntax notation 1, and this RFC 1006, which is the defined way that MMS makes use of TCP IP services operating over Ethernet. That first leftmost column is the protocol, the SCSM, as defined in part 8.1 of the document set. The SOAP services are in development. This SCSM will make use of the same, will implement the same services over SOAP and HTTP, the hypertext transfer protocol that's commonly used in web browsers and that sort of thing, which also operates over TCP IP. GOOS services operate directly over the Ethernet interface and don't make use of TCP IP.


Video ID: 2BHu6ilGibs
Thank you for your interest in Triangle Microworks. This video provides an overview of how our products and services can help you with your industry standard communication protocol requirements. Triangle Microworks specializes in industry standard communication protocols, including IEC 61850, DNP3, the IEC 60870-5 family, Modbus, and IEC 60870-6, also called TASI-2 or ICCP. Our IEC 61850 support includes both Edition 1 and Edition 2, as well as subsets such as GOOSE, 9-2 or sampled values, 7-410 for HYDRO, and 7-420 for DER, as well as IEC 61400-25 over MMS. Let's take a look at how our products and services can help you. Perhaps you need to add support for an industry standard protocol to your device. Maybe you need to integrate industry standard protocols into an existing project, perhaps with protocol translation or data concentration. Maybe you need to test, troubleshoot, or configure devices that use industry standard protocols. Or maybe you just need to learn more about specific industry standard protocols. If you have any of these needs, our products and services can help. The next few videos describe how we help with each of these situations.


Video ID: rMStloTo3aM
This demonstration was shown in the Triangle Microworks booth at Distributech in Orlando in February 2016. The use case being shown in this demo is testing an individual component of a system independently, without requiring the entire system to be set up and configured. As an example of this use case, we'll show how a protocol gateway can be tested on its own, without the other interfacing equipment. In the demo, the Distributed Test Manager, or DTM, is configured to test the protocol mapping of a real gateway. The device under test is the Triangle Microworks SCADA data gateway, our multi-purpose gateway which can perform protocol conversion and functions as a data concentrator. The SCADA data gateway is a software-based gateway which can run on Windows. To learn more about the SCADA data gateway, click on the links below to see more videos. For the demo, the SCADA data gateway is configured to concentrate data from 100 IEDs using IEC 61850 reports. The gateway receives the reports from the IEDs and then translates the data objects in the reports to DNP3 points. On the up-channel side of the gateway, a SCADA DNP3 master sends report-by-exception pulls to the DNP3 outstation in the gateway. The gateway passes the mapped data from the IEDs up to the SCADA DNP3 master. In order to test the mapping configuration of the gateway on its own, we would like to simulate both the IEDs on the IEC 61850 side of the gateway and the SCADA master on the DNP3 side of the gateway. DTM is used to simulate these interfaces and manage the protocol messages on both sides of the gateway. The result is an observed mapping table which shows how the gateway performed during the test. The mapping table can then be used by the tester to verify that the expected behavior was observed from the gateway. Without DTM, you would typically need the SCD file for the network to be completed, including all of the point mappings, the actual devices would need to be present, and you would need test equipment to generate realistic signals for each device being tested. With DTM, you can simulate some or all of the devices. If the SCD file and mappings are not complete, you can limit testing to just those sections that are complete. And DTM can simulate device signals, eliminating the need for expensive test equipment. Now I'll show the DTM application, which is where we will run and observe the gateway mapping test. There are several windows in the DTM user interface. On the left, there's a workspace which shows all of the simulated devices including 100 IEDs, which are IEC 61850 servers, and one DNP3 master, which represents the SCADA system. In the middle of the screen is a custom display which was created using InSight software, which is part of all Triangle Microworks test tools. The custom display uses an underlying image with graphics objects placed on top to create an interactive test interface. On the right, there is a network window which shows us which PC the DTM application is running on. For this demo, we're running a single DTHost application on one PC for all simulated devices. DTM is a distributed application which can be run on multiple PCs in order to distribute the simulation across a network. For example, we could run some IEDs on a computer attached to one switch, and some IEDs could run on a PC attached to a different switch. DTM also supports multiple LAN adapters for each PC, and supports multiple IP addresses configured for each LAN adapter. For the demo we showed in our booth at Distributech, we configured the LAN adapter to have 100 IP addresses so that each simulated IED used a separate IP address on the one LAN adapter. We'll now look at the object model for the simulated IEDs. There are multiple datasets configured for each IED. These datasets are used to define which objects will be reported to the gateway using IEC 61850 reports. During the mapping test, DTM will change each individual data attribute for all of these data objects for each dataset in the IED. Now we'll bring up the data points for the DNP3 master. The IEC 61850 objects are mapped to these DNP3 points using binary inputs, double bit inputs, and analog inputs. The SCADA data gateway supports all DNP3 point types, but for simplicity we're limiting the test to these three data types. In order to run the test, we've created a test script which will run in the InSight scripting engine, which supports standard JavaScript. JavaScript is a scripting language which supports many common programming functions and has an API for direct access into protocol-specific functions. For example, an event-driven script can be created which triggers off-course messages or changes points in the data model for a simulated device. DTM makes it easy to map the script to the object model by supporting drag-and-drop. So, for example, I can find a point in the data model, drag it onto the script, and a new point is created in the script. The test script starts by parsing the IEC 61850 object model, including both complex objects and data attributes, and then creates a list of data attributes that are in the data sets. This list will serve as the list of points to be tested. For this example, we're limiting the test to the common data classes MX, or measure ends, which will be DNP3 analog endpoints, and ST, statuses, which will become DNP3 binary and double bit inputs. After the list of data attributes, or DAs, is created, the script steps through each DA and changes the value. As each IEC 61850 DA is changed, the DTM script waits to see which point changes on the DNP3 master side. The script waits long enough for the IEC 61850 report to be sent by the IED to the gateway, and then for the SCADA master to pull the outstation in the gateway. In this test, the limiting factor is the DNP3 pull, which is performed once a second. As data changes are detected, the script checks to make sure the value on the IEC 61850 DA matches the DNP3 point on the other side of the gateway. If the values match, the script writes the result to an Excel file and then moves to the next DA in the list of test points. Before running the script, I will choose which IEDs I would like to include in the test. For example, I'll choose 1, 3, and 5. And I can choose which data set should be tested. I'm going to choose all of the data sets except for tracking reports. The script will test all of these data sets it finds in each of these IEDs. When I click Start Test, DTM starts executing the test. The custom display shows us which IED is being tested, as well as which data set is being tested, and which DA is being changed currently. The DTM output log shows us the ongoing results of the test. The path for each IEC 61850 DA is printed, along with any corresponding DNP3 data point changes. After the script has gone through every DA in the data sets, it opens the results in an Excel file. The first column shows the data set that was tested. The second column shows the results of the test. The third column shows the results of the test. The fourth column shows the results of the test. The second column shows the path for each DA tested, including the logical node, the data object, and the data attribute. The third column shows the observed DNP3 point which changed. And the fourth column shows the data type for each point. You can see that every IEC 61850 DA was not mapped to the DNP3 point. For this example, we only mapped a handful of points, including the position of one breaker, one switch, and some of the voltages and currents. So as you can see, we were able to verify mappings of the gateway without having the physical devices present for the IEDs and the DNP3 master. DTM simulated both the DNP3 master and the IEDs that were connected to the gateway. By using DTM, we were able to verify the gateway mappings without having all of the devices present.


Video ID: ruRN_Jz9MHc
Good morning and welcome. My name is Beth Kapalis and I'm the Sales and Support Manager here at Triangle Microworks. Welcome to our latest webinar, Tips and Techniques, Gateway Configuration Made Easy. Before we get started, I'd like to go over some housekeeping items while everyone is still joining. The webinar is scheduled to be one hour long. The audience is on mute, so please submit your questions online using the webinar toolbar under the section titled questions. You can submit questions as soon as you have them. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly. We'll try to take a couple of breaks to read some of the submitted questions and are also happy to stay on after the webinar is over and answer as many questions as we can. With that, I'd like to introduce the folks behind the scenes that are answering those questions. Today, we have with us Cyril Vendetti, Product Developer with Triangle Microworks. We also have Fred Van Eyck, our Senior Product Developer specializing in our SCADA Data Gateway, also with Triangle Microworks. Next, I'd like to introduce our speakers. Today, we've got Matt Green. Matt is a Senior Application Engineer at Triangle Microworks and brings over a decade of experience to the role. Matt serves as a technical trainer, providing direct customer support, conducting product demonstrations, providing one-on-one or group trainings, as well as hosting webinars. Additionally, Matt works as a quality assurance tester, specializing in the company's simulation tools and our gateway. Next, we have Jackson Moore, Application Engineer here at Triangle Microworks. Jackson received Bachelor's of Science degrees in both electrical engineering and computer engineering from North Carolina State University. He has a background in power systems engineering. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer, where he designed and developed load management control systems for multi-source microgrids. Currently, Jackson works with our customers and development team to understand and solve the challenges our clients face. Now that you know everyone, I'd like to give you a little bit of background on Triangle Microworks. We were established in 1994 and have over 28 years of communication protocol experience. We're headquartered in Raleigh, North Carolina and have customers in over 70 countries. We work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their cost of development times. Licensing a source code library also includes support for your implementation in any issues that may arise in the field. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They're designed to troubleshoot, simulate, and automate testing for both a single device or complete substation. Lastly, we provide a SCADA data gateway, which is one of the main tools you'll be seeing here today. It's supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. It is available as software only or pre-installed on hardware. We are heavily involved in the standards committees that support SCADA protocols, like UCA Group for 621A50, and DMP user and technical committees. Through our participation, we help define the protocols, stay up to date on the changes taking place, and ultimately provide feedback based on our customers' needs as the protocols continue to evolve. Now that you know a little bit more about Triangle, I'm going to go ahead and hand it over to Matt to kick off the webinar. Matt, I'll go ahead and make you presenter. Thanks, Beth. Today, we're going to be talking about gateways. This is the agenda for what we'll be looking at for the remainder of the webinar. I'm going to start by doing a brief slide or two about what a SCADA gateway is and what its basic functionalities are. After that, I will be going over, I will be going over some of the basics that we've touched on, some of our previous content we've put out. We have several resources to help people understand gateways and specifically our own SCADA data gateway. One of these resources was a previous webinar we did named Introduction to Gateways. The first part of this PowerPoint presentation will be to recap some of those major features of the previous webinar. Of course, if you want more extensive details on these topics, you can re-watch that or any of our other gateway related content, especially our webinars on our YouTube channel. I'll be talking about manually adding and mapping points from the RTUs and communicating that up into our gateway. I'll also be talking about cybersecurity and the enhancements that we have, including the user account and audit logging, and explaining the differences between the different user roles when it comes to those cybersecurity, such as read-only users, super users, configurers, things like that. After I'm done with the PowerPoint presentation, which is probably about 10-15 slides, we'll be moving on to the live demonstration that will be given by one of my colleagues, Jackson, who Beth introduced. We'll be talking about creating and mapping points again, but this time we'll be using some of the tools available to our SCADA data gateway to allow you to do that automatically. This is important by automating this process, you can actually drastically decrease the configuration time of your gateway especially for large configurations. We'll also be looking at equations which include logical or mathematical processes to better view, report, and understand what's going on in your system. Then we'll probably get close to ending with talking about sequence of events, which is one way that you can log information out to a document that you can refer to later. A typical gateway will have this type of structure. We'll be referring to a diagram like this or ones that are similar to this a few times throughout this presentation. It's worth taking a good look at this as a good place to start when it comes to foundational information for a gateway. You'll see here this light blue square, this is our gateway. Above that, we have a control station. In this situation, it would be a DNP3 control station. This could also be a SCADA master or possibly an HMI. Anything like that can be sent here as the recipient of the information from the gateway. You'll also see that there are RTUs in this situation. Again, it's a DNP3 outstation. All of this information will then be reported up to the gateway. The question here is, how can you get all of the data that is relevant and important from all the different RTUs spread across your substation or system into a single place for the end-user to see, especially at a control station or externally. That is the job of a gateway here. Gateways will typically have multiple network adapters and multiple IP addresses. For example, you'll see master 1, which hooks up to DNP3 outstation 1, will be at a different IP address than master 2 and master 3. All of that would be, say, on one logical network. This would be like 192.168.1. Whereas typically, the control station could be isolated to a different network to allow for segregation of different types of data. You'll also want to be able to run all of that through what we call the mapping logic here, which is in the middle. What that mapping logic does is it allows you to pick and choose the important bits of information, maybe your class data, your report data, anything like that, can be all mapped up to the outstation, which is what is presented upward. Now let's talk about some of the functions you'd expect gateways to have. Not all gateways will have all of these functions. You would expect a large percentage of them though, to have at least one or multiple of these functions. The first one we're looking at today is the data concentration. That's actually what we were just looking at here. Data concentration is when you take data from across multiple different devices, and then you put it all through this mapping logic into a singular database, and then you can present that database up to your DNP3 or your control station and whatnot. You're concentrating the data across your entire substation to a singular database, which you're presenting to your HMI or your SCADA master. There's also protocol conversion. This is a feature that is where our gateway originally started out with. What it will allow you to do is translate between protocol. For example, we could have a hypothetical substation that included both 61850 and Modbus RTUs monitoring the electrical equipment. But if your control station is expecting IEC 6870-5-104 traffic, there's obviously a mismatch there. Another thing a gateway could do is take from your Modbus and 61850 devices, go through the mapping logic, and map that to a database for the control center, the SCADA master to see. Not only do you map the protocol, but you can map the like types. Obviously, very simple example would be a 61850 Boolean, probably would get mapped to the similar Boolean type in the 6870-5-104. But there are other things that the gateways could do, such as possibly condensing the data or scaling, doing scaling factors or things like that could all be done during this protocol conversion step. Another function that you'd be looking for inside of a gateway is any way to historically log data. That can be done through external database servers that you export data to. Sequence of events is what I mentioned a little bit earlier. There's a lot of different ways that you want to be able to save that data off. The important part about that is once you have that data off, you now have access to not only what's going on in your system right now, but if you know something went wrong previously, you should be able to drill down to what exactly happened during that time and try to isolate whatever the problem is. The fourth topic for the functions that I'll be going over today is security. This is becoming more and more important over the last few years. I'm sure you've heard of the rise of cyber attacks and things like that when it comes to basically anything that malicious people can try to get into and hold ransom or whatnot. Security is becoming much more important in the SCADA world for both user control and for data. Data could be authentication or encryption, anything that makes it so your data is harder to intercept or do like a man-in-the-middle attack for, whereas user would be providing things like usernames or role protection or audit logging to allow you to verify what's going on from the configuration point of view. And the last function that you could expect to see within a gateway would be any sort of local HMI functionality or setting alarms for people to look into. Basically just getting a localized quick warning if anything's going wrong to alert people to go and repair those or work on those or gather the information that you need to make informed decisions. So we're going to start on the webinar recap, the introduction to gateways. I've got a few slides here just to kind of catch you guys up. We demonstrated our SCADA data gateway. That is what we'll also call the SDG for short. SDG was built 20 some years ago. Like I mentioned earlier, it was set up to be originally just kind of a protocol translator. It was built off of all of our libraries that we still maintain and sell to this day. It grew over those 20 years to support IEC 61850, BNP3, ICCP, or also known as TASI-2, IEC 6870-5-101, 103, 104, Modbus, and multiple flavors of OPC, including UA, Classic, and A&E. And our SCADA data gateway is used by customers all around the world, you know, thousands of them. We've, you know, it's one of our most mature products. So let's talk about the basics of how you can interact with our gateway, specifically the SCADA data gateway. We've made it so that when you're configuring the devices, whether it be the client or masters that are pulling data in from the RTU or the databases that you're presenting to your control center, we have these built-in wizards, like over here on the right-hand side. These wizards will step you through the process of creating everything that is required for your device to be functional, within the SCADA data gateway, including IP addresses, port numbers, specific namings you can put in there, any of the cybersecurity stuff, such as, you know, TLS or encryption or authentication. This wizard will basically step you through the process of creating the device. Once you do that, you can then go to the device you created and add points to it. So, for example, if you did create a DNP3 master, at this point, you could add all of your binaries in individually or all of your analogs or whatnot. Some protocols even have an automatic feature where you just right-click and do autocreate tags, and what that'll do is reach out to your RTU, say, give me all your points, and it'll automatically populate all of that for you. Of course, now that you have those placeholders for the data, you need to keep that data up to date, and that's what this action mask here is for. So, following the example I was just talking about, we'll have a DNP3 master action mask. Well, the action mask is telling the gateway how you want to receive the data. Do you want it to pull it periodically every second? Are you enabling, like, unsolicited messages and getting them as it changes? Are you expecting, you know, an integrity data pull every 24 hours? However you want to get the data in, you can configure the gateway to do. So, for this example down here, you'll see I have it set to read class 1, class 2, and class 3 change events, and underneath it, you'll see that this action mask has a period, and you can do that every second, two seconds. Just make sure you keep up with the change events. So, now we have all of the live information in our gateway. How do we either do a protocol translation or map that to a different database and concentrate it across multiple device? Well, we tried to make that as easy as possible as well. So, for example, you can go to, let's say I had this RTU client for 61850. You can go to the point that you're interested in, such as this MMXU that is our measurement values. Just grab and drag and drop it where you'd like it to go. Once you do that, you'll go ahead and see the value is now mapped within your SCADA database being presented up to your control center. We tried to make it as easy as possible when looking at these databases that you're building up to know where the data's coming from. So, you'll see right here in line, there's a column called mapping, and you'll see Bay Controller, MMXU, all the way down to the path of where that came from. We also added these direction indicators with these red boxes around them just to kind of point that out as I put that in the PowerPoint. This lets you know which way the data is basically flowing. So, it's coming in to our 61850 RTU's client, and then that value is going out into the mapping to our SCADA database. Or if you look at it from the other configuration view, you see that the value from there is coming in to our SCADA database. We also support multiple mapping. So, if you have 100 points, don't worry about dragging them individually one at a time. That can take a lot of time. You can actually just multi-select and drag them all at once and map them at once. We also spent some time talking about cybersecurity as that is becoming more and more popular of a feature these days. The one that we'll be talking about right now is the user accounts. What the user accounts will allow you to do is limit your exposure if possible cybersecurity attacks do happen. So, if you set up username, passwords, and roles for the people who will be interacting with your SCADA data gateway, you can give people who may only need to read information, read-only permissions. This will prevent them from making any sort of mistakes or accidentally deleting something they shouldn't and just lowering your risk there. But also, if you have these user accounts, you're lowering your exposure in case there is a data leak or someone gets access to an account they're not supposed to be able to. If everyone is an admin, then any account that gets leaked could be catastrophic. Whereas, if you limit each individual account to its specific functionality and what it needs to be able to do, you are lowering your exposure there. Of course, we go all the way from read-only all the way up to basically admin or super user. There's also user, configurer. There's all sorts of different roles you can put in there. After you have this user accounts and these role-based access, you can then access the audit logging feature. And what that will do is, again, hypothetically, if there was an exposure or password leak or something, you could go in and search on the account that you think might be compromised, filter down on any sort of actions, and try to help you look through what happened over the course of that time to let you figure out, oh, it seems like over the weekend so-and-so did this. And you talk to them, oh, that wasn't me or whatnot. It helps you really kind of trace down exactly what's going on in your system and help you isolate possibly any exposure. So our SCADA data gateway is now run through a web browser, think like Google Chrome or Firefox or something like that. But it is worth pointing out that even though it is inside of a browser like that, it does not require Internet access. We are really trying to take into consideration cybersecurity. And if you need to limit the access to your SCADA data gateway and you don't want to obviously open that up to the World Wide Web, then you can access it through an intranet or local network. If you want to make it even more secure, you can actually set it up so your SCADA data gateway will only respond to specific IP addresses. So someone can't just bring their laptop in, plug it in and start interacting with the SCADA data gateway. You can actually lock down which PCs are allowed to communicate with the SCADA data gateway. And, of course, we have certificates. You can do third-party certificates for HTTPS. We have encryptions, all sorts of cybersecurity features for the SCADA data gateway. So this is going to be the last slide we'll be looking at today. I really want to give you kind of an overview of what Jackson will be going over. We are building up and putting the finishing touches on a configuration for our SCADA data gateway. You'll see that here in the middle is our SCADA data gateway. We'll be communicating with several things outside of the gateway, but the end goal is to have a 104 database, which we will publish information to, to our SCADA master as a 6870-5-104. Everything here in red is our SCADA data gateway. This kind of gray, dark gray color will be simulated in DTM, and we'll also have an OPC UA server that we'll be interacting with. So the idea here is all of the data will need to come from these RTUs and this OPC UA server, and eventually be put into our gateway through 61850 clients, DMP3 masters, and obviously an OPC UA client. All of that information then through mapping, equations, anything like that will be concentrated into a single database and then published up to our SCADA master. So at this point, I think I'm going to hand it back over to Beth and I think quickly over to Jackson after that. Okay. Thanks, Matt. So I do just have one question that came through here. When you map between protocols like 61850, 870-5-101, 104, or DMP3, are you following industry standards like IEEE 1815.1 or IEC 61850-80-1? I might pass this one off to the panelists. I know that we try to follow standardizations and some of, obviously, the more simple things like Boolean to Boolean or, you know, you can do things like converting from shorts to, you know, floatings or whatnot. I know there are different ways you can do that way. As for those specific parts of the spec, I don't want to misspeak there. So I will pass that off to someone who may have a better answer or more accurate for the second half of that question. Okay. Well, we can address that one online as well because I know we do have a lot to cover. So I'll leave it to the engineers working behind the scenes. And then with that, I'll pass it over to Jackson so he can start his part of the demo. Okay, Jackson, you should be presenter. Sure. All right. Just a moment. All right. Thank you, Beth. And thank you, Matt, as well, for that excellent introduction. So what I'm going to be doing today is really the live demonstration portion. And this is one of the last slides that we saw up on Matt's screen. And it's what I'm going to be configuring in our gateway, SDG or SCADA data gateway. So just to run through a little bit about what we're going to be doing, we've got the gateway here in the middle. At the bottom, we have our data that's coming in. So this will be an OPC UA server. So I'll have an OPC UA client within my gateway that's going to be receiving that data. I'll also have two DNP3 outstations that are sending data, and I'll have two DNP3 masters that are receiving that inside of my gateway. And then finally, I'll have two 61850 servers. Again, these at the bottom will all be outside devices, and then I'll be receiving that data inside my gateway. Of course, 61850 will be with a 61850 client. Finally, I'll be aggregating that data and providing it up to a remote SCADA master. So, you know, of course, what this looks like for you is going to vary depending on what devices you have and what protocols you're using. But today, I chose to use 6870-5-104, pretty common SCADA protocol, at least in certain parts of the world. And I'll be providing that data that I aggregate all together and then passing it back up. So I went ahead and did a little bit of the configuration ahead of time. So I have brought in one 61850 client as well as one DNP3 master. But now we're going to be creating the rest of what we saw in that last diagram. So first, let's start out with 61850 client. So in order to do a lot of what I need to do today, I'm always going to be starting by right-clicking on the gateway root node. And you'll see there's a lot of different options here, right? So this is how I'm going to be adding different client and server communication devices. But in this case, I'll actually be going down to the 61850 section and adding a 61850 client. So the first thing I need to do is give it a name. I like to start with 61850, the protocol that I'll be using, whether it's a client or a server, master, or an outstation, depending on the protocol. And then finally, the device that I'm connecting to, its name. Next, 61850 is a little bit unique, at least compared to something like 104 or DNP, where it does have a model, some sort of file that describes what that device contains. So there's a couple different ways we can get this model. One would be to actually do a discovery. So what we could do if we have our device, our IED, this D1 line 1 LPU available, then I could just fill in my communication settings, connect to it, and discover the data model that is on that device. But if I want to do more of a top-down engineering process, I already have the file for this device defined, then I can instead do build from file. So this might be a CID file for 61850, an individual capability or configured IED description for a file, or it could be an SCD, a substation configuration description. In this case, I'll be using an SCD. Now, it is already uploaded into my gateway because I already did create that other device ahead of time, this ECU, from that same SCD. But otherwise, I just need to upload the file into the gateway. The next step is just selecting which of the multiple IEDs defined in this particular SCD file I want to create a client for. So again, if this was a CID, there might only be one, but because this is an SCD, it describes a whole substation and the many devices in it. So I'll just select the D1, line 1, LPU1. So one of the cool things that this does for me, because the SCD file contains the communication settings for this device that I'll be connecting to, those are filled in automatically. So I see the IP address already filled in, as well as some other parameters as well, such as port. If I come down into our seven-layer stack setting, if you're more familiar with under-the-hood stuff of 61850, this might look familiar. Other security settings and things like that are all available down here at the bottom, depending on what you're using for your specific application. But we do support a host of security features. And with that, I'm ready to go ahead and just hit OK and create my 61850 client. But I'm not done there. All I've done so far is create that client-server connection. So next, I need to start to bring data into my gateway that's available from the server that I'm connected to with this client. In order to do that, I right-click on this client that I just created. And I've got a couple different options again. So I could pull a particular data set or just even a particular point. But if I have done my top-down engineering process, I've created reports already on my device, or maybe even they just came pre-configured in my CID from the manufacturer, then I do have the option to just add a 61850 report control block. So in this particular case, this server that I'm connected to only has a single report control block. But if I did have multiple, each one would be listed here. And then once I pick one of the report control blocks on this device down below, I can see what data is going to be available in that device. So this list here is really every data attribute that's contained in potentially every data object that is a part of this report. So because there's only one, I know this is the right one. And this is the one that I want to add. But if there's multiple, I would use this list down here to tell which one I wanted to add, if not all. OK, so now I've added this report. The report really already existed within the SCD file. But I've now made the gateway aware that I want to gather data out of that report. And I'm going to add some items to this report. So again, just to be clear, if you were to look in the SCD file for this particular device, you would see that report, and you would see the data in it. What I'm doing here is really just the configuration. It's where I'm adding the data into my gateway of what was made available in that device. So I have a couple different options. This is all the data that's available on the device. Each row here represents a data attribute in the data set of this particular report control block. I can also see if the timestamp in quality is available, which in this case it is because they were included at the data object level. I could just add some checks for individual ones that I want to add. Potentially, I could just do a filter, look down for it. Maybe I just know I want to add everything that ends in STVAL, for example. But in this case, because I did do a top-down engineering design, I chose the data that I wanted to be in my reports. I chose the data that I wanted to be available to my gateway ahead of time for this express purpose. So really, all I need to do is just check this top box up at the top. It'll go ahead and check everything for me. And when I hit OK, it'll add all of these data points into my gateway. So we've not yet mapped this to anything going out, but I'm getting a little ahead of myself. Before we start making this data available up to my 104 SCADA master, I want to go ahead and finish the other data that's coming in. So if we pull back up that picture that we looked at before, I effectively have two 61.850 servers. And I'll be working from right to left. We saw that I already have one DNP3 outstation. And I want to create a second. I'm sorry, we already have one DNP3 master connecting to one outstation. But I want to create a second that connects to a second outstation. And that's what I'm going to be doing next. So to do that, much like I did before with my 61.850 client, I'm going to right click on the gateway route node. So we do group a couple of different protocols together within this add TCP channel master. So I'll go ahead and click that. And that's how I'm going to create my DNP3 master. So I do need to specify that it is a DNP3 master from that dropdown. Like I said, we've lumped together 101, 103, 104, Modbus, and DNP. And then I need to give it a IP address of the outstation that I want to connect to. So in this case, I know that this will be 10.10.10.153. And I know that my port number is going to be 20,001. My local IP, I can leave for default right now. If I have multiple IPs on my machine, I can choose one that I want to bind to for my outgoing connection. But for right now, I can just leave that at the default. And lastly, I do need to give a name to my connection that I'm going to be adding my DNP3 master. So it looks like it saved the names that I've given recently. So I'll go ahead and choose the DNP master 2 to kind of complement the DNP master 1 that I already have. All right, a couple more settings that I have the option to look at. Some security settings down here, depending on what your particular setup is and what level of security you're looking for, you do have those options. I'm going to give it a session name. Otherwise, it would just choose a default. And then, again, just other communication settings. In this case, I can leave everything by default and just go ahead and hit OK. So this is where I would add the types of data points that I want to be read into my DNP3 master. Now, there's a couple of different options, different ways I could handle this. So I could do a discovery, so auto-create tags. So if I have my DNP3 outstation out there and available on the network, then it'll go out and do a discovery and say, hmm, OK, what types of points exist on this device? It'll do kind of a general polling of everything that's out there and create the appropriate tags here in my DNP3 master. And there's some cases where that might be exactly what I want to do. But in some other cases, I might not actually have that DNP3 outstation available and on the network while I am configuring that gateway. And this is pretty common. You might be configuring the gateway at your desk before you get out into the field and actually have it installed and all devices available. So we do have some options as well. I could basically create the points manually. And I don't know about you, but when I create points manually, I always like to give my points intelligent names. And that's what I've done up here ahead of time on DNP Master 1. So you can see that I've used alias names such as line 1 breaker 1 position or FTO might indicate failed open, failed to close, or just maybe more general alarm. And I have this for each breaker that I might have. Let's say I have five breakers in this particular bay or line. So now I want to leverage this information that I've already created. I've already taken the time to create my 25 binary inputs and my 25 analog inputs with intelligent names. And I want to apply these that I've already done from DNP Master 1 and use them as sort of a template for creating the necessary points using the same names or maybe slightly modified names in my Master 2 that I've just created. So good news, this is pretty easy to do. In order to do that, I just need to right-click on the device itself, so DNP Master 1, and I'm going to export the points. So I do need to make sure that I say recursive, include children, and that's just because session as well as the point types, binary inputs, and analog outputs are children of the root, which is the DNP Master 1 device. I'll just go ahead and hit Export. When I do that, it exports the file out to my Downloads folder, and I can just right-click and open that up in Notepad++. And now we see a nice overview, kind of a, really, it is a CSV view of what's available inside of that points list. So we do see the user tag name. This is going to correspond with the alias name, the intelligent name that I gave the point versus the tag name, which is really just the device, the session, the point type, T1, of course, corresponding to binary input in DNP, and then P0, the index. So to update this for my second DNP Master, I'm just going to do a basic find and replace, and replace Master 1 with Master 2. See, we've done 50 occurrences. That all sounds good because I have 50 points, 25 of each. And then finally, because this is my Master 2, and let's say that it might be in Bay 2 as opposed to Bay 1, I'm going to change the line 1 to line 2. And I should get, I believe, 100 of these because I have both a user tag name as well as a description. Correct 100 occurrences. All good there. And then I'll just go ahead and give this a save. And I'll name it DNP Master Points 2. And now I am ready to upload this back into my gateway and apply these points that I've just edited to my DNP Master 2 device that I created. In order to do that, I just right-click up on my gateway root node, and I want to select the Import Export Points option. I click that. Make sure that I select Import Points. I'm not exporting anymore. Find that point that I just created, DNP Master Points 2, which is a modified version of 1. Just hit OK. And sure enough, I see that it has created 50 points for me. So I can just close this out, give my gateway a refresh, just to bring in the updated point types. And sure enough, Session Binary Inputs. If I click on that, I can see that it has created 50 points for me. Session Binary Inputs. If I click on that, I can see that I've created those 25 points with the intelligent names that I took the time to apply for DNP Master 1. And now they've been applied down here to DNP Master 2. All right. So we're still not quite ready to begin passing data up to my 104 master. I still need to bring in data from my OPC UA server. So just a quick word before I start doing that. Everything we have brought in thus far in the gateway has been brought into an internal OPC UA server, which is what you see here. The mapping is really done automatically. So if I did have a local HMI or something like that, that needed to read data from my gateway via OPC, that work has already been done. But every time I bring in a point from one of these 61850 devices or DNP devices, I am adding it to a local OPC UA server. That's great. But a lot of times, you also have customers that want to bring in data from an external OPC server. And in this case, we do need a local OPC UA client. So that's pretty easy to do. Much like I did for my DNP and 61850 devices, I start by right clicking on the gateway root node. Now again, I'll skip over the TCP channel section as well as the 61850 channel section. And instead, I'll come to the OPC UA area right here in the middle. Well, I shouldn't say UA. There's OPC Classic up here at the top, AE, alarms and events, and then UA is what I'll be using today. So I'll add an OPC UA client. And just like before, the first thing I'm going to need to do is give it a name. So I like basic names. So I'll just start with OPC UA client. Now, instead of a IP address like we typically have for maybe DNP or 61850 or something like that, I need to provide a URL, which is similar to an IP, but it includes other information as well. So I'll just open up my OPC server that I have running in the background, copy the URL out of that, and paste it in. All right, so again, kind of like the other protocols, depending on my particular device and what security settings I may or may not be using, I am already done in my case. If you are using some sort of authentication or advanced security settings, you'll need to set that up here with your certificate files, private keys, so on and so forth. But in my case, I'm just using the basics. So I can just go ahead and hit OK. And I have created my OPC client. I'm sorry. Did I name it OPC UA server? Which one is it? Sorry, guys. OK, here we go. So I named my OPC UA client OPC. So now it's time to start adding items to my OPC UA client. Now, there's a couple different ways to do this. I can add individual points or multiple points. But either way that I choose, one of the nice things about OPC UA or OPC in general is that it is a browsable protocol. So I don't need to memorize point lists or even provide a configuration file. I can just click and add and browse my device, assuming it is available and connected, and see what data points are available on it. So in this case, on my OPC server that I've created just from this demo, I created a folder called substation OPC that has a bunch of different data in it, including an object called voltages that itself contains 10 children elements named voltage 1 through voltage 10. So I created it for the purpose of adding it all to my OPC client. So I'm just going to go ahead and check all and just add items. Sure enough, when I did that, it has added the 30 items that I created. I can now close out of this and come down. And we'll see that I've added a folder called substation OPC underneath my OPC client. If I click on that folder, I can see the different points that were available at that root level of the folder right there within substation OPC. And then as I mentioned, I had created another object in some ways like a folder within that substation OPC folder called voltages. I see that appear as a child of substation OPC. And then down below, if I click on it, then I can see the 10 different voltages that are inside of that particular object. Again, I call them voltages. They're really just floating point numbers that could represent any data. But we, of course, have floating points, binaries, strings, so on and so forth, common OPC UA data types. All right, so at this point, we have brought in all of our data. If I bring back up our picture one last time, we'll see we have our OPC UA server, our DNP outstations, and our 61850 servers all existing down here at the bottom. We've created their companions within our SCADA data gateway. That would be the OPC client, the DNP3 masters, and the 61850 client. So now it's time to begin passing our data, and if we choose, manipulating it before we do up to our 104 master. But before I do that, I do want to pause for a quick minute and talk to Beth. Beth, do we have time to stop for questions? I have time for one question because we're a little tight, but let's get this one in. And Matt, I believe this is a question for you. So during protocol conversion, is there a chance that we lose some information such as time stamping accuracy and quality information? So it really depends on what you're mapping to and from. Obviously, each protocol has its own unique way of storing data and its own unique way, specifically for qualities that they want to handle quality. So maybe one protocol will have a quality for something that doesn't necessarily one-to-one match up with something in a different one. But we have various ways, and this kind of goes back to the 20 years we've been working with our customers, of automatically setting up these conversions for you. Of course, you can go in and specify things through equations, which Jackson will go on a little bit later. So I would say you could get a perfect one-to-one, especially for data values and stuff. You may not lose any sort of significant values or significant digits. Or maybe if your data types are slightly different, you might lose a little bit. So it really depends on how you set it up. And as for qualities and timestamps, we can map those over as well. And there are several different processes that you can follow to basically try to figure out what is a like quality to a like quality in a different protocol. OK, great. Thanks, Matt. Jackson, I'm going to hand it back to you, and you do have the rest of the hour. OK, excellent. All right, so where we left off, we have all of our data coming in. And we're now ready to create our 104 SCADA master and begin mapping data up to it. So just like I did for OPC and DNP3 and 61850, in order to create that 104, well, really, I need to create a 104. I believe the term is in 104 controlled station. But you could call it an outstation slave. Depending on the protocol, the terminology changes server for 61850. So in order to do that, I'm going to come back in here. And just like I created my DNP master by clicking Add TCP Channel Master, in this case, I'll click Add TCP Outstation Slave. So I do need to specify that this is 104 in particular. And then I do want to give it an IP address. So I could just keep with local host. But in this case, I'll go ahead and do 10.10.10.154. So I do want to point out that I need to have this IP address available on my machine. I have, ahead of time, gone in and added this as an available IP on my machine. The gateway won't do that for you. You need to make sure you do that externally. But you can also just use local host. It'll bind to whatever the local IP is. So for a channel name, again, it just seems to be remembering the names that I used in my trial run here. I'll go with 104 outstation to SCADA. Again, 104 indicating the protocol outstation, just clarifying that data is coming out of this device. And then it's being passed up to my remote SCADA system. And then lastly, let's see, the rest of these settings I can keep as default. And then I just need to, yep, just click through. I can keep these the same. Depending on my security parameters, you might need to mess with them on your particular device. But for myself right now, everything is good. So now I have the opportunity to add what type of data I'm going to be sending out of my 104 outstation. So here's the list of available options. So T just indicates the type. And then you really need to get familiar with the 104 documentation to know the number. T1 corresponds to single point, whereas maybe T13 would be floating point. We'll be working with both of those, as well as once we get into the controls portion, T45, single command. So I'll go ahead and start out by adding single points. And then additionally, I also want to make sure that I add in those T13 measured values as well. OK, so now basically what I've done is I've configured this device. I've assigned it an IP address. And I've let it know what types of data I'll be sending out of it. But now I need to map that data over. So since I already created my single points down here, now I can come down to, let's say, for example, my DNP master, find my binary inputs, the equivalent on a 61850 device. And I can begin to map my data from my DNP master 1 up to my 104 outstation. So we, of course, can basically just hold Shift and click, select all of my DNP points, and click and drag them up to my 104 master. Now, when I do this, it basically defaults to the first available point. So P1 is the first available point on my 104 master that's unused within the T1 type. So that's going to be the single point. And it just continues, increments from there. And it chooses the maps, or it chooses the point numbers or indexes on the outgoing side. Now, I could manually come in and overwrite this if I had a particular point that I wanted this to map to. But in this case, I'm going to let the tool do the work for me and just do the mappings for me. So I can just hit OK. I get affirmation that it has, in fact, worked. Now I could do the same thing for, for example, DNP master 2, come in here, take all of my binary inputs, and also map them up as well. Again, now you'll notice 1 through 25 were my initial 25 points. The next available index is going to be 26. So my DNP master session 2 T1 P0, which is my line 2 breaker position, is going to be mapped on the outgoing side, the 104 side, to P26 of the same type 1. Hit OK. And that is completed. All right. So I could go through the rest of these and show you what the mapping looks like. But I think that's a little bit dry. So I will point out that there's another way to do this as well. So I think, in general, clicking and dragging is pretty easy. And if you don't care what point your data is mapped to, if I'm fine with the fact that my DNP P0 is mapped to my 104 side P1, this is probably the quickest and easiest way to do it. But if I do have a pre-existing data map that defines the functionality of my gateway and how it should be working, I do have an alternative that I just want to make sure everyone is aware of. So it can be tedious to come in and manually update where these mappings are, kind of point A to point B, like I just showed a moment ago, to do it here within a tool. So that alternate method would be to import a mapping file. So this is going to look a lot like what we just saw before when I was importing and exporting points. Whereas importing mappings, instead of just creating the points on a particular device, they clarify where on the other device the point needs to be connected to. So again, if you have some sort of spreadsheet, let's say, that defines the functionality of your gateway, what data needs to be available at what point, then you can leverage something like Excel and maybe export some initial mappings, take the mappings that you have in that descriptor file, kind of merge them into Excel, and create the mapping for an entire gateway in there, and then just have it imported back up here. And again, that's particularly useful if you already have that documentation that defines how your gateway should work and what points need to be mapped where. So a couple more things that I do want to make sure I highlight. One of those would be, what happens when we have more complex mappings? Let's say that if I come back down here to my DNP device and I've got all of these different alarms that are coming in, let's say breaker one alarm for this breaker, and then I have another alarm for another breaker, so on and so forth. What if that's the data that's provided to me from the DNP side, but by the time I pass it up to my SCADA, I want to order those together, aggregate them, if you will, into a single point using something like a multiplexer? Well, that can be done within the gateway and a lot more using equations. So equations are kind of a complex point type that allow you to operate using logic. So in order to create one, just like everything else, start at the gateway root node, right click, and say Add Equation MDO. I'll just give this one a name, alarm combiner. How about that? And then over here in the MDO filter, I'm going to be looking for every point that's available in my gateway, finding it, and then adding it down to this expression builder down here at the bottom. So I mentioned P, or what was it, P4? So P4 was the alarm on DNP master 1, breaker 1. So I'll just go ahead and collect that. And then what I want to do is OR it together with another point. And if either one of those is true, then I want my combined alarm to be true as well. So I've got a lot of different logic operators that I can do down here. I can do some pretty complex things. This could go anywhere from basic ORing like I'm doing today, or some fairly sophisticated control. But for right now, I'll just stick with an OR, which would be this operator, a bitwise OR. And then I'll OR it together with the same equivalent alarm in bay 2, which is going to be DNP master 2, E1, P4. No, I'm sorry. It looks like I already added that one. I'll OR it together with master 1. So the same point on two different devices from two different bays. If either one of those alarms becomes true, then the alarm will become active. So I always want to make sure I validate my expression, just make sure that I didn't have any typos. And sure enough, everything's good. And I can hit OK. Let's say I might want to have another. Just to give you another example of the type of equation you could do, maybe I want to do some scaling. I want to convert my kilowatts, for example, that's coming in to, I believe this is also P4, but in the type 30, which, of course, is my analog inputs. So this is going to be my kilowatts value that's passing through a specific breaker. I could just take that, divide it by 1,000. And that'll let me know how many megawatts I have passing through that particular breaker. So I can just take that, divide it by 1,000, passing through this particular device, as opposed to the kilowatts that I'm getting. Again, just a basic example, but just to give you an idea of what's possible using equations. Validate my equation again, make sure that everything's good. And sure enough, it is. And I can just hit OK. Now, if I do want to have somewhere to keep these, you'll notice that my equations are in this root node. I can create a folder and keep those equations there. And that's what I typically like to do. So just add a user-defined folder, call it Equations, hit OK. And now this will appear in the tree. And I can take these points and drag them over to my Equations folder. And it just makes it a little bit easier to keep track of them. Now, if I want to take one of these and map it up to my 104 outstation, for example, I easily can. So that alarm combiner that we talked about before, just take it and drag it to T1. Available point's going to be 51, so just hit OK. And we are good to go there. So now I've finished configuring all of my data being mapped. I've taken some points and combined them into a single point. The last thing I want to do is make sure that I have controls mapped correctly from my 104 master down to my 61850 device. So this is kind of similar to how we added that report control block at the beginning. I'm just going to right-click and add a control point set. So this is where I'm going to be putting all of my 61850 controls on this particular device. Again, they already exist on the device. I'm just creating almost a folder, if you will, to keep track of the ones that I'm interested in. So I right-click on that Controls folder. And now I can see everything that's controllable on this particular device. What I'm particularly interested in working with today would be the QA1 CSWY1 position. So this is going to be operated with Select Before Operate with Enhanced Security. So I'll just hit OK and add that there. So now I've brought the control into the gateway. I need to map it to the equivalent place on my 104 side in order to make sure that it's available from SCADA. To do that, I just need to right-click and add the appropriate data type. I alluded to it earlier, but T45, our single command, will pair nicely with this Select Before Operate position value. Just hit OK. And then now I come back down to my control and click and drag that point up there. You have different options available. In this particular case, the default is going to be good enough for me. So I can just go ahead and hit OK. And it will map that control for me. I now can operate this breaker from 104 using that single point command. So lastly, special views, just a couple of things I want to point out. I have not created any alarms or warnings. But if I did have a device disconnected or something like that, warnings would be available here in the special views. Typically, the way I think about it is health and performance are always there, always static. Now, if any of these values are in error, then I will see something in the warning. So for example, let's just say channel online status or maybe offline counter. Right now, I have zero devices offline. If this was one, for example, indicating that I did have a device online, it would be available in my warnings. Now, I also might want to map this value up to my 104 side. Just give me some diagnostic information to let the SCADA side know how the gateway is performing. Easy to do, I just take that counter and drag it over to, for example, the measured values. So with that, last thing I do want to point out would be how do we log things that are happening in our gateway. A couple of different ways that this can be done. One example would be using our sequence of events logs. So there's a global setting. If I right click at the top root level node and I edit the workspace parameters, using this, I can edit the global sequence of events log mask. So here, we see different categories on the left side. If I hover over a particular mask, then I can see the data that's available or really the triggers that are available within that category that will trigger something to be saved within the sequence of events log. So like a base one, for example, if I come down here to diagnostic, I could see the data is being updated because it was requested, but the request was unknown, for example. But the reason is unknown. That might be a valid reason for me to want to record that data in my database. Let's say I didn't want to do it at a global level and I was really only interested in capturing individual points and their changes in my sequence of events log. I do have that option for that granularity as well. So coming back down to, for example, this DEP point, let's say that I wanted to trigger a recording every time that the breaker position changed. I could come over here, click my pencil to edit this particular point, click on the pencil again to get to the options, and we'll see that I actually already have that setting turned on here in the log mask. Update change indicated. The source of the data indicates that this update was a change. That, of course, will be recorded in the sequence of events log, and I can then view that later to know, give me an idea of what happened and when in my gateway. So finally, before I turn this back over to Beth, I do just want to give us a chance to see what this would look like in operation. Basically, I can come over here. I am using DTM or distributed test manager to simulate all the devices that I have over here in my gateway tree. So I'm just going to go ahead and toggle one of those points. So let's just say, for example, I'll just change the first point in my DMP master one. Let's just change this from off to on. So when I do that, we're going to see that value be passed through the value change from off to on up here in my gateway. That, of course, will be passed up to the 104 side where I can see it reflected here, the values being sent. Additionally, because I have my equation, my alarm combiner, I guess that would be a different point. You need to do that 4.4. Then my alarm combiner will come true as well in just a moment once that equation evaluates. So with that, I'm going to go ahead and turn it back over to Beth. I hope this has been informative. I hope you guys have a better idea of how our gateway can be quickly and easily configured and some of the advanced tips and tricks that will make that easier. Thank you so much. Okay, well, thanks, Jackson. I'm going to go ahead and just take the screen and wrap up with a couple of final thoughts real quick. Okay. So, again, thanks, everyone, for attending our webinar. Thank you for your time today. I do just want to point out, if you haven't already signed up, please go to our website and sign up for updates on our upcoming webinars. Any future trainings that we're hosting, any product releases, it's right there on our homepage. When you sign off of this webinar today, you will be presented with a survey. It only takes two minutes. We really appreciate your feedback. This is also where, if you need a certificate for a PDH hour, the professional development hours, you can check off yes on that, and we will email you a certificate. The recording for this webinar will be posted on our homepage as well, probably by the end of this week, if not early next week. Should also receive a follow-up email directly from GoToWebinar if you fill out the survey with a link to the recording. So, please, if you'd like to revisit it for just a review of what we went over here today, that's your best way to do that. And then, of course, any questions that you have or any comments, you can always email us at sales at trianglemicroworks.com. So, with that, we do have a couple of questions that have come in while Jackson was presented, so I'll just kind of take those from the top. And I believe this first one is going to be from Matt. How do you organize individual data points from, for example, DMP3 and Modbus to form data objects, logical nodes required by IEC 61850? So, you can provide a SCL file, such as an ICD file, SCD file, as we saw today during the demonstration, to give the model that 61850 supports to our SCADA data gateway. From there, it's just like the mapping that Jackson did today. You would just grab points from things like your databases, such as your DMP3 or your Modbus, and add that over to the model and link that to a data attribute or data object. I think we even saw a little bit of this when we were talking about controls. So, that's how you would normally integrate between the two different kind of models of the protocols, one being a model-based 61850 or CASI-2 and the other being DMP3, Modbus, 101, 104, et cetera. Okay. The next question, how do you access attributes of a data point? I think I'm going to give that one to you, Matt. Sorry, I was answering another question. How do you access the data attributes of a data point? So, all you really need to do, and we've seen this a few times, you just go into the GUI for our SCADA data gateway. You go to the device that you're interested in, so that I think we saw today that you could add, say, a DMP3 master. From there, you just create the data type folder and then add the data attributes, or you could do auto-create tags. However, get that data into the SCADA data gateway, and then you can access the data right then and there. Just make sure you set up ActionMax to keep that information live, so you can do polling every second or unsolicited messages, enable reports, whatever, to keep that information live and accurate, and then you have access to it to use that in equations or mappings or anything along those lines. Okay. I have another question I guess I'll just put out to the group. You now have the IEC 61850 data model that is built based on the data sources we have, but all these data sources are dynamically selected. Is there any way to provide the SCL file? I'm not sure that I 100% understand the question. So it is true that when I was doing the 61850 configuration of the 61850 client, I had two options. I could load my client based on a discovery, so if I had the 61850 server online and on the network and configured, I could discover the data model for that device. The other option would be to provide the SCL file, which describes that device, could be a CID, in some cases even an ICD, definitely a CID, and load that into the gateway, and that gives me the data model for the device as well as what reports are available, what data sets are in those reports, what capabilities the device has, so on and so forth. So from that perspective, yes, we are able to use the SCL file. I'm not certain that that answers the question. I just don't know if I understood it well. Okay, Matt, did you want to add anything? I know you answered it online. No, I think that's fine. Okay. So next question, on what hardware and operating system does the gateway run? Gateway runs on both Windows as well as Linux. Hardware, we do provide a substation-hardened PC, which the gateway can run on, Linux variety, but you can also bring your own as well, and that is commonly done. It's really just up to you. We've seen people run these in the cloud on a totally cloud-based machine or something as small as a Raspberry Pi. Okay. Well, I think that does conclude our question-and-answer session. Again, I'd like to thank everyone for their time today, and please be on the lookout for the recording to this and for our next webinar. Thanks, everyone.


Video ID: Zlp2BosQJQ0
Let's look in more detail at how DNP messages are formatted. The International Organization for Standardization, or ISO, has defined a communication architecture called the Open System Interconnection, or OSI, reference model. This model separates the functionality into seven separate layers. It is customary to reference the OSI seven-layer model in protocol discussions, although very few protocols implement the entire seven-layer stack. DNP3 is based upon a simplified model of the OSI reference model called the Enhanced Performance Architecture, or EPA. This model consists of only three layers – application, data link, and physical. DNP3 also adds a transport function. The transport function provides some, but not all, functionality of a full layer. In theory, each layer of the stack performs a set of functions to communicate with the same layer in another device, using the next lower layer for more primitive functions. Each layer should be independent of the layers above and below it. Each layer in the stack communicates with the same layer in another device, relying on lower layers to provide more primitive functions. When sending, each layer receives data from the layer above it. Each layer adds more information in order to allow the equivalent layer in the receiving device to properly process the message. When receiving, each layer examines the data for the specific information for that layer and processes the message appropriately. That layer's information is then removed from the message, and the message is passed up the stack to the next higher layer for further processing. The net effect is layer-to-layer communications. DNP's transport function performs the layer-like functionality of breaking large messages into smaller messages that can be handled by the link layer. On the receiving side, the transport function reassembles these messages into the application layer messages. A DNP message can be virtually any size. A large message will be broken up into application layer fragments. An application layer fragment is typically 2,048 bytes, although it can be smaller or larger. The DNP3 specification states that the application layer fragment size must be configurable to a size no larger than 2,048 bytes. A device may also allow smaller sizes to be configured. Each fragment is broken into smaller frames. The transport function header is prepended to each frame, and a data link layer header is added. CRC bytes are included after the data link header, after each 16 bytes of the message, and at the end of each frame.


Video ID: mX4oxHfOBp8
Good morning. My name is Beth Capellas, and I'm the Marketing Manager here at Triangle Microworks. I'd like to welcome everyone to our latest webinar, A Holistic Approach to Facilitate the IEC 61850 Engineering Process. This is a two-part webinar series. On May 18th, we hosted Part 1, which covered 61850 Specification and Design. If you missed it, please go to our website at trianglemicroworks.com to view the recorded webinar. You can easily find it on our homepage. Today we'll be discussing Part 2, which is 61850 Simulation and Testing. Before we get started, I'd like to go over a couple of housekeeping items. The webinar is scheduled to be one hour long. However, we typically get a large amount of questions, so we are happy to stay on longer to answer as many questions as possible. With that said, I'd like to encourage questions throughout the entire webinar. Since the audience is on mute, please submit your questions using the webinar menu under the section titled Questions. You can start asking questions as soon as you have them. Don't feel like you have to wait until the end. We will begin answering questions online, as we have several engineers monitoring the questions and responding to them directly. We will also stop a couple of times throughout the webinar to read and answer questions. And then, of course, there will be some time at the end dedicated just to questions. Next, I'd like to introduce our speakers. First, we have Matt Green from Triangle Microworks. Matt is a Senior Applications Engineer here at Triangle. He's been with us for about six years and specializes in QA testing with all our simulation tools in Gateway. He is one of our in-house experts on Distributed Test Manager, which is one of the tools you'll be seeing here today. We also have with us Jim Coates, the President of Triangle Microworks. Jim started Triangle in 1994 to provide communication protocol software to the utility industry. He continues as President with a primary role of product research and design. Jim is currently Chief Financial Officer of the DMP3 User Group and participates in the IEC Technical Committee 57, Working Group 10. With that, I'd like to go ahead and hand it over to Matt to get started. Thanks, Beth. So, just a little background about Triangle Microworks. We were established in Raleigh, North Carolina in 1994. We have customers all around the world, including equipment manufacturers, system integrators, and electric utilities. Many of our employees sit on the standards committees. These committee members help us keep us well-versed in the protocols that we support. We have several categories of products. We have source code libraries for DMP3, 61850, 6870, 101, 102, 103, and 104, TASI-2 or ICCP, and Modbus. We provide technical support not only during the initial implementation of our source code libraries, but we also support any of the communication issues that may arise with your device out in the field. You may have also heard of our SCADA Data Gateway, which is our gateway application for protocol translation and concentration. We also have a collection of test tools, like the 61850 Test Suite Pro, Distributive Test Manager, Test Harness, and Iron. We will be showing off DTM, which is Distributive Test Manager, and Test Suite Pro today. On May 18th, we held the first part of this webinar series. Today, we'll be taking the SCD file that we created in Part 1 and simulating the substation in DTM. Then, we'll verify the behavior and create tests using Test Suite Pro. On July 30th, our next webinar will be about security using Garibaldi. This is the diagram explaining the IEC 61850 engineering process from start to finish. To catch anyone up who missed our first webinar, we created an ICD file or ICD files inside of SEL Navigator. Then we used these files to build up an SCD file inside of Helix STS. And this is the part that we'll be going over today. In the conceptual phase, we will be simulating the entire substation in DTM. Then we can build up test cases and regression tests in Test Suite Pro. Once we're happy with our design, we can move on to the factory acceptance phase. In this phase, we're going to disable devices in DTM and provide those IEDs in the lab. Then we can rerun our test cases from a previous step in Test Suite Pro. In the final phase, the commissioning and maintenance phase, DTM is no longer simulating anything. Test Suite Pro is then brought into the substation and verifies that everything is working properly out in the field using the same tests we designed in the conceptual phase. Another useful application for DTM and TSP is the training of employees. Sometimes you need to be able to train employees in situations where mistakes are acceptable. A simulated environment is a perfect place for this. You can also use this simulated environment to test new equipment before deploying it into a substation. In the last webinar, we designed a high-voltage side of a substation. It has two transformer bays, two line bays, and a bus bar with differential protection. This is the network topology diagram for the system that we designed last time. We have a measurement bay with a bus bar protection and merging unit. We've got our HMI, a test device, and then we have four bays. Each bay has a bay controller, a transformer or line protection, a switch controller, and a merging unit. DTM is going to be simulating everything in red, and TSP will be monitoring the substation's behavior. We'll be doing this by importing the SCD file that we created last time. When we import the SCD file, DTM will create a simulation of these functions. Today we'll be focusing on the recloser function of bay line one. These are the key functions that interact with the recloser function. The logical nodes corresponding to each function are in red next to them. So I'm going to go ahead and start up DTM and get the SCD file imported. So this is DTM. This is what you're going to see when you first launch DTM. Now if you have Helinks STS installed, there's actually a button that you can click that says import to DTM that'll jump you all the way to the import section. But if you don't have Helinks, I'm going to show you the steps to get there. So you go to file, import substation wizard. From here, you need to select your SCD file. This pop-up is basically letting you know that DTM can now monitor this file. So if you have someone making changes and updating that file, instead of needing to reconfigure your entire substation, DTM can automatically pull in those changes and update the devices based on them. So I'll hit yes, and then I'll hit OK. This is our import substation wizard. Essentially, we have some options up here for what the automatic system is going to create for you. So I'm just going to leave everything checked, and I'm going to import all of the IEDs. So I'm going to click import substation and let that run. Now let's jump back over to the PowerPoint for a second and talk about what DTM is doing. So DTM is currently parsing through the SCD file exported by Helinks STS. It'll parse through three different sections of the SCD file. The substation section is the functional part, including the topology information for the one-line diagram. DTM will be reading this in order to figure out how the electrical equipment should behave together. The IED section contains the data models and the capabilities of all of the IEDs. This is where DTM will gather the information needed to simulate the 6150 devices. The communication section contains the configuration of the communication network, such as IP addresses, which DTM can then use to set up the TCP IP connections for each device. In order to properly simulate a substation, we need to know how these devices interact with each other in the real world. Logical nodes are the functional elements, which are the building blocks for application functions, such as distance protection. Logical nodes communicate with other logical nodes by internal connections or with goose messages. These types of communications are used to implement application functions using multiple logical nodes. To interact with the electrical equipment, logical nodes are wired with physical inputs and outputs. So to simulate this in DTM, we'll need to simulate not only the logical node behavior, but also the internal and goose communications, and the behavior of the equipment, including the physical inputs and outputs. We also created a user interface to control the simulation's test conditions, such as fault or breaker failure conditions. To complete this simulation, we'll need a topology and process simulation for high-level power flow. We did all this by creating a library of logical node and equipment simulations. This library is coded in JavaScript or IEC 61131 PLC programming language. The default behavior will work in most cases, but if you need something specific, you can always customize these libraries. So now let's talk briefly about logical node communication. This is a simplified version of the recloser function that we'll be displaying today. Each of the yellow boxes are IEDs, and the lines completely contained within the boxes are the internal signals, while the lines between the boxes are the goose messages. The arrow indicates which direction the signal travels from logical node to logical node. Later on, Jim will be showing how the signal flow display in Test Suite Pro is used to monitor this communication. So now we're back to DTM. We've gone ahead and imported the substation, so I'm going to close the wizard. The very first thing I'm going to do in this is set up the initial conditions for the simulation. That includes the voltages and the currents and the loads. So I'm going to set those up here in this power. So I'm going to click here and add a voltage, move down here, and I'll add a load. This is another applied volts. So after I set all this up, I can go over to our single line diagram, which you can see here. So let's talk about what was actually all imported in the import substation wizard, and that's all in the workspace. So I'll go ahead and open that up. The first thing you'll see is a JavaScript. This is how we do all of our power simulation. The second thing that you'll see is the substation, and this substation is a representation of the substation section in the SED file. So at D1, you can see a voltage level, and then down here, we have all of the bays. We've got line 1, line 2, and the transformers. One more line down, and we have the conducting equipment, such as the VTs and CTs that you can see here in the lines. And if you go one more step further down, you'll see your logical nodes. The next thing we have is the single line diagram, which we're already looking at, and the last thing you'll see is a folder of all the 61850 devices. So what we're going to do now is I'm going to go ahead and right-click here and start the simulation up, and then I'm going to go to the PowerSim and start this script. And what we should see now is we'll see that the VTs are now seeing the voltage on the line, and our CTs are seeing the current that's running through each line. So if I open a breaker on a line, you'll see that current has now gone to zero, but you'll see that the other legs still have their current on them. But if we open both inputs, you'll see all of the currents are going to zero, and the bus bar is no longer energized. So from here, we want to start with our recloser and add in some protection schemes, and that's what these buttons over here are on your right. DTM will automatically go through the substation section and the SCD file and automatically map the inputs and outputs of these internal and goose messages, as we described before. But there's always going to be a few things that you may need to go in and customize yourself, so I'll show you how you can do that. You open the substation section, and you navigate down to the section that may be not configured properly. And you can do that by looking at the indicators next to the node. So a green indicator means that everything that is required inside of that node is configured properly, whereas orange and red may mean you need to take a look at it. So the first thing we're going to do is we're going to take a look at the differential protection. So you can right-click here, manage, and then you do manage substation connections. From here, this is our substation connections configuration window. What you'll see up at the top is essentially your filtering options. Do you want to look at the entire substation, the local bay, or just the local equipment? And then from there, you can choose the node that you're interested in. On the left-hand side, this is the sources. And then on the right-hand side, this is the inputs to the node that you're currently at. So I'll choose the local bay, and I'm going to look for the breaker position. Because what you'll see here is that out of all of the required, only one of them is required, and that's the breaker position. I'll go to local bay, and then I'll look for the XCDR, which holds the position of the breaker. And I'll just grab this and drag it over. What we should see now is when I hit OK, this will change from orange to green, and sure enough, it did. The next thing we'll do is we'll wire up the PTRC to the differential protection. So I'll right-click here, manage, and then manage substation connections. From here, I want to find the differential protection that we just set up. So it's still in the local bay, and then I am looking for PDIF1. And what we need here is we need to start and operate commands. So I'll drag these over and hit OK. So now that's green as well. So what we should see here is now I can click this button, and that will put a fault on the line. And then the differential protection should pop in and open this breaker here, QA1. And sure enough, it did, and all the currents went to zero. So we did want the recloser function is what we were mainly wanting to show off. So it didn't reclose, and that's because the recloser isn't now set up. So we can go into this breaker and find the recloser logical node. Right-click this, manage substation connections. And we want to wire this to the trip function of the PTRC. So again, we'll go to local bay, find the PTRC, here's the trip. And you'll see that we want to drag this to one of the initiates. I'll hit OK. So I'm going to reset our simulation now. So we'll see that all of the currents are now flowing, the voltage is applied. Now if I apply the fault on the line, the breaker will open because of the distance protection, and then reclose due to the recloser, open, and then close. And then the last thing I'm going to show off here is the checkbox right next to these faults will determine whether the fault is permanent or not. So if I click this box here, this means that the fault will be permanent. So when I click on the button, the breaker will open, it'll reclose, but it'll reclose on the fault, so it'll open back up. So this is the general explanation of what we're showing off in DTM right now for the recloser. We're going to pause here for a little while to take a few questions, and then we'll move over to TestSuite Pro to show off exactly what's going on behind the scenes of the simulation. Okay, we do have a couple of questions that have come in, so let me just kind of start from the top. When testing or simulating the IED, what is considered good practice, use of a simulator application or simulate these functions, but simulating the logical nodes on an IED? So when we do the behavioral simulation, that's when we use the substation section, and that's where you're going to see kind of the behavior of each of the logical nodes down here, and that's all being run by the library, as I was describing previously. But if you look down in the IED section, this is where you're going to see the 6150 devices that contains the models and the data attributes, the report control blocks, the goose messages, and all of this is being simulated as well. So I think DTM does both, and they're both necessary if you want to go through and simulate your substation as we're presenting today. Okay, next question. Linking the inputs and outputs of LN inside the simulated IED. So this is the IED process logic. Does this replace the scripting? No. So this is basically setting up the inputs and outputs to the scripting. Now like I mentioned earlier, we have that library of logical nodes and equipment scripts, and essentially what we're doing is we're setting up the values that they're monitoring based on these inputs and outputs. So in order for the recloser to know when to initiate its functionality, we need to tie that to the trip signal, but the scripts are all going on behind the scenes. And this really might just be a continuation of it. It's more of a general question on JavaScripting, but what is done in the JavaScript section? So we've imported JavaScript into DTM, and you can configure that to pretty much do whatever you want in DTM. It's a full JavaScript engine, so anything you can do with JavaScript outside of DTM you can do. And then we added all sorts of hooks to interface with different protocols, different devices, data types, functions, all that type of stuff. So you can pretty much do whatever you want with the JavaScript behind the scenes and set those up yourself. So if I right click here, you can just go down to inside and add a script and make it do whatever you'd like. But what we've done is we've implemented a bunch of commonly used features among the industry and put them behind what you're seeing here in this hierarchy. And that's the stuff that you can go in and edit. So if you right click here, you would be able to edit the simulation node and actually make edits to what our basic behavior is. But the hope is that we're hitting most of the normal use cases for these things, and you'll have to make small tweaks if you have a specific need for your substation. Okay, next question. Is functional naming possible in DTM? In parentheses, he has LD.name defined in SEL, IED name doesn't appear in 620.50 string. Do we support that, Jim? I'm sorry. Yes, we do. Okay. So, yes, I wouldn't, I can't show you that right here live on air, but apparently we do support that. Yes. Thank you, Joel. Okay. One of our behind the scenes engineers helping with questions. That's about it for the questions right now. If you want to move on to the next section, Matt. Okay, so we're going to continue to run this simulation in the background. I'm going to move the simulation off screen so that way there's more real estate for you to be able to see what's happening in Test Suite Pro. But Jim is going to probably still be interacting with it to cause different test conditions in the simulation while he's showing that all. So let me go ahead and set things back to the initial condition. And I should be able to move this over here. And I'll hand it off to Jim. Okay, so this is Test Suite Pro, and we configured it by reading the SCE file that Matt used to create the simulation in DPM. And so we loaded that SCE file and created our own models and mapping of everything that mirrors what's going on in DPM. We could have also done that by scanning a range of IP addresses and doing a discovery of each ID that we found. So on the left side over here, we have tools like Data Miner that's used to find the data. And then on the right side, we have Display that are used to display the values of those data things that we found over there. The displays that we'll look at today to analyze the simulation that Matt created are the Signal Flow that we're looking at now, the Logic Analyzer, the Report Viewer, the Goose Viewer, and the Test Sequencer. Then after we take another break for questions, we'll come back, we'll take a quick look at the custom display and some of the other tools over here. So on the Signal Flow diagram, the IEDs are these big boxes, and then within that you have logical devices, and within those logical devices, you have logical nodes. The connections that go between IEDs are goose messages, and within the IEDs are internal messages that we can get through reports. So if we start a capture here, I can then go over to DTM and cause the differential protection fault, and you'll see these lines. The yellow lines are the goose signals being transmitted. The other gray lines are the internal ones. The black ones didn't change. Down here on the timeline, you see the events that occurred, and so if we stop recording, we can go back and play through those events slowly and verify that they occurred in the order that we expected. So there's the differential fault that I caused. That causes the PTRC, which causes the breaker to open, and you see the signals going back false. Now the recloser kicks in, which is then going to open the breaker again. Then we see those signals going back false again. And so if we hover on these signals here, we'll see kind of a little event history of changes on that data attribute, and if we select these, if you select one of these signals, you'll see that the timeline only shows those signals, only the events for the signals that you've selected. So we can select multiple signals down here, and you'll just see the events for those signals. We can also right-click and send these selected signals to another display, such as a logic analyzer. We can use the logic analyzer to evaluate the timing on these signals. If we go over that, we can use the – we can drag around to find a different portion of the signals that we're interested in, and then we can use the scroll wheel on the mouse to zoom in and see more detail. So the top signal here is the differential protection use message that we saw come in. Then that triggered the PTRC. The XCDR then opened, then the recloser, then the CSWI to close the switchback. We've got these cursors on here that we can use that will show you the value of the data item where they are. So like right here, you see on the auto recloser status, you see the value shown right here. It went into trip protection, and then if we keep sliding over, you'll see the different states that it progressed through in progress. And then over here, if I can get far enough over there, you should see it go back to ready back here. And what this is doing, it's finding the nearest event and highlighting the value at that event that we received. So the times that we're seeing on this axis are all from the goose or report messages that we received over 621850. So the timing is not based on the PC. It's based on the accuracy in the device that we received the 621850 communications from. So the other thing we have here is a goose viewer to look at those goose messages in more detail. And right now we're grouped by the ID, and within the ID we see the different goose streams that are available in that ID, and then these are the messages that we received in that timeframe. And I can slide this divider over here to give it a little bit more room. And if I select these messages, you'll see the values over here. These are the data attributes, and each of these columns is a value over time that we received in a different message. So for example, on the recloser, we saw that in this message it was originally false, here it became true, and then it went back to false. So you can scroll this window over to see as many, the values from as many goose messages as you like. The report viewer works pretty much the same way. You've got your ID, the report within it, and then the different messages there. Okay, so if we go back to the signal flow, this is the sequence of signals that we saw for our recloser event, and we used this to visually inspect what was going on with the recloser function. If we want to automate that testing, we'd go over to the test sequencer, and we can see all the steps here that we can generate graphically. And then here's more of the details on exactly what we're doing, such as timeouts and triggers and so forth. And then here will be an execution log. So to create one of these, we have a couple of different options. We can just create a new one. And then come back over here to data miner, grab the wrong slide. There we go, and if I just grab any random data, I don't care what it is, we'll get a dialog box that shows you the functions that we can do. We can verify that that data attribute has a specific value. We can write to a configuration option to change the configuration value. We can do control operations. We can also get a dialog box for a user action. We can do 61850 actions, such as connecting or disconnecting to a device, and then we can also interact with external test equipment, such as a mega, to put, say, the simulation voltages that we've created on our simulation network, we can put those out to the mega so the device under test that we're interfacing with receives that voltage. So with this, we can create test sequences that could be used later in the process. We can develop them on the simulation and then use them at, say, factory acceptance test or the site acceptance testing, and then those will be automated so that we don't have to step through them manually. So that's how you create a test manually. We can also take this sequence here that we captured in the signal flow diagram and send that to the test sequencer, and that generated steps here in the test sequencer for each of the signals, each of the events that we saw in the signal flow. So here you see the differential protection going through, you see the PPRC going through, you see the breaker opening, you see the differential protection going back false, and so you follow each of these events in sequence and verify that they occur in that sequence. Some of these signals going false end up getting an erase condition, and so what we need to do there is merge those into one step that they could occur in any order and it won't cause an error when one comes in before when you're running the test. So again, this is a test step. If we had checked this box, validate order of events, it would verify that this one came before this one, but since we didn't, these could come in in any order. So if I go back over to DPM and, well, let me start the execution of the test first, and I go back over to DPM and cause a differential protection fault again, you'll see this guy running automatically through there, and this is what we found in our testing. This is why we do the automated testing. This PPRC didn't generate. We have an intermittent problem there, which is why you want to do the regression testing to keep running through things again and again. I bet if I run this test one more time that I could make that run successfully since it's an intermittent problem. So that time, all of the tests ran successfully. So we need to look into why this signal is periodically not acting correctly. That's what the value of these regression tests is when you're integrating the new equipment in, working with DPM. You disable that device in DPM, put the real equipment on the line, and then it can interact with all the equipment and see if you can pass all these regression tests again. Okay, back on the signal flow, if we're looking at different functions, then we drew this diagram for the reflows or function. If we wanted to have a different layout to look at, say, a breaker failure, we could look at that diagram and evaluate the breaker failure operation, see how we have the breakers from other bays in here. This is the breaker that we're looking at, so this is the differential protection that will generate the PPRC and then generate the breaker failure signals for the other bay. So depending on what we want to look at, we can have a whole slew of different signal flow diagrams here, and we can generate, we can save captures of those events by storing them in this snapshot buffer. So if we see something in the field that we want to show to an engineer when we get back to the office, we can save that and then play it back later. For example, we can select that one and then we can play through it. So one more thing that I want to cover before we get back to some questions is the way we find data. Typically, a 620-50 client is going to show data as a model view with a tree where you expand out different, oh, I'm going to cover this afterwards, I'm sorry. Why don't we stop now for questions, Beth? Okay, yep, we do have a bunch of questions coming in. Let me try to take them from the top. In test sequencer, do we verify time of event occurrence as it could be delayed, which may not be expected? Right now, we verify the order of the events that occurred. We could, yeah, you could drag a timestamp over and validate the timestamp with a certain value. Okay. Next question, and the signal flow, is it created automatically when you get the ICD? At this point, no, but I will cover how, like what you're developing here is different functions, right? So you're going to want to, like what I'll show later is if we drag over the CSWI, it will automatically pull over the subscribers to it like this XCDR and the inputs to it like this recloser. And then once you get that basis there, you can right-click and get a context menu here and see what other logical nodes subscribe to this message, or I'm sorry, that this device subscribes to, and then click here and look at the other subscribers to this signal. So it's fairly easy to expand out the functionality that you're looking for to test. What we'll probably do long-term is take the data that we came up with to create the simulation in DTM and then have different schemes that we can apply those to, so you can only draw like the recloser function here or the break or failure function here and just isolate. So you don't have this big diagram that shows all of the reduced connections in the entire substation, because that can get overwhelming. You want to isolate it down to a list of specific functions that you're focusing on testing today. Okay. Can TestSuite Pro be used to event fault record on a live system triggered by a Goose event, for example? I'm sorry, could you repeat that? Sure. Can TestSuite Pro be used to event-slash-fault record on a live system triggered by a Goose event, for example? Yeah, so TestSuite Pro in the commissioning and maintenance phase that we were talking about during the engineering process definitely can be taken into a live system and have these same tests run in the actual substation. The triggering options to start the signal flow in our other tests, I mean, obviously there's the record button, but for the test sequencer, when you run a test, you can put a variety of time starts or how long you wait for each step. So you can definitely set it up for TestSuite Pro to kind of autonomously sit there and monitor values and then be able to run the tests on it later once you trigger certain test conditions or if the system triggers out certain conditions as well. Okay. Does the Report Viewer and Goose Viewer provide all details like Wireshark MMS packets? So we've tried to filter down the information that most of our customers are most interested in with the Report Viewer and the Goose Viewer. This is for people who may not feel as confident digging into every single message that you'd see in Wireshark. So if you open up the Report Viewer, if you open up the Goose Viewer, you're going to see the individual Goose streams, you're going to see the reports that are coming in, the values inside of them, the timestamps, the qualities, all those important 61 and 50 bits. But you may not see absolutely everything that you might see in Wireshark, but that's because we're really trying to filter it down to the stuff that is most necessary for the on-site. Okay. This is kind of going back to DTM. Do you have a transitionary simulation on DTM? Say the... I'm not sure if we might need more clarifying information. Do you have a transitionary simulation on DTM? Christoph, you got any input, insight on what a transitionary... Transitory simulation on DTM. Can you repeat the question? I was answering other questions. Sorry. Do you have a transitory simulation on DTM? What is meant with transitory simulation? I mean, we have transient data objects in 61 and 50 that can be included in this simulation as anything else. I'm not sure what otherwise could be meant with transitory simulation. Yeah. Whatever question is being asked here, can you give us a little bit more qualifying information? No. I think what they're saying is that if you get a pulse, a short pulse signal where a signal just goes true for a very short period of time and then goes back off, yes, we would catch that because that would be a goo signal. And we would display that in all of the logs with various tools. But if we're not answering the right question, feel free to clarify it and we'll either tackle it in the next question section or definitely by the end of the webinar when we're going over just questions. Let's take one more before we get back into the presentation. Can test sequencer be programmed to run repeatedly and output results to a report? It does have this execution log that we can output. If you have a test plan here that has a group of it has a test group within it. Right. You can you can add test and test groups and test plans and then you could run a test plan that ran a whole series of tests within it. And then the output from that you can export as well. See export results. And you can add as many or as few tests to that test plan as you want. So for the respects of can you continuously run it? You could just, you know, re add the same type of test, you know, to the point where you've got 10, 15, 20 of them. And that way you can vary the exported results to make sure, you know, over time, is your test consistent? Are you getting the same results each time or something buried? Yeah, I think what we might want to do is add a configuration on this guy where you can put a counter. So if they were doing like burning testing, you know, in a heat chamber, you could execute this test under time or something so they could run it overnight. The test would keep running and they'd give you the results in the morning. I would say with that, let's go ahead and put, you know, continue to ask your questions online and we'll continue to answer them online for to just stay on time. If you guys want to get back into the demo. You bet. All right. So, like I was saying before, most clients display the 62050 object model as a hierarchy. And so and also there's a lot of data in here that you don't really care about when you're just looking at the process data for the. When you're debugging things like this, for example, the descriptions and. Configuration values for various parameters. So on the on our advanced client, we will show the value here and how it was read. Some of these, especially configuration parameters, will only be available from the from the SCL file. Other ones we get by goose and other ones we get by report. And then if we don't have it in a goose or report data set, then we can just read it and that would be pulled. But what we've done in order to find the data easier, we've created a tool called data minor. It filters out all of the. It only leaves the process data so that you see the data that you're interested in. So each row is a data object and the columns are properties about that object. And so we can we can filter out stuff that we're not using. For example, we have a property here called used in a goose or report control block. And if this column is checked, I can filter based on that to only see data that was actually used in this substation design, because there may be a lot of data attributes in the SCL file or SCD file that was used to create the substation that were not actually used for that substation design. We don't have to weed through those to find the data that we're looking for. So so this filter was created to find the signals within the that I might use for a signal flow diagram. And so I group I can take a column here and bring it up and group by a bay. And then by the logical device name. And then by the logical node name. And so that's what I've done here. And so these are data objects within that logical node. And then so these are the properties this one has. So if I want to see what the external references that this data object has, then I can navigate to them right here. So, for example, I could I could navigate to the reclosure. And it'll find that for you. So you can see the inputs and how you can navigate to them. And it'll find that for you so you can see the inputs and how data objects are related to each other very quickly in here. The same thing for subscribing logical nodes. You can you can look at who subscribes to this guy. You can put that in a column as well if you like. So once you find a layout that you like, you can say that as a preset up here, for example, this is a preset that I created for drawing one line diagrams. It's going to have grouped by bay and then by logical device. These are all the devices within the each bay of the substation. So it's real quick to find the things you need to draw the one line diagram in the substation. So if we before we had a question about creating a new signal flow diagram. So if we. Create a new one. And we grab, say, this. That up close when and when we bring it over, it's going to bring over. We brought over the CSWI, it brought over the XCVR as well, and the inputs to the CSWI, the recloser and sync that then we can go here and add an input to this guy. So you all the inputs that we have. And so I can add the other one. I can add the other one. We can look at it. If he subscribes to it. And say, add the breaker failure. The. Close signal. We can also look at the output. We can look at it. If he subscribes to it. And say, add the breaker failure. Get back into the. It's part of this ID, so it's a different logical device within this ID, so it draws in the same box. And so you see that output gets drawn and it adds the logical nodes that are required to do that. So you can quickly expand on the drawing that you have in signal flow and look at what other signals are available. Like I could look at this guy and add other inputs to him. Looks like we've already got them all. So you can kind of explore and see how you want to expand this signal flow diagram. So if we look over at the custom display, that's going to mirror the one line diagram that Matt had automatically created in. DTM. If we zoom in on this a little bit, you can see it clear. So here's line one. This is transformer Bay one. This is the breaker. We can we can open and close. And switches. And data objects in here. Like if we go to the measuring. We can see that we have a lot of data objects in here. So another valuable tool to see what's going on in substations is the use tracker. So on the end of the chart, if we look at the tool, the second thing I'm going to do a quick outline, it is actually from the front of the chart, we see something called the consumption table, this represents all the assets are collected and dispersed and let's zoom into the measure and we we draw things on this diagram, say I want to put the phase voltages over here. See how that draws three data attributes at once instead of having to pull those individual data attributes over. So another valuable tool to see what's going on substations is the use tracker. So what you see on this chart are the view streams that are available in the substation. And the columns are properties of those view streams. And so if we I'm not going to try, we're getting short on time. So if you look right here, for example, the SCL wire match column, if you want to see what that is, you hover over the column heading, and it'll give you a tool tip that explains what it is. And it will also explain what errors it can cause in this status column right here. So the status column tool header also shows what columns will generate will change the status value here. And you can verify the status column here. So it brings the important errors to the top. So you can see anything that's wrong in your substation very quickly and verify that signals that maybe a device is not receiving or something are there and that they're not having problems. Notice one of the signals here has a warning on it, and it says that the simulate bit is set. The reason that's set is because in our goose publisher here, we're publishing that simulated signal. If I want to change, so this contains a list of all the view streams available in the substation that we could simulate, and you expand it out to see the data values here. If you want to change one of the data values that's on the simulated stream, you can just change it like that and then publish the new goose message containing that data. It will then continue to be retransmitted. If you wanted to publish a real stream instead of a simulated stream, you could change these checkboxes to publish a real stream in case you needed a signal from a device that you didn't have presently, and you didn't want to simulate that whole device in DTM. Before I wrap it up and give it back to Matt to go over a few more things in DTM, I just want to mention that we can also do a compare model. We can go out and do a discovery of a device on the network and verify that that matches the SCL file that we have loaded in TestSuite Pro. That saves a lot of problems when you've got version problems. You're wondering if the right data is in that IED. The other thing that can save some time is SCL verify. A lot of problems in 620.50 are caused by errors in the SCL file, and this will check the syntax and make sure everything is synced up in the SCL file and that you're not getting invalid data in. So with that, back to you, Matt. Thank you. So one question that I'm sure we're hearing a little bit of is we talked about the SCD file, the substation section, the IED section, all that stuff. And some people might be saying, well, what if I don't have that fully configured? Can I still run tests in the simulation with DTM? And the answer is yes. So I'm going to ignore the substation section as of now, and I'm going to show you other ways that you can change data in DTM. So I'm going to go straight to the 620.50 devices in the IED folder. And what you can do from here is you can right click and do simulate, and these gives you your different ways that you can simulate data. The first one I'm going to say is just auto data changes, and I'll just show you what that is. What that's going to do, since I did it at the folder level, it's going to look in that folder and find all the devices. And that's 1960.50 devices. And it says, do you want to create the default data points for all 19 devices? That's going to be anything in a data set. So I'm going to click yes. It's going to go ahead and populate all the points. And what that's going to do is it's 119 or so points in this substation that is a data set. And what it's going to do is it's going to change these variables, these data attributes. And essentially, it's going to change it in a way where, like, the user doesn't really care. So it's going to toggle binaries on and off. It's going to increment integers and analog values up until a certain point. It's going to go through the different states of double bits. And what you're just trying to do here is you're trying to generate traffic. And you can do that on an individual device level or at the whole substation level. This allows you to verify that goose messages are being sent at the right intervals, that reports are being reported, that your devices are basically functioning at a 6150 compliant level. You can change this with the sample interval. So right now, it's going to change all this data every five seconds. But you could come in and change it, say, down to every two seconds or so. That's normally what I do. Of course, you can open these things up and add and remove values as you want. You can drag new things in from the model into here or drag things out. You can insert all the indexes, all the STs, all the data sets like you did before. Once I hit OK here, that's going to go ahead and put a hammer at the folder level. But it's also going to put a reference node at each of our devices. So you can actually start this at a single device or you can do all of them at once. And the other thing I'm going to briefly go over is a little bit closer to simulating behavior. You can add a CSV file. So if you add a CSV file, what you'll need to do is import it, which I've got this breaker position CSV. And what this is is this is a CSV file that has different columns as your states. So you can say when the, say, state one is breaker open. So what you can do with that, which I can just right click here and do apply state. In this, I have breaker open, in between, and closed. So what you can do is you can time up certain data attributes that make sense together. So when the breakers open, the current should be zero. So you would put the breaker in the open state and the current at zero. In between, the current is still going to be zero, but you might be in an intermediate state for the breaker. And then once the breaker is completely closed, then you can then set the current to run the specific amperage that you need. So there are other ways that you can do these simulations. Of course, you could go in and write your own JavaScript as well, but we're not going to go into that for this webinar. I'm going to jump back over to the slides and hit the last few things before we start to wind down and answering questions. So in the last two webinars, you've seen our SCL Navigator, our 61850 Test Suite Pro, and Distributed Test Manager. Navigator was used to create and verify ICD files. DTM was used to simulate the entire substation and create testing scenarios for device behavior. And Test Suite Pro was used to monitor, troubleshoot, and test simulated or real substations. We actually bundle all three of these applications together and sell it as our IEC 61850 Toolkit. And if you're interested in the 61850 engineering process, we highly recommend looking into this. So just in case you came late or you want to review what you saw today, we do post all of these webinars to our GoToWebinar page and our YouTube channel. We also have handouts in the GoToWebinar app that you can go ahead and download for product information, more information about Triangle Microworks. And soon we'll be opening up a beta for our next system toolkit release. And if you're interested in what you saw today and you'd like to participate in that beta, please let us know on the survey that you'll be getting after this webinar. As Beth said earlier, we are going to stay online to answer questions. But if you have to leave early, feel free to drop any of your questions or comments at sales at trianglemicroworks.com. And with that, I am going to open up the floor to questions. OK, well, we've had a lot of good questions come in throughout the whole webinar, and I see a lot of good answers going back out online. So, yep, I will try to get through as many of those as possible for anyone that would like to continue to stay on. The first question that we have here, the next one is, can we build certain logic and test sequence to verify a switch or logic to verify DA and do write control certain actions? In which display or tool were they talking about? Is test sweep pro test sequencer? Yeah, so you can you can monitor all sorts of values in that, and you can also perform rights as well from there. I think Jim kind of showed that off a little bit where you've got the verify, you've got the rights, and you've got several other things that each test step can do. I have a question here that I think goes back to DTM, but perhaps it can be answered on both tools. If we simulate all devices in a single, how the network latency or delay taken into consideration? Is that possible to simulate devices in a multiple PC or run test sequencer in APC? Yeah, so we've been referring to it as distributed test manager. We actually have a lot of training and information on our YouTube channel as well. But the reason we call it distributed test manager is because for large systems or for specific systems, you do need to distribute the load, right? Which means that we can take all of the devices that we simulated instead of just doing it on my laptop like I did today. You can actually set up multiple computers and distribute the individual IDs to each computer. That will be able to better simulate things like if you want to test your network or if you need, if you have such a big substation that maybe one computer can't simulate altogether. Maybe you need to go across some specific hardware or simulate things in a more realistic situation. Yes, you absolutely can distribute it to as many computers as you need to get your proper testing. And to kind of follow up on that question, Matt, it's related to this next one. Is there any limit in the number of devices that can be simulated? So there are two limits. The first is your resources. So if you have a computer from the early 90s, you're probably going to be able to simulate a far less amount of 61850 or any protocol devices on it than if you got something that's much more modern day. That kind of relates to what we were just talking about. So if you have multiple computers, you can then distribute that load across multiple of them, allowing you to simulate more devices. Also, the device count that you can simulate in DTM is a configurable option for when you purchase DTM. So I think the trial, when you download a trial for it, you get 25 or so devices by default. But you can up that number to whatever you're interested in. So if you don't need a thousand devices, you can save yourself some money. But if you do need higher device counts, we can scale it that way as well. Okay. For DTM, in terms of timing accuracy, do you measure the latency and jitter of the simulated goose messages? There must be some difference between simulated goose from a PC versus actual goose messages from an IED. Any comments? Yes. So DTM solely runs on Windows applications right now. So we do have some sort of restriction when it comes down to the timing requirements of Windows. That being said, where we can, we monitor timestamps and things set by the 61850 device. And we try to be more accurate with timing based on those type of parameters. Okay. Sorry, we've got a lot of questions coming, jumping around. Okay. How buffer and unbuffer information really work in IEC 61 is completely based on the FIFO concept. How does this really work when a controller is working as a client and a relay is working as a server, where information goes to the SCADA through 104 communication protocol? And that might be, we might have to call in one of our engineers that are working behind the scenes on that one. Doesn't matter, Jim, just let me know. Could you repeat that question one more time? Sure. How does buffer and unbuffer information work in IEC 61850? It's completely based on the FIFO concept. How does this really work when a controller is working as a client and relay as a server, where information goes to SCADA through the 104 protocol? So we implement both 61850 and the 104 protocol you're describing through our source code libraries. So the simulation tool of DTM isn't really just pulling up smoke and mirrors and pretending like we're simulating devices. We're actually simulating based on the stack that is out in, you know, plenty of devices all around the world of customers who use our stack. So we actually simulate 61850 down to its spec requirements. So if you set up a buffer control block or an unbuffered report control block, it's going to work as if it would in a physical device. The exact same concept can be set for the 104 protocol that we use. So I think the answer is it works just like you would expect it to. Yeah, so if a data attribute or a data point in 104 changes in the simulation, then the stack will monitor that event, put it in the buffer, and then send it out just like it would if it was a real device. And then the client that's receiving that would process it just like a regular client would. Okay, I've got a specific question here on a use case, and basically they're asking if DTM can be used in this purpose. I'm looking to upgrade a currently service substation, which is using 61850 goose messaging for the substation interlocking. As I will be upgrading IEDs on a bay-by-bay basis, I will need to simulate goose to support the existing interlocking system, maintain goose quality bit as each bay is upgraded. Can I use DTM for this? Yeah, absolutely. That seems like a really good use case for both DTM and TSB. I mean, that's one of the things it's designed for is upgrading old substations to more new equipment. So, absolutely. Great. I've got a question here, Matt. Maybe we can touch on one of the past webinars that we had done. He's asking, is there going to be a webinar for the SCADA data gateway? Now, I know we did. So, we've done a few things that have had gateway kind of running in the background or as the device under test, but there hasn't necessarily been a webinar specifically about gateway. We are constantly reevaluating what is going to be in our next webinars, what we're next going to be talking about. And gateway is a very popular application here. It's used all over the world. I would not be surprised if there isn't something coming down the pipeline. We had the past webinar where we showed how DTM and gateway can work together in testing your gateway. So, certainly, I can send out a link to that individual of that past webinar where it did put our gateway a little bit on display. Yeah, it showed off the performance and activity of gateway as the device under test. Can DTM be used to test buffered reports? Yeah, absolutely. I think Jim and I were talking about that with a few questions back. But, yeah, so one thing I like to say when I'm giving demonstrations and trainings is as far as the outside world is concerned, if you bring up Wireshark, if you bring up any sort of sniffing application, all the protocol traffic you would expect from a device is going to be seen on the wire. It's as if the physical device is sitting there right next to your laptop, just talking out of whatever IP address or port number network configuration you provide it. So 6150 supports that, and so does DTM and TSP. Okay. We still got some questions coming in, but I think we've answered most of them at this point. So I think for now, I think we're good to wrap it up. What I would mention to everyone is that once we do close up this webinar, you will receive a survey, just a five-question survey. We'd love your feedback. Please go ahead and answer that. Whoever does answer the survey will also receive the PowerPoint slides that we have here today. And everyone, whether you answer the survey or not, will get an email with a recording to this webinar. Please stay tuned for our next webinar. We are scheduled for July 30th to really show off a new security product out there called Garibaldi. So stay tuned for more information on signing up for that webinar. And with that, we thank everyone for their time. And we will be following up with additional information if you respond to the survey. Thanks, everyone. Take care. Thank you.


Video ID: iDJm-gNPRew
This video will be a tutorial on creating devices in DTM, including creating single devices, creating multiple devices, and importing an entire 61850 substation. This is the screen you'll see when you first launch DTM. It is a blank workspace with the DTM homepage shown. The DTM homepage allows for quick loading of workspaces. You can either launch the Load Workspace window, import a workspace, or load one of our many sample workspaces. You also have quick access to your favorited workspaces or recently opened ones. You can also jump straight to the Help Center, which includes links for instructional videos, resources such as technical support requests, and documents like the user manual. You always have access to the Start page by going to Views, Start Page. To add a device to this empty workspace, you'll right-click on New Workspace, Manage, Add, and from here, we can pick which you'd like to add. We have DNP3, ICCP or TASI2, IEC 6870-101 and 104, 61850, and Modbus. You can also add Test Manager nodes for automated testing, or folders for organizing your workspace. Each protocol can be broken down into their different types. For DNP, we have Master and Outstation. Let's start with an Outstation. This is the device configuration window for a DNP3 Outstation. On the first tab, Device, we will have the name of the device and where you want the device to be simulated. If you have multiple computers in your DTM network, you can select any computer in your network to simulate this device on. The second tab, Channel, is where the connection information is configured. Because we have a DNP3 Outstation, we have automatically filled in this configuration tab with the DNP3 Outstation's default configuration. This includes behavior and port number. You can select a connection type, Serial, TCP IP, or TCP IP and UDP. Today we'll be using the default, TCP IP. Since TCP IP was selected, we will need a local address, remote address, and port. By clicking the local address dropdown, you can select any IP address the selected computer has. 0.0.0.0 will use any possible IPs. You can also type in manually the remote IP address, or leave it as star.star.star.star, which will allow anyone to connect to this Outstation. The port number 20,000 is used by default in DNP3, but you're free to change it, especially if your DTM network does not have many unique IP addresses. The next tab is the Sessions tab. From here, you can configure protocol-specific connection information. For DNP3, you can configure source and destination addresses, unsolicited messages, or enable secure authentication. The Database tab can be used to configure the specific database of the simulated DNP Outstation. By default, we've included a database that includes a wide array of different types of data, but if you'd like to edit this, you can. You can reset the database to remove all points, and then add points as you desire. Each point can be configured further based on your protocol-specific requirements. For example, this Binary Inputs class can be configured here. Once you've built up a database you are interested in, you can then export that database to a CSV file. When creating future devices, you can import this CSV file to automatically build up your custom database. The last configuration tab we'll be looking at is the Advanced tab. This is the area where you can really drill down into the configuration options of specific devices. An example of this would be configuration parameters for DNP3 Authority. For most tests, I can keep these to defaults. The final tab is the Multiple Devices tab. Here we can create multiple devices based on the configuration in the previous tabs. This is especially helpful when you're trying to do a load test and quickly need a large number of devices. Once you click Create Multiple Outstations radio button, you'll see that a single device is created in the table below. From here, you can just click the up arrow to create more, or you can type in a number. By default, we change the port number to prevent duplicate connection configurations, but you can still manually go in and change the connection configuration as you need. Now that we're done configuring, we can just hit OK. Now you'll see the workspace has been populated with the devices we created. We can follow the same steps to create master partners for each of these outstations. DNP3, Modbus, and 6870, 101, and 104 are all created using the same process. Following these steps will allow you to simulate any of these types of devices. Next we'll be going over how to create 61850 devices. Using the same steps as before, we'll go to Manage, Add, 61850, Servers. 61850 uses configuration files that we can import directly into DTM to simulate those devices. DTM supports many forms of these configuration files, including ICD, CID, SCD, and many more file types. You can do this by clicking the SCL file icon and clicking Import Configuration. From here, just navigate to your file, select it, and hit Open. Now that that file has been imported, you can click OK. You will see that all of the configuration options are now filled in. If your SCL file is an SCD file or contains multiple IEDs, you can click this drop-down to select which IED you would like to load. You can also go and change from the default IP address found in the file to something that is on your computer. If your device needs GOOSE, you can select the GOOSE adapter here. You can also add IP addresses to your network adapters, but that will be shown in a different video. The Advanced tab is similar to the one shown in DNP. This is where protocol-specific configurations are updated. This is also where you'll find sampled values adapters if your device needs to have access to sampled values. The last tab is the same as DNP, Multiple Devices. From here, you can create multiple instances of the device configured, but I'll just create one. Following the steps as before, we can create the 61850 client. Since we've already imported the SCD file, we will not have to import it. Just click the SCL file icon and select the file. Hit OK. Again, we'll need to select the proper IED to load and give it the right IP address. If you did not have the SCL file to configure your client, you could always click Use Discovery and the 61850 client would discover the model based on the server's configuration. If you were to create multiple 61850 clients, you could go to the Multiple Devices tab. This is slightly different than what we've seen before. In 61850, you can create multiple clients to connect to one server, or you can create a client based on a server configuration found in the imported SCL files. Since I imported an SCD file, you'll see the configuration for any device found in my SCD file, but for the sake of this tutorial, I'm only going to create the one that I already have selected. The last thing we're going to talk about today is import substation. SCD files are substation configuration files. You may want to simulate an entire substation, but you may not want to go through the previous process multiple times. This is where our Import Substation tool comes in. To access Import Substation, you'll go to File, Import Substation. Again, we'll want to select my SCD file, and hit OK. This is our Import Substation tool. There are three modes you can run in this. The first mode is SCL File Restrict. In this mode, you are tied to exactly what the SCD file defines. This includes IP address, subnet masks, and port numbers. This mode is used to verify everything that you have is properly configured. The second mode is User Defined. In this mode, you'll have a full range of editing configuration options. This will allow you to tweak connection options freely to quickly configure devices for your computer. The last mode is Loopback Addressing. This mode is the quickest way to get from nothing to a full simulated substation. Every device in your substation will be assigned an IP address in loopback addressing. This means devices outside of your computer will not have access to this simulated substation, but you'll be able to connect to everything locally. After you're done configuring your devices, you can either click Import All, or drag devices independently. For more information on DTM, or to start your eval of the application, check out the links in the description below. Thanks for watching. https://www.kenhub.com


Video ID: GVPyPthOyP8
Hello, everyone, and welcome to today's webinar. My name is Beth Capellas, and I'm the sales and support manager here at Triangle Microworks. Thank you for taking the time and joining us. Today we will be talking about securing grid communications, unleashing IEC 62050 routable goose and sample values. Before we get started, I'd like to go over some of the housekeeping items while everyone is still joining. The webinar is scheduled to be one hour long. The audience is on mute, so please submit your questions online using the webinar toolbar under the section titled questions. You can submit your questions as soon as you have them. Please don't wait until the end. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly. We'll try to take a couple of breaks to read some of the submitted questions and are also happy to stay on after the webinar is over and answer as many questions as we can. With that, I'd like to introduce the folks behind the scenes answering the questions. Today we have with us Phil Young, senior developer specializing in 62050 with Triangle Microworks, and also Tom Stone, software engineer specializing in security, also with Triangle Microworks. Next, I'd like to introduce our speakers. First, we have Jackson Moore, application engineer here at Triangle. Jackson received bachelor's of science degrees in both electrical engineering and computer engineering from North Carolina State University and has a background in power system engineering. Prior to joining Triangle, Jackson spent five years as a microgrid system engineer where he designed and developed load management control systems for multi-source microgrids. Currently, Jackson works with our customers and development team to understand and solve the challenges our clients face. Next, we have Herb Fuch, senior system architect at PCI Tech. Herb has over 40 years of experience working in the automation, information exchange, standardization activities, and system integration. Previous employment was Westinghouse Pneumologic and Cisco, where he was responsible for 62050 and SIM integration projects and products. Herb has been involved with 62050 technology since 1982 and cybersecurity since 1993. Herb is an editor of IEC 62050 8-1, editor of several cybersecurity standards, and is the vice president of testing for the UCA International User Group. His work for the UCA has involved the coordination of 62050 interoperability testing since 2011. Now that you know everyone, I'd like to give you a little bit of background on Triangle Microworks. So, we were established in 1994 and have over 28 years of communication protocol experience. We are headquartered in Raleigh, North Carolina, and have customers in over 70 countries. We work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs in development time. Licensing a source code library also includes support for your implementation and any issues that may arise in the field. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They are designed to troubleshoot, simulate, and automate testing for both a single device or a complete substation. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. It is available as software only or pre-installed on hardware. We are heavily involved in the standard committees that support SCADA protocols like UCA Group for 620.50 and DMP user and technical committees. Through our participation, we help define the protocols, stay up to date on the changes taking place, and ultimately provide feedback based on our customers' needs as the protocols continue to evolve. So now that you know a little bit more about Triangle, I'd like to go ahead and hand it over to Jackson to kick off the webinar. Jackson, I'm going to go ahead and make you presenter. All right. Thank you, Beth. Okay. We can see the screen. Excellent. All right. So before we get started, I just want to provide a little bit of a background of 620.50. So development on IEC 620.50 began in the late 1990s, and this was really in response to the need for a standardized approach to substation automation. Prior to the introduction of 620.50, there was just a lot of different proprietary protocols and technologies used for substation automation, and this really made it challenging to integrate devices from different vendors. This lack of standardization hindered interoperability, increased costs, and ultimately, it limited the flexibility of power system designs. To address these challenges, the International Electrotechnical Commission, or IEC, initiated the development of a new standard that would define a common language and common communication protocol for devices within substations. The goal was to enable interoperability between devices from different vendors, improve system flexibility, and then, as I mentioned, ultimately reduce associated costs with substation automation. So the first edition of IEC 620.50 was actually published back in 2004 and incorporated not one but three distinct communication protocols as well as an accompanying engineering process. Goose, or generic object-oriented substation event, was introduced as a high-speed communications-based alternative to traditional copper wiring used for control and status signals. Sample values, on the other hand, serves to replace CT and PT wiring by passing analog values such as currents and voltages over communications from a measuring device to one or multiple protection or control devices. Finally, MMS was introduced as the report-based SCADA communication protocol, and this was used to pass status information and supervisory controls back and forth between the substation and a control center or SCADA system. The accompanying engineering process is a discussion all of its own. I encourage you to check out some of our past webinars for a deeper dive on this topic, but in brief, it relies heavily on the use of SCL, or system configuration language. SCL is a standardized XML-based language which provides a common format for specifying, designing, and configuring an entire digital substation. Since its introduction, IEC 620.50 has continued to evolve, incorporating new features and capabilities to meet the changing needs of the power industry. So now that we have a little context as to what 620.50 is and how it came to be, let's take a look at where we are today. So now in its second major revision, IEC 620.50 has achieved widespread support from both device manufacturers and electrical utilities, and in fact, it's considered the de facto standard for substation automation and communication in much of the world. We've seen continued development and updates to address new requirements of technologies such as wide area protection and control, cybersecurity, and interoperability with distributed energy resources, or DERs. Today we'll be discussing two specific additions to the standard and the use cases that they enable. First is the introduction of routable variants of the goose and sample value message protocols that I mentioned before. Originally optimized for use within a single substation, these protocols were implemented in layer two of the OSI stack. Practically, this meant that their use was limited to a flat network architecture, and any use case which required goose or sample values to extend beyond one physical network would share this same limitation. The introduction of routable variants, which package goose and sample value messages into UDP multicast packets, allows for this technology to extend beyond a single network. This change is in part what necessitated the second addition to the standard that I'll be discussing, which is security. While security for MMS communications was first introduced in 2007 as part of IEC 62351-4, and it's been enhanced more recently by the end-to-end variant, end-to-end, excuse me, specified in 62351-6, there has historically been no standardized approach to security and sample value, I'm sorry, for security for goose and sample values. Increasing cybersecurity threats, coupled with the introduction of applications in which critical grid communications extend beyond a single isolated network, this has led to the requirements for a method to both authenticate and encrypt messages, so goose and sample values. So the standard which introduced methods for both authentication and encryption for goose and sample values is known as IEC 62351-9. And I should mention that while this technology is commonly applied to the routable variants, it can also be employed for traditional layer 2 versions of the protocol. So before getting into the specifics of how it is accomplished, I do want to clarify what I mean when I say authentication and encryption. In short, encryption protects the contents of the message from being understood by unauthorized parties who might be listening in the middle, whereas authentication verifies the identity of the sender to prevent impersonation. While IEC 62351-9 specifies methods for both, only authentication is required, leaving encryption as optional based on the needs of the user. For instance, in a wide area protection scheme, it might be necessary to ensure that a trip signal sent between substations carried via goose message is coming from an authorized sender and has not been altered in transit. In this case, authentication would be used to verify the source of the command and ensure that it has not been tampered with, but encryption may not be deemed necessary if the command itself is not considered sensitive. On the other hand, sampled values messages containing real-time information about grid characteristics sent back to a control center as part of maybe a remedial action scheme may be deemed an operational or even national security risk if not both authenticated and encrypted. In either case, the mechanism for distribution of the cryptographic keys used for signing and encryption is the same, utilizing GDOI, or Group Domain of Interpretation protocol. GDOI is a protocol used to manage group keys for secure multicast communications, such as goose and sample values, where data is sent from one publisher to multiple subscribers. The GDOI server establishes and maintains the shared encryption keys among all group members. In the case of 61850, a single group consists of an IED publishing either goose or sample values messages and any IEDs subscribing to that one message, each of which would contain a GDOI client, each one of those IEDs, and that's going to be responsible for receiving and requesting those keys. I should mention that a single IED can and likely will be part of multiple groups, depending on the control scheme that you have present in your substation. Each message or goose control block that your IED is publishing and any subscribers constitutes one group, but if that same IED is subscribing to other messages, it will be part of that separate group as well. So what you see on my screen now is a diagram depicting a simplified network architecture in a substation that's utilizing security for goose and sample value messages. Up at the top, depicted with the key icons, are the key distribution center or KDC servers. As mentioned before, these use the GDOI protocol to securely distribute keys to participating IEDs, each of which contains a KDC client which received these keys. As part of IEC 62351-9, the standard that describes this mechanism, there's also defined the ability to have redundant KDC servers for increased resiliency, as shown here. Depending on the use case, the KDC servers may be co-located in the substation with the IEDs, or they could be spread across multiple substations if you're using routable goose to send message between substations, or they even could be at a remote third-party site. As mentioned before, GDOI stands for Group Domain of Interpretation, and in this diagram, the three IEDs at the bottom represent a single group that I was speaking about before, consisting of a single publisher up here at the top, and one or more subscribers. In this case, we have two. So this means that they will each receive the same set of unique keys for their group, and this will be used for signing and encrypting in the case of the publisher, and authenticating and decrypting in the case of the subscribers. Now this transfer of keys can take place via a pull request initiated by the IEDs, or it can occur a periodic push from the KDC server, and that's a process that I'll go into a little bit more detail now. So pull versus push. So as I mentioned, pull is going to be initiated by the IEDs themselves, and this has the advantage of mutual authentication. What I mean by that is, well, it's more than just a simple checking for certificate validity. It's checking to see, is that certificate expired? Has it been signed by a trusted authority? And additionally, has it not been revoked since that initial time that it was received or generated? On the other hand, push, well, this is going to be initiated by the KDC server itself. So that was up at the top of the last diagram. So one benefit here is that it does allow for KDA, or key distribution authentication. This basically makes sure that all IEDs have, in fact, received the keys, and a confirmation of receipt is sent back from those IEDs, so you can pause rotation. I distribute keys out to, let's say, all 25 IEDs in my substation, and they send a confirmation of receipt. Once I receive that, then I give the okay to go ahead and roll over to the next key. If less than a certain percentage of my IEDs have received the new and updated keys, I have the option to pause key rotation and continue to use the last set of keys. Another advantage of push is how it scales for large systems, so you can push keys out to a multicast address. And if I have many IEDs that are subscribed to that multicast address, it takes one single message to distribute keys to all members of a specific group, instead of each one of those IEDs needing to request keys individually. So here we have a diagram that just illustrates this point a little bit further and shows how a push would work or how a pull would work. So on the left-hand side, we have a group member. In 61850 terms, this is going to be an IED, and the first thing he'll do when he boots up, indicated by this icon here, is initiate a pull exchange. So he'll reach out to the KDC server and request a new set of keys. When those new set of keys come back, he'll be in a wait period until the key rotation period expires. So this might be two days, this might be two weeks, but some period of time to wait until the group member or IED needs to request new keys. When that time expires, he'll go out and request new keys again, and all IEDs that are a member of this group will have the same key rotation period, and they'll all be requesting keys at the same time and know to switch over at the turn of the clock. On the other hand, a push, as I mentioned before, is initiated by the KDC server. So in this case, when it powers up, initially a pull will be requested by the group members, but all future key exchanges will be initiated by the KDC server, and it will, at the expiration or prior to the expiration of the key rotation period, push down new keys to all members, and then wait until the key rotation period has expired, and then push down a new set of keys. So finally, I want to take some time to highlight part of what makes this solution unique from traditional methods of data security, such as VPNs, and really that's that this was developed from the ground up for use in an operational technology environment, not information technology. So while OT and IT share many common goals, the priorities of these goals that we rank them in often differ. In the world of power distribution and transmission, where reliability is our key metric of success, availability often becomes the chief concern, followed by integrity and confidentiality. In the world of IT, we often see these things reversed. So with this in mind, many availability-focused features have been incorporated into the standard. Listed here are a few that I want to highlight. First, the standard was designed in such a way as to give flexibility to those who implement it. I touched on this already when I mentioned that encryption is optional based on the needs of the user, but this extends to other policies as well, such as the key rotation period and key delivery assurance. Key rotation period I alluded to before, this defines how frequently cryptographic keys change. Halting a would-be attacker's ability to decrypt future messages should an old key be compromised because new keys are now in order. So if we estimate that it might take an attacker two days to, you know, with the supercomputer to crack a set of keys, we set the key rotation period to less than that, we always have forward security. Key delivery assurance provides a mechanism for the GDOI server to confirm that group members have received the necessary keys and can halt rotation if required. As I mentioned before, this is a benefit that comes when using the push method of key delivery. So these features and others, such as the option to add redundant KDC servers that I mentioned before, well, these were incorporated to ensure that the increased complexity introduced by security does not lead to a less robust or reliable power grid, but instead quite the opposite. And now I'm going to open up Darabaldi, the KDC server available today from PCI Tech and resold by Triangle Microworks, and take a look at just how easy it is to use and configure. So what you see up on my screen now is Darabaldi, and this is the web interface in particular. So this allows us to log in, configure, and set up the tool. So I start out by just clicking Submit. And as I said, this is the KDC server. So this is ultimately what's going to be responsible for distributing keys out to my different group members. So the first thing I need to do when setting up a new KDC server is configure my certificates. So I come down to KDC Admin, Security, and Certificate Management. I start out with my Certificate Authority Configuration, and I begin by uploading this certificate here. So this is really a root of trust, or a root of trust. And typically, this is the issuer for certificates that are going to be distributed as part of the GDOI, or Group Domain of Interpretation. So this is going to be a public key, and it's going to be used to verify this identity of certificates that are exchanged later on. So after I have this initial certificate, I come down and set my local KDC certificate. And this is going to be the identity certificate of the actual KDC server used as part of the GDOI exchange. So once I have those two certificates in place, the next thing I'll do is come over and set my global policies. So in just a moment, I will actually be importing a SEL file. In this case, it'll be an SED file defining all of my substation, but most importantly, the data exchanges that will be happening as part of goose and sample value messages. So when I do that, this global policy that I set here will be applied to each and every goose exchange that's configured to use security. Well, I can't specify whether a specific one will be encrypted or authenticated, but these are the initial policies that will be applied. Now if there's many, many policies in my substation, because there's many goose control blocks, I do have the option to go in afterwards and edit individual ones, but I do want to take time to set up the default policy before importing the SED file. So this gives me the option to specify which encryption algorithms I will be using. Will I be using group pull or group push and other things such as the key rotation period defined here as interval for generation. So after I have my global policy set, I come down into SEL management, and this is where I would upload my SED file, which as you can see, I've already done here. So these represent the different goose and sample value control blocks that are present within my SED file, and here I see four different ones. This one in the middle I see has been configured to use rgoose or routable goose. Selecting it, I can see the different subscribers highlighted in this window below that are configured to subscribe or receive this particular goose message. Now, to configure further, I could either click this Go to IED Management page or back from the dropdown up at the top, go to IED Management, and then select that same IED. Again, down here, I see the different publication groups present on this IED, or in this case, the goose control block. So I click on that same stat goose control block that's configured to use rgoose, and then down here, I see the subscribing IEDs again, and now I can edit the individual policies for this particular publishing group or goose control block. So once that is complete, I'm really done with my KDC server. Everything is up and running and ready to go, and now the only thing I need to do is configure my group members or IEDs. So to represent that configuration, I'll actually be using Distributed Test Manager, or DTM. So in this tool, I am simulating an IED. It's a multi-protocol simulation tool, but today I'm using it for a 61850 IED. And if I want to configure this particular IED to use security, the first thing I need to do is modify the SCL file just to make sure that those goose control blocks are configured to use rgoose and make sure that security is enabled. And then all I need to do on the actual IED itself outside of the SCL file is come in here to manage KDC client configuration. In here, I'll just put in the IP address of my KDC server. If I have redundancy set up, I'll put in the IP address of the second KDC server, an identity certificate for this local IED, and then a password that is associated with that certificate. And then I'll just finally click Enable KDC Client, and then I am done. So this is the process that you would use in DTM, but ultimately, these are the same four parameters that you would need to enable on any IED which supports this technology. With that, I'm going to go ahead and turn it back over to Beth for a quick Q&A session if we have time. Yeah, we did have a couple of good questions come in. I'm going to direct this first one to Herb. So Herb, you might want to take yourself off mute. So we had someone come in and ask about the interoperation between 62050 and DMP. So, you know, are we going to discuss it? I know this presentation is focused specifically on 62050, but is there anything you can say concerning how it interoperates with DMP? There are two totally different protocols. So DMP is a SCADA protocol, and the equivalent in 61850 is the MMS profile. So you can't use DMP to communicate with an IEC 61850-only device, but there are devices out there that support both. So I hope that helps. Okay, another question that came in. Are GOOS and RSV in this demo are transmitted over Layer 2 or Layer 3? They're transmitted over UDPIP, so Layer 3 and 4. Okay, great. And the next question that came in, since we are sending the GOOS messages and sample values to a multicast address, do we know which IP is subscribed to it in advance? If so, why do we need to use multicast? No, this is a complicated question. So the first thing is the development of GOOS was the Layer 2 GOOS, which happened 20 years ago, was based upon an actual substation issue where a tornado went through and tore down three transmission corridors. In that mode to protect the equipment, the use case said you had to communicate protection information to over 100 different devices in under 4 milliseconds. You cannot do that with TCPIP. So you can't even do it with unicast UDPIP in a lot of situations. So that's where the concept of multicasting came into play. And without security, anybody can plug into the network and receive the publication. So you don't even necessarily need to configure who subscribes to it in that mode. But anyway, that's why it's multicast. OK, thanks, Herb. I think with that, now we're going to switch over to you. So I'll go ahead and make you presenter. OK, hang on. And you should have the screen. Yeah, I'm just getting ready. Yep. Can you guys see my display? Yes. OK, so just to reiterate a little bit about what Jackson was talking about, this is a view inside of a real implementation of two GEUR relays that are participating in a single group via multicast. And there are a couple of things that I just want to show kind of internal to how resiliency is actually achieved. So you can see here are the two keys. One is activated. One is activating. Same thing over here. So Jackson did cover that there are two sets of operational keys delivered. And you can see the time to expire, the time to activate in seconds over here. And you can see there's a direct correlation in the ID for that key. And this is how the devices can match up which keys are in use. We discussed the activated and activating. And you can actually see there's an overlap between the time of expiration and the time that the next key is going to be activated. So all of these features provide resiliency within a device. And you can see where KDA has not been received yet. So the other thing that comes into play, and Jackson briefly touched on this, is digital certificates or what's typically referred to as public key infrastructure. And if you think about a system of 100 devices where there are certificates that have to be installed in those 100 devices, those certificates expire every so often. So in 62351-9, one of the two protocols that allow devices to request new certificates or renew expiring certificates is the Simple Certificate Enrollment Protocol or SCEP. There's also a methodology to a protocol called Online Certificate Status Protocol, OCSP, that allows devices in the KDC to check if the certificates that are exchanged as part of GDOI have been revoked. For those of you in the security realm, this is how you can do real-time queries of certificate revocation lists. Now, in utilizing SCEP, what happens is there's a trust anchor that gets established and then typically an intermediate CA or more than one intermediate CA. And the lowest intermediate CA is typically the one that is used to actually provide and sign certificates. But SCEP does provide the entire CA chain, which would also be available to devices that implement SCEP. And what you see in front of you is just a example interface that you might see in the KDC or the KDC client. So use cases for security of goose, routable goose sample values are quite extensive. The routable variants we'll get into, but really are best utilized for transfer trip and remedial action, which goes across large geographic areas, and fault location isolation and system restoration FLISR. There are utilities that we'll talk about that are requesting authentication for layer two sample values and substation. And there is an impact on how do you test a secure substation using this technology. So let's talk about how current utilities are using layer two goose without security to do transfer trip and remedial action. Well, you can do it by using L2TP over a VPN to tunnel layer two goose from one substation to another. This provides wide area network protection between the substations, but it creates a flat layer two network that most IT and OT staffs don't like to maintain since they're in geographically disparate sites. If you do layer two goose without the 62351-6 security methodology, there's no end to end authentication between the publisher and the subscriber. So it's easy to gain access to one end or the other of the VPN tunnel, and actually create threats from those locations. Our goose was designed to always have security, which was end to end authentication. It runs over UDP IP, so it's normal IP networking. And you can encrypt it either by using VPNs or the IEC 61850 security policies that are called out in 62351-6. Vault location FLISR is very similar to transfer trip because it spans several geographical areas. And in this single line diagram, this is a representation of an actual layer two goose system that's operational in Raleigh, North Carolina, which was implemented by GNW. They do have fiber between the vaults and the vaults do offer some type of secure physical access. And this design is being used as the first U.S. basis for the first U.S. standard regarding 61850, a basic application profile. For FLISR being created by NEMA, and it is very close to being published. There are utilities and security people within those utilities that want to have authentication on merging units so that they can't be spoofed. And the answer to that is to use the 62351-6 extension to provide authentication for layer two sample values. Now, one of the interesting things about testing in a secure environment is in most environments, you would have to decommission security. However, with the design of 62351-6 and the use of GDOI, that's not necessary. So you aren't faced with decommissioning security, running your test, and then remembering to recommission security when your testing is done. So if you were to bring in a transient cyber asset, like a DOBL test set, an intrusion detection system, or an Omicron test set, they have a mechanism of actually obtaining the keys and the policies to interact with the system. And once they do that, they can do the 61850 simulation mode as normal. So what's required? The test system, or the TCA, needs a valid X.509 identity certificate that's recognized by the KDC. Obviously, it hasn't been configured in the SCD to be a subscriber. So it has to use pull to get the keys and the policies so it can synchronize with what it's going to be subscribing for or publishing. Once it has the keys and the policies, it can set its simulation bit, do normal testing. And just to note, overpulling may be required just prior to key rotations. But other than that, it can fully participate and test a system without decommissioning security. The 61850 security standards are based on a whole slew of standards, which are not 61850 standards. They are 62351-6, which includes security profiles and usage definitions for TLS and MMS client server security. 62351-9 contains the key distribution specifications for GDOI. And 62351-6, in conjunction with 61850-8-1, specifies security protocols that are used and extensions to provide end-to-end authentication, optional encryption, and key and policy management and delivery related to GOOS, RGOOS sample values and routable sample values. There is a UCA IEC 61850 sponsored IOP occurring in September of 2024 in Birmingham, Alabama. And we're going to continue testing all these cybersecurity options that were started in 2022. One of the questions that typically comes up is, what is the impact on doing crypto on the performance of GOOS or sample values? And we're working on doing research in this area for an IEEE technical report or white paper that is going to be published about using security for GOOS and sample values, believe it or not. And these are some initial results that show that when you have crypto acceleration, which is basically available in most modern CPUs these days, you're looking at less than 1.5 microseconds impact. If you use AES GMAC or the AES Galeric methodologies. And the reason of the research was to come up with a recommended default for best performance for the encryption and decryption and MAC algorithms that are specified in 62351-9. Now, you saw a slide. Oh, let me just say, obviously, if you have to do this in software and you don't have crypto acceleration, depending upon your CPU, it can go up to as high as 50 microseconds. But still, that's not bad. OK, so there is an evolving ecosystem of devices and vendors that have declared support for the routable GOOS or 62351-9 key management. And these include GE, Schneider Electric, Siemens, Toshiba, and other vendors who don't want to be named until they actually release their products. GE and Toshiba were at the 2022 Interop testing this, and I hope Schneider Electric and Siemens will be at the 2024 to test. Meinberg, which is a PTP clock vendor, is looking at using 62351-9 key distribution and policy management to actually manage the policies for the power profile security tag. So this can bring authentication to PTP as well. There are monitoring and testing sets that are available or will be available shortly from Triangle Microworks. You saw DTM and TSP also has this capability. Doble has committed to providing this capability very shortly. And Nozomi has an IDS that can interact with the KDC. And GNW is working very hard to be an integrator for this technology. And with that, what's going on in the utility world, the user space? Well, Denmark theoretically has a national standard about this. There is a FAT for an Argo system over in Europe about to be executed, and it's appearing in multiple RFPs, especially for transfer trip and wide area network protection. In the U.S., NEMA is publishing the first basic application profile for 61850, with four more potentially to come. Southern Company is talking to merging unit vendors to set up a lab to test SV authentication. And Southern Cal Edison is evaluating, updating to use Argoose with security for several aspects, including its centralized remedial action scheme. This technology is installed in several labs over in Asia, and they're working towards actual deployment. And with that, I will turn it over to Jackson and Beth about next steps. Thank you, Herb. Yeah, so next steps, we kind of broke this out into two groups, and it depends on what camp you find yourself in. So for device vendors and labs, maybe associated with a vendor, potentially even with a utility or someone else, a research group, if you find yourself within that camp, we want to let you know that Garibaldi, the KDC server that I demonstrated earlier today built by PCI Tech, it's available today, and Triangle Microworks is a reseller, and we're happy to speak more with you. Go ahead and email us if that's something that you're interested in. Our KDC client source code library, also available from Triangle Microworks, this is what would run on your individual device or IED, and that's what's going to be able to receive the keys and pass them down to your IED, which will be used for authentication and encryption and those. So that is available with the Triangle Microworks 61850 source code library, as well as other source code libraries. You can purchase just the KDC client source code as a standalone. And then when it comes to testing and simulating, we have two different tools that fit into that camp. So TSP, or Test Suite Pro, from Triangle, as well as DTM, or Distributed Test Manager. Test Suite Pro, as you might have seen in some of our past webinars, is a client test set, and it allows you to subscribe to goose messages and can communicate with the KDC server to decrypt and authenticate those messages. Whereas DTM, or Distributed Test Manager, multi-protocol simulation tool that I showed briefly earlier on, and that can be used to simulate a 61850 server that is either publishing or subscribing to goose or sample values messages that are regular layer two or routable and including authentication and encryption for all of that. So for utilities, I guess we would ask you to start asking vendors for this technology. It's a major enhancement over what's available now. It obviously has the benefit of security, which is becoming more important in our world. So start writing it into RFQs and start requesting it. And when you are ready to move forward with the proof of concept, that can be accomplished today using Garibaldi, DTM, and Test Suite Pro, as I mentioned. You can recreate the demo that I did earlier and start sending and receiving authenticated and encrypted goose and sample value messages and seeing the impact it will have on your system. So with that, I just want to say thank you to everyone for attending, and I'll go ahead and hand it back to Beth for a final question and answer session. Great. Thanks, Jackson. Before I get to the rest of the questions, we do have several to go through. I do want to just wrap up with some of my final thoughts here. So again, thank you for attending today's webinar. This does conclude the presentation part of it. If you are not already signed up to receive our newsletters, please go to our website and go ahead and fill out a very short form on that page. You'll find out about future webinars, any training events that we have, any conferences that we're attending, and about our product releases. When you leave the webinar today, you will be presented with a survey. It's very short. It's about five or six questions. We'll only take about two minutes. Really appreciate you taking the time to fill that out and give us your feedback. You will also receive a recording of this webinar if you fill out the survey. It will also be posted to our website in the next week or so on our homepage. So you'll be able to see it there. And you can also find any of our other previous webinars located on our website. And then, of course, if you have any additional questions or want any additional information, please email sales at trianglemicroworks.com. So with that, let me go ahead and start to go through the other questions that were coming in. We did have several more really detailed questions coming in. I'm going to kind of back up a little bit here. And Herb, I believe this one is for you. What is the preferred IEC 62351-9 approach, EST or SCEP? That depends, as Alex Apostolou would say. So EST came about because SCEP didn't have the challenge mechanism at the time. So they needed a secure transport. But SCEP has been upgraded to have a challenge mechanism so that the keys or the certificates can be controlled. Most utilities use SCEP currently. And talking to some hardware vendors that are dealing with TSP chips, the TSP chip vendors have chosen EST. So it really depends. But SCEP is very prevalent in the utility space. I hope that helps. Jackson, I think this question is for you. You did just touch on it. But just to reiterate, make sure it's clear. The question is, which TMW tools can be used to configure RGOOS and RSV and used to test them? Yes, so it takes a couple. So ultimately, you need to begin with an SCL file that has configured for GOOS and sample value messages. So we do provide a sample workspace with DTM that can get you up and started. That could also be created with a system configuration tool such as Helinks or even edit an existing one with SCL Navigator or SCL Editing Tool. So once you have that, then you need two pieces. So one would be Garibaldi, as I mentioned before. So this is the KDC server responsible for sending out keys. And we are a reseller of that. And you can try that as a demo. And then you'll need to be able to simulate a server that's publishing and subscribing to GOOS and sample value messages. Or you'll need a physical IED that's capable of it. So DTM would be my recommendation for that, for publishing GOOS and sample value messages that are configured to use security. Although additionally, you could use TestSuite Pro as well. So TestSuite Pro, its primary focus is not simulation of servers. It's more a client tool that can subscribe to GOOS and sample value messages. But it does have the capability to simulate 61850 servers. And you could publish GOOS and even sample values messages that are using security with that tool as well. I know that was a lot. I hope that answers the question. OK, next question. I believe this one will be for Herb. Is it mandatory to have access to the Garibaldi KDC tool? Or can one configure our GOOS and our sample values and verify communication between publisher and subscriber? So there are two answers to that question. The standard says you must have keys in play and policies in play for our GOOS and operational systems. For testing, there's an exclusion in a lab, let's say, where that's not required. And some vendors start out with pre-shared keys and pre-configured policies. But that's really not very useful in managing a secure environment and an operational system. So operational systems really do require a KDC, in my mind. OK, next question. How does the KDC client know that the KDC server is the authentic server? And there's a second part there. Is there a method for the KDC clients to talk to a certificate authority or is it provided at the configuration of the client? OK, so the identity certificates are exchanged during the pool exchange. That's part of the authentication mechanism that both the KDC client and the KDC have to perform prior to giving keys and policies. So both sides have to have an identity certificate that the other side can verify and validate as being authentic. And what was the second question, Beth, part of the question? The second part is, is there a method for the KDC clients to talk to a certificate authority or is it provided at the configuration of the client? So we touched on that as being SCEP. And then there was a question about EST. Either of those mechanisms allow either side to talk to an intermediate authority that's in a certificate authority chain. OK, next question. Is the management interface of the KDC support secure communication? It can do HTTPS. It requires login via either locally configured username, password, radius, or a two factor attribute certificate. So I would say it can be, it is secure. OK. Here's kind of a broader question. What is the main reason to choose either goose over sample values and vice versa, or what is the response to have both? Do you want me to take that, Beth? Yeah, I can. Oh, either one. Jackson, go ahead. Yeah, so going back to kind of the first slide, when 61850 was first introduced, it wasn't one communication protocol. It's something like DNP or 104. It's really three distinct, right? So MMS, our SCADA protocol, that's probably most comparable to 104 or DNP, even Modbus, something like that. Whereas goose and sample values kind of fit two different holes. So goose replacing copper wires that are traditionally sending, I mean, it's not limited to binary signals, but that's kind of its bread and butter there. So like a trip signal sent between a protection device and a breaker controller, whereas sample values is replacing the CT and PT. So you're sending high speed analog messages in a 60 hertz system. Like in the US, you would have 4,800 samples per second. In a 50 hertz system, it might be 4,000. But so you can reproduce that entire waveform. Think of a current waveform maybe that's going up and down 60 times per second. You can reproduce that whole waveform by sampling it 4,000 or almost 5,000 times per second, and then send that out at very high speed from the device that's actually got the PT or CT on it, and then send it out on the network to any device that needs that value for either protection or usually protection, but potentially a control application. So that's, I guess, when you would use the two different things, and they're commonly used in conjunction, right? So you would have your CT or PT measuring current and voltage on the line, publishing these analog values out at high speed. And then in another location, you might have your protection device that's receiving these analog values and then sending back. Maybe a most basic example would be a trip signal if the current exceeds a certain threshold. So like an overcurrent trip and that message back to trip the breaker would be sent via a goose message. OK, I do have a question. I just have one other thing to add to that. So the routable sample value variant was designed to do phasor measurement, you know, PMU type functionality in a secure way. I wouldn't ever recommend routing merging unit class of traffic over a wide area network. So sample values is a protocol that can be used for multiple purposes, but within a substation, it's typically merging unit, distributing digitized CT and PT information, as Jackson said. OK, a question came in that really just wanted to clarify. When you talked about the ecosystem, Herb, you mentioned GNW and you were referring to GNW Electric. Is that correct? Yes. Yes. To make sure that's clear. Next question, I see some challenges on how to renew certificates, which must be replaced from time to time to be in line with cybersecurity requirements. Is this being addressed? That is the SCEP and EST discussion. So, yes. 62351-9 specifies to use either of those two protocols to do that. Great. Well, I think we've gotten to all the questions. So I just want to thank everyone for their time today and submitting excellent questions for us to answer here. And we look forward to seeing you on our next webinar. Thanks, everyone.


Video ID: d5pYP4dq2fc
Hello, everyone, and good morning. My name is Beth Kapalis, and I'm the sales and support manager here at Triangle Microworks. I would like to welcome everyone to our latest webinar, Charting a Course to a Digital Substation, How to Integrate IEC 62850 in Your Existing Solutions. This is a three-part series. Part one and part two have already been held, and recordings can be found at our website, trianglemicroworks.com. I see a lot of familiar names on the attendee list, so I appreciate everyone that's been with us for the whole series, and welcome back to part three. For anyone that is new, and this is their first time joining this part of the series, you do not have to have watched part one and part two to participate and understand part three. They are related, but they can stand on their own. In today's webinar, we'll be focused on strategies for integration of DMP3 and IEC 62850. So for all the new folks that have joined us today, I would like to go over some housekeeping items while everyone is still joining. The webinar is scheduled to be one hour long. We have quite a bit of material to cover, so we are going to try to take at least two breaks to answer questions. The audience is on mute, so go ahead and submit your questions online using the webinar toolbar under the section titled Questions. You can submit your questions as soon as you have them. Please don't wait until the end. We will begin answering questions online, as we do have several engineers monitoring the questions and responding to them directly. And we're also happy to stay on the webinar afterwards and answer as many questions as we can. So with that, I'd like to go ahead and introduce the folks behind the scenes answering those questions. Today, we have with us Jim Coats, President of Triangle Microworks. We have Greg Gitalewski, Engineering Manager, Triangle Microworks, and Dave Gaukenauer. He's our lead developer for DTM, which is one of the tools that you'll be seeing here today. Next, I'd like to introduce our speakers. We have Christoph Brunner, President of IT4Power. Christoph is a well-known utility industry professional and 61 and 50 expert with over 30 years of industry experience. Christoph's been involved in the development of the standards since the beginning. He's the convener of Working Group 10 and member of Working Group 17, 18, and 19 of IEC TC57. He's also an IEEE Fellow and a member of several IEEE Working Groups, as well as a member of PSRC Main Committee and the Subcommittee H. Next, we have Jackson Moore, Application Engineer here at Triangle Microworks. Jackson received Bachelor's of Science degrees in both electrical engineering and computer engineering from North Carolina State University and has a background in power system engineering. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer, where he designed and developed load management control systems for multisource microgrids ranging from 1 megawatt to 30 megawatts. In his current role of Application Engineer, Jackson serves as a bridge between our customer and our development team, seeking to understand and solve unique and complex challenges for our clients. So now I'm just going to spend a couple of minutes telling you about Triangle Microworks and give you some background. We were established back in 1994 and have over 28 years of communication protocol experience. We are headquartered in Raleigh, North Carolina, and have customers in over 70 countries. We work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their cost and development time. Licensing a source code library also includes support for your implementation and any issues that may arise in the field. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They're designed to troubleshoot, simulate, and automate testing for both a single device or a complete substation. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. It's also available as software only or pre-installed on hardware. We are heavily involved in the standard committees that support SCADA protocols like UCA Group for 62850 and DMP user and technical committees. Through our participation, we help define the protocols, we stay up to date on the changes taking place, and ultimately provide feedback based on our customer needs as the protocol continues to evolve. So with that, I'd like to go ahead and hand it over to Christoph, who's going to kick off the webinar. Thank you, Beth. And welcome everybody from my side to this webinar. Today we will talk about three use cases where we can combine DMP3 and IC62850. And we are starting with a quite straightforward use case. That's when you have basically a DMP3 based substation and we add IC62850 to this substation. So the use case looks like it is shown here. We have our relays in the system. The relays have a DMP3 communication as an outstation to our gateway. The gateway has on one side DMP3 for the local substation, our Ethernet, and we also talk DMP3 towards the control center. However, what we would like to achieve is to reduce the bias that we have for application between the relays. So now we also add goose messaging capability to our devices. So the devices will have goose publishers and goose subscribers. Through the webinar, we will look at this use case in different examples. So the use case where we have a protection device somewhere that initiates a trip and in a conventional approach, you would have binary output, which has the trip signal that goes to trip the breaker. But this signal would also be routed maybe on the binary input of a device that implements the breaker failure function and in case the breaker failure function has to operate because the breaker failed, it will send again over a wire the external trip to the devices of the neighbor breakers that need to be tripped in the case of the failure of the original breaker. So again, in a conventional approach, there will be a wire going on binary inputs on all the other devices that interface to these breakers. So as we discussed, the first approach to start using IC62850 would be to get rid of this wiring and apply instead the goose communication service. As both goose is using Ethernet and DMP3 can be run over the Ethernet, we don't need to have additional communication network infrastructure. We just can use the Ethernet network that we already apply for our DMP3 communication. So what you will see then is kind of goose messages coming from these devices sent over the wire with the information, with binary information that represents the strip and external operate signal. To design such a system, we have basically two approaches that we can apply. The first approach, what I would call a bottom-up approach, would be that we simply configure in the IED tools, Vista IED tools for these devices that are able to speak both DMP3 as well as 62850, that we configure their goose messages. We more or less map the IEO points that we had before on our binary outputs and inputs on GGIO logical nodes, create data sets, a goose message with GGIO. And on the subscriber side, we will more or less receive these signals to virtual inputs and can then perform the required action. This is relatively straightforward, but it's not very, very well using the semantic data model 5C6150 as we are using GGIO logical nodes. So a little bit more enhanced approach could be to do what I would call the top-down approach. So now we are using not just GGIO, but we would really use the semantic data model that we have built in into our 62850 devices and use the system engineering tool to configure. So use basically the IC61850 engineering process as we have introduced it in our part two of this webinar series. So now we would use the system tool. We would start with the ICD from publisher and subscribers, typically these protection relays that support both DMP3 or other communications, as well as IC61850. They also support an ICD file for 62850. So we can use that as a basis to create now our signal flow using the standard 61850 engineering process, and then based on that, build our goose messages with the related data sets from the signal flow. So if you look now at the use cases, we would have our protection in this example, distance elements. We have the key element, which is our trip conditioning logical node, so the trip logic that will go on the device that is the protection device to the breaker interface to trip the local breaker. But the signal will also go to the breaker failure logical node where we implement the breaker failure functionality. So we verify if the breaker did open. If it didn't open, we may do a re-trip and if it still doesn't work, we have to do what we call the external operate or the external trip. So we have to send now that signal and that signal will be sent as a goose message to all the devices that are interfacing to the neighbor breakers and it's to be going to the logic node XCBR there as the interface logical node and then we will trip the breaker. That's more or less the application, how it will look in a 62850 model. As a result, we will then now have the goose messages that publish these signals like PTRC.trip or RBFR.opX publish these signals over our communication network. They may be received by the BCUs, by control units that interface to the other breakers. So that's the principle of that use case where we can have DMP3 and IC62850 coexisting. Now let's do a little example followed by a demo later on. So for our example, we are using one ID per feeder which combines basically protection and control functionality. So in case of a breaker failure, an external trip is sent to the other feeders using goose. The device that we are using in this case will be an SEL401 merging unit and control device with an integrated breaker failure and backup protection function. This device supports both IC62850 as well as DMP3. And if we look into the data model of the device, that's the ICD file that we can get from that device. So we see here we have a couple of protection functions, the backup protection. We also have the trip logical node and the key element that we are using for this application is the breaker failure function or the logical node implementing the breaker failure function RBRF, where we have the start object indicating that the function started. We have an op-in which would be a local retrip, but the data that has to be sent over the goose message would be the op-x. And in particular, in our example, the general, we are not doing in the example single phase break operation. So we just need to send the general trip. This would be then sent to the logical nodes XCBR in the other device. So how do we do now the engineering of that? So using the engineering process, as we discussed last time, we would start with a single line diagram. So using an example that I will show here is the Helix SDS system engineering tool that we have also shown in the last webinar. So we start with the single line diagram. And in this case, I have two feeders that we have. So if one breaker here fails, the other breaker needs to be tripped in order to clear the fault. Next we import our ICD files and we create our ID instances. So down here, we see now we imported the ICD file for this CL401 device and we created two instances called F1BCU for feeder 1 bay control unit and F2BCU. We then have to define the functions and as we are only using the goose messaging on 61850, the functions that we are defining are limited to what we need for our little use case, which is basically the trip function of the protection, the breaker failure function, and then the breaker interface function in order to receive the break, the operate external from the other device. So very limited data model, but still we have the semantic model defined in IC61850. When we follow our engineering process, we have to define the single flow for our application. And that's quite simple. In this case, we have to operate external from the breaker failure function in feeder 1 that we have to send to the breaker interface function in feeder 2. And in a similar way, the operate external from the breaker failure function in feeder 2 needs to go to the breaker interface function in feeder 1. So quite simple, straightforward design. Using the top-down engineering process and tools, the next step is now that we have defined this single flow. Next step is to generate the goose control block and the associated database. The HALINX tool basically allows us to do that automatically with various options that we can set. And at the end, what we get is our goose control block. So we see in the logic device, in the device F1-BCU, logic device for configuration. We have a goose control block, which this name and this goose control block is sent to the F2-BCU. In a similar way, we have in the F2-BCU the goose control block that is sent to the F1-BCU. It contains the breaker failure of X signal. Here we see an extract that shows our goose matrix. So we see that goose message is subscribed by the F2-BCU. And this goose message from the F2-BCU is subscribed by the F1-BCU. Okay, now that we have made the design, we can load that design in our tools. And Jackson will show how that is done, and we'll do a simulation of that basically. All right, thank you, Christoph. So what you see on my screen now is Distributed Test Manager, or DTM. So DTM is a test and simulation tool applicable to multiple protocols. Today we'll be using it with DNP3 and 61850. So what we saw in Christoph's last couple slides kind of setting up this demonstration was the engineering process that you might use to configure the goose communications, which went between two devices. So we've taken that SCD, or System Configuration Description file, which Christoph set up using Helix, and we've imported it into our tool, Distributed Test Manager, where I've simulated both of those 61850 devices, seen here on the left. So up here at the top, F1-BCU is a bay controller, which is protecting the QA1 breaker in feeder F1, whereas F2-BCU is a bay control unit protecting QA1 in feeder F2. Now, additionally, I've also set up a DNP3 outstation in this same device. And this is to simulate a device which might have multiple protocols enabled within it. Additionally, I've gone ahead and I've taken my breaker failure signal, which Christoph just described, which will be sent from F1-BCU over to F2-BCU, and set it up so that when this breaker fails, it publishes out that goose message, which is received and will open this breaker should this breaker fail. And additionally, that'll be published out in the form of a DNP message on my DNP outstation that's configured on that same device. So over here in the middle of the screen, you see a one line representing this substation that we've configured. And over here, you see a couple very large buttons, the first of which is my F1-QA1 fault. So when I click this button, it'll simulate a fault being present on this breaker over here on the right, QA1. And if I go ahead and click it, we can see that that breaker does, in fact, open. Now, if I was to reclose that breaker and reset my simulation, and then click these two little check marks over here, this is going to change it such that this breaker is now disabled. Now, when I click the same button to indicate a fault has occurred, my breaker will no longer be able to open. And that will initiate my breaker failure scheme. As part of that scheme, that RBRF, which is the logical node for a breaker failure, will publish out its OPX general, OPX being operate external. And that general message will be set to true. And because it's configured to be part of the dataset that's part of the goose control block, it's going to be sent out across the wire. And this second BCU, which is protecting this breaker, will receive it and open. So let's go ahead and give it a try. As I click the button, you see that this breaker, sure enough, did open. So that's all well and good. But now let's take a look at what it looked like from a protocol standpoint. So first, I want to take a look on the 61850 side. So what we see here on the right side of my screen is a data model which corresponds to this F1 BCU device. As I drill down into the logical device protection, I can see my RBRF. And sure enough, I can see that that value has been set to true. And again, just to reiterate, if I come up here to my goose control block section in my LN0, I can see that goose control block has been configured. I can locate the dataset for it. And I can see the two signals that make up this goose message. And that will be OPX.general, which we just discussed. And then the quality is included as well. And in just a moment, we'll take a look at what that looks like on the wire. But before, I first want to come over to my DNP device or DNP outstation that I've also configured on the same device. So you'll notice it only has one point. I thought it would be ideal to keep it simple. I've named that point F1 QA1 breaker failure. And sure enough, we see that value is set to on as well. I can reset my simulation and reclose my breaker. We see that it's gone back to false. And as I click OK or click Default, it will, in fact, come back to true. So now, in order to just kind of drive this point home, I want to take a look at a couple of our other tools so we can see this visualized on the wire. To do that, I'll be using first TestSuite Pro. So 621850 TestSuite Pro, which is what you see up on my screen now, is a visualization and testing tool that's specifically used for 621850. You might recognize it from some of our other demonstrations that we've done as part of our other webinars in this series. Specifically, what I want to use today is our Goose Viewer module, seen selected down here in the bottom right. And you can see I've got that selected, and I've already begun capturing. And as such, I've already captured several Goose messages that have come across the wire as I've been triggering that fault. And I can click on one at a time, and then over in the right pane, I see the contents of that Goose message. Every time a new Goose message is sent with updated values, not necessarily retransmission, but a new one, the state number will increase, and it will appear in this list. If I was to hold down the Shift and click multiple of these, I can see that the value has changed as I've gone ahead and clicked that button a couple of times over in TTM and generated several Goose messages. And each of these that I've highlighted and selected appear as separate columns over here on the right. And then what's highlighted in yellow is what's changed from one to the next. My quality, indicated by this Q down here, which is one of the data attributes within this data set, stayed constant throughout, but the op-ex or operate external signal from my breaker failure logical node has changed from true to false and back to true. So that's just a way to visualize the Goose signal that was sent between these two devices when the breaker failure occurred. And now in a traditional substation, that might have been a wire, but now we've replaced it here with Goose. But now for the DNP3 component. So while Goose was replacing a wire, we still need to report the fact that this breaker failure occurred back up to SCADA, and that's what we're using our DNP3.4. This might be captured in an alarm on an HMI locally or passed up through a gateway to a control center. To visualize that, I want to show one more tool, which is Protocol Test Harness. Protocol Test Harness is one of our legacy tools. It's used for creating outstations and masters in a variety of protocols, including DNP3, Modbus 101, 102, 103, and 104. In addition to a device simulator for those different protocols, it's a very powerful network analyzer tool. It can also be used for conformance tests for some of those protocols. So now I want to create a DNP3 master in order to be able to read the values from the outstations that I've just created. In this case, I've just used default settings on both ends, so I can go ahead and hit Open. And when I do that, it is, you can see in my protocol analyzer window up here, it's going to do an integrity poll. And so it's going to reach out from my master that I've just created and request all of the data from my, I'm sorry, from the master that I've just created to the outstation. And probably most important, I can see that my binary input zero, which is the only data attribute that I have, is in fact set to true. And sure enough, if I come into my data view window within the tool, I can see the master that I've just created, and I can see that my binary input flag is set to on. And with that, we've kind of demonstrated what it would look like to have a multi-protocol device with both 61850 and DNP, and how that could be used in conjunction in a basic way. Goose to replace our wires, but still holding on to DNP to pass information up to both our local HMI and SCADA. And with that, I'm going to go ahead and pass it back over to Christoph, who will continue with slides. OK, Christoph, you should be presenter. Yes, thank you. So you should see my screen again. OK, so next, the next use case that we are looking at, we'll talk about a second use case, which will be configuring a gateway using DNP XML file with IC61850 mapping. So basically, the situation that we have now is we assume we have an IC61850 substation that we created. So the local communication is always IC61850. We also have a local HMI. But as the communication to our control center, we are still using DNP3. So our gateway now has to convert from IC61850 to DNP3. To perform that, there is an IEEE standard that was developed that is discussing how we best can do that. So the standard that I would like to introduce is IEEE 1850.1. And it is about exchanging information between networks implementing IC61850 and IEEE standard 1815, which is the DNP3 protocol. The standard basically addresses two use cases. The first use case where we have a 61850 client in the gateway. So we have locally a 61850 system. But then remotely, we have DNP3. So in the gateway, we have a DNP3 outstation. The second use case addresses the opposite direction, and it's typically used when we want to integrate in our IC61850 substation some DNP3 devices. So in that case, the gateway will be on one side a DNP3 master to communicate to the DNP3 device. And on the other side, it will be an IC61850 server to integrate the data from that device into the IC61850 system. The standard defines mapping rules to map IC61850 data attributes on DNP3 data points. It also defines extension to the DNP3 XML device profile to describe such a mapping, which can be used to configure the gateway. And we will explain how that works. So this slide shows the use case again that we are looking at for our example, which is the case A, where we have a local IC61850 system and remotely we are talking DNP3. It's a typical use case that we have in many substations today that are using 61850 locally, but still DNP3 is used as communication to the SCADA control center. So from a configuration perspective, we have on one side the ICD files from the devices that we have to integrate into the IC61850 system. But we also have the DNP XML device profile files, which in the early stage describe the capabilities of these links. So basically, what can the master, what can the outstation support with regards to DNP3? One example is typically the indication if they support double bit information or if they only support binary inputs or single bit, because that has, as an example, an impact on how we would map the position of a breaker onto the DNP3 system. When we have done the engineering, we may have an output file, which is a DNP3 XML device profile that may contain the IC61850 mapping. And I will show how this is integrated. And we also may have, or we have, of course, as well from our design of the local substation, an IC61850 STD file. Also in this file, we can optionally include the DNP3 mapping information. The standard describes, so the IEEE 1850.1 standard describes multiple scenarios, how we do such a design. The scenario that we'll have a closer look at is called scenario A1. And that's basically the scenario where we assume we have a new system so we can create the point map of our DNP outstation that communicates to the SCADA system. We can create that point map based on the signals we have, so we don't need to map on an existing point map. So the workflow would basically be we configure and select the 61850 signals that we need to send to the gateway. We can do that by selecting the data, configure reports and data sets as we would have as we are sending them to the gateway. And then from these data sets, we can extract the information that we need to send forward. So in the next step, we can create DNP3 point list from the data sets associated to the reports. Based on mapping rules defined in this standard, we can identify what kind of DNP points we have to create. And then you can create the configuration file, which basically will be the DNP3 XML device profile with mapping information. So in order to better explain and understand how that works, I will go a little bit more in the details of the IEEE 1850.1 standard. So that standard basically defines mapping principles. The mapping depends on the IC61850 function constraint. If you have status or measurement information, so the typical operational information that we cannot write, we map this on inputs in DNP on DNP input points as these data are provided by the process and we cannot really write on them. However, if you have data attributes with the function constraint like set points, set points can be written. So we use DNP output points in that case. The standard also defines some specific rules for certain 61850 data attributes. As an example, enumeration attributes that we have like the data attribute control model that we use to configure what control model that we are using like select before operate or direct operate. This is an enumeration and this is defined to map on DNP3 analog output. The large part of this 1850.1 standard defines mapping tables that are provided to specify mapping on one side of the IC61850 common data classes, but also of the what we call the constructed attribute classes that are defined in the part 7.3 of IC61850. And I will give you a few examples, extracts from that standard. The first one is the mapping of analog value, which is a constructed attribute in IC61850. It contains both an integer representation of the value as well as a floating point representation of the value. This DNP mapping standard defines that in both cases the preferred, so the first choice of mapping would be to use an analog input of the type floating point in DNP3. And if that is not available as a second choice, we would use a normal, an integer analog input. Depending on what we are using, the mapping rule that are defined is either int to analog input or int to analog input floating point for the integer value in IC61850, where we have in IC61850 a float value, the mapping rule is float to analog input or float to analog input floating point. This mapping is applied when we have the analog value in a data attribute that is a measurement. If you have the analog value in the data attribute that is like a configuration attribute or a setting set point, then we would preferably use analog outputs as we can in that case also write the values. However, IC61850 provides the possibility that an IED may have certain configuration attributes predefined and they cannot be changed. As an example, a deadband parameter could be a case where you have a predefined value and it cannot be changed. So in the case if that cannot be changed, which we can see in the ICD file from the device, we also will map it on an analog input. So that's more or less what is defined in this IEEE 1850.1 standard. Now let's have a second mapping. Let's have a look at the second mapping. And I use here the mapping of a common data class. And the common data class we are using is the ACT, Protection Activated Information, which has multiple attributes like general, phase A, B, C, neutral. All those, if present, each of them shall be mapped on the individual binary input following the rule Boolean to binary input as those data types in IC61850 are all Booleans. We also have quality and timestamp associated in IC61850 and DMP3 has as well quality and timestamp elements. So there is some specific mappings defined in DMP for that. Certain information like the description that we have in IC61850 are not supported directly by a mapping into the online data model in DMP3, but the DMP device profile supports the possibility to have a description on the data point. So the description here will only be mapped into the device profile, but will not be available as a DMP data point. OK, now let's have a look on the mapping of quality and IC61850 has a generic definition of quality. However, DMP3 has different variations for binary control and analog points. What I show here is the mapping of quality for binary information. In IC61850, what are relevant for binary information is the validity, where we have basically valid, invalid or questionable DISMAPs on the DMP3 flag online. And like in IC61850, when validity gets invalid, there may be other attributes in the detailed quality set as well in DMP3. So in IC61850, some of our attributes that are what we call in the detailed quality like oscillatory, old data, we also have a corresponding element in DMP3, which will be the chapter filter or communication loss for those. IC61850 also supports the possibility of doing substitution, in which case the source instead of being processed would be substituted in DMP3 that will be reflected with the DMP3 quality local forced and remote forced. Test or operator blocked, which are additional elements in IC61850 would also test would be either local forced or remote forced. Operator blocked would map on remote forced. So again, that's the mapping as it is defined in our DMP in this 1815.1 mapping standard. Mapping of timestamp, we basically have besides the timestamp that we can map into the DMP3 timestamp, IC61850 also supports a timestamp quality, a time quality. So the time quality directly is not mapped, not supported by DMP3, but we can map the attributes that we have in the time quality on binary input and analog input data points. So there's also a mapping defined how this can be done. The other attributes of our timestamp, they directly map into the DMP3 time, preferably if DMP3 doesn't support time, they can map on two analog input or analog output points. But as I mentioned, the time quality cannot be directly mapped, but the 1815.1 standard also defines a way how we can map them on individual binary inputs and analog inputs. OK, now that we have seen how the 1815.1 standard is basically built up, we will talk a little bit about the DMP3 XML device profile. DMP standard defines an XML based way to describe the device profile, which contains multiple sections. In the mapping standard, there is one section added, which is called the IC61850 device mapping. And this section, together with the section of the profile that defines all the data point lists, can be used to describe the mapping. The mapping can either be rule based, based on the rules we have just had a look at, or it can be equation based, based on the equation. As an example, when we have to map a double point on two binary points, we will use an equation based mapping. But in the most cases, we'll use a rule based mapping. The slide here is just an example how this device, how this mapping is defined. We have one example of a rule based mapping, where we have the rule, we have a DMP3 pass, and the IC61850 pass, and we will look at that in our example later. But here we also have an example of an equation based mapping, where we basically have an equation that converts some analog values based on scaling. OK, with that, we will go to a second example and demo. And the example is basically how we map 61850 to DMP3 in a gateway. So an example, we assume we have a substation with IC61850 that was configured using the standard 61850 engineering process. We have reports and associated data sets that we use to configure the reporting from the devices to the gateway. And from those data sets, we will use a tool that can extract our leaked data attributes. That's where we have the values and can apply the mapping rules defined in IC61850.1 standard to determine the DMP3 points. And that's what Jackson now will demonstrate. OK. All right, thank you, Christoph. So as Christoph mentioned, I'll be talking about how you can automate the process to map between 61850 and DMP3. But to kind of do that, I want to start out by showing what it means to configure 61850 MMS reports. So to do that, I'll be using SCL Navigator, which is what you see up on my screen now. SCL Navigator, short for System Configuration Language, is a tool that's used to edit and view SCL files. In this case, I'm viewing the SCD file, which we discussed earlier. So this is the same one that I used to create my simulation over in DTM. Last time, I talked a little bit about what the different sections are. Today, we'll be focusing mostly on the IED section with a little bit of a data template section. Now, remember, the IED section is where we define the data models and the capabilities for our individual IEDs, which make up our substation. In this case, we have three IEDs configured, the two bay controllers that we saw over in DTM, as well as a client configured. Now, the data template section down below, let's say that we were to use some of the same data objects or logical nodes between these two different IEDs. Instead of needing to define the data models completely in each IED section, we can just reference different logical nodes. And those logical nodes can reference different data objects. And those are defined only once down in the data template section. And that will come into play here in just a moment. Starting with this top IED that we've configured, I'm going to expand it out, come down into the access point section, look at the server that we have configured on this device, and then look at the top logical device config. Within that, we have our logical node 0, where we can hold much of our configuration information. And specifically, this is where our report control blocks are configured. So here you see we have two different report control blocks configured, sorry, down here. And each of those is associated with a data set defined in this first tag. Now, those data sets are linked to just right up here above. And if I expand out those data sets, I can see the data that's actually going to be sent out in the form of the 61850 report. So just to recap, we have 61850 reports enabled on this device. So these are messages or collections of messages that will be sent up to SCADA. And any time, based on our trigger options, that any of the data contained in one of the data sets associated with this report, anytime it changes, we're going to send out that associated message. So now here we see, as I mentioned, the different data objects that make up this data set. But data objects aren't really where the story ends. So data objects are just containers, much like logical nodes. It's just one step lower. And data objects contain different data attributes. Data attributes are really what correspond to individual points in DNP3. So in order to know what data attributes are going to be included in this report, I first need to look at the DO, or data object that's included, and look at the functional constraint that's associated with it. And then if I right-click and locate, navigate to data. Okay, so this is a definition for the RBRF, that op-x that we are looking at that was part of our report data set was in the RBRF logical node or in an instance of it. And then this op-x is type ACT. So if I want to know what data attributes actually are contained within op-x, I can right-click and navigate to my template for this specific data object. And as I expand this out, I can see the actual data attributes that will come across the wire when this MMS or 62850 report is published. And the point that I really want to drive home here is these are the leaves at the end of the tree. This is kind of where it ends. And this is what maybe most directly corresponds to an individual DNP3 point, whereas the data object is just a container which holds multiple of these, in this case, 1, 2, 3, 4. The data attribute is a single point in my DNP map to relate the two. So now I've created my MMS report. So this is what's going to be published by my devices and going to be received by my gateway. But now I want to configure my gateway with what DNP3 points are going to be coming out of it. In this case, we do have a tool here in SEL Navigator in this build that allows me to create that DNP3 point map. So what it's going to do when I click this button, it's going to go in and look at the reports that I've generated across all of the devices in this SED file. It's going to look down into the data sets of those reports. And then based on the data attributes that contain that data set, so for example, this opx.general data attribute that I see here will correspond to this opx.general that's here at the top of my DNP mapping list. And it's going to do that for each and every data attribute in each and every data object that I have within my data sets of my different report control blocks. Now, when I click generate CSV file, I've already done this and saved it out. And we can take a peek at what that's going to look like. So this is a template for this DNP3 data map that our gateway is going to be outputting on the other end. Names and descriptions are something that Christoph is going to be going into a little bit more detailed later. But I do want to draw our attention to point type and index. So point type, as I mentioned, this opx.general is a binary value or a Boolean in 61.850 parlance. And that corresponds to a binary input in DNP3. In the DNP3 standard, 1 is used to indicate that data type, binary input. Whereas this position value for my XCDR position in 61.850 is a double bit indicating four possible states, open, closed, intermediate, and unknown. And that corresponds to a double bit in DNP3, which we define with point type 3 in the standard. And then finally, down here at the bottom, we have some analog values, our three phase voltages and currents. And those are going to correspond to analog inputs, which are defined by the number 30 in DNP3. And then lastly, what the tool did for me was create these indexes. It really just does this automatically. You could come in and edit them should you choose. But it basically starts, well, I've got three different binary inputs. I need to start at index 0. And there's just increment from there. Once I switch to a new data type, in this case 3, we said corresponds to double bit. Again, we start at 0 and increment, so on and so forth. Now, in this case, we've exported this as a CSV, which I'll be using a little bit later to give you a preview in DTM. But we could imagine a future where you would also export this into a DNP XML device profile should your gateway support that as an input. And to give us a little bit more insight into what that DNP XML device profile that contains that mapping looks like, I'm going to turn it back over to Christoph, who's going to explain that a little bit more. Christoph, back over to you. Yep, thank you, Jackson. And let me share my screen again. So in order to have an easier view, I created here a screenshot that summarizes what we have seen. So basically, as we mentioned, the DNP XML file will define all the DNP points. An example, the binary input points, it will define each binary input there with an index. And we can give name and description. The key is also that we have to change event class. In our mapping section in that same file, we have now for this data point that we have just looked at to operate x, we have the mapping rule. So we have a rule-based mapping that we are using here. The rule is from the standard, which is called Boolean to Binary Input. And what we see, we have the path of the DNP data, which refers down to the data point list in the same DNP XML profile file. It goes down the hierarchy to the input point with the index 0. But then it goes also to the data, and it's the element state of that input point that we are mapping. The same way we have the IC61850 path, this points now into the ICD, into the SCD file. And it basically points to the device, F1BCU, Logical Device Pro, what we see here. The Logical Node Breaker Failure, RBRF, the prefix BFR1, RBRF, the data object of x, and the data attribute general. And we also indicate the common data class, because that's what we are using for the mapping, the data type of this, and the function constraint. So with that, we have completely defined the mapping between our 61850 world and the DNP world that we are sending on the other side of the gateway. And before we go to the next use case, I would hand over to Bess for the first series of questions. Thanks, Christoph. Yes, we have had several questions come in that I see are being answered online. I will pull a couple of them, and we are running a little bit tight on time, but I will try to get at least two questions in here. The first one, this is for either Christoph or Jackson, whichever one wants to take it. We have a controller which does not support DNP3, but supports IC61850. Is it possible to support an external DNP master or outstation to integrate to our controller which supports 61850? Yes, I mean, I think that ties in really well with Christoph's last point. So I think you would want to use a gateway or maybe just a protocol converter, and those two really overlap in this case. But yeah, you would want to have some device such as our gateway, SCADA Data Gateway, which could convert between DNP3 and translate that over to 61850. And you could leverage the 61850 engineering process as just discussed in order to make that configuration a little bit easier. SCADA Data Gateway, our solution for that exact problem, is a software-based solution. So you would install that on a Linux or Windows PC. OK, thanks, Jackson. Like I said, we are running a little bit short on time. We only have about eight minutes left in the webinar, so the questions are continuing to be answered by the engineers online. So please continue to submit them. But I'd like to switch back to Christoph so we can make sure we get through all the material. OK, thank you, Beth. So yes, now we will have together a look at the last use case, which is basically using the 61850 engineering process by combining IC61850 and DNP3. So the use case is a little variation of what we just discussed in the gateway. Now we would assume we are still using DNP3 in the substation to communicate to our gateway. But we also have 61850 support, so our device is multi-protocol support. It supports also IC61850 for the communication to a local HMI. The basic idea of the approach now is that we use, of course, the 61850 engineering process to configure the 61850 aspects of the substation. But we use that process as well to configure the signal flow to the gateway, even that this will be used DNP3. The reason we can do that is because typically the model includes mapping to device internal variables and we show how we use that. So basically using the same approach that we discussed for the configuration of the gateway, we can determine the DNP3 points that we use for the communication to the gateway based on our signal flow that we have configured in 61850. So the workflow is summarized here, but I think it will get a little more clear when we then just go through the example. So we would import our ICD files for the IEDs in the 61850 world. And as I mentioned, this device that supports both IC61850 and DNP3, they also have an ICD file. We will then configure the signals that we send both to the HMI and gateway using 61850 signals. But having in mind that the ones to the gateway, we will use DNP3 points. From the data sets, we will, as it has been shown before for the gateway mapping, we can now extract the information that we need to create as DNP3 points. And instead of using that extract for the mapping, we use it now to define the flexible point map from the DNP device. So we create now the configuration file for the device, which can be a DNPXML device profile, but it can also be a vendor specific file. So how does it look? So basically, the ICD that we are using, the cell F401, the ICD file of that device, we have seen before already. We have, as an example, our breaker failure logic and notice signal OPX with the attribute general. But what this ICD file contains as well is a reference to the internal variable. So for all of the data that this device supports, we have reference for the internal variables. So when we extract the data points that we need to send to the gateway, we can also extract the internal variable that this corresponds. So for an engineering process, we now define our data model, the full data model, not just what we used initially for the goose. And we can configure with a tool, we basically can select with our system configuration tool, the Helix STS. We can select the data points that we want to send to the SCADA gateway or that we want to say to local HMI. And based on that, we can configure our reports with the corresponding data sets with all the data members. Now we can use the same tool that Jackson has shown before to extract the data that has to be sent to the gateway. So we can extract and create out of that a DMP3 point list. As we have as well the internal variable name available, we can also include that variable name in our DMP mapping point list. And depending on the device, you can now use that to configure your DMP point list, the points that will be sent to the gateway. In our example device, the device accepts a file that basically introduces the points and links them to the internal variable that we found based on our ICD file. So with that, we can use this SCI-C6950 engineering process, extract the information that we need to send over the DMP link to the gateway and configure this DMP points in the device. And Jackson will demonstrate how this will then look. OK, Jackson, you should have the screen. All right. All right, thank you, Christoph, and thank you, Beth. So just to briefly recap, we're back over here in SEL Navigator where we've set up our data sets. And now instead of just creating a CSV file that we want to use to configure our gateway, we want to create a CSV file that we can load directly into our device to configure a multi-protocol device. And this is in a situation where we might be sending, as Christoph described, MMS out to our local HMI, but DMP up to a gateway, for example, for our off-site control center. So just to reiterate, once I have my MMS reports configured or my 61850 reports configured in my data set, I came up here and I created my DMP point map, and that gave me this CSV file. Now, using the proprietary format, in this case, we have it configured for DTM, but you could, with small changes, have this be set to, for example, the SEL format that would allow you to configure their DMP outstation. In any case, once you have this CSV file, we're going to come back over to DTM or Distributed Test Manager where we started the day. Now, remember, I have the same two sets of devices or IEDs, and one of them has a DMP outstation on it. This is my multi-protocol device. So now I can edit the database, the DMP3 database, on this DMP3 outstation, and instead of having just a single point that I had earlier, I'm going to import the CSV that I output as a result of the engineering process using SEL Navigator. Now, when I hit OK here, you can see that it's gone ahead and created all these points. I'm just going to hit OK one more time, and then we'll see that it does import all of my tags right here into DTM on my DMP3 outstation. So this is really applicable in a case where you have a configurable DMP map on your outstation. So now, just like we did before, I could do the same test, write trigger a fault on feeder 1, and it's going to open my breaker on feeder 2. But now we see I have more points configured in my map. My operate external, my operate input, and my start all have changed to true. And then the same thing over on my 61850 server before we looked at goose control box, but now I also have reports configured on this device as well. And we'll see if we look at the data set for this report that we've got that same information available. So now to bring it all back together, I could look over in Test Suite Pro to see this report being published. And to do that, I'm going to just drag Test Suite Pro back over onto the screen. And you can see several different reports have come in. So remember those three values that changed in DMP? That was the start, the operate, and the opx. Because those changed at three different times, we actually generated three different reports. If I was to do a reset, for example, I'm just going to click that real quick, we'll see that I generate one more report that comes in that does contain all three data attributes. And that's because they're all three reset at the same time. And again, this is just showing in my report viewer over in Test Suite Pro the SCADA messages that will be sent out to your local HMI. In order to see the DMP3 messages that would be sent up to maybe my gateway or my remote control center, I'm going to go back over to the tool that we looked at earlier, which was Protocol Test Hornets. And I'm going to create a SCADA master again using the same process as before. Just open DMP3 master. I can use the default settings in this case. And when I do it, it does do that same integrity poll. But in this case, instead of seeing a single binary input like we had before, we see that we've retrieved all signals or points that exist on this DMP3 outstation. And if I come back over to my data view, I can see that a little bit more clearly in the form of what those values are currently at. Now, if I do want to incorporate the names that I've set up as a part of my engineering process, I can do that as well. By default, DMP3 can discover the data points that are on your external device, but not the names. But if I want to apply that same file, the CSV file that I set up before, I can just do that here. And we'll see that now the descriptions have updated to include those that I defined as a part of my engineering process. So now I've kind of been able to leverage that process, which we traditionally think of being associated with 61850 communications, and apply just the whole 61850 engineering process to define my DMP3 data map, and apply that not just to my device or my outstation, but also to my master as well. So with that, we are kind of coming to the end. I'm running out of time. So I do want to just wrap it up with a couple conclusions before I turn it back over to Beth for some Q&A. So bear with me for just a moment as I pull up my conclusion slide. Okay, so I hope that we've been able to demonstrate today that both DMP3 and IEC 61850 can and should coexist, and there are many reasonable use cases to have both in the same substation. That we've shown that 61850 can be introduced stepwise. You can start out with goose and kind of work your way up to have a full 61850 system. And you don't need to redesign your whole system to introduce individual pieces. For example, replacing wires with goose. And then finally, I hope that we've demonstrated that applying the 61850 engineering process to DMP devices introduces a machine-readable format in the form of the SED file and in the different proprietary CSVs or DMP XML device profiles that you generate from that, and that automating these steps allows for tools to better visualize your design. Thank you so much for attending. Please stick around for questions. And with that, I'll turn it back over to Beth. Thanks, Jackson. Yeah, so we are a couple minutes over. I appreciate everyone hanging on the line a little bit longer. I do have a couple of final thoughts on the webinar I'd like to review real quick before I switch into questions. When you sign up for the webinar today, you will be presented with a survey. It only takes about two minutes. We are always looking for your feedback, so I really appreciate it if you take the time to fill that out. The recording for this webinar will be posted on our homepage in a couple of days, so you can get it that way. If you fill out the survey, you will also get a follow-up that will include a link to the recording, as well as the presentation will be attached. You can also go to our website and sign up for our email newsletter, which will keep you notified about future webinars, training events, any product releases. The sign-up form that you see on the right-hand side of the slide is what you'll see on our website, so please, if you're not already signed up with that today, go ahead and get on that email newsletter list. And I also want to mention that any of the tools that you've seen here today, in addition to the SCADA Data Gateway, which came up through the questions, you can go and download an evaluation license and try out the tool at any time from our website. So if you go to the website and you set up an account and log in, you'll get a three-week evaluation license. Again, that'll be for our tools and for our gateway. Specific to the gateway, a couple of questions came up about a proof of concept. We can provide longer-term evaluation licenses for that to support your proof of concept. So please just email sales at trianglemicroworks.com at any time if you need a longer-term license. So with that, we've still got a number of folks online. I will switch back over to questions. We've had a – actually, this one is for you, Christoph. I'm using IEC 6870-5-104. Is there a similar standard using – to 1815.1 for the mapping? Thanks, Beth, for the question. Yes, there is. So like we have the IEEE standard for the mapping of 6150 on DMP3, there is within the IEC 61850 series the technical specification IEC 61850-80-1, which addresses the mapping between 61850 and 875-101-104 protocol in a similar way. Okay. This is more of an opinion question, really, for both either Christoph or Jackson. Is it common to use gateways to translate between 61850 and DMP3? Yes. Yes, I think, as I mentioned – I think it's very common. It's common to use MMS in your substationing – please elaborate. Sorry, go ahead. You're breaking up a little bit, Jackson. Christoph? Okay. So, yeah, it's very common. Typically, when we have existing systems, you have the DMP3 communication to the substation, so maybe 875-101. So when you introduce IEC 61850 in your substation, that does not mean that you change now as well the SCADA communication up to your control center. So applying there 61850 is – applying there and mapping a gateway that maps between the traditional SCADA protocols and the IEC 61850 is very common. Okay. Next question. Christoph, this is for you. It's specific to 61850, I believe. What would happen to a network when there are two IED devices with the same IED name? That would not be a very good idea because the standard requires that the IED names are unique on the network because the IED name is part of the address that we use to identify the data. So the IED needs to be unique. And typically, you have naming schemes how to create these names for the IEDs so that they remain unique. Okay, well, I believe that wraps up all our questions. Everything else has been answered online. So again, I just want to thank everyone for attending and for staying on with us a little bit longer than scheduled. Please fill out the survey and we will follow up with additional information. So thank you, everyone.


Video ID: KFIEPXH0XjU
Welcome to the Triangle Microworks IEC 60870-5 communication protocol training videos. This video will give a brief overview of the 60870-5 protocol, which sometimes I'll refer to simply as 60870, and will also contrast the protocol somewhat with the Distributed Networking Protocol, or DNP3, which I'll sometimes abbreviate as DNP. The complete training consists of five videos. This first video contains an introduction and provides an overview of the protocol layers in IEC 60870-5. It also dives into the stack layers, particularly with regards to 60870-5-101 and-104. The second video goes into detail about ASDUs, or Application Specific Data Units. The third video describes the application layer. And the fourth video discusses the link, transport, and physical layers, diving into a little bit more detail about polling, controls, and the link layer in general. And then the fifth and final video describes security in 60870-5 and provides the conclusion. All right, so let's start with a quick history of 60870-5. This protocol goes way back to the early 1990s. They started working on standardizing a protocol for telecontrol and SCADA systems. The first subpart was published in 1990, and the original subparts talked mostly about the lower level layers. And finally in 1995, the 101 spec came into being. The 101 spec basically leveraged all of the original subparts. In 2000, the 104 spec was introduced, which is very similar to 101 but focused more on the TCP IP applications, whereas 101 is serial. DNP is also based in part on the 60870-5 subparts, so it is somewhat similar. So throughout this presentation, we'll point out some of the major differences between the two standards. In the standardization effort, there were a lot of objectives for SCADA. Basically, the main idea is to be able to send data from the outstation or the remote device to the master. Also, they wanted to be able to send controls from the master to the outstation and to do both of those in a very reliable way. So the protocol includes some quality bits and a CRC and checksum to make sure the data and the messages are correct. Another goal of the protocol was to make sure that bandwidth was optimized so it's not sending a lot of unnecessary messages. Now remember, in the early 90s, bandwidth was pretty expensive. So unlike today where we kind of take gigabit Ethernet for granted, when you're dealing with dial-up lines or slower, minimizing protocol overhead is a big concern. The design committees also wanted to provide the ability to have timestamps and other functions like that. And some security aspects were recently added in order to prevent unauthorized access. One of the most important goals was to serve as an open standard so that you would have interoperability among different vendors. Something we kind of take for granted now, but was a pretty big deal in the early 1990s. Obviously there's a lot of benefits to open standards. You're probably aware of many of them, including interoperability among vendors and fewer protocols to support for utilities. In the case of IEC 60870-5, there's really not an official users group. The protocol is controlled by the IEC working group, and that working group meets on an as-needed basis. In recent years, the working group has mostly been focused on work relating to security features. 60870-5 is primarily used in Europe and in the Middle East, but in several other parts of the world as well. Typically it's either DNP3 or 60870-5, which is primarily used for distributed SCADA systems. Kind of a general rule of thumb, and it is just a rule of thumb, so of course there are exceptions, but the general rule of thumb is that English-speaking countries tend to prefer DNP3, whereas non-English-speaking countries tend to prefer 60870-5. 60870-5 is being used primarily for electric utilities, but we are seeing it be used in some other industries as well. DNP3 went into other industries a little bit faster, but 60870-5 is spreading. And of course, as I mentioned earlier, security features have been recently added to the protocol. If you look at the standard itself, there's several different parts. It's an IEC standard, so the IEC typically likes to break things up into lots of different parts. If you're familiar with 61850, there are lots and lots of parts to that standard, and 60870-5 is kind of similar. In 60870-5, the first parts here basically are talking about the different layers. So for example, 5-2 is primarily talking about how the link layer is built up. 5-3 through 5-5 are focused on the application layer, and so it's following that IEC architecture, kind of a similar way that 61850 does. In the first five sections, it's talking in kind of a very generic way, and then in the 101, 102, 103, and 104 sections, they're basically referencing the first five sections and explaining how those layers can be used to accomplish these different requirements. 101 is designed for serial-based communications for telecontrol, 102 is used for sending measurements, 103 for protection functions for substations, and 104 is really telecontrol for 101 that maps to TCP IP. So it's kind of building up the different layers and referencing several different standards in the overall architecture. One big difference here is that D&P 3, once it was standardized by the IEEE, everything was put into one document, which is kind of convenient. The IEC does not work that way because of the way it's architected, and so you have these several different sections. All right, let's move on into the real stuff. 60870-5 is designed to support several different topologies, very similar to D&P 3. One of the more common topologies is where you have a controlling station and individual point-to-point connections to the controlled stations. I should point out that 60870-5 uses this terminology of controlling station and controlled station, while D&P 3 uses master and outstation. So the controlling station in 60870-5 is equivalent to a master station in D&P 3, and a controlled station in 60870-5 is the same as an outstation in D&P 3. They're basically the same, just different names. Much like D&P 3 serial connection, 60870-5 can also support a multipoint topology. There's a couple of different names for this, but basically N101 uses the unbalanced mode, which we'll talk about a bit later. This supports where a controlling station can connect to several controlled stations on a common physical layer and then pull those stations using the link layer without having any collisions. So that's basically used for multi-drop serial support. And of course, with the point-to-point approach, hypothetically, you can also have multiple controlling stations for a single controlled station. This isn't really addressed in the standard, but the point-to-point approach would support that. When we get into the layers of the stack, there are some major differences between 101 and 104. In this section, I'll point out a few of those differences. The 101 stack is based on the Enhanced Performance Architecture, or EPA, which is the same architecture that D&P 3 is based on. However, D&P adds a transport function, sometimes called a pseudo-transport layer, which is not used in 101. So basically, you just have, in 101, the three layers. There's the application layer, the link layer, and a physical layer. The application layer, of course, deals with all of the data objects, polling, reading, creating events, sending commands, supporting timestamps, and clock synchronization. All those kinds of things. The link layer is really there to provide things like the checksums to make sure that messages are sent without errors. There's also the ability to have addressing at the link layer, so that's very similar to D&P 3, and that feature helps support multidrop topology. It also supports link layer confirmations, and with one special case in 101 with multidrop, it allows the master to control how the outstations are replying. That's especially used for things like polling. But that's what the link layer is for, and of course, the physical layer is intended to support serial applications. In the standard, they reference V.24 and V.28. This is basically the equivalent of RS-232 and RS-485 serial. Of course, you can support other serial mediums like serial radios as well. So that's very similar to D&P 3. The only big difference here, as I mentioned a minute ago, D&P has that transport function, which can break up the application messages. But that is not done in 101. The 104 layer is dramatically different, and this is because of the common architecture that IEC defines. Basically, everything under the application layer is completely different, and it's mapped to the TCP IP protocol suite. So the application layer is basically the same as 101. It has ASDUs, which I'll get into in a moment. They're basically the same with a few differences. But there's a special 104 application to the TCP interface, which we refer to as the 104 link layer. This is basically put in since you don't have the link layer that you have with 101. So they added this in to add some of the same capabilities and to interface to the TCP part of the stack. After that, everything is basically the standard TCP IP layers. So of course, it does support Ethernet. So there's a big difference here with D&P 3, because the way D&P 3 interfaces with TCP IP is to take the entire protocol stack and encapsulate it in a TCP IP message, whereas in 104, they actually pull out the link layer and replace it with the TCP IP stack. So that's a fairly big difference between D&P 3 and 60870-5. The 101 and 104 application layers basically provide the same functionality. So it includes things like addressing for data objects, data transfer and processing capabilities, control operations, and polling. We'll get into all of these and basically how that's structured in just a bit. I pointed out one big difference here in the polling, which we'll get into in more detail. But basically, there's no equivalent to polling for a sequence of events like you have in D&P 3. So we'll talk about what that means in a little bit more detail later. Some of the big differences at the application layer between 60870-5 and D&P 3 are shown in this slide. In 60870-5, you're only limited to one data type per message. In D&P 3, you can mix data types if you want to. 60870-5 also limits you to controlling one point per message, unlike D&P 3, which can allow multiple points to be controlled in one message. And 60870-5 does not have the equivalent to D&P 3, where the master can confirm that it's received events before the outstation clears its buffer. The standard does kind of refer to an option of using link confirms before clearing the buffer. And I want to point out that we do that in the Triangle Microworks stack. So the Triangle Microworks stack uses link confirms to try to verify that events were received by the master before those events are cleared from the buffer. 60870-5 has no equivalent to D&P's internal indication bits. There are some similar capabilities, although it's not implemented in the same way at all. Instead of the IIN bits, probably the most similar thing in 60870-5 is the cause of transmission, which is included in the ASTU. With 60870-5, there's also a different way of addressing data. There's a capability to have multiple sectors to differentiate the data address space. We'll go into that in a little bit more detail in a bit as well. So those are kind of the high-level differences. And as we go on on several of the slides, I'll point out specific things that are different. So this concludes the first video on the introduction and protocol layers overview. So please continue on to the second video in this series, ASTUs.


Video ID: o442gb8GimQ
Hello, everyone, and welcome to today's webinar, Virtual Isolation and Testing with 61A50. My name is Beth Kapalis, and I'm the Marketing Manager at Triangle Microworks. Before we get started, I'd like to go over some housekeeping items and some introductions. The webinar is scheduled to be one hour long. We will be stopping for questions a couple of times throughout the webinar, and we will stay on afterwards to answer as many questions as possible. Please submit your questions as soon as you have them. Don't feel that you have to wait for a break or wait until the end. You can submit them online using the webinar menu under the section titled Questions. We have a team of experts ready to answer questions and will respond directly to you, not the entire audience. So the questions are actually not seen by the whole audience, they're only seen by the folks answering the questions. So don't hold back. We do want to keep these guys busy. We will stop a couple times throughout the webinar to read and answer some questions. And as I mentioned, we'll stay on afterwards as long as possible to answer as many questions as possible. I'd like to start with introducing the folks that are behind the scenes. These are the folks that will be responding to your questions directly. I'll start with Dave Gafner. He's our Senior Developer for Distributed Test Manager, also known as DTM. That'll be one of the tools you'll be seeing here today. We also have Sri Engla, who's our Senior Developer for TestSuite Pro, also known as TSP. That's the main tool that you'll be seeing here today. And last and certainly not least, we have Christoph Brunner. He's the Convener of the Working Group 10, which is responsible for the core parts of 621-850. Next, I'd like to introduce our speakers, the folks you'll actually be hearing from. So first, we have Joel Green from Triangle Microworks. Joel has worked with communication protocols for over 20 years, with the last 10 years focused in the power industry. He's the Lead Engineer for 621-850 Development and Products at Triangle Microworks and is heavily involved with 621-850 industry groups. He also works as the Co-Editor of 621-850-7-2. He is a member of the UCA Test Committee and Test Procedures Working Group and involved in the IEEE Standard Association. Our second presenter is Matt Green. Matt is a Senior Application Engineer here at Triangle Microworks. He's been with us for about six years and he specializes in QA testing with all our simulation tools in Gateway. Matt is also one of our Lead Product Trainers and Presenters for webinars and for customer demos. So now that you know everyone, let's talk a little bit about Triangle Microworks. So we were established in 1994 and we have been in business for over 25 years. We're headquartered in Raleigh, North Carolina, and we do have customers in over 70 countries. Our customer base consists mostly of equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. The first category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs and their development time. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They're designed to troubleshoot, to simulate, and to automate testing. And lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with all the SCADA protocols as well as OPC. As a result of all our work with the SCADA protocols, we are heavily involved in the standards committees that support these protocols, such as UCA Group for 61A50 and the DMP User and Technical Committees, which are just a couple of examples. Through our participation, we help define the protocols, we stay up to date with the changes taking place, and ultimately provide feedback based on our customers' needs as those protocols continue to evolve. So now that you know a little bit more about TRIANGLE, I would like to go ahead and hand it over to Joel Green to kick off the topic that we're here to learn more about. Thanks, Beth. Hello, everybody. Let's talk a little bit about the testing features of 61A50 this morning. 61A50 was originally designed to incorporate new technologies to improve system robustness in the power industry. As we were – as the folks who were designing the standard originally, we saw the need to provide testing methods to improve what's done in traditional power systems, which we can take a lot of advantage of software-based devices as these intelligent devices are being added to the substations. One thing we talked about a little bit in a previous presentation is design for test. In the system engineering process, it's important to allocate resources for these testing features that we're about to discuss. Keep that in mind as you start building systems. Let's get into a little more detail. In a traditional substation power system, we have copper wires connecting the control devices to process equipment. So traditionally, we'd have our CTs and our VTs, our current and voltage measurements, and those measurements would be piped in to the devices through analog signals. We've got control points monitoring here. Very simple to understand what's going on here. Follow the wires. Very labor-intensive and sometimes dangerous to maintain. When we test this system, we bring in a piece of test equipment that can simulate these analog signals, our voltages and our currents. Then we take our screwdriver and we remove test jumpers throughout the system so that the devices under test are responding to the signals injected by our test equipment. Pretty straightforward stuff, easy to follow, easy to understand, difficult and expensive to maintain. In the 61850 world, we kind of go about this a little bit differently. With the digital substation, we now have devices near our analog signals that collect that information, turn those analog measurements into digital signals, send those over an Ethernet, what we call the process bus. So these are now Ethernet packets carrying the data from the input devices. And all the systems, all the devices that need those data, that information throughout the system can respond to these multicast Ethernet packets. So this changes our testing methodology quite a bit. We can no longer disconnect a device using a hardware jumper because all these signals travel over the same Ethernet network. So what we do in this case is we've got simulated messages defined. And so we bring in a piece of test equipment that plugs into our Ethernet network. And it injects packets very much like the ones coming out of our IO devices and puts them on the network. The way that we'll do this requires us to identify those messages as simulated messages since they're coming from a piece of test equipment. And we have to be able to control which devices are responding to those simulated messages rather than normal messages because the normal messages also stay on the wire at the same time. So let's look into a little bit about how that all happens. We've got a couple of different distinct functionalities in 61850 to allow all this testing to go on. And two of the main concepts are the simulated mode we talked about briefly and the test and block data qualities. So we've got the mode and behavior. Each logical node has a set of data objects. Each logical node is a controllable object that allows us to change the setting on each logical node or logical device. And devices can be configured in a hierarchical manner to inherit this behavior all the way up and down. So behavior is the status that controls what the device is doing and what it's responding to based on the mode setting in the local logical node and not the hierarchy. So when we do this, when the logical node is in test mode, any data coming out of that logical node has a quality attribute marked as test. And any control service parameters, this is when the client is sending a command to the device to operate it, that gets set with a test flag in it. And we'll look in a minute and see how all those things come together to operate the test system. Similar but completely distinct is the simulated messages coming from the test device. And we have a data object in the logical node representing the physical device that indicates whether the device is supposed to pay attention to those simulated messages or not. So we've got two distinct concepts here. Test and blocked apply to particular data signals. Simulated messages apply to the communications channel itself. And we have a mechanism in the standard. We're not going to look at this today, but just so you know, there's a method of mirroring service control requests. So you can programmatically verify that operations happen. That's important when you're in blocked modes. We've seen a little bit of hesitation in adopting some of these technologies in the field. And the problems that folks tend to be running into is people need to be able to trust this stuff. When you have copper wires that you can look at with a meter and see what's going on, it's pretty easy to visualize how the system's working. When that data is flowing through Ethernet packets, it's much harder for humans to visualize what's going on in the system. So there's a lot of hesitation based on that lack of trust. And to get that trust, to get past these hurdles and understand what's going on in the system, we often need a lot of training and more often retraining to get folks that are used to using meters and screwdrivers over into using laptops to monitor these devices. And what that requires to get over those hurdles, we find, is some good tools to visualize what the devices are doing and how they interoperate. So that's what we're trying to bring to the market here at Triangle, and we're going to look a little bit later on at how some of those tools help you with these topics. Back into the standard and how the concepts interact. The mode and behavior I talked about, this is an enumeration that has five values, but there's really two separate concepts going on within this enumeration. The five values we have here are off, on, blocked, test, and test blocked. Test refers to the digital communications, the signals going back and forth between the devices, each have a quality attribute attached to it. And so we carry that information about whether it's in test mode or not in that quality. The blocked, on the other hand, refers to whether the physical outputs are affected by the process. So if we have a device in blocked mode, our communications can indicate that everything operated as expected, but we don't actually, our physical outputs, our process, our electrical system. In testing and seeing what our system's doing when we set certain functions into test mode so we can test those, we need to understand what devices, what logical device, logical nodes are in these test modes. This is a screenshot from our TestSuite Pro tool that lets you quickly visualize all the devices in the system. This is a very simplified example. But this tells you what the status of each of these is. That's important to verify that all the devices you need for a particular test are in the correct modes to respond correctly and not respond correctly in some cases. It's important to verify all of those are set up for the test we're doing. It's also important to verify that they are restored to operating conditions before we go home. So this mode and the quality interact to influence how signals are received. We have our publishing device over here. He's putting out a trip signal. His quality is good. He has no test fits. So all of these devices are going to receive a signal. The device that's in test is going to receive the normal data. The device that's not in test is also going to receive the normal data. These arrows here are indicating that the receiver is accepting that signal. When the quality is true, test, quality.test is true, only logical nodes will receive and react on that data. So our device up here that is in an on or blocked mode, he no longer is receiving this data because our quality shows test data. That same signal is coming down to this device. Both of these logical nodes are in test. So these logical nodes continue to receive those signals. In a similar manner, but completely separate, we've got the simulated messages we discussed earlier. And each IED at the device level, at the IED level, has a logical node, LPHD, which represents the physical device. And in that LN, we've got a data object called SIM. And when that data object is set to true, the device can start to receive simulated messages. So in this diagram, we've got our BCU1 here, his LPHD.SIM is false. He's not going to receive any simulated messages. Our BCU2 here, LPHD.SIM is set to true, so he is prepared to receive simulated messages. So now we plug in our test equipment and start generating these simulated messages. This is our test device down here. TSP will provide this functionality. So he's going to simulate these various signals. Our device down here, with LPHD.SIM equals true, he's now showing the signals that he's acquiring from the test device. He's receiving those simulated signals, and at this point, he's ignoring the normal signals from the device in the system. One thing that's not shown here is if one of these signals were not produced by the simulator, by the test device, this logical node would continue to process the real signals from the devices. So we set the whole device to SIM true. It will continue to process any signals, any normal signals, until it sees a simulated screen with that signal, and then it switches over and starts monitoring that. Blocked is the other part of the behavior of the logical nodes. Blocked doesn't affect reception of digital signals at all. It only affects the output. So in this case, we get a trip signal. All these devices are going to interpret it. These two logical nodes are going to open their breakers. These two blocked logical nodes are not going to open their breakers. Another feature of 61850 that's related to this testing is LGOS. LGOS is a logical node that lets a server indicate the status of each goose subscription he's receiving. This is a simplified screenshot, again, from the TestSuite Pro tool. This is just showing one publisher and subscriber. The tool Matt will show you later shows a matrix of all the ones in the system. But this is telling us that this device is subscribing to this signal from these IEDs. He's getting the signal. This is saying that the normal signal is on the line and valid. It's also saying that the simulated signal is on the line and valid. And we have LPHDSIM true, so we're looking at simulated signal in this case. Being able to see what all the devices in your system are doing with regards to receiving the goose in one clear display is very helpful to see how the system's working. So we've got a few tools for 61850. Our 61850 toolkit includes all of them. Two we're going to look at here today are the 61850 TestSuite Pro. This is basically a diagnostic client tool and goose and sample values monitor. And Distributed Test Manager, which is our simulator tool. This tool can bring up your entire SCD and simulate your whole system in the lab before you get equipment in place. It's great for testing designs and validating concepts, including virtual isolation. Now Matt's going to show you a little bit about these tools and how they help you test your systems. We do have a couple of questions. We're going to just take a quick stop for a couple of questions while Matt changes to presenter. So, Joel, first question. What is the difference between MOD and behavior? So MOD or mode is a control object that the client can operate to change the mode of the logical node. It exists optionally at each logical node, but at LLN0, the logical nodes in a logical device inherit that behavior from LLN0 and possibly from other logical devices. Behavior is the status that shows you what the behavior of the device is based on that hierarchy of modes through the LLN0 and perhaps other logical devices. So MOD is a controllable object. Behavior is the status. Okay. Next question. What happens when the simulated GU stream goes away while the IAD has still LPHD.SIM set to true? So once the device starts monitoring a simulated stream, it stays on that simulated stream until LPHD.SIM is set back to false. As I mentioned, it will continue to monitor the real device stream until it sees a simulated stream because this allows you to only have to simulate the streams you need for the test. But once the device is receiving that simulated stream, it will continue on that. And if the stream fails, the functionality will go into the failure mode just as if the real device stream had stopped in operation. So it will show an error. It will show a loss of communication. It will not revert to the normal stream until LPHD.SIM is set back to false. Okay. Okay. Next question. How should blocked mode apply to protection communications going outside the station? Should they also be blocked? Communications outside the station? And we do have Christoph on the line if he wants to jump in and answer as well. I know he did address that one. Okay. Can you hear me? Yes. Okay. So basically, blocked mode applies to physical outputs of a device. So when it's now something that goes out of the station, if it goes out of the station like a transfer trip to the other side as a non-61850 communication, then somehow the signal needs to get into that communication device. Either that's a physical output as well that will be blocked so it will not go out. Or there would be 61850 communication involved between the protection equipment and the communication equipment. And in that case, the signal would have the quality set to test. And the communication equipment would not be in test mode so it would not consider it. And there will definitely not go a trip signal out to the other side. Okay. Thanks, Christoph. With that, we just do still have a good amount of questions coming in, so keep them coming. I see that our panel of experts are answering them online. So we'll continue doing that while we switch over to Matt, who's going to continue with the demo part. Okay, Matt. Yours. Thanks, Beth. Just to verify, everyone can hear me and everything? Yes. Perfect. So the first thing that we're going to be looking at today is DTM, or Distributed Test Manager, as mentioned earlier. The majority of this demonstration is going to be done in Test Suite Pro, but I do want you guys to have the fundamental understanding of what each of the applications is doing. So when I refer back and forth to them, you will be able to tell them apart and know what each is doing. So DTM is our system simulator tool and is currently simulating a hypothetical Raleigh substation. We'll be monitoring and testing this substation with Test Suite Pro today. We're simulating all of the 61850 devices over here in this IED folder, and we're simulating the physical connections between the devices in this Raleigh section up here. We can interact with this simulation multiple ways, but ways you'll see me interacting today is by interacting either directly with the single line diagram, such as double-clicking on a breaker in order to open it or close it, or we can also interact with these buttons, which will simulate process behavior, like generating a fault on the line or simulating a failing breaker. If we'd like to validate operations of some of our physical equipment that we may use in our substation, you can actually go in and disable the devices in DTM and add the real device into the network to verify that it responds as expected to virtual isolation like we're doing today or any normal operation. Let's take a look at our Test Suite Pro tool now. On the left here, you'll see a selection of tools that you can use to interact with the devices or the system, and on the right section, you can see there are various displays to show off data in different ways. Such as if I go to the data miner here, this is our filtering and grouping tool, and go to, say, our one line, I can drill down to what I'm interested in and then drag it over to our custom display, and you'll see here that a new kind of breaker has appeared. So that's how we kind of diagram to match our system. When you load up your workspace and run some baseline tests. These baseline tests run continuously while you're working in Test Suite Pro to give you continual feedback on the health of your system. These tests are like automated sanity tests that let you know if everything is expected or if there is warnings or errors that you need to address or acknowledge before starting to use the other tools and displays in Test Suite Pro. Once a warning or error is acknowledged, it'll be hidden away, allowing for attention to be drawn back only to new issues. Let's take a look at a few of these sanity tests. We've got the compare model, and effectively what this does is it verifies that you're using the most up-to-date SCL file when it compares all of the SCL files configured in the workspace to a discovery of each of the devices that you're connected to in Test Suite Pro. Even if a device was added to the workspace via discovery, the model compare tool will show the differences since the device was originally added to the workspace. So if I open this up, you can see everything matches here because the simulation running in DTM is using all the same files. But if something happened to be off, you would see either red or yellow in here. And on the tool, instead of having a green check mark, you'd have something similar to what the goose tracker's seeing. So let's take a look at the goose tracker. The goose tracker verifies that you see all required goose streams and only the required ones. It also will alert you to any intermediate problems that may occur while testing. So what you'll see here is there are a few, everything down in the valid section is obviously running as expected, but there are a few things that we're seeing here that are a little bit of anomalies. Now, it is a yellow circle. Yellow is kind of what we use for warnings, whereas red is for errors. But I can go in and look at it. So if I just mouse over this, you'll see that there were two retrans drops here instead of the expected zero. And then there were a few streams that were on the wire before that are missing. Since none of these are going to upset our test today, I can either go and acknowledge them directly, or I can acknowledge everything from the top level here. And then we'll see that that's good to go. But if more retrans happen to pop up issues during the presentation, you can see that that will show up or any other warnings or errors. The next tool that we'll be looking at is the LGOS tool. The LGOS tool combines data gathered from the LGOS logical nodes with the goose messages test suite pro has detected on the wire and provides information on subscription statuses and possible errors. In order to see that a little more closely, you can actually just hover over and you'll get various information based on the subscription status for these specific devices. Now let's take a look at a few of the displays and tools that we'll be focusing on today. The mod behavior and sim tool is also run kind of as the sanity startup test, but it's also something that we'll be interacting with a lot. It allows us to configure the type of data that devices will respond to and how it will respond. So at the top level node here, you'll see if we interact, we can actually change the LPHD dot sim as Joel discussed earlier. And then at the logical device level, you can actually send the control over to the mod or mode in order to influence its behavior. Under the right circumstances, when you send a control to this mode, the behavior should follow suit. We're also going to take a look at the goose publisher today. The goose publisher is how test suite pro can publish goose messages into the system. By default, we simulate, we set the simulate bit true as if it is coming from a test set and the quality of the test. This is all configurable, but we figure by default you would want those things marked that way. We'll also be looking at the signal flow quite a bit, which I'll be showing off now. This is our first display that we're looking at. It allows you to clearly see how signals are transferred around your system. For example, if I hit record here and I go back to DTM and I'll click this button and what this button is effectively going to do is it's going to put a fault on line one. So if I click this, you'll see very quickly that this breaker opened and closed, but you'll also see if I look at the goose viewer down here, a lot of messages were sent in that short period of time. But if we go back to test suite pro and stop recording, what we can then do is step through everything that happened during that time. So if I click this next area, you'll see that a P diff was triggered as a differential protection, which is routed to the PTRC. The PTRC does some internal communication, but importantly it sends a goose message out to the SCU. And again, you can then kind of click through all of these options and you'll see eventually around this fifth event that happened, the breaker did open, the reclosing event happened, and then the breaker closed back. So you can now decide if you would like to keep these events as a snapshot. If you click here, you can save these off in case you want to take this back to some engineers back in the office after you record something in the substation, or you can clear the data out in order to be prepared for your next test. I'm going to go ahead and clear this because we won't necessarily need it. For simplification sake, we'll be using a smaller signal flow for digital isolation of line one BCUs, and I'll move that to here just to kind of clean up the display a little bit. I'll only need the devices that interact directly with the logical device from line one. So all I need is the LPU as the inputs, and the SCU also has an input, and I'll also need anything that it gives outputs to, which would also be the SCU. So now that we have a baseline understanding of what each of the tools and displays do, let's go ahead and start our first demonstration in the virtual isolation. This demo will be broken down into three different parts. The first is we're going to virtually isolate line one BCU. The second is we're going to generate all the simulated inputs that it needs and verify that its outputs qualities are set to test in order to not affect the rest of the system. And then after setting all that up, the third is going to be actually running the test. So let's virtually isolate line one BCU. We'll go back to our mod behavior and sim tool. And what we want to do is verify that these guys will then be set to test. So since the logical device here is the BCU control, I'll right click here, I'll send a control to the mode, and I'll set that to test. So when I perform this operation, it's actually sending this control back to my simulation in DTM, and sure enough, you'll see here that now it's in test. You'll also may notice that on the line one SCU, we now have Xs instead of arrows, and this effectively means that the SCU is no longer going to receive the inputs coming in from line one BCU because it's not configured to look at test data. The next thing is we're going to set up the one BCU with to receive simulated goose streams. So in order to do that, we'll go to the top level node here. We'll change LPHD, and we'll set it to true. So now you'll see in the signal flow, this is automatically updated to the LPHD sim is equal to true. But what you won't see automatically is Xs appearing on the input, and that was explained by Joel a little earlier today and in the question. And effectively, the reasoning is, is because until it sees a simulated stream, it's going to continue to look at what it already has. But as soon as it gets a simulated stream, that'll take its place, and it will lock onto that until LPHD.sim is set back to false. So now that we've set that up, let's go ahead and set up the generated inputs for this system. To do that, we'll go over to the goose publisher, and we need to find the inputs from this LPU control and the input from line one SCU. So here is the LPU, and here's goose control that uses for that. So I'll go ahead and start that in our goose publisher. So two blocks have now appeared, and effectively what these are is this is the simulated stream coming from Test Suite Pro. So just for organization's sake, I'm going to move this around, and I'm going to put the reclosing function under this one, and then the release from the synchro check under this one. And then we also need to simulate the breaker position coming out of line one SCU and going into line one BCU. So I will click play here. I can move this under here. So this will go ahead and give the breaker position now in. And what you'll see is all of the physical inputs coming out of DTM, the goose messages, are now being blocked off by the X because the simulated ones are there instead. So now that we've virtually isolated line one BCU and generated simulated inputs and set the BCU outputs to contain quality tests, we can now interact with the BCU however we want without it affecting the rest of our system. As a first test, we'll want to verify that it performs the close operation following a request from the reclosing function from the protection device. In order to do this, we'll need to set up some initial states. The first thing that we're going to need to do is open up the breaker in DTM. This represents the physical breaker and should remain open throughout all of these tests. This will prove that the devices were digitally isolated and that any closing operations are not reaching it. So I'll just come here, double click on this guy, that'll open the breaker, then we can go back to TestSuite Pro. Since the BCU is not subscribed to the position of the actual breaker, it doesn't know that the breaker is open. So we'll need to open the breaker in the simulated stream as well. So we'll just open up the Goose Publisher from the SCU and we'll set this, we'll just toggle this to make sure it's got the right value, we'll set it to off. So now the simulated stream coming out of this is telling him that the breaker is now open. We'll also need the Synchro Checks release to be set to true. This will allow the control function to allow the reclosing to take place. So again, we already have it set like this, but I will just toggle it to make sure it's got the up-to-date value here. So in order to simulate the reclosing function from the protection device, we'll also need to send an operate close from the LPU. Before I start, I'll record this using the signal flow, so that way we can watch the data back. So now I will send a pulse from the LPU to tell it to reclose, I'll send that to true, click republish, and then I'll set it back to false, and I will stop the recording. So what we'll see here is the very first thing that happened is the reclosing function sent an operate close to the BCU. Because the BCU is looking at test information and the simulated stream, it accepted it. If I click the next option, you'll see it sent an op close to the SCU. But if I hit the next button again, you'll see that the SCU did not respond in any way, and that is because line one BCU has been completely isolated. And if we go back to DTM, we'll see that the physical breaker is still actually open, proving that we are able to test the functionality of line one BCU without affecting our physical, or in this case, simulated system. So that's the end of just digitally isolating just line one BCU. But we can expand this test even further in our system by setting up line one SCU to accept test signals. This time, since we still don't want the physical breaker to respond, we'll need the input of line one SCU and monitor this op okay. Let me see if I can move this around so you can see it a little bit better. Monitor this pause.opokay. If the op okay goes true, the physical breaker would have opened if the SCU was. But before I move on, I'll be ready for the, let's create a snapshot here and save that off. Okay. So I'll go ahead and clear the data. Then I'm going to go back behavior and line one SCU, send a control over to its mod. And in this case, I'm going to do test blocked again, because we want to block the physical output to our breaker. So I'll click operate here. Now that this is in test block, you'll see that, and you'll see that the red Xs have disappeared, is expecting test data. Because all of the inputs to line one SCU are coming from another device and test. We actually don't need to use the goose publisher to mimic any of its inputs. And now because it's in test block, the output would also carry the same quality. All inputs and outputs are already in the right state and the test should be isolated from the physical hardware. One thing I will need to do is I need to tell the original line one BCU that the breaker closed from the first test. This will reset him to a state where I can run the test again. Of course, I don't actually want to close the breaker. So to do this, I'll go to the goose publisher. I'll go down to our simulated stream of the SCU. And I will again tell it that the breaker has closed. And then I'll need to open the breaker again. So the reclosing function can go through. We can now run the exact same test again. But this time we can monitor a larger system of devices and verify the behavior based on this pause.op okay. So before the test, I'm going to go ahead and record this so we can play it back. And then I can go back up to our protection reclosing op close here. Send a true. And then since it's a pulse, I'll send it back to false. And then we will stop the recording. So as we step through, we'll see the exact same sequence of events as before. The original close signal comes into our line one BCU. That triggers the op close like it did last time. But now what you'll see is that because it accepted the input, the op okay went high. But you will notice that the pause.stval did not change. And that's because the physical output has been blocked. And so now we've both verified line one BCU's performance and line one SCU's performance without actually affecting the physical breaker. So at this point where I'm going to hand it back over to Beth for another small Q&A section based on test suite pro and DTM. And then afterward, we're going to talk about how you can use the test sequencer to automatically configure the devices and prepare them for test. Okay, so we do have quite a few questions coming in. Some of them are multi part questions. So I'm going to start with a couple of probably shorter questions. How does the signal flow get the data in its displays? That's a that's a really good question. So when you bring up test suite pro, you provide you provide it some of the SCL files that you're using. You can also use discovery. And there are a few different ways that it can get the information. It can get it via goose messages, reporting or polling, depending on how you need it for all of our different displays. Those are the ways that all of our displays can generally get data. Okay, next question, how does goose publisher know what goose streams it can publish? So the the goose publisher here, it does the same thing as I was kind of describing before. Once you provided an SCL file to test suite pro to build up these workspaces, it can go through and it has all the goose information inside of those SCL files or your SCD file. So it can go through, find all of the signals or find all of the goose streams. And at that point, it can try to mimic those, which is generally why we default to the simulated stream just in case. So you don't overwrite, you know, have two different goose streams at the same time with the same values. Okay, next question. How do you differentiate the physical output in the test output? So there's a few things kind of going on there. And I'll have Joel tap in if there's something I leave out. But so there's a few things going on behind the scenes here. When we look in at line one SCU, there are 6150 communications going on, such as these op closes and the pause.op okay. And those are going on at the 6150 level. Those can hold a quality of test, normal, any of the different quality options that you can choose, whereas the physical output is like a hard wire going directly to the hardware that will actually open or close the breaker. So in this situation, I'm in test blocked, the 6150 outputs coming out of this logical device are not being blocked, but any of the physical output to the actual hardware would be blocked. Okay, next question. Is it possible to define in a test sequence, excuse me, is it possible to define in test sequence a loop to repeat many times automatically a test? So at this point, we don't have inside, we'll actually talk about that much more detail. But what you could do currently is set up a situation where you duplicate a test multiple times. So say you need to run 10 times, you can have that same test duplicated multiple times. And you could run it that way, or you could set up kind of repeating steps. But we don't have like, as far as I know, we don't have a like run forever type framework. Granted, that is something that we are kind of discussing internally of how we can expand and enhance the test sequencer. And honestly, most of our application and displays. Okay, next question, and I'll probably open this up to the group. How do we drop a data attribute level in simulate? Can you ask the question one more time? Sure. How do we drop a data attribute level in simulate? I'm not particularly sure what they're asking there. Does anyone else want to weigh in or I see I see who that who just asked that question if you can provide some clarifying details? Yeah, I think we need a little more information is not quite sure what he's meaning there. If I was going to try to guess what that meant is earlier, I showed how you could drag and drop from the data miner over here. And in order to drag and drop from that, you'll go to the data miner, you say I grabbed this position and drop it over. But those are just the the data attributes inside the data miner. If it's in if it's coming from our simulated stream here, you'll see that there's actually these little red stars next to it. And what that effectively means is that simulated goose is available for these data attributes. And you can actually switch over to the simulated stream rather than the actual physical stream running from our physical devices, or in this case, DTM, our simulator tool. If I had to guess that might be what they're talking about. But I'm not 100% sure. So if it's not, feel free to follow up with some more information. And we'll try to answer it a little more clearly. Okay. Okay, I'll let you know some more details coming on that one. With that, I would say because we're only about 10 minutes left, if we want to continue on here, and then we can come back to the questions and answers. Yeah, that sounds great. So now that we've looked at how to configure these devices manually with the mod behavior and sim tool, let's take a look at how you could configure them automatically. To do this, we'll be looking at the test sequencer. Let's get a little more real estate here. The test sequencer can be viewed in multiple different ways. The first is kind of this flowchart at the top. This is a very high level view of what's going on in your recorded sequence. The second, if I slide it up here, is a detailed steps view. From here, you can see everything that's happened all the way down in each step, even to its sub steps. The test sequencer allows for a sequence of events to be recorded and automated and verified. Three frequently automated in the test controls, protection scheme validations, and digital isolation for test preparation. In this case, we'll be looking at digital isolation and test preparation. The main benefit for automating the setup and breakdown is that you can verify that this works on a simulated system, such as we're doing today, and be confident that all devices will be put in their proper state when you test on an actual physical system. So what I'm going to do is I'm going to go to remove devices from test, and I'm going to go ahead and run this. Since we just ran the demonstration where we have digitally isolated a line one BCU and line one SCU, when I run this test, what it's going to do is it's going to go through the setup. It's going to run a few operation commands here, which will allow for it to start receiving the regular goose again. That's just interacting with the LPHD. Then the other two operations down here are sending controls over to the mod again. Then we've got four verification steps here. Aspectively, these two are guaranteeing that it goes back to the normal goose stream and that that's still there for it to get values from. Then the second two verifiers down here are making sure that it does return, that its behavior was then returned to normal after we sent the control, just in case something had prevented it from changing back. So if I go back to the signal flow now, you'll see that all of our devices have now returned to the on state and everything is back to its nominal state. Of course, you can do the exact opposite thing. So I could go to this put devices in test, which is going to look almost identical. But the interesting thing about this is instead of creating the test from scratch, as someone asked earlier, what if you want to do a while loop or continuously run these things? You can actually go up here and duplicate your tests, which is what I did to build up this put devices in test. So effectively what this is, is it's the exact same thing, but I double clicked on these actual steps here. And instead of setting this value to off, when I was setting it the other way, I set it to on. So I was able to take the same test that I already created, tweak some of the parameters to it, and then recreate kind of a brand new test. And again, if I run through this, it'll then go through and automate it, automatically put all the devices that we're going to want to run tests on back into the mode that they need to be in order for us to successfully run those tests. So now we very quickly turn to lphd.sim to true. We put this in test and we put this into test block. Having sequences in your test environment that will configure your system for tests will give you confidence when you have to do these type of tests in an emergency situation on a live system. You don't have to worry about missing a step or any human error. And that's really the conclusion of the live demonstration today. I'm going to pass the presentation back over to my colleague for, I think, the last two or three slides and answer the questions that we happen to don't get. Okay, Joel, I'm making you the presenter. Okay, and so based on that, I am going to wrap up real quick, but we do still have a lot of questions coming in. As Matt mentioned, we will get back to that. I do want to let everyone know that the webinar will be posted on our website, as well as our YouTube channel. Once you sign off on this webinar, you will receive a short survey. We just want to get some feedback from you guys and some other topics that you'd like to hear about. Once you respond to that, you'll also get a link to a copy of this webinar. One other thing I want to touch on is we are offering what's called a substation simulation program where we're looking for your SCD file to help use that to build a workspace in DTM. So if anyone would like to participate in that, you please reach out to me directly afterwards at sales at trianglemicroworks.com. And I can give you a little bit more details about what's involved there, but essentially, if we're able to use your SCD file to build a workspace specifically for you, we'd like to set up a web call with you afterwards and kind of discuss feedback on how that works for you. And if you participate in that, you are able to get a discount off of our 620.50 system toolkit. There is limited participation for that, so it is done on a first-come, first-served basis. And then, of course, if you have any additional questions or comments, please email us directly. Again, sales at trianglemicroworks.com. So with that, I would like to jump back into the Q&A section of the webinar. We have about six minutes left, but as I mentioned before, we will just stay on as long as needed. So certainly feel free to keep sending those questions in. So to back up a little bit on the questions, and I will open these up to the entire panel, you know, speakers and the folks behind the scenes, I'm going to kind of start a little bit. Actually, we have clarification on that last question I did just ask, so we're going to go ahead and address that one now. So originally I had asked, how do we drop a data attribute level in simulate? And the additional clarification was, if we want to simulate ST, can we drop this in simulated directly in their meaning at an individual data attribute level? So again, I kind of opened that up to the group, but go ahead. So in TestSuite Pro, when we're sending out this simulated data, it's based on the Goose Publisher, and the Goose Publisher has access to all of the data sets and the Goose control blocks inside of your SCL files. So if you would like to simulate a data attribute, just make sure that that data attribute is within a Goose control block that is being Goosed out, and then we'll be able to publish that in simulate, and you'll be able to tweak or change that value in any way you want. Okay, and the next question, is it possible to test SV with this tool, assuming sample values? Yeah, we have, just like we had the Goose tracker that I showed off at the beginning, there's also a sample values tracker as well. So you'll be able to use the same kind of layout and form that we use the Goose tracker for, that you can use to make sure that your sample values are also on the line. Okay, I'm going to back up a little bit to some earlier questions that came in, I believe they referred to Joel's slide, but again, I open it up to the panel. On one of the slides, with the one with the test signals where all were normal, why would test mode enable LN except the normal controls? So in this case, the LNs that are not in test mode will, that are in test mode will accept normal signals, but LNs that are not in test mode will not accept test data. And that is a fine detail that gets confused a lot, but LNs that are in test mode will accept data with quality good, but not test. Okay. And then I, there were some follow up questions that one, that Christoph, I see you did address online. So let me know if that does not fully answer the discussion that was going back and forth there. Yeah, okay, there was some discussion going on on that topic. Also, it's related to simulated messages, when you receive simulated messages, I mean, these concepts are independent, but they are still some relations. So on the first question, as Joel said, it's a little bit asymmetric, but there were some arguments that were going in the direction that when a device is in test mode, it should still receive regular messages. And that's of course only true as long as the stream, the goo stream where this regular message is coming from is not affected by the test and there is no simulated one there. This is typically meant for parts of a system that may still be in operations and are not affected by the test. However, as soon as you have a simulated stream present and your device is configured to receive the simulated stream, then you cannot continue to receive any more of the regular messages independent of the test mode or not what is in there, because then the message is switched over. I think that's a key point to understand. The test mode, this affects the individual signals and how you treat them on your logical node, whereas the goo stream, simulated or not, this is really the message at the reception that is switched away. I hope that clarifies a little bit. Yep, and the gentleman that had asked that prior to, please feel free to send in any additional details if that does not answer your question more fully. Next question. Mode and behavior can be set independently for different logical devices in a logical device. Okay, that's a question. I didn't phrase it as a question, but that's a question. It can be set for different logical nodes within the logical device if the device implements that mode is optional at the LN level, except for LN zero. So if the device chooses to implement it fully, it can be set at an LN level. Okay. So I have a rather long one here. It's just the new, based on what's coming in just at the end. Okay. Is there any default test models of IEDs of relay manufacturers available in DTM? If yes, any addition of new generic IEDs available? So we have some sample SCL files that we provide when you install DTM. This is stuff that we've made in-house for testing and demonstration purposes. We tend to not give out other companies' SCL files because we don't want to step on any toes and we want to make sure that we're not giving away any private information or anything like that. But it is completely compatible with any form of SCL files, ICDs, IIDs, CIDs, SCDs, anything along those lines. So we do have some demonstration and some sample files that we provide in DTM. And when you install the latest version, any updates that we've done to those files, you'll get automatically. But I wouldn't say we have a library of simulated or of SCL files. Okay. All right, so I've got one long one here. I'm going to try to read this as slow as possible and I open it up to the panel. By definition, the sections of inputs that define XREF for data received from goose messages has no defined application semantics to match, for example, protections or CB inputs. How do you associate incoming goose data with the functions of receiving IEDs, such as tripping, closing, and blocking? Where is this semantic of incoming goose data taken from? This is certainly a place in the standard that needs some improvement. There's a source REF that's not widely used that goes in this direction, but essentially the current state comments can be provided in the XREF and with addition 2.1, there's some suggestions coded in attributes for to give some indication of what the input signals are used for, but it's not well documented in SCL right now. Christoph, do you want to add anything to that? Yeah, maybe as I have answered already in the answer as well. So basically, there have been various improvements in steps over the time of the XREF on the semantics of the XREF, and with addition 2.1, what we now have is that on one side, we have the concept of later binding, so an IED manufacturer in the SCL file can already express the inputs that he expects on the logical nodes, and he can express through a proposed data object or proposed data attribute what kind of data he wants. So by that, you typically see if it's a tripping input or if it's a control input, and he can also express in the description basically some semantics. So when this is well done in the IED, it's more or less obvious what you need to connect. There is in addition, in ongoing work now, and I think Dave has mentioned that in his answer, there is this OSMOS project, which is a European research project. There we are investigating a little bit further in the way to be able to document signal flow in addition to goose messaging and independent of IEDs so that it can also document that when you do a requirement specification. And for that, we are working on introducing a source REF attribute, which is basically pointing also towards sources, but this is more on the specification part. So while there is already quite some progress compared to the first edition of the standard, there is still work ongoing in that area to further refine it. OK, thanks for that answer. Next question. How could I simulate a goose message burst to test PRP protocol and substation LAN saturation? So the goose publisher in TestSuite Pro allows you to generate those goose streams. You could easily set up a SCL file with as many streams as you need to load to carry the load conditions you're trying to test, and then enable those, you can publish those either as simulated messages or as normal messages from the TestSuite Pro goose publisher. OK, next question. As sample value is working over process bus only, the medium is Ethernet-based or fiber, whether it can be mixed, bridged with the Internet? I'm not entirely sure I understand the question, but if we're talking about routing goose or sample values over the Internet, that would get into the newer routable goose and sample values mechanisms that have been, well, they're including the 2.1 and the First Amendment of A-1 published a year or so ago. They've been tested in the field in relation to a technical report 90-5 for several years now. Traditional goose and sample values will not route. I'm not sure if that's answering the question or not. OK, and that question actually just came in. So the gentleman that just sent that in wants to add any additional details. I can go back to that. Can we make use of Wireshark for an output analysis for throughput and other analysis? Is this relative to the goose question? It's a different person. OK. Wireshark can be used for analyzing network traffic, yes. There. OK, if there's any additional details, if we're not fully answering that question, please feel free to go ahead and send in some more details there. Although I would say on that note, I do think we've gotten through majority of the questions. So, again, I'd like to thank everyone for taking the time to sit through our demo and through our presentation. Hopefully, you've learned something. Like I said, we look for your feedback in the survey that you receive once you do sign off of the webinar. And please feel free to, if you have any additional questions, just contact us directly at sales at trianglemicroorks.com. Thank you, everyone. Thanks, everybody. Good day. Yep, take care.


Video ID: ujCpWKGdtd8
Hello, everyone. Welcome to today's webinar, Troubleshooting and Conformance Testing for DMP3. My name is Beth Kapelis, and I'm the Marketing Manager here at Triangle Microworks. Before we get started, I'd like to go over some housekeeping items and introductions. The webinar is scheduled to be one hour long. We will be staffed for questions a couple of times throughout the webinar, and we'll stay on afterwards to answer as many questions as possible. Please submit your questions as soon as you have one. Don't feel you have to wait for a break or until the end. You can submit them online using the webinar menu under the section titled Questions. We have a team of experts ready to answer questions, and we'll respond directly to you, not the entire audience. To be clear, the questions are not seen by the entire audience, only by the folks answering the questions, and we'll respond directly to you. We will stop a couple times throughout the webinar to read and answer questions, and as I mentioned, we'll stay on after the webinar to answer as many questions as possible. So next, I'd like to introduce the folks you'll be hearing from today. First, we have a Triangle panel of experts, starting with Steve McCoy. So Steve is our senior software developer and protocol expert. Steve has been with Triangle for over 20 years and brings a tremendous amount of protocol and industry expertise to the table. We also have Alan Morrison, our senior technical support engineer. Alan has been with Triangle for over 10 years, and anyone that has worked with Triangle has most likely worked with Alan at some point when contacting technical support. Alan also has a tremendous amount of protocol and industry experience and works every day to help customers solve their protocol issues. Our main speaker today is Matt Green. Matt is a senior applications engineer here at Triangle. He's been with us for about six years, and he specializes in QA testing with all our simulation tools in Gateway. Matt is also one of our lead product trainers and presenters for webinars and customer demos. So before we get started with today's topic, I'd like to give you a quick overview of Triangle Microworks. We were established back in 1994 and are headquartered in Raleigh, North Carolina. We have customers in over 70 countries and work mostly with equipment manufacturers, electric utilities, and system integrators. The products that we offer all support the standard state of protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs in development time. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They are designed to troubleshoot, to simulate, and to automate testing. Lastly, we also provide a skated data gateway supported on both Linux and Windows for protocol translation and data concentration with standard skated protocols as well as OPC. Now that you know a little bit more about Triangle, I'd like to go ahead and hand it over to Matt Green to get started with today's topic. Thanks Beth. So let's take a minute or two and talk about the history of DNP3. DNP3 stands for Distributed Network Protocol Version 3. It was initially developed by Westronix and was based on an early version of the IEC 6870-5 protocol standard from 1990. The standard wasn't actually officially published until 1995, but DNP3's roots can still be found in the version from 1990. You might be more familiar with 6870-5 from the 101 or 104 protocols. DNP has two different roles, outstations and masters. Outstations have a database composed of the inputs from system devices and outputs controlled from masters, while masters are able to replicate the outstations database and issue controls as previously mentioned. Unlike propriety protocols at the time, DNP3 was an open standard which encouraged different vendors to use compatible protocols to allow for interoperability. A DNP3 users group was founded in 1993. They helped enhance and maintain the protocol. This slide is going to talk about the way that DNP3 connections can be created over TCPIP. DNP3 connections can be issued from either the master, the outstation, or from both. While either can initiate the connection, the outstation can limit the IP addresses from which the master can connect from. And as with all TCPIP connections, you'll need to designate a port. DNP3's spec defaults to using port 20000. If your devices are not connecting, as you'd expect, there are a few steps that you can do to troubleshoot the issue. The first thing you'll need to do is to understand the architecture of your devices. This includes understanding the roles of the devices, which acts as the outstation, and which is the master. You'll also need to understand the type of connection you're attempting to communicate on. The next thing you should check is which of the devices is set up to be the initiator, or TCP client, and which of your devices is the listener, or the TCP server. Also, remember sometimes both can initiate in a dual endpoint configuration. If all of this seems to be configured properly, you can check the IPs and the port, and then the source and destination addressing. While checking the IP addressing information, your outstation isn't restricting connections from the IP that your master is using. Once all of this has been checked out and your devices are connected, you should see data link status requests. These are a form of keep-alive messages to verify that your devices are remaining connected. While TCP IP does have a built-in keep-alive message, the rate at which is checked is far too slow for some DNP3 connections. Before we start with our first demo today, I'd like to talk a little bit about the applications that you'll be seeing today. Wireshark is a network protocol analyzer. It listens on the network adapter and analyzes packets that are being sent across it. It can be used almost anywhere, including in labs, testing environments, and out in the field. The Protocol Test Harness is an application we'll be looking at the most today. The Test Harness is a diagnostic protocol simulation tool used to test and monitor an individual device's behavior. It can be used in a wide variety of situations, including in development, in testing, in labs, and even in the field. The Test Harness supports DNP3, Modbus, 6870-5, 101, and 104. The last application we'll be looking at today is Distributed Test Manager, or DTM. The Distributed Test Manager is another protocol simulation tool, but this one is more focused on simulating entire systems of devices. DTM is generally used in the lab or in testing environments. Today, DTM will only be simulating the hardware that we're performing tests on, but if you are interested in more information on this tool, we have other webinars whose main focus was on using DTM. So now that you've been introduced to the application shown today, let's take a look at how you can troubleshoot a DNP3 connection over TCPIP. So this is the applications we're going to be looking at today. We have DTM, or Distributed Test Manager, on the right-hand side. And then we have different folders, depending on which demo we're going to be looking at. For the first view, we're going to have the outstation as the device under test, and that's simulated in DTM. And we're going to have the master inside of the protocol test harness. And what we'll see right off the bat is that we're getting a bunch of errors when trying to open a connection. So what we can do here is there's two ways we can troubleshoot why this is happening. The first is you can bring up Wireshark. I'm going to go ahead and search at the IP address 1516.17.102. I know that is my master's IP address, so I just want to see what is going on. So I'll go ahead and start that up. And then I will stop it, just so we don't have it continuously screening past. So what we're going to see here is we're going to see a connection request, and then a response to that connection request. Then we're seeing an acknowledge. Everything seems to be connected. And then at the end, what we see right immediately after is this FIN, or finish, which is effectively closing down the connection. And that close is coming from 1516.17.100, which I know is my IP address of my outstation. So it looks like the outstation is closing this. You can also see this in the test harness. So in this middle section here, we have what we call the protocol analyzer. And we're seeing the traffic going across the wire that's involved with the devices in test harness. Currently, I have the target and physical layer filtered out. But if I enable this to get more information on what's going on, and then I'll just pause the display, what we'll see is a TCP channel is being opened. It was successfully opened, and then it was connected. And then almost immediately after, we're seeing that it was closed due to the remote side or a connection failure. So again, we're seeing that the outstation is the one that is closing the connection. So let me unpause this, clear it to keep going. So now what we're going to do is we're going to go through the troubleshooting steps we talked about before. I've already laid out that which is the master and which is the outstation. So we know that architecture. We can also look to see what we're going to be communicating on. So if I right click in the test harness and do modify session and go here to the channel, we'll see that we're communicating on TCP, which is what we are trying to connect on. So that's correct. We can also go into the advanced settings here to look to see who if we have a correct initiator. So if I scroll down to this win TCP mode, we'll see we have a client. So yes, as the protocol analyzer is saying, the master is trying to initiate the connection. The next thing we're going to want to do is look at the addressing. So I'm going to look at the sessions tab, which has our link layer addressing on our DNP3 master in test harness. And I'm going to try to make sure that my device under test or my outstation here is configured correctly as well. So I can just come here and do edit the session. So what we're seeing here is that the outstation is configured to have a source addressing of four and a destination of three. So the source of this outstation is four and the destination of the master is four. So these will communicate and vice versa. We have the source of the master as three and the destination of the outstation is three. So that all seems correct. So now let's go look at the IP addressing for TCP. So I'm going to go up to the IP information on the configured device under test and the same for the DNP3 master. So we'll see that the host, which is the device that the master is trying to connect to should be at 1516.17.100. And sure enough, it is. So that's correct. They're both communicating on port 20,000. So that's no problem. But then on the last stage, what we see is where our issue is coming about. The remote addressing on the outstation side is 1516.17.101. So it's only going to accept connection requests from 101, whereas our test harness is simulated and is running at 102. So I could either change this to be 101 on the master side or realize that my device under test is misconfigured. I can go ahead and change that to 102. Hit OK. Close this down. And now what we'll see is those error messages have stopped coming across. So now we've got our connection and some information is being sent across. And immediately afterward, what you'll start seeing is those link status requests if your device is configured to do so. So if we sit here and wait a little bit after they finish kind of all their handshaking, you'll see, let me pause it so it doesn't go off screen, we do start getting these link statuses. And it's just going to send those back and forth to make sure that the devices are still connected and that we have not disconnected. You can also see this again by going to Wireshark. And this time, I'm going to go ahead and start it back up. But at this time, instead of searching on the IP address, I can actually just type in DNP3 here and click Filter. And it'll filter out everything on that network adapter except for anything related to DNP3. And sure enough, we're seeing these link status requests being sent back and forth. So that is the first demo we're going to be going over today. I'm going to jump back over to the PowerPoint for a few slides to talk about monitoring. So in the last example, we were able to simulate one side of the communications. But sometimes you need to be able to decode communications between two real devices, or you are unable to simulate either one to recreate a problem. This is when you can use monitoring to help you diagnose the issue. Monitoring is the act of listening or sniffing the wire to gather information about communications on it. Once a monitor has been configured properly, you can use filtering options to narrow down the amount of information that you're seeing in order to help you locate your issue. You can also use a recording feature that is included in some monitoring software in order to keep a log of what you're seeing in case you need help from others, such as technical support. So now we're going to talk about how you can set up a monitor within the test harness. So I'll minimize these for now, and I will bring up a brand new empty test harness. So in this situation, I don't have the device pre-configured or anything, and I'm going to walk through those steps with you. So what you're going to want to do with the empty test harness here is go to open, and then monitor channel, and then you want to do DMP3. From here, there are a few different ways that you can communicate with the devices. You've got serial, such as a standard RS-232, as well as a radio network or RS-485 multidrop. So we can use that type of connection. We can use TCP IP, which we will be today, but it's also worth noting that we have the ability to input bytes captured by another tool and parse them directly in the test harness, and that's what raw input bytes is. We're doing TCP IP today. I know the master is at 1516.17.102, and the out stations at 1516.17.100. I know the port is still 20,000, and I'm going to choose the adapter in which it will be able to communicate on, and hit open. So what's going to happen is we're going to get a monitor channel here on the left-hand side, and that's effectively all we're going to see right now. Now, any sort of traffic that's being sent along that channel of communication, we should see pop up in our test harness now. So I'm going to go over to my devices under test, and I'm going to issue an integrity data poll, which is effectively just the master asking the out station to give them everything in their database. So I'm going to issue that command now, and sure enough, now we see it here in the test harness. So even though the test harness is not the master or the out station in this communication, we're still able to see the information traveling across, and so we're seeing all the double bits come across, the binary inputs, just everything inside of that out station's database is being sent over to the master, and we're able to see that on the line. So how can this be used to actually track down, say, troubleshoot an issue that you might be having between the two devices? Well, I'm going to go ahead and issue a control to .5 or index 5 from the master to the out station. So I'm issuing that control now, and what we're seeing here at the bottom is that there was a parameter issue. The value did not update in my out station, the control was not successful, and there was some sort of parameter issue. So I will bring up the control editor in my master and bring that over. So you've got many options here. You've got your different modes. I'm looking at all the parameters since that's the area of the issues. We've got select before operate, that's correct. Qualifier code, that seems correct. We're doing a latch on, that seems correct, but I was trying to control the point at index 5, and I have 50 in here. So just a simple mistake here. If I go ahead and clear that, and I'll clear the display, bring this guy back up, and now if I issue the command, what we'll see is it come across, and sure enough, there's no more error. The value did change in my out station, so that was all successful. So just a way that you can use the test harness to track down a very simple issue that could be potentially causing a problem between two hardware devices or a device that you won't be able to simulate. So now that we've talked about connections and getting devices up and monitor channels up, we're actually going to stop for our first section of questions. So I'm going to pass it back over to Beth and see what she has. Okay, yep, we do have a couple of questions that have come in, so let me just take it from the top. First question, I saw opening UDP endpoints in the test harness protocol analyzer output when it was configured as a DMP master. I thought we were using TCP for the connection. Can I hand that over to the panel? Steve, Alan, do one of you want to take that? I'll take it. So yes, it did say that when you use TCP, typically you also open a UDP port that's used by the master for sending broadcast requests to the outstations. The outstation similarly would open a UDP port to listen for those broadcast requests. Your broadcast needs to be sent to a broadcast IP address, so this only typically works on a subnet. A key thing about broadcast is there are no responses to broadcast. So while the requests are broadcast over UDP to the outstation, no responses come back to broadcast requests. The master does not listen for broadcast, does not listen for UDP to get responses. There is a UDP only mode in which requests and responses would be sent both ways, but that's a different scenario. Okay, I've got a second question here. I think this one's going to be for you as well, Steve. Do I always have to use port 20,000? Suppose I want to allow three masters to connect to the outstation. Don't I have to listen on three different port numbers? So the DMP spec says the default is 20,000. Every outstation has to be able to listen on port 20,000. Every master has to be able to connect to port 20,000. However, if your device is capable of it, you can choose to listen on different ports. You can choose to connect to that same matching port from the master. So it is not required to always listen on 20,000, but it is required to have that capability. And as far as multiple connections, if an outstation supports multiple masters connecting to it, it can still listen on just a single port number like 20,000 and can accept connections from multiple devices on that port number. Or it could choose to use three different port numbers if that was desired. So either way works. Routing is done for you even if you use the same port number. But it depends on the outstation, what it supports. That is a key point. Okay, we do have a bunch more questions coming in. So let me just keep going for a couple more of them. Do we have keep alive messages and other SCADA protocols like 61850, 101, 104? What are they? Steve, do you want to take that one? Yeah, I'll take it. So let's see. 101 would be a serial link. It can either be balanced or unbalanced. If it's unbalanced, the master's polling, so there'd be no need for, I mean, that's the equivalent of the keep alive. I mean, it's constantly talking. 101 balanced, there are no keep alive, but it's also not over TCP. The point of the keep alive was to make sure that the TCP connection doesn't go down and need to be reestablished. So 104, yes, there are link layer keep alive, they're called test frames. 61850, I can't really answer, but I'm sure the, I would imagine the answer is yes, but I couldn't tell you what they're called. Does it, I don't know if anybody else has. Yeah, I think, I think 61850 depends on TCP keep alives. Okay. Next question. Could the test harness import PCAP traces? So the four, so yes, you, yes, sort of. Yes, you can. You can output Wireshark or PCAP traces to a file and you can massage the data in the file so that it can be imported to the test harness. One of the features we're actually currently working on is to simplify that process, but yes, it can be, and there can be value in that because sometimes Wireshark, I'll talk about Wireshark, you asked PCAP, but Wireshark, sometimes it doesn't know the latest things in the DNP protocol. For example, secure authentication is not, I'm sure it depends on the version of Wireshark, but it's not completely implemented. So yes, you can. It's documented in the manual how to do it, but there are commands that you can use to import bytes of data that were outputted from Wireshark. Okay. I will take one more question and then we'll move back over to the slides in the demo. Alan, I think this one might be for you. How can and how do I save the protocol analyzer log? Alan, do you want to take that one? You're on mute. Hey everyone. Sorry about that. Yes, you can select the file tab on the protocol analyzer GUI and then go to save protocol analyzer and then it'll pop up two options. Either you can save it in text format or XML format. The text is just plain text. If you save the log in XML format, you can load it back into the test harness and it provides the display capabilities of the protocol analyzer in a live capture. It allows you to filter, deselect, search. Also, when you save the protocol analyzer log in XML, it gives you the capability to store the configuration for the channel and the session. So those can be examined and looked at so you know what settings were used when the protocol log was captured. Great. Okay. Thanks everyone. I think what we'll do now, a lot of questions are continuing to come in so we'll go back to answering those online. And then Matt, if you want to go back to the presentation. Yeah, absolutely. Let's see. So now that that question segment is over, let's get back to a few more examples. When testing an outstation, you might want to use a test set. A typical test set is simply just a simulated master. A DNP3 master simulator allows for sending messages and controls to the outstation from a single master or even from multiple masters. This will allow you to test your outstation in various situations without requiring the actual hardware. Other tests you can perform with a simulator is sending bad messages from the master to the outstation and verify it performs properly under these abnormal situations. These bad messages may not be easily sent from an actual hardware master device. So let's take a look at how you can simulate a DNP3 master in the test harness. So again, we have our outstation or our device under test simulated in DTM and we've got the master from before. And then what I'm going to do is I'm going to take a look at showing you how to set up a master checklist. So I'm going to go ahead and click on the master checklist and I'm going to click on and then what I'm going to do is I'm going to take a look at showing you how to set up a master channel. So we do have a device here and as mentioned before you can you know modify this and go change your parameters but if you don't have any device you need to open a new device you just go to file, I mean you just go to open and then you do a master session and then do DNP3. This is going to be bring up your configuration for that master. So we've got our link layer addressing which we talked about previously the source and destination. If you do have secure authentication as I think one of our questions had some information about you could enable that here. The channel tab this is when you'll choose either serial or TCP IP. If you're doing TCP IP you put the address of the outstation you want to connect to. If you want to specify the IP address that you're communicating on you can choose that from the local IP. Just do a drop down here and by default the port number is 20,000 but as mentioned you can edit that if needed. There's also an advanced settings on both of these tabs to allow you to go in and change some configuration information for you know very specific situations. So if you want to monitor something or if you want to simulate something that may be exactly out in the field you might need to come in here and change buffer sizes and timeout times and things like that but I would say 80% of the time everything that you need to get a simple connection up and running you'll be able to do directly from these three windows. I'm not going to create another device right now as we already have one up and connected to our device under test but there are a few other things I can show off on the master side. The first is if you do open data window what that's going to do is it's going to give you kind of this default view of all the data in our system. So if you drill down into it you'll see all of the devices that are inside of the test harness. Currently we just have the MD&P device and if I continue to open these up we'll be able to see all the different data types inside of it. So we've got our binary inputs, our double bit inputs, our analog output statuses, we have analog inputs, we have all the different things that you might want to see but what you'll also notice is that all these values are fairly static. They're just sitting there nothing's really changing and on my outstation nothing's really changing either. So what I can do is go over to my outstation and I'll just interact with it and cause data values to change. So I'll just click change once. Now the data points inside of my outstation have all changed but again in the test harness we're not seeing those values coming across and that's where commands can come in. So if you go to open the command window for d&p3 that'll bring up a window that looks like this and what this is is this is a way for us to be able to issue commands and controls from the test harness. So the first thing you want to look at especially if you have multiple devices here is this target. This is effectively who is sending out the command. Currently we only have this one mdnp and so when we send any command or control it'll come from that and then we've got some default data or default commands out here. We've got our integrity data poll, we've got our control relay output block, enabling and disabling unsolicited messages, just to name a few. So when I click integrity data poll here we'll see all those values come across the screen into our protocol analyzer and then from there now if I bring back up our data window sure enough we'll see just because I left it on analog inputs we'll see all of our analog input values have now updated to what they are into our outstation. You can also run, can someone, one of the organizers mute? Hearing a bunch of keystrokes. You can also update, I mean you can also issue controls in other ways. So we have say our control relay output block. If I want to edit that I can either right click and then do configure command or I can just double click on it and this will bring up the configuration editor we were looking at before. So again we can do select before operate, we can do direct, no acknowledge, just a select. There's the different qualifier codes. You can interact with which index or which point number that you're going to be performing this control on. You can do latch on, latch off, you've got trip, close, everything along those lines. And you may want something to happen not just once but say periodically. So you can actually set up this certain control to happen periodically. So there's the repeat interval here you can set up what that is. Now maybe for control that doesn't make too much sense but maybe if you were doing integrity data pull you'd want to set it up for once an hour or something like that just to make sure your database is up to date. And while we're not talking about it now it is also worth noting that once you configure a command how you like it it actually will give you a tickle command and if you copy this out this is the exact command you would use while scripting to perform that exact same control that you have configured. And scripting will be one of the demos we're going to talk about later on but I did want to point it out while we were here. So if I close this then I can issue a control and the control will go across you see that the binary command is being sent and that will be able to be sent from the master to the outstation as well. So there's one more thing I wanted to talk about when it comes to simulating masters and that is simulating multiple masters. Again you could go to open and then do a master session a master session DNP. We talked about this before you can set up your IP address your port number you can select your your local port or local IP address but what if instead of communicating on port IP 102 I wanted to do 101 I could change this and connect up to a totally different thing and then just hit open and that'll bring up an entirely new simulated master with individual session information and TCP IP information here. So you might want to do that if you're connecting to two different outstations or if you're connecting two different masters to the same outstation. There's another way that you could do it and that you could do that by adding an extra session to already previously created channel which is the TCP IP connection. One reason you might want to do this is if you have one TCP IP connection to a remote terminal server which then will split that up into multiple serial connections to different outstations. So to do that you would just right click on a previously created channel and add a master session and it will give you the configuration just to create another session down here. So let's just pop these up to and click open and sure enough now we have another session down here that could connect through like we were mentioning earlier remote terminal server or something like that to allow one TCP IP connection with multiple link layer session information. So let's jump back over to the PowerPoint. So now let's talk about simulating an outstation in Test Harness. You may want to simulate DNP3 outstation traffic for multiple reasons. One reason is to test large volumes of messages being propagated in your system to test your master device under a heavy load. You can test performance of your master device, how it will respond to large buffer sizes, or even overloads. You can also generate data from a single or multiple outstations in order to verify the behavior of your master device. By simulating outstations you can also verify that your master supports all the features that you'll need once it's been deployed to the field. You can also use a simulator tool to verify that your device has also been properly configured before deploying it. So in this scenario we're given a master device who seems to disconnect at seemingly random times. So we've configured our Test Harness's simulated outstation to match the field outstation as much as possible. Our hope here is to recreate the disconnect and track down why it's timing out. So let's go ahead and look at that. I'm going to go over to our DTM, our device under test tool. We need a master in here, not the outstation, so I'm just going to right click manage and disable all the devices from the first three demos. Then I'm going to go down here to demo four and enable this device. Now in order to test our master, which is our device under test, I need an outstation in Test Harness. So I could configure a new device here for you, but I've already created a workspace before the webinar started to match the configurations of this theoretical device in the field. So I can just go file open workspace and then go to demo four and just open this up. So in this situation, what we're seeing is we have our master here simulating DTM and our outstation in Test Harness. We are seeing that the devices are up and connected and the information is being sent across. So what we can do is test to see when the master times out and when the failure occurs. So we see that it's not disconnecting, there's no errors with it just sitting here, that all seems fine. So let's add another layer of complication to it. So to do that I'm going to go from my master and just issue an integrity data poll. So I'm just going to go ahead, let me clear the display, issue the command. So sure enough, the integrity data command is coming across, it's grabbing all the information inside of the outstation, it's being packed up and sent back to him, his database is updating, so that all looks good. So now let's add another layer of complications on top of it. Let's go ahead and change some of the data inside of our outstation. So I'll go to open data window. As we've seen before, this time because we have an outstation in here, we can actually interact with the data. So I could go to these binary inputs and I could toggle that value, I could go to these analog inputs and all these guys are 50, I could edit one at a time or I can multi-select and let's just change these analog values. I'm going to change them to say 200 from 50 and click accept. So what that's done now, it's actually gone into our simulated outstation in test harness and changed all those values that I interacted with. And the reason I did that is because now I'm adding event data on top of the values that we're already sending across. We're going to see if not only the integrity data poll can be successful, but if there's event data, will it also be successful? So let me clear this as well and go ahead and issue that command. And we're seeing the same thing come across. It looks like everything looks fine, the values are coming across, but what I'm seeing is the database in the master is not updating. And sure enough, after a little while, we do see that time out. So I'm trying to think about what exactly we did in between when it was working and when it wasn't working. And what we've done is we filled the packets that we're sending across with more data. So I'm thinking maybe like some sort of overflow has occurred. So I can go in and look at and reconfigure my master. So if I right click here, I can go and edit its configuration information. And I'm going to go over to the advanced tab here. So I'm looking through here for overloads, for sizes. And one thing I noticed because I just configured the outstation and test harness is that the receive fragment size is 1024. I know the device that this master eventually needs to connect up to in the field actually has a receive fragment size of 2048. So what's effectively happening is at the 1024 level, we're able to get all of the information across all the static information. But as soon as we start layering on event data on top of it, it breaks this fragment size and the timeout occurs. So I'm going to match the configuration here to 2048 that I know is the device that it will be connecting to in the field and how I've configured the test harness to connect to it. So I'll hit OK. It'll restart the device up. And then we need to go through kind of the same steps. We see the devices have connected. We're not seeing any issues. So I'll go ahead and clear this. I'll go ahead and issue another integrity data poll. All that information is coming across. Again, we're not seeing any issues. And then I will come down here to this event data. And I will change the values. And I'll change it to say 500 this time. Those values have all updated now. And if I issue this, well, let me clear just so we can make sure you don't see it. If I issue the control here, we see all the values come across. We're not seeing that timeout. If I go and look in my master device, I actually do see that the values have updated. So that all seems correct. So that's one way that you can use the protocol test harness as an outstation to track down maybe configuration errors or something along that in your master device. Let's say you were unable to find the issue and you had some information from your history of the test that you've run. And you did want to output the log as one of the questions asked during the last Q&A session. Just to tag along more information, yeah, you would just go over here to file. And then you've got your save protocol analyzer. We've got text and XML. You could then output the test that you've run, the diagnostic information you have, and potentially take that to a coworker or someone who maybe has more experience with the protocol and see if they can help you figure out what you saw potentially in the field after you send it back to them in the office or something along those lines. So now we're going to go back to the PowerPoint. And now that we've talked about how to simulate outstations and masters in the test harness, we're going to have another question segment. So I'll hand that back over to Beth. Okay, yep, we do have a bunch of questions coming in. Let me start with this one. I think this one's going to be for you, Steve. I noticed that error messages were in red in the test harness protocol analyzer window. Is that always the way? Are there other message, are other message certain colors? So yes, they're red. But that's just configured that way. You have the ability in the test harness to open a configuration window for the protocol analyzer. And you can choose colors and fonts for each layer of the protocol for specific types of messages like errors or different data or events, you know, static data or events. And you can color the transmit and receives differently if you choose, or like secure authentication, you can make a different color. So you have the ability to choose colors or fonts that maybe would help you isolate what it is you're looking for quickly in the protocol analyzer. You know, you also have the ability to turn layers off in terms of filtering. But if you want to set the colors in such a way that, you know, red's good for an error, but whatever is good for you, you can set. And it gets saved, you know, if you save the workspace, well, actually you don't, just when you exit the test harness, it saves that configuration as you're in a settings file. Okay. The next question might have a couple of different answers to it. So let me read it. I'll hand it over to the panel. And Matt, you might even want to add a little bit to this with some insight into DTM, the other tool. How can you change points continuously? Example, take analog input in an incrementally, increment automatically. So, yeah, I can, I can take over that one. There's a few ways. Let's, let's talk about both the test harness and DTM. In test harness, one of the last demos that we'll be going over today is scripting. And, you know, Steve, correct me if there's a better way, but how I would do that in test harness is you could just write up a quick script to change the values. And then you can use the previous value as the new value. If you have an analog value, you could increment that by adding a certain just flat value to it, a random value. I mean, with scripting, you have all the power in the world and you could just pretty much run that to allow changing. And the same thing could be done in DTM. If you are using DTM as a simulator, we have the ability to use scripting as well. We also have two other ways in DTM to simulate, you know, a full system of device tests. You can use a CSV file and just play that back continuously. So you have different states for all your different devices and all your different data attributes inside of each of your data, your devices. So you can run that on playback. And we also have a tool called auto data changes, which is basically just dummy data. You go from, you know, analog values from zero to a hundred, you can ramp, you can do sign waves, you can do different types of things like that. You can even do random values between a max and a min point, and you can periodically change it every second, minute, hour, however you want to configure that up. If there is a better way or you have another suggestion I may have missed in the test harness, feel free for any of our panelists to unmute and let them know. That sounds like lots of answers. I mean, you can also do in the command window that, you know, it can do something every time period. You could have it change values. And I don't know if you said it, but C sharp or tickle C sharp plugins you could write. So lots of possible ways to do it in the test harness and in DTM. Okay, thanks. And just a follow-up question to that. Someone had asked a little earlier in the chat, do we require a separate license for DTM or can it be used on the same dongle as test harness? And I can answer that. I mean, DTM is a separate license that is purchased through Triangle. It can work with test harness. It can be available on the same dongle as test harness, but it is in fact a separate license from test harness. I'm going to do one more question. I think we'll go back to the presentation. So we make sure we cover everything in time. The test harness log looks very similar in usage as compared to Wireshark. I was under the impression that Wireshark is actually embedded in test harness. Does someone from the panel want to take that? I'll answer. Yeah. So Wireshark is not embedded in the test harness. Monitor mode over TCP, so when you're monitoring TCP channels, does use WinPCAP or whatever its new name is, but there's a new name for it, but WinPCAP. So the same base that Wireshark uses to actually do the sniffing, but the decoding is done by the test harness, not by Wireshark. So that, you know, that they are actually the display and the decoding is completely separate. The sniffing of the TCP is the same between Wireshark and the test harness. Okay. Thanks, Steve. Matt, if you want to go ahead back to the presentation. Yeah, absolutely. Okay. So there's two more demos I need to go over today. These can be seen as more of the advanced topics of the things that we're going to be talking about, and the first is conformance testing. The DNP3 users group maintains a testing procedure that will verify your device's conformance to the DNP3 spec. The tests can be executed by the vendor or a third party, but the results logs must be verified by the DNP users group. Recently, the users group has dedicated some more time toward improving the certification process. The benefits to having your device pass this test is to prove that your device is more likely to be interoperable with other vendors DNP3 devices. The conformance test is customizable based on what features your device supports, and all of that is defined in your XML device profile. The test harness application has a tool that allows you to pre-test your device and find issues early on in the development process. In fact, some of the third party test houses use the test harness to test the DNP3 conformance. So let's take a look at an example of conformance testing. In this situation, we want to run the conformance test on the device under test, and that device under test would be an outstation. Again, I'm going to disable the master here, and I will go down to my demo 5, which is the conformance test one, and enable an outstation for us to perform these tests on. I'm also just going to close the workspace here and start with a blank page to allow us to show you how to build up the conformance test all together. So again, as we've done in the past, you go up to open, and then you've got conformance test and DNP. So from here, you're able to then take a look at all the tests that will be performed. You can select these tests individually. You can select all level 1 tests, all level 2 tests. You can select everything, deselect everything. Reset state will just put everything back to how it was when you first opened it up. And then we've got a few other things here for connecting and executing the test and things like that. But I mentioned earlier about how all this is customizable based on your XML device profile, and so the conformance test needs to be within test harness as well. So to do that, you go to file and do edit configuration. So if I do that, what it will do is it will bring up the configuration for the DNP3 conformance testing. So from this very front tab here, this general setup, we're able to choose what data types we support, the application layer information. So here's some of the fragment sizes we talked about for if you support unsolicited messages or not. This will allow you to kind of customize all of this. For the sake of this demo, I don't, you know, want to go through and show off the entire conformance test just because it might take a little while. So I can show that, you know, you can disable these different data types, and along the type and along the top here, you'll see that the different configurations for the tests are being enabled or disabled. So for this case, I'm going to be looking at analog outputs. I can go over the analog outputs and configure the tests. How this configuration works is effectively anything that needs to do select before operate, it's going to use whatever this point index is. This is the initial value that it will be set to, and this will be the value it will be controlled to. So all of this is completely editable. So if you're, you know, analog output index one doesn't support select before operate, you would want to change this. Otherwise, you could leave it as is. And then all the different functionalities that we might be performing on analog outputs will be listed here, and the values, initial values, the test values, things along that can be configured as well. So now that we've done that, the next thing we'll want to look at is how to actually connect up to our device under test. So that would be under the connections tab here. And as we've been doing today, we've just been using TCP IP, so I'll stick to that. I know my host, my outstation, that I'll be connecting up to is, again, 15, 16, 17, 100, and the default port number of 20,000. So if I hit close here, what we'll do now is we have now configured our conformance test to match the information on our outstation. So I can click connect, and what that will do is we'll create a master based on the connection information we have. If we want to just, you know, be super confident that everything's connected, I can do a general interrogation. And sure enough, all that information is coming across. So now let's actually run some tests. So I'm pretty sure I had binary output tests. I'll just go down here and grab a few of these guys. So I'll select maybe these four here, and I will execute those tests. So what we will see is the log is now filling up here to allow us to see what's going on during each of these tests. And then at the very bottom, once I only did four, these four tests have run. We'll see the number of tests was completed was four. Two of them passed. There was a warning for one of them and a failure for one of them. And you can see that here at the bottom. You can see that as you scroll through the log, the different colors, so red is a failure, orange is a warning, and green is a success. You can see that there. Also, if you don't want to scroll through the log, because say you did, you know, every test, you can see over here on the check marks, a green check mark, obviously pass red failure. So you can get the information like that as well, to be able to see what's happening. You can actually save this off. So if I do file, save test execution, I can then save that off to an output file somewhere. So maybe we can test today's performance with tomorrow's, you know, maybe update or something like that, to see if we've resolved whatever the failings or warnings might be. It's also worth noting, if you're going to be testing the same type of devices, instead of editing the configuration every single time you come in, it might be a good idea to after you edit it to what you want it to be, save that off again to an external file somewhere. And then when you come in and load up test harness next week or tomorrow or whatever it is, you can just load that configuration in rather than having to reconfigure it each time. And as I mentioned before, you can click reset state here, and that'll reset the state of all of your tests being run. And you can clear out the log here as well to get you back to the original state. So that's how we work with conformance testing. One other question we get asked a lot is about automation testing. So let's take a little while to talk about automation testing. Automation testing is a large part of regression testing. There are many reasons to implement automation testing, and a few are shown here on the screen. Specifically, I'd like to talk about testing variations and its effectiveness. Certain tests can be repetitive and follow the same process over and over, and the only difference between these tests are slight variations of the inputs. This is a perfect type of thing to automate as testing one or automating one process can test many different requirements all at the same time. It also frees up the human who used to perform those tests to perform more complex things, which may be harder to automate. Automation testing in the test harness, as mentioned in the previous Q&A, is done through TCL and the C-Sharp plugins. So I'm going to go back. I am going to go ahead and disable all of my devices inside of my DTM tool, and I'm going to show you how automation works. So what you can do with automation is you'll be wanting to interact with the TCL interpreter down here. So you can do just, for example, mdnphelp. If I hit that in, it's going to give us a detailed description of every sort of command that's built into the test harness about the mdnps. It'll give you the name of what the command is you need to type and a little bit of information about what it does afterward. So if I scroll here to the bottom, I'll just look at this bottom one here. It's mdnpdb for database, and I'll just put a question mark afterward. That'll give you all the parameters that are required and optional for this specific test. From here, you could obviously learn what all of these functions do and what they are and come in here and type out any sort of things you want to do, say creating device, issuing controls, et cetera, et cetera. But once you get comfortable with that, you can actually bring up something like Notepad++ and then type in, say, mdnpdb and then all the parameters that you need and build up a script. We don't have a lot of time left today in the presentation, so I'm not going to try to script something for you here live, but what I can show you is we do have this demo script. This demo script is something I made beforehand. It's based completely on a built-in script that we provide with every test harness install. I've simplified it. I've cut it down by maybe half or maybe cut it into a third. I effectively have a main function here. I've got a command that I've built here that runs all the commands. I've got a section here that just creates devices, so it creates one master, one outstation. Then, of course, I've just grabbed all these functions from the script I was talking about to actually issue commands. Now that I've written up this script, I can just go ahead and type source and give the location of where the file is and hit okay. Now that I've done that, test harness not only knows about all the pre-built-in commands but also all of the new functions that I have in that script. If I do run DNP now and hit enter, it's going to execute the main function, which I named run DNP. What it did is it created an outstation, it created a master, and now it's waiting 10 seconds. Sure enough, there's a master and outstation here. After the 10 seconds is over, the master then issues a binary command, an analog command, and does an integrity data pull. There's no errors or warnings. Everything ran through properly. Sure enough, we're seeing all this information come across in the protocol analyzer here. It's also worth noting if I just deleted out this run DNP and this closing bracket here, when I sourced this file, it would learn all of the functions that are built in and then just issue these commands one by one. You could also just copy and paste from your script into this output or into this interpreter here to run your test as well. There's a lot of different ways. I've only touched on TCL today. There's also the C-sharp interface that you can do. There's a lot of different ways that you can automate your tests and have these things run while you're working on more complex tests or getting a coffee or overnight or some people you do nightly build type things. Automation is definitely a feature we focused on with the test harness. Let's get back to the PowerPoint. We've looked at the demo for scripting. I am now going to hand it back over to Beth for our final thoughts. Then, of course, stick around afterward. We will be finishing up our Q&As and answering as many questions as people put out there for us. Beth? Thanks, Matt. We made it just in the nick of time. It's 11 o'clock now at one hour. The recording of the webinar will be posted on our homepage at trianglemicroworks.com. You will also receive a follow-up email that will have a link to the recording within the next 24 hours if you respond to the survey. Once you close out of here, you'll get a very short survey. It's three questions. We'd love to get your feedback. As part of the response from that, you'll get the recording to the webinar itself. We also will be posting it on our YouTube channel. Please go ahead. If you'd like to keep tabs on our upcoming webinar, if you go under the training tabs on our website, you'll always see the latest webinar that we've come up. We typically are doing them on a monthly basis. You could also sign up for our newsletter. That sign up is right on our homepage of our website. That will allow you to get email updates for upcoming webinars and product releases. Now, with the tools you saw here today, Test Harness and DTM, you can also download evaluation licenses to both of those tools and try them out firsthand. Certainly, if you have any questions or additional comments, please feel free to email us directly at sales at trianglemicroworks.com. With that, I want to thank everyone for their time. As we mentioned a couple of times, we'll continue to stay on. We do still have some questions coming in. I'm going to read through some of those and answer that. Whatever we did not answer today online, we do send out the question and answer spreadsheet with all the answers in a follow-up email probably about in a couple of weeks. You will get an answer at some point if we were not able to answer you online today or live on the webinar itself. With that, again, anyone that wants to stay on for a couple more minutes, please feel free to continue to enter some questions. I'm going to go ahead and engage the panel again and read off a couple more questions. With that, I'm going to hand this over probably quickly to the panel. Someone did ask about support for subset level 3. Does Test Harness have support for subset level 3? Does someone from the panel want to take that, Steve or Alan? I'll take it. Specifically, I think that the question was related to the conformance testing, but I'll answer it in general. Test Harness supports well beyond level 3 in terms of the functionality of DNP. If you all know level 1, 2, 3, 4 are defined for DNP. They have different functionality. The conformance tests were recently updated to include level 3 testing. The version of the Test Harness that Matt was showing is 328. That did not have level 3. The next version will include level 3 testing as that spec is released. Thanks, Steve. Does any of conformance tests require pre-approval from Triangle for patent issues or so on? I think that might be another one. Yeah, I'll answer that one too because it's all related. As long as you license the Test Harness and license conformance testing tool in the Test Harness, you can use it however you want. So, no, there's no issues about how many devices or anything about patents or license. Again, as long as it's licensed, you can use it to self-test. You can test other people's devices that you're buying or whatever you want to do. No restriction. Here's another one on conformance testing. Which protocols does Test Harness support conformance testing for? Really, it's just DNP. It's mainly DNP. We do have Modbus conformance testing. That's undergone some changes lately, so I couldn't tell you the status of where the Modbus group has gone with their conformance testing. We do have a version of conformance testing for Modbus in the Test Harness, and then DNP is being updated to match the latest spec. Okay, next question. Is there any plan to include Python as an option to do scripting in the Test Harness in the near future? I'm going to throw that one over to you, Steve. Yeah, so Python scripting already exists in the Test Harness. It's documented in the Test Harness manual, but I would have trouble winging it right here. Python scripting is different. It's very programmatic and is different than TCL, but from the Test Harness point of view, yes, you can use it, and there are examples in the Test Harness manual as to how to do it. And I do see a number of questions coming in on automated testing, but they were coming in just as right before Matt did the demo or as he was doing the demo, and I think a lot of them got answered through the demo, but if you still have questions on the automated testing functionality, please go ahead and submit those while we're still on. No, as far as automated testing, if customers want example code or, you know, it would be better if they can just send us a kind of a support request, and we can craft some example scripts to show them rather than trying to do it through the chat, I think. Okay, yeah, that's a great suggestion. So if someone has specific questions on the automated Okay, yeah, that's a great suggestion. So if someone has specific questions on the automated testing, absolutely, you can send that, I guess, directly to, you know, either sales at trianglemicroworks.com or support at trianglemicroworks.com, and you'll be lucky enough to work one-on-one with Alan on those. If I can add to that, you know, someone asked about whether that demo script.tcl that Matt was using is available. A superset of that file exists already with the test harness. We have samples for DNP and all the protocols that are automatically loaded to the test harness. So, you know, the specific function, the specific procedure he called to run the demo is not in there, but one that does more than that is in the sample. It's a good starting point for writing your own tcl scripts. Great, I think that actually just answers one of the questions that just came in on that, so thanks for that. And I'm sorry, one more, and again, I don't mean to tell you to read the manual, but the details of where to find it are in the manual, and rather than me try and wing it, I would look there to see where it is, because I know you can say, well, where is it? But look in there and see. It'll tell you what directory they're in and how they're used. Okay, I've got a couple of questions that were asked a little bit earlier. I'll just spend a minute or two on those. Some devices report analog database on deadband configurations. Does that exist in test harness for a slave simulation? Yeah, since I answered that one, I'm sure I know the answer for that one. Yes, the test harness does have, for DNP, has deadband built into it, so if you set deadband values for the analog inputs, they'll be used to determine when to generate events. Deadband algorithms, there are lots of them, and they can be very complicated, but the one that's used in the test harness is the simple obvious one. If a value changes by more than the deadband value over, since it last reported, it will generate an event, so there's not time deadband, and there's not lots of options for deadband, but there's the simple one, and it shows how deadband is used, and if you're exercising, if you're testing the master, it doesn't really matter why the outstation decides to generate events. The key thing is it generates events when deadband is successful. Okay, thanks, Steve. Okay, I think I'm going to stop there. The questions have slowed down a little bit, and we've kind of covered a couple of the ones that were asked earlier on. Again, as I mentioned, we will go through and answer all the questions that were submitted here online and send them out to anyone that attended the webinar, so be on the lookout for that. I want to thank everybody for joining us today. Thanks to our panel and to Matt for presenting and for taking the time to look a little bit more at our deeper dive into our testing tools, so thanks, everyone. Enjoy the rest of your day. Thank you.


Video ID: gzkp4S3EiSg
Good morning and good afternoon. Welcome to today's webinar, IEC 61850 Tools and Techniques, Simulation of a Specification for a Digital Substation. My name is Beth Kapalis, and I'm the Marketing Manager here at Triangle Microworks. Before we get started, I'd like to go over some housekeeping items and some introductions. The webinar is scheduled to be one hour long. We will be stopping for questions a couple of times throughout the webinar, and we'll stay on afterwards to answer as many questions as possible. The audience is on mute, so please submit your questions online using the webinar tool under the section titled Questions. We have several folks answering questions online behind the scenes, and I will read and answer questions during the breaks and at the end. Next, I'd like to introduce our speakers. We have three presenters today. First, we have Thomas Sturcks from ELEA Engineering. Thomas has been with ELEA for more than seven years with a focus on IEC 61850 engineering and modeling. He is also a member of the IEC 61850 Working Group 10 and SIGRE B568. Next we have Christoph Brunner, President of IT4Power. Christoph is a well-known utility industry professional and 61850 expert with over 30 years of industry experience. Christoph has been involved in the development of this standard since the beginning. He is convener of Working Group 10 and member of Working Group 17, 18, and 19 of IEC TC57. He is an IEEE Fellow and a member of several IEEE Working Groups, as well as a member of the PSRC Main Committee and the Subcommittee H. Lastly, we have our own Matt Green. Matt is a Senior Application Engineer here at Triangle Microworks and has been with us for over six years. He specializes in QA testing with all our simulation tools in Gateway, works as a technical trainer on Triangle software tools, creates training videos, and participates in product demos for customers and conferences. So now that you know everybody, I'd like to give you a little bit of background on Triangle Microworks before we get started. Triangle was established in 1994 and we are headquartered in Raleigh, North Carolina. We have customers in over 70 countries and work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs in development time. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They're designed to troubleshoot, simulate, and automate testing. Lastly, we also provide SCADA Data Gateway, which supports both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. We are heavily involved in the standards committees that support SCADA protocols, like UCA Group for 621850 and DMP User and Technical Committees. Through our participation, we help define the protocols, stay up to date on the changes that take place, and ultimately provide feedback based on our customers' needs as the protocols continue to evolve. So now that you know a little bit more about Triangle, I'd like to go ahead and hand it over to Thomas to kick off the topic we're here to learn more about. So Thomas, I'll go ahead and make you a presenter. Yeah. Thank you, Beth. Okay. You should now have the screen. Do you see my screen, Beth? Yep. We can see it. Okay. Thank you. So good morning and good afternoon. My name is Thomas Seix. I work for the Belgian DSO Alia, and I will speak today about a sales specification from a user's perspective. Before we start, first of all, the agenda. So I'll introduce Alia and the Alia context, so you have a good idea of what our starting point is. Then I will specify what we call the definition of a good specification. Then a small part about how we specify or build our specification today. Then what we consider as what specification should become in the future. Then a little bit of explanation how the Osmos project contributes to that. And then I end with some next steps and the conclusions. So the Alia is part of the Alia group, which consists of two DSOs, one in Belgium, one in Germany called 50 Hertz. And to start with the Alia context, I'll first give some details about the technology that we use today and the way that we work within Alia related to secondary systems. So what you see here on top is the current architecture that we apply in our current frame agreements for protection and control. We are implementing substation bus 61850 between the level of the bay and the gateway. Communication between protection and control cubicles per bay is still done in copper wiring, as well as the wiring between the protection and control cubicles and the field outside. What we are moving to in the future will be a full implementation of a digital substation, including substation bus and process bus. So let's start by saying that we use frame agreements for protection and control cubicles. These are frame agreements of eight years that we have with two manufacturers. Our current frame agreement, which consists of this type of architecture is called SPEX3. What you see here is the ongoing frame until 2026. For the other general applications like busbar protection, RTO, DCS, etc., we have separate asynchronous frame agreements. We do our engineering maximally in-house, so our competences stay maximally in-house. We are focusing heavily on standardization and on templating. For example, we have 52 bay cubicles, which we apply in all of our projects, and we engineer and commission around 200 cubicles, protection and control cubicles, each year. So what do we call a good specification? First of all, a specification should allow a correct interpretation. It should be complete, sufficient level of detail, and it should be centralized. You don't have to look for information in five or six different documents. No, everything should be centralized and should be complete and clear. A good specification also allows a correct evaluation of what you receive back from the manufacturer. So with clear criteria, measurable, and really easy to understand what the manufacturer is offering you. It should also allow a fast startup after your procurement stage. So you want to recover the effort that you put into your specification so you don't have a long startup term and you can recover everything that you did before when starting your project or a frame agreement. It should also maximize testing possibilities already at specification stage. So you want to make sure that the specification that you build is of good quality. And you want to verify that by testing it as much already as possible in the specification stage. It should also be reusable so you don't have to redo everything for a next project or a next frame agreement. Very important also is that you are able to trace what you specified into the templates and the project files that are built afterwards upon that specification. And in that way, also enable the automation of updates to all of these files when your specification is updated. And the last but also very, very important one is specification should not narrow down the market. It should keep open the market so you can get offers from as many manufacturers as possible. So now we've seen the definition of what we consider a good specification. This is a slide explaining how we specify and how many other utilities also specify their requirements today and how they implement them. It's in a first stage creating specifications and requirements which are mostly non-machine readable elements, just raw text, non-intelligent drawings, etc. With that, they go to the manufacturers and they have a selection and purchasing stage where they get an offer from the manufacturers and negotiate with them. But most of the times, there's not a lot of time to go into real deep technical discussions with them in this stage. Then after the final definition, we start creating templates which we then can use afterwards for executing the projects. And this is also a very effort intensive stage because until now, we could not recover anything from the earlier stages. We cannot recover the effort that we already put into our specification into the creation of the template. So this is everything needs to be started up from scratch, very effort intensive. And again, we have a very limited time because the need of the projects is behind it. And so we need to move forward very fast. It's also the first time that we can provide feedback to our specification because it's the first time that it's really been used for projects. So this means that we have to wait until the creation of templates before we can really verify if our specification was well built. And then again, if we detect errors. It means that all the templates that are affected need to be modified manually because we did not recover any effort. Yeah, so indeed, the last point about feedback I already covered. And when we compare the current process then to our definition of a good specification, we see that on almost each of the points, almost all of the points, our current process does not respect the definition of a good specification. So we identify here that this is not the best way to work. So what we did is we tried to enhance the process in order to be more in line with the definition of a good specification. And what we identified is that we need to extend the part that is before the selection and the purchasing. And we do that on different levels. So we want, first of all, more the ability to prepare and test things in advance, because we want to have the ability to provide feedback to the specification in a stage where we did not yet select or purchase anything. So we can go to the manufacturers with a specification which is of a high quality. And in order to do that, we also want to include more automation and efficiency by making things more automated, not only for the user in the creation of the specification, but also for the manufacturer which needs to respond to your specification. And in this way, by preparing more in advance and allowing more automation, speeding up the phases after the start of the project. And because we prepare more in advance, we can create our templates easier and more efficient so we can start with projects in an earlier stage. And what you see here is the updated process where you can see that there is now in the earlier stages already more feedback possible to the specification. And how do we do that? By creating vendor independent templates which are machine readable for the vendor and so that we can move forward in a more automated process. How does the OSMOS project contribute to this? So first, a small introduction of this project. So OSMOS is a research project which is sponsored by the European community. And a part of this project is dedicated to the enhancement of the 650 engineering process. And the result of this research has provided ways to now specify system data flow inside SEL without having to allocate anything to an IED. So we can now really specify in SEL the way we want our system to behave, the way data needs to flow between elements, without having to introduce IED elements or vendor dependent elements into it. So we can really make an independent specification. And also, we can now trace the specification into the files that reflect the implementation of the specification. So these are two very important points which allow us to respond more to our definition of good specification. And how do we implement these two points? By extending the SEL language with additional elements, which allow us then to remove the layer of the IED and the vendor dependent layers of the SEL file in order to get a vendor independent file. We did that in close collaboration with the 650 standard, more specifically the 6100 part, to guarantee that all of the things that we are identifying and developing within OSMOS get introduced into the standard. So the 6100 will implement the recommendations and the developments that we provide within OSMOS. We also demonstrated our developments. We demonstrated the extensions within the OSMOS project with tool and product prototypes from different vendors. Now, what is the impact of all of this on tools? First of all, specification tools now allow to create a vendor and IED independent system specification. There is a new need for tools identified, which we call comparison tools, which allow to compare what you specified with what you get back from a manufacturer. And for example, also allow to compare what you specified with the final implementation of the IEDs of the manufacturer. So this is a new role for tools that we identify and which can help the user during the engineering process. Configuration tools can now use also the data flow that we include in our specification to implement it in an automated way. So if you already provide specification with included data flow, a configuration tool can now use this specification to automate the data flow between real IEDs. For testing tools, we also see new possibilities in, for example, now testing a specification which does not yet contain any IED or vendor information, allowing a user to verify the quality of a specification in already in a very early stage. But the work is not finalized, so we identified a lot of things within OSMOS, but we are still identifying other things that are still missing in order to really get to a good and final specification, which is conform all our points of the definition. For example, we want to be able to specify also physical interfaces of IEDs so that we can have the ability to specify if we want a signal on a binary input of an IED, for example, without having to specify the IED itself. Or, for example, wanting an analog value on an analog input of an IED. So we really want to be able to model the IED already in specification without having to include real IED or vendor information. We also want to be able to define application templates so we don't have to repeat many times the same actions in creation of the specification. So we want to create a specification site template, which we can then reuse during the next steps in the process. We also would like to introduce functional variants, for example, so that you can quickly, with some easy selections, modify your specification in order to make it correspond to specific specification cases. For example, your specification is slightly different depending on the voltage level that you work on. With functional variants, you could easily then adapt in an automated way your specification to the specificities of that voltage level. And as a last point, we also want the possibility to link what we call the description of a logical behavior of your specification and link that to the SEL file directly in order to get a more centralized specification and have all data in one place. And so these are only some examples of the things that we think are still missing in order to really get the best possible specification. But as I've shown here, OSMOS is already providing many, many improvements. So this was my last slide. Thank you for listening. And that's my presentation. Great. Thanks, Thomas. Now we're going to switch over to Christoph Brunner. So I'll go ahead and make you presenter, Christoph. And you should now have the screen. Yes, thank you, Beth. I just need to select the screen that I want to show. You should see my screen now. Yeah, we can see it. OK, good morning as well to everybody from my side. What I want to present now in the next presentation is a little bit the concept of an application in the context of IC61850 and the way how we can specify that from an IC61850 perspective. So what we see on this slide is that applications, when we look at applications in general, they are built by exchanging signals between different functional elements. That's how we typically create an application. In IC61850, the functional elements are logical nodes. And the information exchange between these logical nodes that can be internal in a device if they are implemented in the same device, but it can also be external. And that's when we typically use GUIs if the logical nodes are in different devices. So what we see here is such an example of a logical node that has outputs, which are the data objects, data attributes standardized in the standard. These outputs may go as signals to other logical nodes, to other functional elements. We also get inputs. Inputs today are not standardized, but we can at least describe where we get an input from. And we also may have parameters and controls as elements of these logical nodes. The next slide here gives us an example of an application in IC61850. And the application is basically a breaker failure. So we have somewhere our protection elements. In this example, I show a few distance elements. The signals from these protection elements go to a logical node that is the trip logic, the logical node PTRC. From there, the trip, if we need to trip, is going to the logical node XCBR, which is the interface to the switchgear. And we may have a physical wire going out of the device to the opening coil of the breaker. But at the same time, this trip signal will initiate the breaker failure function, which is the logical node RBRF. The breaker failure function will supervise that the breaker opened. If the breaker didn't open, it can either do a re-trip, and if it's still not successful, it has to clear the surrounding breakers, which is done with the standardized data object operate external that is sent to all the other XCBRs of the concerned breakers that may need to open. So that's basically the application, how the different signals between the different logical nodes work together to implement such an application. Now we can allocate that to devices, and in this case, we have a first device here, the IED1, which would be the protection device. In our example, we implement the breaker failure function in the control device, so that will be a different device. And then, of course, we have the additional IEDs in the other base that would interface to the neighbor breakers that we need to clear in the case of a fault. What we see now is we still have internal signal exchanges between these logical nodes like before, but some of our signal exchanges are now going between the devices. And that's where we typically will apply goose messaging. In the traditional world, you would have applied wires there, you would have routed this signal to a physical output, you would have created a wire going to a physical input in the other device. With IC62850, we now can use goose messages there, so we basically have virtual wires, virtual signals, virtual connections. But now we are using goose messages between the devices to distribute that information. But in principle, the concept of the application stays exactly the same, whether there is devices there or not. What we can already do today in IC62850, we can describe when we have a goose message between the devices, we can describe through the use of extracts where signals are getting into a logical node. As an example, the logical node from a neighbor breaker that would need to trip when the external signal gets activated would have, among other, an extract, which basically represents an input, an extract pointing to a signal OPX that he has to subscribe somewhere from another device, from another logical node, RBRF, in another device. That's what we can already do today with IC62850 addition, the part 6, as we have it today. Now, when we look in general at how we have to implement an application in IC62850, we have basically three steps. We start, like Thomas explained, with the specification of the application, the details of the application, which means the signals that have to be exchanged between the functions. This corresponds to a wire diagram that we had in a conventional design. We then will allocate functions to devices. And at the end, we have to translate the signals that need to be exchanged between the functions of the devices into goose messages. With IC62850 part 6, as it is today, this can only be described at design when the IDs are added. And that's why, what Thomas already mentioned, we wanted to get a step further. And in this part 6-100 that we are working on, we define a couple of extensions. Among them is also the extension that we already can specify interactions between logical nodes in the specification stage. So when we do not yet have IDs associated to our system. And that has been done by adding a new element source reference as inputs to L-nodes. The L-nodes are the ones that we have in the substation or process section. This is comparable to the extra that are inputs to the LNs in the ID section. But with that, when we create the specification, we can already define the details of our protection schemes. Like Thomas mentioned, he would have the possibility in the future to do that. And as he mentioned as well, this allows to test the scheme already at the specification phase using simulation. And that's what we will see later on in the demonstration. Now, let's get to the step of verifying the concepts. As Thomas had mentioned, the OSMOS project has worked together with the working group 10 of TC57 to define those extensions to the part 6 in the 6-100. And we have established a demonstrator where 6-100 extensions are tested with tools and products prototyping those extensions. The demonstrator emulates in principle two substations at two ends of a power line. It implements the two feeder bays in the two substations. Functionality includes protection and control, including reclosing and a permissive teleprotection scheme. We have real IDs that are connected to a real-time simulator that simulates the power system. The demonstrator is set up in the laboratories from R&D Nester in Lisbon. Tools are supporting the extension from 6-150-6-100 are used to test the whole engineering process, as Thomas has mentioned that. One of the applications that we now will later on demonstrate how we can simulate that is a teleprotection example. This slide is the teleprotection example from the OSMOS demonstrator. So we basically have two protection zones. Zone 1 protection, which is the protection up to maybe like 75% of the line, which will do an immediate trip. But then we have the second zone, the teleprotection zone that is overreaching. So that's to cover the areas of the line that are far away from the own substation. But as the zone goes beyond the other substation, we need to get the permissive signal from the other side to make sure that if the fault is between the two substations, so that we don't trip when the fault is beyond the substation. So that's why we have to exchange also teleprotection signals. So if you have a fault, if the substation on the left side sees the fault somewhere here, it will see the fault only in the teleprotection zone and the teleprotection zone will only trip when it gets the permissive signal from the other side, which is sent when the other side sees the fault as well on the line. Then we get the permissive signal and we'll trip as well. If the fault is beyond the substation, this substation will not see the fault on the line. It will not send a permissive signal and we will not trip. And that's an example that now will be demonstrated in the next part by Matt, simulated in DTM based on the STL files that we have created as part of the OSMOS project and that includes this 6-100 enhancement. And with that, I hand over back to Beth, respectively to Matt. Great, thanks Christoph. Yep, Matt, I'm going to make you presenter. And while Matt just gets his screen set up, I just want to remind everyone you can feel free to submit any questions that you have online, even while the presentation is going on. We do have several folks sitting behind the scenes ready to answer questions, so it's just a great time to kind of ask the expert. So with that, Matt, I'll go ahead and hand it over to you. Thanks, Beth. So now that Christoph has gone over this in PowerPoint, let's take a look at an example. We're going to look at the OSMOS sample application, specifically at two protection schemes triggered by different fault conditions. The OSMOS sample application defines two different substations, SSA and SSB, that communicate via teleprotection. The first fault we'll be looking at today is noticed by both SSA and SSB in zone 1. The second fault will be outside of SSA's zone 1 and will rely on SSB's teleprotection. We will be looking at these protection schemes and how they're handled within the SCL and how they can be simulated in DTM. So on my screen right now is part of the full signal flow diagram for how these protection schemes work. But for the first fault, we'll be looking at this section here. This is the signal flow for the three-phase distance protection scheme to trigger a trip on phase A, B, and C breakers when a fault is detected within zone 1. So let's take a look at this inside of the SCL file. As defined in the 61850 engineering process, substation design tools will generate a system configuration description file or an SCD file. This file contains the substation architecture and all of the IEDs required to implement the system. The OSMOS project enhanced the specification step within the IEC 61850 engineering process. The specification step generates a system specification description file or SSD, whereas the design step is when the SCD file is generated. The SSD contains a substation section, like here, and the substation section defines the architecture of the substation. It can also optionally contain the definition of virtual IEDs. Virtual IEDs are start to splitting up the functionality of the substation into IEDs, but they don't necessarily correspond to any actual IEDs yet. The OSMOS project relies on the functionality of the substation The OSMOS project relies on the fields labeled with 61850-6-100, and you will see these fields in the SCL files in Navigator. So the OSMOS project's sample SSD file for substation A contains the complete definition of substation A and a minimal definition of substation B. The only parts from substation B that are necessary for the functionality of substation A are included in the SSD file for substation A, and this goes vice versa as well. So for example, we have the entire SSA description here, but if we drill down into the SSB, you'll see we only have the function definitions for the teleprotections. In 61850, a substation is defined by one or more voltage levels, and then inside the voltage levels you have one or more bays. Inside the bays, you have conducting equipment and a set of functions. Conducting equipment is like breakers, switches, current transformers, things along those lines, whereas the functions are a list of signals and logic required to run the functionality of your substation. So if I open up one of the conducting equipment here, you'll see terminals, and these terminals are defining the connectivity of the lines in your power system. And if I open up one of the functions, say this trip logic here, we'll see that we have all of the logic and signals required to implement all the functionality for this specific function. For example, in this trip logic, we'll see that we can drill down into the three-phase power, and we'll see that there is a PTRC here. And there would be a PTRC for both the single-phase and the three-phase protection, but we're going to look at the three-phase right now. In 61850, a PTRC logical node monitors all the possible sources of trips as inputs, and then determines if the trip signal should be sent. And if it should be sent, it'll go ahead and generate that trip signal and send it. The OSMOS project uses the IEC 61850-6-100 namespace that augments the 61850 specification to allow for more complete definition of your system. So the first thing we'll look at here is just this power system resource. This power system resource element associates which piece of conducting equipment that the function we're looking at applies to. The next thing we're going to be looking at is this PTRC. The default behavior for the PTRC is defined in DTM, but the inputs to this behavior is defined by the 61850-6-100 source ref elements. So I can open that up and show off all the source refs here. So for example, one of the inputs that could potentially be a trip is defined on this line here. And you can see the input is defined here, and you can also see where the source of that is coming from from within the SSD file. So it's a distance protection scheme, and it's op general. So now let's take a look at what that looks like in DTM. When DTM imports an SSD file, it creates a substation structure like we just looked at in Navigator. So we can take a look at the SSA, that's the substation, we've got our voltage level, we got our bays, and now we have our conducting equipment and our functions. If we look in one of the conducting equipments, you can see that the terminals defined in the SCL are also shown here, which helps us define the power flow of our system and also helped us hook up a single line diagram to show off the substation. We can also navigate down to that trip function we are looking at, open up the three phase, and look at the PTRC logical node we were talking about before. And if I open this up here, since I already pointed it out, we'll see that one of the inputs this operation general is in fact coming from that protection, distance protection, and op general. Since everything has been properly linked in the SSD file, I can actually go ahead and just immediately start interacting with my simulation and verify its responses. It's worth noting that we're not simulating the reclosing or the busbar protection functionality, to better demonstrate this distance protection functionality, but all of those protection schemes would be handled in the same way that we are showing off today for the distance protection. So I can create a three phase distance protection fault on zone one by clicking here, and verify that both of these breakers have been opened. This is because both substations see in their zone one, the fault, and any time a zone one fault is detected, the breaker should open. So let me reset this and run a different test. If I generate a fault here, which is outside of SSA zone one, we'll see that they both still open as well. And this is because any time a fault is detected in the teleprotection or TPR zone, and you receive a TPR permission signal from the other substation, the breaker will still open. So in SSB, the breaker opened because this fault is in within its zone one, like previously, but over here we have both the TPR section where the fault was being detected, and the permission signal being detected in SSB. So I can go ahead and reset it one more time. The last test I'm going to run here is specifically for distance protection for SSA on a bus fault. So if I click this, we'll see that neither of the breakers opened. SSB still sees the fault in its TPR zone, but it does not receive the TPR permission from SSA, so the breaker remains closed for that reason. And on the other hand, SSA does not see the fault in its TPR zone, because it's behind it, and the breaker will remain closed, even though it did receive the permission from SSB. So let's take a look at this teleprotection functionality in a fully formed SCD file and see how DTM will handle this. So just for time's sake, I'm going to go ahead and load up the workspace that has the SCD file, and then we can jump back over to Navigator. So this is our SCD file in this other tab. So similar to the SSD file, you can see we still have the substation sections for SSA and SSB, but now we've added all the IEDs to add a more complete design. Just like for the SSD file, for substation A required a minimum data from substation B to define its functionality, the SCD file requires a minimal amount of IED data from substation B to support the teleprotections between the substation. So let's take a look at the teleprotection fault, and let's see how that's defined in the SCD file. So following the same process, we can open up the substation, we can go down to the voltage level, the bay, we've got our conducting equipment, and our functionality, but this time I'm going to go to teleprotection. And so what we'll see here is we still have the source reps like we did in the SSD file, and we still have what the input is named and the source it would be coming from, but what we've added to the teleprotection file is the IED data that's coming from the substation. What we've added in the file now is the XREF address. This XREF address is the input to the logical node from an actual goose signal from an IED, and it replaces the source when it is used as if the source was used in the SSD, now we're in the SCD, so we're using the XREF address instead. So let's go back over to DTM and take a look at some of the new areas we've added. The first thing we've added is obviously the IEDs to the substation, and because we have these IEDs, which are doing reporting and goosing and everything that you'd expect from a 61850 server, I can even open up this model here and see all of the logical devices, the logical nodes, data objects, data attributes, everything like that. We also at the bottom here have a goose tracker, so when any goose message is sent, we'll be able to see that happening in real time as well. So I'm going to go ahead and activate that zone 1 fault again here, and what you might see if you're paying attention is that the start general does go true for a second, but even if you miss that, we'll see that the time has updated, verifying that the 61850 IEDs are having their database updated, and we should also see some goose messages being sent out as well. So if I click this, again, you'll see these breakers open because the fault was in both SSA and SSB zone 1. We'll see that the time period here has updated, and then if I grab a little bit more area down here, you can actually interact with some of these goose messages and see what values are being sent across the line as the goose messages are being sent out. So that is the full live demo that I was going to give with our two applications here, SCL Navigator and Distributed Test Manager, or DTM. I will be handing it back over to Christophe now to discuss what if you don't need the default behavior, but you need to customize the logical node behavior using 61131. Okay, thanks, Matt. As I hand it over to Christophe, we did have a couple of questions that came in, so I'm going to pause and just take one or two questions because we are running a little bit close on time. So the first question is, I think directed for you, Matt, what was the tool that you used to look at the SCL file? Oh, yeah, absolutely. So this is our SCL Navigator. It is a tool that you can use to build and edit and verify all forms of SCL file for 61850. It's got a ton of functionality. It's really been designed from the bottom up to support 61850. And correct me if I'm wrong, you can just go to our website and download a trial of it and give it a shot and see how things load up like that. Great, thanks, Matt. We have another question that came in. I think this is directed to you, Christophe. Are you going to use LGOS? And if so, can you elaborate? This demo does not use LGOS in this context. LGOS, if it would be used, would basically indicate that a device that is supposed to subscribe to the message actually received the message. We demonstrated the use of LGOS extensively in a previous webinar. And I think that webinar is still online so that you can download and see how we would test and how we would use LGOS in a real application. And just to throw a little bit more information there, the webinar he's talking about, it's still out there. You absolutely can still go watch that. You can actually follow along with it if you download the applications we use in the webinar. Because we have built in the workspaces that we use for that webinar straight into our samples. So you can download them and follow along, write along with us in the webinar and see how LGOS can be used within the simulation tool and our debugging tool. Great, thanks. I think I can take one more question before we'll switch back over to Christophe. Does the wire diagram also show network switches present in the 62050 topology? I was interested in evaluating different network topologies and routing algorithms. I'll open that up to anyone on the panel. So if that was directed toward DTM, we just kind of have a single line diagram here for the electrical equipment. This doesn't necessarily show off any routers or switches or things along those lines. If that's not directed toward DTM, I will be happy to let someone else continue to answer this question. I'll jump in real quick. So the single line in this demo was developed using our insight display capability, which is really a generic drawing tool. The reality is, yes, if you wanted to overlay communications and network communications or topology on this diagram, it would be fairly easy to do. It's a generic drawing tool. In this case, you draw the single line. Yeah, that just maybe add on the 62050. Basically, we have the possibility as well to describe, include network switches in the communication and describe the connectivity of that in 62050. So that's basically using the extensions from the part 90-4 where we have model of switches as well. So this supports as well the possibility to describe a network diagram in details. Great. And I did just bring it up a little bit. This is how you would build up a display. So we built up a single line diagram using some of these tools here, some of these items here. But we have tons of little pallets where you can come in and add in processes and valves and pipes and counters. And, you know, all of these kind of symbols can be used generically or overlaid. Thanks. I appreciate you chiming in there, Dave, and following up on that because these displays can be made however you want them to be. Great. Thanks for the detailed answer. And just for everyone in the audience, that was Dave Gaukenauer from Triangle Microworks. He's our lead DTM developer and part of the panel of experts, folks answering questions behind the scenes. So with that, Christophe, I'm going to go ahead and make you presenter again so that you continue on with your demo. Okay. So you should see my screen again. So in that last part of the webinar, I will show how a feature that will be included in the next version of DTM can be used to prototype an application. As Matt has mentioned, it's basically adding the capability of using 61131 to specify behavior and simulate that behavior to DTM, which is basically in line with what Thomas has mentioned that he wants in the future as well to be able to include logic behavior description in a specification so that we can even easier simulate the specification. The example I'm using is again from the OSMOS project. An additional part of the OSMOS project, the OSMOS project as such, deals a lot with storage system. An additional part where we had worked on was also related to verify the concepts of 61850-7420, which is a part of 61850 dealing with models for distributed energy resources. And the little use case that we have invented for that purpose is the usage of a storage system to basically prevent an overload of the line. So what we see here, we have our two substations that Matt has shown before that we simulate. We have the power line between them. And we assume now in the first step of power flow going from the one substation to the other substation. So this will be the collective sum of all the other feeders except the ones to this power line. And the same this on the other substation that will be a summary load out of that substation. And the energy has to flow over this power line. In order to prevent an overload that may happen in certain situations, we now introduce on both sides a battery system as a storage. The idea is when the power flow here goes above the threshold that we have from the line ratings, the battery down here will take the lead, will feed in this excess energy out of the battery instead of pulling it here from the line. And to compensate on the other side, the battery would store this excess energy that cannot be pulled over the line into the battery. So with that, the power flow, the overall power flow remains the same. But we have a kind of a virtual power flow between the batteries. The battery on the load side will drive that with a function called load following, which is described in 61850 as well. It's a standard function that exists in the DR world. So it would drive that and it would tell the other battery what it has to store to compensate what this battery would generate. Now, the first thing we wanted to do, we wanted to verify that as models in 61850. And 61850 part 7420 basically has a high level abstract models for DRs. We talk about operational functions that basically implement the grid codes. And in our example, the load following function, that would be one of the operational functions. We then have a power management. That's the function that is doing the complete algorithm. So that will decide what the battery has to do. And the interface battery would be to the group of what we call DR resource logical nodes, which could be generators, load or storages. Just to quickly say that the load following function. So that's basically what we need. If the load goes beyond the certain threshold, instead of pulling it from the line, the amount we pull from the line will be constant. But the additional energy, the additional power that we need will be produced by the generation following exactly up the demand of the load that we have. Now, in 61850, the model looks like the following. We have on one side our operational functions. So that would be the logical node modeling this load following function. We also use active power generation. That's what the other side would do when it gets the request from the other side to act as a load. So this basically requests generated energy. This would request to store energy. In the middle, we have our power management function. That's where we implement the whole algorithm. So that's where we decide when we have our requests that we should follow up load that we need to generate. We have an output that goes to the generator. So here we have the DR resources. We have an output that goes that requests to generate energy, which goes to the battery. In the case, we would get the request to from the other substation to store energy. The power management function would analyze that and probably request to charge the battery. But it also supervises from the storage element the state of charge. I mean, if we are too low, we cannot generate any more. So that is also supervised here so that we don't generate requests that we cannot fulfill. So that's basically the application that we had in mind. And now we will see how we can simulate that in DTM using this new feature of the 1131 language. What we see now here on the display is another workspace of DTM where we now have defined an application, an 1131 application. We can do that with function block diagrams. That's the function block diagram that you have seen before in my slide. That basically models that complete application of this DR control of this battery system management system, basically. So this has been described in function block diagram. And each of these logical nodes its own is described in structured text. So we can program the behavior of the logical node, in this case, the load following. We can program that in structured text and then we can run the application. We also created a second application, which is basically the simulation of the battery. As we are not connected in this case to a real battery and to a real power system, we also simulate the battery behavior. So we have a known program running in this 1131 environment that simulates the battery. And now we can see how that works out. What we see here, we can watch certain variables. I have added the variables like the load that we have to follow, which is currently zero request from the other substation. I may go back to the diagram so that you see the signals. So that's basically the load that we supervise. If that goes beyond this threshold, then we will have to generate energy from the battery. And that's a request from the other substation that would request that we have stored energy in the battery. And these are the outputs from the complete function, but the request, what we have to generate, what we have to charge, a request that we would send to the other substation. We see here the state of charge, the initial state of charge of the battery is 50 percent, which represents 100 kilowatt hours that we have from the battery simulation. And we can now change some of our variables. So we see the threshold is set to minus 1500. The rating of the battery is 200, so the state of charge of 50 is 100. And now I can simulate a load that we have to follow if I say our load is minus 1000. We still should not generate anything because we are still in the range of the threshold, which is minus 1500. If I increase that to minus 1600. We now see we got a request to generate 100 kilowatts. And at the same time, we sent the request over to the other substation to store this amount of energy. And what we see here now that we are generating this amount, the battery state of charge is going down as we are consuming energy out of the battery. On the other system, the amount, the state of charge would go up in the same way. Now, if you go back now, we are again in a normal condition. Let's assume we go back to minus 100 here as load. Now we are back in a normal condition. You see, we are not requesting anything anymore. The battery stays where it was. And now let's assume that the other system, the other side requires us to store energy in the battery. I can set the value here like minus 200. And now we see we have now a request from the other substation and we are supposed to charge with 200. And we see the state of charge in the battery is going up again. And the energy as well in the battery is going up. And just to make it a little bit higher, to speed up, we can also charge more. So now it's still charging faster, but so it's still charging. So with that, we can simulate the behavior and we can verify that the concepts work in this environment. And with that, I'm at the end of the demonstration and I hand back to Beth for the final questions. Thanks, Christoph. That was actually perfect timing. So with that, we have filled our hour. We have ended it perfectly at one hour. So I do want to wrap up with some final thoughts. The webinar that you've seen here today is being recorded and will be posted on our website at trianglemicroworks.com. You'll be able to find it there in a couple of days right on our homepage. Just scroll down to the bottom. It will also be on our YouTube channel, which has tens of other videos of our past webinars and training videos and the protocols and our tools as well. You can download evaluation licenses, as Matt had mentioned earlier, of any of our tools. The ones you saw here today were SEL Navigator and Distributed Test Manager. So feel free to try those out. And certainly I welcome any questions or comments. Please email me directly at vcapellis at trianglemicroworks.com. When you do log off of the webinar, you will receive a short survey. It's only four questions, but we appreciate if you take a minute just to answer those and give us any feedback you have. And any additional topics that you'd like to see in the future for a future webinar that we're happy to host. So again, I thank everyone for their time and look forward to seeing you on the next webinar. Thanks, everyone.


Video ID: NVBci8CivlI
Good morning. My name is Beth Capellas, and I am the sales and support manager here at Triangle Microworks. Welcome to our latest webinar series, IEC 62050, Substation Design and Testing. This is a two-part series, and today's webinar is part two, Test Your Design, Simulating and Validating an IEC 62050 Substation Design. Part one, which was titled IEC 62050 Substation Design Workshop, was previously held on August 10th. You can go to our website to view this webinar at any time. It's okay if you haven't watched part one to participate in today's webinar. They are related, but the material does stand on its own. The webinar is scheduled to be one hour long. We have a lot of material to cover today, but we will try to get to all the questions. The audience is on mute, so please submit your questions online using the webinar toolbar under the section titled Questions. You can submit questions as soon as you have them. Please don't feel you have to wait to the end. We will begin answering questions online, as we have several engineers monitoring the questions and responding to them directly. We are also happy to stay on after the webinar is over and answer as many questions as we can. So, with that, I'd like to introduce the folks that are behind the scenes today answering those questions. Today, we have with us Matt Green, he's a Senior Application Engineer here at Triangle Microworks, and we also have Dave Kuchenauer, Senior Developer for DTM, also at Triangle Microworks. Next, I'd like to introduce our speakers. We have Christoph Brunner with us today. Christoph is President of IT for Power. He's also a well-known utility industry professional and 61 to 50 expert with over 30 years of industry experience. Christoph has been involved in the development of the standards since the beginning. He is Convener of Working Group 10 and Member of Working Group 17, 18, and 19 of IEC TC57. He is an IEEE Fellow and a member of several IEEE Working Groups, as well as a member of the PSRC Main Committee and the Subcommittee H. Next, we have Jackson Moore, Application Engineer here at Triangle Microworks. Jackson received Bachelor of Science degrees in both Electrical Engineering and Computer Engineering from North Carolina State University and has a background in Power Systems Engineering. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer where he designed and developed load management control systems for multi-source microgrids. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique complex challenges our clients face. So, now that you know everyone, I'd like to just go over a little bit of background of Triangle Microworks. We were established in 1994 and have over 28 years of communication protocol experience. We are headquartered in Raleigh, North Carolina, and have customers in over 70 countries. We work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs and development time. Licensing a source code library also includes support for your implementation in any issues that may arise in the field. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field.
 and automate testing for both a single device or complete substation. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. It is available as software only or pre-installed on hardware. We are heavily involved in the standard committees that support SCADA protocols like UCA group for 61A50 and DMP user and technical committees. Through our participation, we help define the protocols, stay up to date on the changes taking place, and also we provide feedback based on our customers needs as the protocols continue to evolve. So now that you know a little bit more about Triangle, I'd like to go ahead and hand it over to Christoph who will kick off the webinar. Christoph, I'll go ahead and make you a presenter. Thanks Beth, and welcome everybody as well from my side. Let me share my screen. Okay, you should see my screen now. I hope you can hear me well. So in the first part of this webinar series, Jackson has demonstrated how the design of a digital substation can be made using 61A50 system configuration language or SCL. In the second part, we will now show you how you can verify that the design works before you need to configure the devices. With IC61850 and the SCL, the system configuration language, we have basic information that describes the system functionality in a machine processable format available. So let's have a look at that. So we will first discuss what can be done with the design for a digital substation by using IC61850 and the IC61850 SCL. In the second part, we will discuss why that is possible, what are the features in the standards that support that, and then finally, we will introduce the tool that we have that supports this simulation, which later will be demonstrated by Jackson in the second part of the webinar. So now let's have a look in the substation protection and control system. Traditionally, we have relays that are wired together, that are wired to the process. Signals are exchanged over these wires as binary or analog information, and that's basically what realizes the applications in a classical design. Let me now move to a digital substation. Things have changed a little bit. A digital substation is typically characterized through the fact that our devices or relays are now intelligent electronic devices, microprocessor-based, that have communication capabilities. The information exchange between the devices is now done through communication, and another characteristic of the digital substation is that we integrate merging unit switchgear interfaces to bring communication down to the process. So that's more or less the key characteristics of a digital substation. Now, how does this influence the way how we can look at applications that we realize in our substation protection and control system? An application is typically realized through various functional elements, like distance protection elements, and these functional elements exchange signals between them.
 850, those functional elements are characterized as logical nodes. So we see here three logical nodes for three different distance protection zone elements. We see a trip logic. So the application that we have illustrated here is a breaker failure application. So the trip logic will decide if a trip shall happen. The trip would go through the breaker interface and as a wire into the breaker. In the same time, the trip signal that is sent to the breaker is also initiating our breaker failure function. The breaker failure function will supervise if the breaker has opened, if the breaker failed, maybe doing a local retrip, and if it still fails, we'll do what is called in 6150 external operate data object op x that is basically going to the neighbor breakers that we need to trip in order to clear the fault. So what we see for this little application, we have a bunch of functional elements, 6150 logical nodes, and we have single flow between these elements. The next slide shows another look at the concept of an application focusing now a little bit on the logical nodes that are representing the functional elements that work together. So a logical node has outputs that are our data objects in the 6150 world that are standardized. We may have parameters that influence the behavior. An example would be a operate delay for a protection element. We also may have controls. A control could be to enable or disable a protection element or it could also be a control to request an opening or a closing of a breaker when we do a controlled break operation. But then we also have inputs that are typically signals coming from other logical nodes and the inputs and outputs together flow between these logical nodes to implement the application as we have seen before. Some logical nodes are interfacing process equipment like the logical node XCBR that we have seen which interfaces to the circuit breaker. So the equipment in that case would be the circuit breaker. So in these cases, we also have the logical nodes are associated to physical inputs and outputs of the system. Information exchange between the logical nodes, if that's within the device, that's typically done through software. If it's in different devices, then it will use Goose messaging, one of the communication services of IC61850. So basically, we are getting now to our key topic, the function simulation. As in a digital substation, everything is digital, we can as well simulate things in software. As you have seen, the key aspects of an application which are the functional elements and the single flow, those are both described in the system configuration language in a machine-processable way, which basically allows us that a piece of software can use that information and simulate the behavior of the system. So basically, we can create a digital twin of the system or like Jackson would say, maybe a digital cousin as it may not be exactly one-to-one.
 the same behavior. Why would we want to do that? I think the first obvious use case is that we want to test the design in an early stage before implementing it in the devices. But there is more that we can do and that's what we'll discuss today. But there is more that we can do with a functional simulation. Another example is a unit testing of a device. Let's take the use case that you have a device in your life system that has failed, you need to replace it. So you get a replacement ID, you have to configure it and you want to be sure that the configuration is really working as expected, that you configured it right. So before you put it in the real system, you can put it in a testbed where the remaining devices are functionally simulated and interacting with the physical device that you want to verify the communication. Use cases may verifying test scenarios, test scenarios that you need to test in a live substation, the whole virtual isolation concept that IC61850 has. So you can on one side test these scenarios using functional simulation but another opportunity is as well as this involves a certain sequence that maintenance staff needs to do. You can also use the simulated environment to train maintenance staff. So we can also use simulated environment for trainings. Operator training may be another thing. So when you want to train an operator on your on your SCADA system, you can simulate the complete behavior using functional simulation and the operator can be trained on the SCADA system. So that's some of the use cases for the functional simulation. Now that we understood what can be done in a digital substation design, let's have a look of what features in IC61850 support that. The key element here is the SCL or system configuration language file that I hope you are partly familiar. I mean Jackson showed in the first part of the webinar how such a file was created. So basically it's a XML-based documentation of relevant configuration aspects and it's structured in different sections where all the sections have a dedicated purpose. The substation section that's where we have the key information for our functional simulation that basically describes the topology of the substation, functional requirements, the structure of the substation. Then in the IED, we have an IED section for each of the IEDs that we use in our system and it basically describes the data model of the IED as well as the communication configuration like goose report and sample value messages, how those are configured. Last we also have the communication section that's where we include all the information related to the communication network. So basically addressing information for our system. Now how does this look? So with regard to the functionality, we have in our SCL substation section the hierarchical structure. It starts with substation voltage level, then we have the bay and then we have our conducting equipment. Conducting equipment can be a circuit breaker, a disconnecting switch and we can also associate to conducting equipment logical nodes that we specify for the functionality like associated to a circuit breaker equipment we would
 have a logical node of the class XCBR associated. In the substation section we can as well describe functions, hierarchically structured functions like an overcurrent function with the sub functions and logical nodes associated like the overcurrent element, the measurements, the current transformers where we get the signals from in this case there are all the groups here and the overcurrent function. This whole structuring of function is completely user-defined when you do the specification like Jackson had demonstrated last time that's when you basically did build up this structure so we have another function here for the trip and we have a function breaker failure where we find our logical node breaker failure. This is all described in SCL. When we realize the system we start with the specification and then we implement it which means we associate our specified functionality to IDs and again this is implemented and documented in the system configuration language file. We see here an extract where we have in the function breaker failure specified logical node RBRF to be used and then what we see here we see the reference to the ID which is implementing that particular function that we have specified here so with that we can associate our specified functionality with the realization within the devices. So that's about the function and the functional elements. Next is the signal flow. Signal flow can be described as well in SCL and basically already with the part six that we have today it is possible to document signal flow based on goose messages mainly by external references so basically we can have here we have a logical node the breaker failure that is realized in an ID and what we see here is the input to this logical nodes documented through an xref that points to the signal which is the the trip signal from a logical node ptrc that is used as an input to this breaker failure function it's the breaker failure initiate input that we have seen before so that can be documented through an xref we can also indicate what service we will use and we could also indicate where the control block is associated to that goose message and as an element that I would like to point out here so we not only have the status value which is the general attribute here as an input we also configure the quality to be an input because it's a good practice to always associate as well the quality use the quality in your signal flow and use the quality in your application so that's the way how external references can document signal flow between the devices that is realized with 6150 communication but as we have seen earlier multiple function elements can be realized within the same device and then we have an internal signal flow and would also like to document this and for that we are currently working on an extension to the standard which is the 6150-90-30 technical report it's a draft that defines various extensions associated to that is a namespace
 like 50-6-100. The reason that this number is not identical is that within this namespace we include as well formally other extensions that are defined in other parts of the standard. So the key element now for our topic today that we add there is an element source ref which can be used to document signal flow between logical nodes from our specification, so from our substation section, signal flow independent of allocating functions to IDs, independent of a communication mechanism. So this allows us basically to document the complete application as we see it. We have our logical nodes in the specification in the substation section and with the source ref we can document these internal signal flows or these signals flow in general from a specification perspective. When we now realize the application, then we have to associate IDs and what we now see is that signals that have before been flowing between IDs are now going on virtual outputs and virtual inputs and are carried over with a goose message. These signals are still internal, the blue ones have now been translated into a goose message. All the signals are documented with the source ref in the substation section, the blue ones that are between the devices are documented with a next ref in the ID section associated in the specific ID that implements this logical node. And again in SCL we can document how our specified input is realized with the real reference. So we can specify our input reference but then we can also specify how this is realized with an extra ref to an extra ref address. So here we see the extra ref address that points to where this specified input is realized. Also in SCL we can document once we have realized the interaction with goose, we can document how the goose publisher, how the message is constructed, what is the control block, what is the data set and the subscriber that receives the signal through our extra refs. So that means when we implement an application in IC61850, we first have to specify the details of the application with the signals that we want to exchange between our functional elements. This is similar to a wire diagram in a conventional design. We then have to allocate the functions to devices, to IDs and then based on this allocation and the complete signal flow we have to translate the external signals here into goose messages and associated data sets. And that's basically when we have all that together that will create all the information in our SCL file that we then can use to design our system, to configure our system but also to simulate. So now that we have understood the elements that support the function simulation, let's have a quick look at our tool that is supporting this before Jackson will then demonstrate that tool. So the tool we are talking about is our distributed test manager. It's a tool that can simulate scalar communications in the substation, perform automated tests to confirm systems.
 system, or individual device behavior. It supports multiple protocols. It includes a script and action-based test manager. It has a 650 GUI and report viewer. And the focus for our discussion today, it supports a script-based simulation of a system that is described in STL, in the system configuration language. Just about the protocols we support. So we see we support IC61850 with all the services. MMS GUI sample values, but also secure and routable GUIs and sample values, where the key distribution center from Garibaldi gets involved. We also support the other protoscaler protocols like EMP multiples, 101, 104, and TARZ2 or ICCP in the control center communication protocol. But the focus today is our behavior simulation. So we have one feature of DTM is that it supports behavior simulation. The behavior can be described either in JavaScript or in 61131 structured text or function block diagrams. We have mechanisms built in to debug, to test, to run the stuff. We can watch variables live as the simulation is executed. So we can also have a look at the internal variables. When it now comes to our function simulation, so the application is now a digital substation. That's what we have seen earlier. Our application is decomposed into logical nodes, which are function elements, which exchange data. And they may have single exchange to equipment. So at the same level, that's also how our function simulation works. So we have elements in our function simulation that simulate individual logical node behaviors. But we also have elements that simulate compacting equipment, like a breaker or a switch. So it simulates the opening, provides the feedback to the logical nodes that interfaces with it. In order to perform this simulation, we need some additional control. One of them is a control through a user interface. If you want to run a test, you may want to simulate a test case where the breaker opens or a test case where the breaker fails so that you can verify your breaker failure function. So through a user interface, we can influence our function simulation in DTM. Also, of course, we need to consider the behavior of the process, the topology. When a fault has been cleared, the current stops flowing. So the simulation of the protection element that discovered or that determined the operate signal needs to know that it has to set that back to false because the fault has been cleared. So there is also some additional information going into the function simulation for the different logical nodes. So to summarize, DTM basically includes a library with a default functional behavior of more or less every major logical node as well as major compacting equipment. Here illustrated is the simulation of the XCBR, the behavior that we have in our XCBR logical node simulation. The behavior can be customized.
 so we have a default behavior available. You can customize that using either JavaScript or 61.1.31. And with that, I think it's time to hand over to Jackson that will now demonstrate what I have tried to explain here based on the design that he created in the first part of the webinar a few weeks ago. Great, thanks Christophe. We are going to just pause for just one or two questions real quick, if that works. So Christophe, this question is for you. It's a question with a comment as well. What standard is used for IAD name and do you only have six characters for the name? I thought that IEC 81346 is recommended. Okay, I was muted, sorry. Okay, so for the 61.850 perspective, there is no requirement how you create your IAD name. The IAD name can be completely configured in SCL, so it's a configuration element that we have to design to use to determine when we build the system. Yes, there is a recommendation on this standard that you mentioned that could be used, but it's not mandated to use that. About the names, I think there is more than six characters allowed. There is somewhere in the standard the limitations are defined, but the limitation is such that the total of your logical device name, which is created by the IAD name, the logical device instance, this should not be more than 64 characters. So I think the IAD name can be way bigger than six characters. Did that answer the question? Yes. The next question, does your simulation have HMI of the breakers in other objects? Yes, I can take that one, yes, and we're gonna get to see that in action here in just a moment. Okay, sorry, I think we did double speak. Okay, well, perfect. With that, let's go ahead and segue over to Jackson. I'm gonna make you the presenter. Okay, Jackson, you should have the screen. All right. All right, thank you, Beth, as well as Christoph. Thank you, Christoph, for kind of explaining the what and the why, as well as some of the how of what we're accomplishing today, and that is simulating our SED and all the reasons that we're capable of doing that. So before I get into the demonstration, I do just want to provide a quick explanation of the tool that I'm using. So Christoph gave us a brief introduction, but I'll show you a little bit more here. So what you see up on my screen now is DTM, or Distributed Test Manager. It is fundamentally a multi-protocol test and simulation tool, so it does include support for DNP3, 101, 104, Modbus, TASI-2, but today I'll be using 61850, and it is capable of simulating both servers and clients in all of those different protocols. Particularly for client functionality today, I will actually be using another one of our tools, TestWii Pro. You might be familiar with from past webinars or from using yourself, hopefully, but I'll go ahead and save the introduction of that until the end when we actually pull that up. So aside from just communications, DTM is also capable of simulating device behavior, so that will include JavaScript that we'll take a look at here in a little bit, as well as IEEC 61131, structured text, and function block diagrams.
 And finally, we have the substation simulator, another tool that's built into DTM. And that's what I've used to build the layout that you see on my screen now, which includes the one line, the substation section, and our IDs. And what I'll be doing today is walking through how I created this simulation and what I can do with it now that I have it. So the first step is to actually use the substation simulation import wizard. So to do that, I just came up here to file and imported my wizard. I selected my SED file, and I hit OK. That brings up this screen here, and this is what you would need to do should you have an SED of yourself. And I'll briefly walk through a couple of these options, but most of them will make more sense once we've actually completed the demo. So for example, functional nodes, this will be the scripting elements that allow our system to actually function and have behavior. Generate single line diagram. You can get a little bit of a peek of what that looks like here behind the window that I have up now. Generate XY coordinates from the SED file. So there is the ability within 61850 to specify XY coordinates. So where on a screen, for example, an HMI, will a specific piece of conducting equipment live? So that would tell us where a specific breaker is going to be in space in relation to other conducting equipment that it is connected to. Similarly, I won't be using the power simulation, but you can use that to simulate voltages and currents flowing through your system just to give it an extra element of realism. And then finally, generate simulation controls. That will allow me to automatically trigger protection elements over here on the right, and we'll get to see that in action here in a little bit. But once I select my SED and bring that in, I'm able to do all of this. To use an analogy, I think of an SED as kind of like a blueprint for a car, where the frame of the car captured in the blueprint would tell us the different pieces of conducting equipment, where they are within my system, and what they're connected to. You might think of the logical nodes as the different components of the car that perform functions like the different brakes, engine, transmission, so on and so forth, and those will exist down on the different IKDs that we have down here at the bottom. And then finally, our signal exchange. That's also documented in my SED, and that might – in a car analogy, you could think of the wiring that connects different components to each other, or maybe fuel lines or different hoses. So once we have all that information contained in an SED, just like if you had the blueprint for a car, you could build a pretty good approximation of that car, we're also able to build a realistic simulation of what our substation should actually do. So again, we've already talked a little bit about the single line and how that was generated. You'll notice that it looks pretty similar to the SC – I'm sorry, to the single line that we had over in Helix. Just to refresh your memory, or in case you weren't a part of that last webinar, I encourage you to go watch it. It is available on YouTube. I'll go ahead and pull up that up now. So I won't be using Helix much today. I really just want to point out that the single line and the layout for the different components, we did set that up and design that in Helix, that information was captured in the SED. And when I did that export – I'm sorry, import wizard over into DTM, we were able to recreate it. The next thing was that we were able to also bring in all of our different IEDs. And these are visible down here in my IEDs folder. So our SED contained a data model for each and every IED contained within our system. And you might remember we had six different bays visible here on the one line, and each bay over in my SED has six different IEDs contained in it. And those perform –
 different roles. Conveniently, you can tell where the IED exists and what its function is based on its name. That's, of course, because we named it so over in Helix. So E was the name of our voltage level, which everything exists in. 01 denotes the bay. BCU refers to a bay control unit, where BPU would be a bay protection, backup protection, and process interface unit. And this is the IED that interfaces directly with the physical breaker opening and closing it. I wanted to see what the configuration for one of these IEDs looks like. I will point out that this is done automatically once I imported the SCD. But here you can see the IP address of this IED was captured and set. I was able to bind it to a specific network adapter on my PC for publishing goose messages. If I was using routable goose, I could specify a separate IP address for that. In this particular case, we won't be using routable goose or secure and authenticated goose. But if that is something that you're interested in, we do have a webinar in the past, and we'll likely do another one in the future. Please let us know in the questions if you guys would be interested in more information on that. So aside from just communications, we also have the data model of our IED. So to view that, I'm just going to come in here and show model. So this would be the data model from our BCU. Much of the information contained in this data model really came from the ICD or IED configuration description, which came from the manufacturer. It defined the capabilities of this IED and gave us a layout for the different logical devices and logical nodes that would be on it. However, once we imported that into Helinks and configured it, it also contains other information such as the signal exchange. So these would be the goose control box and report control box that are contained on my device. Now, again, fundamentally, what DTM is a simulator for industrial communication protocols. So if I was to, say, open up one of these goose control blocks, navigate to a data set on that device, and then navigate to a specific point that was within that data set, I could manually come in here, manipulate this value. In this case, it's an OPX or an operate external general signal on an RBRF, the break or failure logical node. If I was to change this, because it is part of a goose data set, it would publish that message out on the wire. And any IEDs which were subscribed to that particular goose control block or goose message would receive that message. Those could be simulated IEDs like I have over here on the left, or even a real physical IED that you might have set up in a lab. But by itself, it's powerful and useful, but it doesn't yet include behavior. I would have to come in here and manually toggle this value in order to elicit a response on another IED. And that's really where behavior comes in. So as Christoph talked about earlier, the all-important substation section. So the substation section is designed really early on in the engineering process, typically when you're building an SSD or System Specification Description. However, that information is all carried over to and even added to once we move into the implementation stage when we're trying to create our SCD or System Configuration Description. So all of the conducting equipment contained within that substation section has been represented over here. So again, we see our substation name is Raleigh. E represents our voltage level, just like we saw down here for our different IEDs. And then we can see the different bays that are contained within it. So for example, if I open up Bay 1, I can now see the different pieces of conducting equipment that make up this bay, as well as some different functions that are inside of it. You might recall from last time we talked about how functions are really just collections.
 of L nodes or logical nodes that are all working together to perform some common goal. So as Christoph alluded to, there's kind of a mirror of each logical node in both the substation section as well as on a particular IED. Now depending on how your substation, I'm sorry, your project has been implemented, you may or may not have the exact same logical node down in your IED section that you have up in your substation section, but that is typically the goal. And then there's the opportunity within the SEL to link those two together. And that's what was done here in our particular SED that we created using Helix. So if I come in here to one of these specific L nodes from the substation section and I go ahead and manage IED associations, what's happened automatically is it has been associated with a logical node, and that's because of data that's contained within the SED. If that wasn't done automatically, then I could come in here and click this association. But effectively what it does is it links the output parameters and controls of this specific L node with a logical node that exists down on an actual IED. So in this case, this L node from my substation section has been associated with an XCBR that exists on the PIU in Bay 1. So we're getting closer. We've now linked our substation section down to our IEDs, but now it's time to link the communications that are flowing between our different IEDs to those goose messages, all-important goose that replaces wire, signals that go between different IEDs. And we now need to link them up to our functional nodes up here or LNs up in the substation section. So to do that or to see how that was done, I'm going to come over here to my input section. So for this XCBR, we've defined a couple different inputs that we think a typical XCBR might have. So those would include a trip signal, for example, and we can see this particular XCBR has been configured to have up to seven different trip inputs, five of which are used. And they are configured to refer to XREFs that point to goose messages that are coming from other devices. So for example, here would be a RBRF op external. So this is going to be a breaker failure signal. If a breaker in another bay, in this case, it actually will be Bay 2. If that breaker fails, the RBRF or breaker failure logical node on that device is going to publish the op X general signal. Now that is configured as an input into the LLN0 specifically of my Bay 1 PIU. So I didn't have to do this mapping manually because the information needed to do this was contained within the SED in really two different ways. So first and most importantly, it's contained within the substation section. So that defines sources, source REFs or contains source REFs as part of the 6-100 extension that's currently being worked on within the working group. And it contains what inputs are necessary for a given L node to function. Now crucially, once we moved into the implementation stage, that source REF included a link to an XREF that actually implemented or realized that signal. In other cases, a source REF might point to a connection that's within a single IED. And that's what we see here. You might notice that this path looks a little bit different for this connection up here at the top than these ones down here at the bottom. These lower ones point to specific points on IEDs down in my IED folder section. This path that you see here is basically a file tree hierarchy that we refer to as a DTM.
 path that points to a specific point on an IED that we're simulating. Whereas what we see up here, this Raleigh E01 trip PTRC, that actually refers to a point on another functional node or simulated L node that we see over here in the substation section. And that's because this is an internal signal. In this case, this particular manufacturer chose to put two PTRCs, one on the protection device, as well as one on the PIU itself. And the PTRC on the PIU is actually receiving that trip signal and then forwarding it on as sort of a proxy over to my XCDR. Finally, these different, I guess, pieces of data that exist within the substation section are linked to my single line. And this is what allows me to actually animate. So, for example, if this L node, the XCDR, had its position value set to open, this is, of course, in Bay 1, then this breaker down here would also reflect open. I know this is a lot, but I do have to really, you know, I hope you guys can get excited here, because there is one last piece that's not necessarily contained within the SED itself, but some of the secret sauce that we contain here in DTM that actually makes this all possible. We've now got everything linked together. We've got our IED simulated. We've drawn a nice one line. But you might still be wondering, how does the behavior actually occur? For example, when a breaker gets a signal, a trip signal, how does it know to try and update its position? Well, what we've done is we've implemented behaviors for different logical nodes in a variety, well, not a variety, really two different languages. One of those would be JavaScript, and the other one would be 61.1.31. If you don't have a preference between the two, then you really never need to worry about which one it's implemented in, and you don't need to ever touch it. But should you want to modify the implementation we have for the behavior of the different logical nodes, we do make that available. So, for example, for the QA0, this is not actually a logical node. This is actually the physical breaker itself. I'll show you what the scripting looks like for that. So, I come up here to my breaker simulation. This is just a piece of code that exists every time I have a breaker. So, that would apply here for QA0 and Bay 1, but also every other bay that includes a breaker. This bit of code will be instanced and executed. So, this is structured text, which is one of the languages that is available as part of IEC 61.1.31. It's probably most widely known and widely used within the control community, specifically on PLCs. So, ladder logic would be a cousin of this. We do also include support for function block diagrams, and you could do all of your scripting in function block diagrams, which provides a nice visual representation. So, as I mentioned, this logic here just describes the behavior for how a breaker would work. We basically started out by drawing a state machine that describes a breaker, and then we've implemented it here in structured text. Should you be more comfortable with JavaScript, then you would need to come in here. Let's see. So, a different element that's been implemented in JavaScript might be XCBR. I could come in here, and if I go to manage edit behavior, that will bring up the JavaScript editor, and you can see how this is the code that we've written for the XCBR in particular. Depending on your level of comfort with different programming languages, we find that some prefer JavaScript, some prefer 61.1.31, and some prefer neither, and the good news is if you don't prefer either, you really don't have to come in here and ever make changes to these behaviors in either JavaScript or 61.1.31.
 131, because we have created base implementations for you. It's really only something that you would need to come in and tweak. Should you want to maybe, let's say, make your digital cousin a little bit closer to a digital twin? To come back to our car analogy, we could say, we know that our car has a carburetor, and we've implemented a base version for how a carburetor might work. But should your carburetor operate a little bit differently than the one that we've designed and implemented, you do have the ability to come in and make tweaks to make it more similar to how your specific implementation on your specific vendor's IED actually works. So, just to recap, before we move on, we've simulated, or we've drawn our one-line, we've linked our one-line over to elements contained within our substation section that exists over here on the left-hand side. In order to animate it, we've linked behaviors to these different components within the substation section that are done in both JavaScript and 61.131. And we've linked our substation section down to specific logical nodes that exist on IEDs down in our IED section. So, of course, what this all means is that it can all work together to create a realistic simulation that actually includes real 61.850 communications flying between our devices, just like you would have in an actual substation. So, let's take a look at some of this in action. What is it that we've all been building towards? So, over here on the right side, you'll see I have a couple different buttons. These are broken up by bay, and these are generated automatically because of one of those selection boxes that I selected when I imported my SED into DTM. So, up here at the top, I have Bay 1. I've got my distance protection elements and my overcurrent protection elements. I have three different zones for my distance protection, and the functionality for those is contained within a PDIS logical node that exists on the bay protection unit. Over here, it would actually be this particular IED. So, when I click that button, what we're going to see happen is that this QA0 is going to open, but what's happening under the hood is that distance protection element is actually sending a start signal or an operate signal over to the PTRC logical node that also exists on the bay protection unit, and that's being passed down, in this case, to actually a second PTRC that exists on the PIU, which is, in turn, passing that message over to the XCBR, which then tries to operate the breaker. The breaker will, in fact, open, and we'll see the position value updated over here on my single line. So, I'll go ahead and click that button, and sure enough, very, very quickly, we see that that breaker did, in fact, open. It's a small change, but this little curve line over here on the left did move back. So, let's do something a little bit more exciting. You might recall during our last webinar, when I was configuring all of this using Helinks, configuring my whole SCD, I also put in some breaker failure functionality. So, you might be wondering what these two green check marks do next to each breaker. Effectively, the first one disables the trip coil that you might have as part of an actual breaker out in the field, and the second one disables the retrip coil, which would typically be energized as a secondary effort to retrip if the first one should fail and might be used by a breaker failure function, for example. Before it issues an operate external, the operate internal that we'll see in just a moment might try and flip that breaker. So, now, if I issue the same fault again or simulate the same fault, the zone one distance protection in bay one, what's now going to happen is we're going to see it fails to open because I've disabled those two trip coils associated with that breaker, and instead, we're going to see the breakers in bay one.
 Bay 1 and Bay 5 are two transformer breakers, they're instead going to open, thereby clearing the bus and protecting us from the fault. So I'll go ahead and click that button again. And sure enough, if we come down here to Bay 2, we see QA0 and Bay 2, and QA0 and Bay 5 have opened, protecting us from the bus. So originally this is where we thought we'd end the demo, but we decided, well, you know, we wanted some opportunity to show you the communications that are happening between these different IEDs, and we already have the perfect tool to do that, and that's going to be TestSuite Pro. As I mentioned earlier, TestSuite Pro is a testing tool that really is specific to 61850. It's full of visualization features, testing features, it has a whole built-in test sequencer that can allow you to do some pretty impressively complex tests, and it really can be used in both a lab setting or in the substation itself, used by either a substation engineer or a technician to test and diagnose and troubleshoot events that might be occurring in your substation. So I don't have time to go through all of those features today, so I'll really just focus on two. The first of these will be the signal flow diagram, and that's what you see up on the screen here. So these different teal boxes that you see represent different IEDs that I contain within my substation. These names might be familiar, E01BPU, that's going to be the bay protection unit in bay one, and then over here on the right, we have E02PIU and E05PIUs. So PIU stands for process interface unit, and this is the last device that exists before the physical breaker, the piece of conducting equipment, and it's what will issue the open and close commands to the breakers. So what this whole signal flow is intended to capture is the breaker failure that I just simulated over in DTM. Now there's not some back-end communication between these two tools. TestSuite Pro is simply monitoring goose messages that come across a wire, as well as MMS reports, which are necessary for messages that are internal to a single device, and I can talk a little bit more about that in just a moment, but let's go ahead and start a capture here in TestSuite Pro. I'll wait just a moment for us to gather the signals, and I'll come back over to DTM once that is completed. All right, and I'm going to disable the trip and re-trip coils over here on breaker QA0 in bay one, and then I'm going to trigger that same protection fault again, and just like we saw before, these two breakers in bays two and five should open. All right, they did. So now let's come back over to TestSuite Pro and take a look at what happened. So it all happened pretty quickly, so I'm going to stop the capture and step through it to give us a little bit better idea of how these signals actually flow. So the first thing, as I step through, is when I click that button, the distance protection operate.general message picked up. Now this was documented via source ref that this should be passed into the distance protection PTRC that exists on the BPU, or bay protection unit, and that's represented by this box right here, which represents that logical node. Now very shortly after, that distance protection unit is going to send out a goose message and let the PTRC over in the PIU know that it should tell the XCBR to open and actually update the breaker position, and depending on your familiarity with 62850, you might be surprised that we do have PTRCs in both, but this was a design decision that was made over at Helix because the manufacturer of the PIU typically does use a proxy PTRC. So the op.general came in, and sure enough, the PTRC did its job and it sent out the trip signal, and that was routed to two different places.
 places. One, documented internally by SourceRef, was over to the XCBR itself, over in the Bay 1 PIU. So this XCBR is now going to try and open its breaker. But recall, we disabled the trip coils on that breaker, so it will not be able to open. The trip signal is also sent down to the RBRF, or the Breaker Failure Logical Node. And the Breaker Failure Logical Node, again, this is defined by SourceRef and implemented by an XRef. When it receives that message, it's going to begin two different timers. The first timer will be how long it waits for the position to update from the breaker before it sends out an operate internal, which is basically a secondary trip to the XCBR. If we hit Next, we see that first timer has expired. And sure enough, the operate internal signal has been sent over to the XCBR. We did also disable the retrip coil on this breaker, so the breaker position still will not update. Next, the second timer in the Breaker Failure function will update. And it's now time to do its job. It's in the operate external signal, which will be passed over to the two XCBRs in base 2 and 5, indicating to those breakers, or XCBR breaker interfaces, that they should, in fact, update their breakers or open their breakers, which they did. And we can see that reflected over here by the position value updating for those two different breakers in base 2 and 5. Now, depending on your background and how familiar you are with the 61850, you might prefer to see this same information over here in the logic analyzer. It really is the same set of tools. I'm just going to take a peek at a capture that I already have. But basically, what we see here is the same thing, but in a different format. This looks a little bit closer to maybe a digital analyzer or oscilloscope that you might have used in a traditional substation. If I turn on my cursors, I can take a look at the timing that occurred between the different signals as the same thing occurred. So the distance protection picked up. And as we saw, almost immediately, the PTRC issued an open signal or an operate signal that was received by the PTRC over in the PIU, which then, in turn, operated its trip signal that was received by the RBRF, the breaker failure logical node on the bay control unit. And when it didn't see the breaker position update, it went ahead and issued its operate internal a short time later. And then when that still didn't update, then we can tell that the operate external signal was issued out from that same breaker failure logical node. And finally, the position of those two breakers did open. And that would be the breakers in bays 2 and bay 5. So with that, I hope I've explained to you how a simulation such as this is useful, how it can be accomplished, really, with minimal effort over in DTM, how the import substation wizard can make this whole thing so much easier, especially when you have a well-formed SCD that contains a substation section. And I hope this is something that you guys are willing to try on your own. I will mention some of these features, specifically the 61131 is unique to our DTM 2.0, which is an upcoming release. However, everything else that you saw today, including the JavaScript, is available in the release build of DTM 1.6, which is available on our website. If you are interested in learning more about any of these tools, such as TestSuite Pro, I encourage you to check out some of our webinars that we've done in the past, all of which are available on our YouTube channel. And additionally, send any questions that you might have about future webinars that you'd like to see. And with that, I'm going to go ahead and turn it back over to Beth. Great. Thanks, Jackson. That's actually perfect timing. So we're at the end of the hour right now, but we are happy to stay on longer to go through some questions and answers.
 What I'd like to do just real quick before we do that is just wrap up some final thoughts. So, Jackson, I'm going to kind of just take the screen from you for a second. So, once you do sign off on the webinar, we do have a survey that we really appreciate if you took a minute or two to answer. It's only a couple of questions. You are also eligible to receive a professional development hour for attending this webinar. So, please let us know if you'd like a certificate for that. The recording for the webinar will be posted to our website in about a week. So, look for that there if you'd like to see it again. And then if you already have not, please go to our homepage and sign up for our newsletter. That's where you'll be notified of future webinars, training events, any product releases, and just a good way to stay in touch with what Triangle is working on. And as always, if you have any questions or want additional information, please email sales at trianglemicroworks.com. So, with that, I'd like to go ahead over to the Q&A session. And I'll just take it kind of from the top. We have a couple of questions. Some of them have already been answered online, but we can share them with the entire audience. So, the first question, and Jackson, this might be for you. Based on the existing SCD, how long did it take to configure the test scripts on DTM in order to put this demo together? I'm just trying to get an idea of the amount of time it takes for someone that is already trained in the tool. Sure. So, that's a great question. So, the scripts themselves are packaged with DTM, and they are already available. So, when you download the tool, the scripted behaviors, the scripted behaviors, if you were to download 1.6 from a website today, which is available for a free demo or free trial, I should say, when you import your SCD, if it has a substation section, it will be created. And if it doesn't have a substation, you can do it manually. But the scripts themselves are already there and already available. You don't need to come in and manually edit them or make any changes to them if you don't need to. Really, the only reason to is if your particular substation implementation differs greatly or widely from what we have scripted. But from our experience, that is generally not the case. We have tried to create a nice general solution. And most of the logical nodes that are used in a typical substation don't really have that complex behavior. They only become particularly complex when they all work together. The biggest exception to that would be probably some of the protection elements, which are typically unique to vendors. But we're not simulating that level of detail. If you noticed in my simulation, I just clicked a button to simulate a distance protection fault. I wasn't actually injecting sample values. That is an option that you can do. But typically, for this level of simulation, we just click a button to simulate a fault. The work that you would have to do in order to bring the simulation up, if you're not using source refs, then you might have to manually make some of the linkages between the L nodes that you saw within my substation section and the different logical nodes that exist on the IEDs that you're simulating. It's not a hard process. I guess it really depends on the time of your substation or the size of your substation. We've done everything we can to go ahead and make those links automatically. But in some cases, say your design uses GGIOs instead of the functions that are defined in the standard or logical nodes defined in the standard to perform a specific function, you would need to come in and make those manually. That does take a little bit of time, but depending on the size of the substation, maybe an hour or two. Okay, thanks, Jackson. Next question can go to either Jackson or Christoph. If the SCD file was created, for instance,
 with IET600, would it be compatible with this tool? Okay, I can take that. I think I answered it already in the chat. So, basically, the compatibility with the tool, okay, there's the standard compatibility. So, if IET600, which is the system design tool from ADB, for those that are not familiar with it, if IET600 creates an STD file that is compliant to the standard, the tool, our DTM, will be able to import that and use it. But then, as it has been mentioned also in some of the other questions and answers, and like Jackson explained before, at the end of the day, it's less about what the tool produces, it's also how you make your design. Most of these tools, you can add more or less information when you do a design. So, as an example, the xrefs that are the signals from the goose messages going into the logical nodes of the IDs, they are better there. If they are there, then you can automatically create the link. If not, you need to do it manually. But you still can load the file and do it. So, the compatibility is less about the compatibility of a tool when it's compliant to the standard, it will work. It's more about how much design details do you put in your STD file, which makes it then easier or less easy to implement the simulation. And that's where the impact comes. How fast can you build such a simulation? So, the more complete the STD file is, the easier it is to simulate it. Okay, thanks, Christophe. Next question, I believe this one will be for you, Jackson. Do you need an outside tool to create a compatible single line diagram for DTM? I guess the answer is yes and no. No, you do not need an outside tool. There is a built-in drawing tool that allows you to create whatever you want. And many people who use DTM outside of 61850 build their own one lines for a TNP or 104 substation, or they just build control screens to perform functions that they're doing outside of simulating a substation. However, if you do want to define the substation section portion of an STD, which contains conducting equipment and how it's connected and XY coordinates, that would need to be done in a system configuration tool, such as Healings. And then instead of having to draw it yourself in DTM, we could import that. Okay, next question. I believe this will be for you, Christophe, or maybe Jackson, you guys can decide. I see how you selected inputs for an IED, many of which are points from other IEDs sent over GOOSE. How is that information stored on a CID level for an IED to parse out specific points from GOOSE message datasets? So maybe I can start, and then Christophe, you can fill in the gaps. So the XREFs are contained within the CID. So the XREF will tell you what GOOSE message a specific IED is configured to receive. Now, depending on the IED manufacturer and depending on the system design, those XREFs might be contained within the, let's say, consuming logical node, the actual logical node that needs that message, or they could be contained somewhere else, like, for example, LLN0. Now, we basically have an algorithm that tries to make those connections automatically, but it's a lot easier if they are.
 are placed in this specific logical node that will be using that message. But sometimes two logical nodes will be using the same POOFS signal, or for other reasons, they might be placed in LLN0. And that's really where the source REFs come in. And that's the problem. I wouldn't say it's the primary reason that source REFs exist, but it's one of the primary benefits from a simulation perspective of source REFs. We now know in the substation section which L node was intended to receive that message. And we now know where the X REF lives that implemented that specific connection or input. And therefore, we're able to make that connection automatically if the source REF is present and configured. Christophe, could you elaborate? Yeah, I think I also answered that question in the chat. I think the question was also including a little bit about graphic visualization and how you can do the design. When you have seen the first part of the webinar, Jackson had shown how the things are developed in the Healings tool, where you basically do a graphical way to define the signals that are flowing. Additionally, what we are working on is in visualizing, as well, an application. So when you have all the source REFs, you have all the connections in the SCL file, that's correct. But then the question is about visualizing it. And we are currently working on adding extensions to DTM to visualize, as well, such interactions. There is ongoing work in a project with Alia, which is the Belgium TSO. They are doing a proof of concept currently. And within that, we are investigating a couple of these things. And we probably will have a future webinar on that later this year or early next year. OK. Next question. Will measurements be visible in the single line diagram? So I guess when you say measurements, I'm assuming that refers to things such as voltages and currents. I guess that is an assumption. But yes, that is possible. You could simulate a device which subscribes to sampled values. I will point out that we do have a device I will point out from a computational perspective that it is more difficult to do on basically a Windows platform. So depending on the number of sampled values that you needed to subscribe to, you could need a more powerful PC. And then, of course, you do need some way to generate those sample values, which is even more computationally intense. You can do that in DTM. But we typically, we've also seen people use external test sets to generate the sample values. But once you have those messages, the single line canvas that I showed, it really is more of a generic HMI tool, or at least visualization tool. And it is just a single line tool. And you do have the ability to bring over basically like just like a number element like you would have on a typical HMI. And you can link that to anything, including a sample value measurement or something else, maybe something that's coming in via report periodically instead of multiple times per second. And that value could be updated. Additionally, we do have the power simulation that can provide kind of a basic power flow. And you can set voltages and currents. And you could see those updated on the screen. But that does not resolve all the way down to like a sample value level. OK, next question. This one might be for you, Christophe. What about defining the L nodes used in the IUD section without having real IUDs?
 Okay, so the L-nodes are basically created, the L-nodes in the substation section are basically created during the specification phase, so when we are using Healings and the top-down engineering process, we create functions and associated logical nodes for these functions and out of that the L-nodes are created in the substation section. So later then, when the design is made based on the specification, the system configuration tool links these existing L-nodes that we have to the L-n in the IED that is realizing this L-node, but the L-node is already there from a specification. Assuming we do a top-down specification, if we just do everything together, it's more or less at the same time that the L-node will be created and then the L-n links to it. Not sure if that answered the question. I would just elaborate that, if at least you could, I don't know that this would be a common use case, but you could in DTM just open a blank project, create a substation section, begin to create voltage levels and different bays and different pieces of conducting equipment within those bays and different L-nodes within those, all within DTM. It's not going to generate an XML. It's not going to create any SEL, but if you just were curious and you wanted to create basically the equivalent of a substation section here within DTM, now you're not going to have IEDs. You couldn't link up signals between different logical nodes as if they were goose messages, but you could link almost like everything was an internal signal directly from one L-node that exists to another just directly. Now that's not going to be visible with something like Wireshark. That would all just be internal communications, but you could build a simulation that way with kind of a simulated substation section. Okay, the questions are wrapping up. I do have one here that I want to end with that I can answer. So the question is, how does one go through the initial ramp-up? These webinars are super exciting, but trying to do that myself with the software for the first time means I have tons of questions and it slows down the process incredibly. So a couple of ways that we can help with that. When you do purchase a license from us, we do offer a one-hour training free session that can be with you or your entire team to get you familiar with the tool so you can get started as quickly as possible. DTM itself does come with a number of sample workspaces that helps you get more familiar with the tool very quickly. You also would have access to technical support that can help you with any questions you have while using the tool. And lastly, we do offer consulting. We can build workspaces for you and help you with your projects through consulting hours. So there really is a number of different ways to help you get ramped up on the tool as quick as possible using it to the full extent. So with that, I do see a couple of other additional questions coming in. We will, in our follow-ups, if you answer the survey, we do send the question and answer in a spreadsheet along with the follow-up to that. So you can get a full list of all the answers that were provided today. On that note, I'd like to thank everyone for their time today and hope to see you at the next webinar. Thanks everyone.


Video ID: Ylb3EOeD4Y4
Good morning. Welcome everyone. My name is Beth Capellas, and I'm the Marketing Manager here at Triangle Microworks. I'd like to welcome everyone to our latest webinar, Simplifying Secure Routable Goose and Sample Values. Before we get started, I'd like to go over some housekeeping items. The webinar is scheduled to be one hour long. We typically get a large amount of questions, so we're happy to stay on longer to answer as many questions as possible. With that said, I'd like to encourage questions throughout the webinar. Since the audience is on mute, please go ahead and submit your questions using the webinar menu under the section titled Questions. You can start asking questions as soon as you have them. Go ahead and just start typing them in. Do not feel like you have to wait to the end. We'll actually begin answering those questions online as we have several engineers monitoring the questions and responding to them directly. And again, we will have some time in the end to go over questions and answers. With that said, I'd like to go ahead and introduce our speakers. First, we have Herb Falk from PCI Tech. Herb has over 40 years' experience and has been involved with the 61850 technology since 1982 and cybersecurity since 1993. He's an editor of IEC 61850 8-1, editor of several cybersecurity standards, and is the vice president of testing for the UCA International User Group. Second, we have – our second speaker today is Mark Adamiak. Mark is an independent consultant for the electric power industry with previous employment at American Electric Power and GE. Mark is an original member of the 61850 working group, a life fellow of IEEE, a registered professional engineer in the state of Ohio, and a GE Edison award winner. And last, but certainly not least, we have Joel Green from Triangle Microworks. Joel has worked with communication protocols for over 20 years, with the last six years in the power industry. He's the lead engineer for 61850 development and products at Triangle Microworks and is heavily involved with 61850 industry groups. He also works as the co-editor of IEC 61850 7-2. He is a member of the UCA Test Committee and Test Procedures Working Group and involved in the IEEE Standard Association. So with that, I'd like to go ahead and hand it over to Herb to get started. Thank you, Beth. We're going to discuss today the need for security, how the 61850 routable goose and routable sample value security paradigm brings resiliency to security and operational technologies. We're going to discuss the need for simulating the impact of security on your system, and we're going to introduce a key distribution center and show how you can configure security in five minutes, and then we're going to have some questions. So I will turn this over to Mark Adamiak. Thank you, Herb, and good morning, everybody. So to start off, why multicast and security? The industry uses a technique called use cases. A use case identifies a potential application and how it can best be served, in this case,
 mechanisms. There are many use cases that went into the development of the routable goose as well as routable sample values, but we're going to just start here on a detailed focus on one, which is transfer tripping. There are many lines around the country where you have, say, a medium voltage line or a sub-transmission line, in this case 69 kV, where a number of transformers are tapped off of the 69 kV line. There is a region or a zone of protection that is not visible by the protection at the 69 kV bus, and in particular this is the region between the transformer and, in this case here, a 13 kV circuit breaker. Transformers in this region are not visible to any protection at 69 kV. One solution that's been in place for many years now is implementation of what is known as a ground switch. The ground switch, as shown in the figure here, you can see multiple ground switches, is up when a fault is detected in this region. The relay sends a signal to the ground switch, which operates it and forms a ground fault. So, Herb, next slide. So, in this case here, the ground, give me one more click, Herb. When the fault comes on, the relay operates, next click, the relay sends a trip and operate signal, a trip to the breaker, and an operate signal to the ground switch. The ground switch literally places a ground fault on the 69 kV system. Give me a click, Herb. And when that ground is made, now the 69 kV protection can see that fault. And one more click. And in that case, when it sees the protection, it can then operate. So, there are two significant drawbacks to this technique. The first one is, it's not really the best idea to put a fault on your system to clear a fault. And as the system continues to get stronger, the ground faults become larger and larger. And this presents another problem then, because the higher current ground switches are more difficult to find and or maybe not even available, depending upon the voltage at which the ground fault occurs. So, this presents itself as a, what we call, a use case for transfer tripping. Next slide, Herb. In this case now, we have a similar scenario, but instead, at each relay location, we have a device that can be connected to the existing relays. And then the relay can see when the relay picks up. A message is sent to the relay. And, Herb, give me one more click. So, again, the fault comes on. When the fault comes on, one more click. The relay, seeing the fault, sends out a routable goose messages. This is a multicast or unicast. Both are supported. And it's, instead of putting
 a ground fault on, it sends a transfer trip message. This message would typically go over the utility communication network, but if secured, it allows any communication network to be used for the communication purposes. So our GOOS, as you may or may not be aware, contains security mechanisms to prevent a wide range of potential attacks, and Herb will go into detail on those. Another very interesting aspect, to do this, to secure the message, a key is needed, and the main focus of the rest of the seminar will be on how the key is distributed from what is called a key distribution center, or KDC, to all of the relays involved in the transfer trip mechanism. And one of the things that's going to come up during the discussion is the concept of what if communications are lost to the devices. There is something called a KDA, which Herb will go into detail on. Herb, take it away. Oh no, one more slide, one more slide. Now this was just one use case. Several other use cases have been identified for application of the routable GOOS, specifically remedial action schemes, and the route, the RGOOS is actually in place right now on a remedial action system. Synchro phaser data transmission, which is known as routable sample values, is actually in service as we speak right now. Microgrid control, since the routable GOOS can hit hundreds or even thousands of devices simultaneously, the configuration of a microgrid can be easily and securely affected with the secure routable GOOS. Surgical load shed. Many remedial action schemes today, when called upon, will strip an entire feeder. But if you look at household load, being able to dynamically trip, say, an air conditioner, a pool pump, a dishwasher, a dryer, this would be called surgical load shed and it could be affected, effectively removing a significant part of the load without affecting day-to-day life. The new technology is coming out now in the concept of grid forming and black start, specifically using inverter-based devices. Inverter-based devices will require sequencing. So that means turn on this inverter first, then this inverter, then this inverter, and to bring the grid up from black start. Routable GOOS is a perfect mechanism for performing this function. Okay, Herb, now it's your turn. Excuse me, frog in the throat. Operational people typically look at security with some degree of apprehension, especially in regards to their focus versus IT's focus on threats. What you see on the left-hand side is the operational technology prior, typical priority of the list of threats that need to be countered, which is people tampering with the packets, hackers injecting packets that aren't supposed to be there, which is
 spoof or replay, packets needing to be authenticated, and lowest on their totem pole is the need for information leakage protection and confidentiality. As Mark talked about, routable goose, routable sample values, and even the layer 2 goose and sample values have been updated to support security that counter these threats based upon messaging authentication codes, public key infrastructure certificates, and the distribution of common keys for signing and for encryption. For those of you who don't know what a key is, it's a big number and it's generated with what is called cipher randomness. The theory is they do not repeat. There are two forms of encryption that are in typical use today. One is asymmetric and the other is symmetric encryption. In asymmetric encryption, the owner of the key pair has a public key that it gives to the publisher and a private key that when the publisher encrypts with the public key, only the holder of the private key can decrypt it. This works for peer-to- peer client-server relationships, but as you can see in the top part of this diagram, the inability of the second node here to decrypt the published message is a problem with multicast and it cannot decrypt the published message because it does not hold the private key and private keys should never be distributed to other non-owning nodes. There's another technology called symmetric encryption where there's a group key that is managed which is distributed to all the members of the publication group. The publisher publishes and encrypts the value using the group key. Any member that's been provided the group key can now decrypt that message. So this is the mechanism that's used by routable goose, routable sample values, and the 61850 multicast security. One of the other issues that operational people typically have is what happens if I lose security? The 61850 multicast security starts addressing this resiliency issue by delivering two sets of policies and keys in one distribution. And the policy specifies how much longer the key provided is valid for, the encryption algorithm to be used using that key, the authentication mechanism, in other words the signature or the MAC algorithm, and an activation delay. The activation delay allows for an overlap between the current and the next policy and keys to actually overlap. So if there's been a group member that hasn't quite gotten the notice of the second key pair, it can still operate for a while.
 The next thing that's been added to really improve resiliency as Mark talked about is key delivery insurance. It's the responsibility of the key distribution center to track the percentage of group members that have acknowledged the key delivery. Once a certain percentage level is reached, it can tell the publisher that when it is time to rotate to the next key, the group is ready to use those keys based upon that percentage. It is noteworthy that it's almost impossible to reach 100% members of a group being updated with the new keys in an operational environment. And the reasons for this can be several, but think of an IED being offline or decommissioned for testing or some other reason. It's up to the system design and the security people to determine if that was the case, is it still okay to rotate the keys? And therefore, the system design and resiliency needs needs to be weighted against the percentage of key delivery. There are two forms of key delivery allowed. One is group pull and one is group push. In the pull situation, the KDC responds to key requests from the group members and the push mechanism, the KDC actually pushes the keys out to the group members. Obviously the asynchronous process of the group pull makes it difficult for the publisher to receive the KDA and therefore push is the preferred mechanism if you are using key delivery assurance. There's another part of resiliency is that when group members power up, they are responsible for pulling the keys from the KDC for the groups they are interested in. This allows them to re-sync with the current and next keys immediately upon power up. If there is a push that the group member cannot decode because it doesn't have the correct encryption keys, decryption keys to decrypt the push, it needs to pull in order to re-sync. This mechanism is very important because it allows the KDC to revoke members based upon revoked PKI certificates. This is a look at a very high level of the Layer 2 and routable security wrappers for IEC 61850 multicast security. It is possible to secure Layer 2 GOOS and additionally routable GOOS. The routable GOOS is a primary concern because of NERC SIP requirements and the fact that it is typically used to go outside of an electronic security perimeter. It is not recommended to encrypt Layer 2 sample values for security just because of the CPU
 needed and the high bandwidth that would be required there. So you can see that the policy determines if the payload which is the goose and the sample value packets are encrypted and for layer 2 goose it determines if there's a signature or a message authentication code. In the routable scenario message authentication codes are required and encryption is optional. So at this point I think we will break and ask are there any questions on this section. We do have several questions that have come in so let me just pick a couple of them and I'll put them out to you know all the speakers any one of you can feel free to answer them. So the first one is please describe the logical nodes that contains the distributed keys and other security management settings. So there are a couple of places to look for that. The keys are never exposed and the logical nodes but the fact that there's a key delivery issue would be indicated in the supervision function of goose or sample values which would be the ALGOS or sample value. Also there's a generic security logical node that could be a place to indicate a key failure as well as there are there is a logging mechanism to put those type of events into. Okay. One more feature and that is the policy of what encryption algorithms actually comes from the KDC. I guess you're going to get into that but it's relevant here. Okay next question we have. In what way is SynchroPhaser data different than 61850? Wow that could take months to describe. Mark I'll let you do it. All right so data is data. 61850 carries data. Most recently in the standard now it's been identified that SynchroPhaser data of course has a time sync on it that has to be compliant with the IEEE C37.118 standard which is the SynchroPhaser standard. It defines the time synchronization requirements as well as the timestamp requirements. 61850 has been augmented to include all of this information. There is a new logical node that is a SynchroLogical node. So when the device is configured the SynchroLogical node identifies the fact that the data in this node is synchronized per C37.118. Subsequently, since SynchroPhasers are synchronous, similar to sample values, the profile R routable sample values R-SV was created such that it maintains the synchronous transmission like sample values but instead of having samples in it it would have SynchroPhasers mapped into it as complex values.
 one more thing to add to it. C37.118 has two parts to it. One is part one that is the measurement technique to generate synchrophasers. Part two is a templatized packet format that can run over various transports that are used to deliver that measured information. C61.850 and the routable sample values can define what's called a data set to transfer the synchronized measured data just as if it was normal data in a secure and standardized fashion. Okay, all right guys thanks for that very detailed answer. I would say at this point we do still have a lot of questions coming in. Keep the questions coming we will get to them but to make sure we can get through the presentation Herb why don't you pick it back up. Actually this is Joel that needs to pick up. Oh I'm sorry. Thanks Beth. Good day everybody wherever you are in the world. I'm Joel Triangle. I'm going to talk a little bit about how to test these systems. As Herb mentioned at the beginning of the presentation OT and IT staffs in the utility industry are inherently in a in a state of conflict. They have different goals in their job descriptions and security tends to land straight in the middle of that conflict quite often. Security has a history and reputation of causing trouble, confusion and fear especially in this industry where keeping the lights on is absolutely the goal for the whole organization. Now while security is in conflict with some of those goals in the short term it's becoming more and more clear that long term reducing the risk of cyber attacks and unauthorized access is is necessary to those goals keeping the lights on. So this leads us to a couple of needs in the process of securing these systems. As engineers implementing security in the communications here we need to test these systems. We need to be able to show that they work and understand how they fail and what the what the failure modes and results are when there are problems security. So we've got a couple of tools here to help with that. We also have a further goal to try to help out with some of that conflict we mentioned with the with the OT and IT staff and other stakeholders. We need to be able to demonstrate those other concerned folks that this system is working it is reliable and it will help increase the reliability of the network. So the first tool we're going to look at here for a minute is the DTM product from Triangle Microworks. It's a 61850 system simulator. It lets us bring up and simulate all the devices in a configured substation. As you can see there in the middle we have a single line diagram showing the various components in the system and how they interact. And we can bring that up and we can have those devices simulating what the substation will do in normal operations, prove that that works, and then we can start injecting faults and analyze how the system reacts. This is particularly useful in analyzing what happens with the security concerns. For instance if the devices in the field can't get the keys that we're talking about here today from the KDC, that's going to result in it not getting the information it needs from other devices. That information it's getting from other devices or input
 to logical nodes in the subscribing device, so therefore it can't make the same decisions as the outputs of that device, then can get affected because of not being able to get the inputs he needs, and you can get a potential cascade failure. And we need to be able to observe that and show what it looks like when those cases happen. Also, as I said, demonstrate that to other stakeholders to build confidence in these systems. So on the right-hand side here, we've got a status indicator, and the red lights and green lights simply tell us from the subscriber side for each device if I'm getting each of those new streams that I need. Being able to set this up, simulate the whole system, shows us that we can demonstrate how the system will react in all these failure modes. In order to further analyze what's going on in the system, on the next slide here, we've got a diagnostic tool that we provide, TestSuite Pro. 61850 subscriber servers are able to help implement functionality in a logical node called LGOS, which lets the server indicate the status of the good subscriptions it has. So this matrix here is showing us for each subscriber publisher pair what the subscriber is seeing about it. So the purple boxes here are telling us that we can't read the LGOS right now, but generally those will be green when everything's happening. The red boxes are telling us that we're not getting the signals we want, and so we can quickly analyze from a third-party perspective, whether against the DTM simulator or later as we've proven the system and gotten into the substation against the real devices, we can quickly monitor here and see what those subscribers are doing, whether they're getting the signals they receive or not. If we back up a step to the next slide, we've also got a goose tracker here. This is not looking at the servers, but just looking at the streams and the wire and comparing that to the SCD file, the system configuration description, 61850, tells us what goose messages should be available in the network. And this tracker looks on the wire and shows us what's on the wire, how it compares to the streams we expect from the configuration file. We can integrate here with the keys. We can get the keys from the KDC so that we can decode those messages and validate that the security on those messages is correct. And show all of the necessary information to prove the system out. So, it's very helpful in, as I said, diagnosing the system, proving that the system works, and demonstrating those to other stakeholders. Any questions on that? Okay, I'm not seeing any questions come in specific to the tools, but I will go back just to cover some of the earlier questions that were asked so we can get to some of them. And again, this is open to all the speakers. How do devices get their initial certs? Are these provided by the manufacturer? In an operational environment, those certs should be provided by the utility, not by the manufacturer. They should come from a PKI infrastructure, a certificate authority, or a regional authority that the utility trusts or operates itself. Okay, next question. Which part of the IEC 62050
 do I need to look at to understand and learn RGOOS and RSV? There was an old technical report called 61850-90-5 that has all the use cases and some out-of-date technology in it, plus informative text, but the new routable protocol is actually defined in 61850-8-1. And the security is defined for Layer 2 GOOS in 61850, no sorry, 62351-6. And we're going to get into some of that coming up here. Okay. Can we simulate and view the data associated with IEDs without a physical component? I assume by physical component, you mean the device itself. I believe we're referring to the DTM simulator here. The DTM is a Windows app that runs on a fairly average PC to simulate those devices. How does the GOOS tracker account for MAC address filtering on the switch that is in part of the SCD file? It does not. The GOOS tracker shows what it can see on the connection that it's plugged into, of course. You can get into using things like mirror ports and that to expand that, but yes, where you've got a situation where you've got MAC filtering in the network or potentially VLANs can restrict traffic, you do have to manage to get the observation point of the tracker at a place in the network the traffic's available. Okay. What are the differences between 6218.57-2 with that compared to 6218.59-5? I don't believe there is a 9-5, 90-5 or 9-2, I'm not sure which we're referring to there. Scott, coming in, 9-5, whoever submitted that question, if you want to send a clarifying, 90-5 is what he's referring to. Okay, 90-5 was the technical report that laid out the original implementation and Herb, you can feel free to jump in anytime here, Herb was one of the authors of that paper or that TR, that demonstrated the routable GOOS and sample values. 7-2 is the abstract services description for 6218.50, so it describes the concepts of multicast publisher-subscriber information, but 8-2 is the schism, the specific mapping to a protocol in the wire. It provides the information for the Layer 2 GOOS traditionally, and now with Amendment 1 of Addition 2, it has absorbed the information that was in 90-5 with a few updates for the routable protocols. And I just want to add, 90-5 theoretically no longer exists from a standards perspective. I've seen requests for information and for proposals that actually specify 90-5, they need to specify.
 and be upgraded to addition to Amendment 1. Okay, with that, I would say we'll put the other questions on hold for now and address them a little bit later if you want to go ahead and get back into the presentation. Okay, one of the primary fears of both IT and OT people is how much effort does it take to configure and maintain security. So at the tail end of this section, we're going to demonstrate how you can configure key distribution in under five minutes. And we're using a new product called Garibaldi. It's from PCI Tech and Triangle Microworks has the rights to resell it. And we're going to concentrate today on the 61850 key management aspect because we're talking 61850 today. The concept in key distribution is the keys are delivered and over what's called a security management plane. And this delivery provides the policies and the keys we were talking about previously to group members that are concerned about publishing and subscribing to particular data from a publication control block. Those publication control blocks and the publication of that information is what's actually used in what's called the real-time plane and to TRIP, as in MARC's use cases. 62351-9 is the specification that starts the chain of standards that defines the exchange mechanism and the management mechanisms that are actually implemented in Garibaldi. There was an RFC 8052 written that allows the delivery of two keys and two policies plus some other extensions to the normal group domain of interpretation, which is an Internet RFC I believe it's 6407. So it's very standards oriented. 62351-9 introduces the concept of standardized key delivery assurance. There is more clarity provided in how to do a group push to both unicast destinations and multicast destination addresses for key delivery. And it uses X.509 digital certificates to authenticate group members to the KDC as well as the KDC to authenticate itself to group members. And I think that was one of the questions in the question menu here. Garibaldi took a step back away from manual configuration and since its 61.850 component is 61.850 centric, there was a decision made to leverage the power of the 61.850 engineering process, which is the system configuration language. And the reason that decision was made is the system configuration description file contains the communication...
 addresses of all of the IEDs in the system, the destination of the data that's being published, all the destination addresses for the goose and sample value publications, all the control blocks, and actually allows you to express who are the subscribers for each publication control block. When you bring in the SCD that we're using for the demo into the HeLinks tool, it can actually show the network topology of the system. And this is based upon 61850 subnet information. This is an image from the HeLinks substation tool set tool, STS, and you can actually see here the destination addresses for routable goose and layer 2 goose. And unfortunately it's cut off, but you can see that the L92 IED is a subscriber for this goose control block and routable goose. And you'll see a different representation of the same information in the GUI of Garibaldi. The way the subscriptions are defined is here in the SCD file is a goose control block. These rows that contain IED names actually reference the names of the IED, other IEDs in the SCD file, and these are the subscribers for the data that's being published by this control block. In order to configure efficiently, global policies need to be decided upon, such as what type of certificate validation is going to be used to authenticate group members, what the encryption algorithm is, what the hash and signature algorithm is, whether using group push, the percentage of KDA members that are required in order to give KDA to the publisher of a group, and the interval for key generation, in other words the key rotation, and whether to use key delivery assurance or not. These global policies are enforced upon import of the SCL file. Garibaldi does allow you to change the individual policies per publication group, but if you can get close with the global policy you shouldn't need to change the individual policies. So this is where you select group push or group pull. It's advised to use group push because KDA adds another protection for resiliency. Garibaldi was designed with two types of certificate validation. You can either validate with just certificate validation, or you can match the IP address providing the certificate as being in the SCD file and the certificate being valid. This mechanism here allows for test equipment to be brought into the system, become a member of a group as long as they have a recognized and validatable certificate, perform their testing, and then disappear from the group.
 with no reconfiguration of the system required. If this is the mechanism used, well, they've got it. The IP address has to be well-known and basically be designed as part of the system. We support the standard specified encryption algorithms and HashMap algorithms. The KDA percentage specifies the percentage of group members that are required to acknowledge the keys prior to the publisher for that group being given the okay to rotate the keys. And the interval for generation is the rotation periodicity of the keys and the policies. And typically it's 24 hours, but the default we provide is 12 hours. You can also in Garibaldi define your own username and passwords. It is not provided with a default password. We use a multi-factor attribute certificate that's provided with Garibaldi for the initial login. These are the standard roles from 62351-8 versus their rights as they apply to Garibaldi. You can define custom roles. You cannot define custom rights. In order to validate certificates from group members, the certificate authority or regional authority public certificates need to be loaded into Garibaldi. This is used as a first level of validating that it's a trusted certificate. Of course, revocation list and checking real-time will also be available. In order to allow Garibaldi to negotiate the secure channel to deliver the group pull of keys and to allow the group members to validate the KDC, you must also upload an identity certificate for the KDC into the KDC. So what are we going to see in the demo? We're going to see the login. We're going to see the configuration of database credentials. We're going to create some username and passwords as well as defining a demo user login. For the system that's in the lab we're going to be talking to, we actually need two systems. In the lab we're going to be talking to, we actually need two certificate authority certificates loaded. We're going to import the identity certificate for the KDC. We're going to import the global policy and then import the SCL. And this, if you gather all of your information and plan ahead, this is definitely achievable in under five minutes. But it requires planning. So what to expect? Our starting point is that the GE UR relays in the lab have expired keys. When we finish the configuration and one to two minutes later, we would expect that the GE UR relays
 actually had show that they have received their keys and are communicating. At the endpoint we would expect, and this is a Garibaldi display, that we would see that an IED has, which is the publishing IED here, has been delivered its key and all the group members, and there's only one in this group, have been delivered the required keys. So basically we've met 100% KDA and the IED has been told it's okay to rotate the keys for this particular publication group. There is a column here called KDA Fail. Garibaldi's designed to assume that the first push out of the keys probably won't reach and be acknowledged by a hundred percent of the group members and therefore Garibaldi continues to retry to deliver the keys. If the key delivery fails on three retries Garibaldi continues to retry, but it'll let, it'll enunciate a true here that there have been three attempts to deliver keys to non-acknowledging group members and you can see the yellow light here. And in more detail you can see this is false and that is false as well, and zero percent of the members were delivered the key. Any questions? So we do have a lot of questions coming in. We're getting a little tight on time so I'm gonna maybe address one or two questions, Herb, and then we can wrap up and then address the rest of the time for all the questions. Okay. Okay, so one of the questions that just came in. Can I have a redundant hot standby on KDC as well as we have domain controller certificate authorities? Yes, and the design of Garibaldi is for redundant paths to the CAs as well. Redundancy is not available in the demo version that'll be available for download though. Okay. If KDC is out for more time for, and I'm just trying to read it as it's coming in, if KDC is out for more than time period of two keys, including rotating key, what happens to the system? In the new 62351-9, it's going to be specified that there should be a configuration item in the group members to continue operation with the last delivered key and policy. Okay, and last question before we go back to the presentation. Is KDC an IED communication through PKI? No, it utilizes PKI certificates, but it uses Diffie-Hellman and some secure channel creation per GPI.
 to actually deliver the keys. Okay so we still got questions coming in and answering them online. If you want to we can wrap up the end of the presentation Harp and then we can address the questions. Well we have the demo still coming. Okay so what's going on in the demo? I am sitting here in Troy, Michigan and the lab I'm communicating with is in Gainesville, Texas. I almost said Florida. So we're going to go into the demo now. Just to show you this is a remote desktop session to a computer down in the test lab. And you can see all of the keys and the URs have expired. There is Garibaldi does not have a GUI except for a web-based GUI. So this is not an RDP connection. This is a connection over the internet to the KDC in the lab. So what we're going to do is we're going to log in with our two-factor authentication attribute certificate. We're going to configure the database that we are going to connect to and what the mechanism for authentication is. And this is SQL Server. We test the connection, we save the connection, and now we can go in and configure some information regarding the demo and the location. And we're going to call this NARC1 just to be consistent. And save it. Now we're going to go into security and define a new login so we don't have to use the attribute certificate any longer. I'm just going to give myself all rights. Obviously you would want to pre-plan what the username and passwords versus roles would be. Now we're going to go in and load our Certificate Authority information. And remember we need two for this lab. Then we go in and we actually load the identity certificate for the KDC. Now we can go into the global policy and we'll set this to 15 minutes. We'll set this to 50%. We will set this to group push. We will set this to what the relay support. And we will turn on KDA.
 Now at this point we should be ready to import the SCD file which describes the system as you saw in the previous slide. These filters down here specify which type of control blocks this particular instance of the KDC is going to manage. So we are going to go in and submit it. And you can see it's upload, it was uploading, it is uploading. And now we are 100% configured in two minutes. The KDC will start trying to deliver keys. And just so you can see this control block is not being managed. This control block is being managed and we haven't delivered any keys yet. There are also some other nice menus which are system status menus. Tells you how long the KDC has been up. How many data streams are actually in use versus the license. The demo license allows you to do eight data streams. And I'll show you where you can see that. Tells you the number of local username and passwords configured. The normal of active sessions. So somebody else is logged in to this particular instance of Garibaldi. How many security associations are actually in place. So somebody is actively trying to pull keys at this point in time. We don't disclose who that is. If you want to see your license information this is it. Demo licenses expire in 30 days. There's also a logging menu. So if you ever see a non-green LED this is where to come. And the last 30 logged messages are shown here. And you can download your log files when you're remote. Let's see here. Let's go back and see if somebody has keys. And you can see the L90-3 has delivered key, has delivered the key, received its key for this. And the T60 no response IED has received its key. Now let's see if the L90, the L90 has received its keys. And if the T60 has received its keys, we're all good. And now if we go back and refresh these screens, you now see the L90-3 has received its keys.
 they have been delivered their keys and they are actively communicating with each other via routable Goose. And I think it's time to go back to the slideshow. So in conclusion, Goose and sample value security is real and can be managed because there's a KDC available for it now. I hope you all find that the use of SCL to configure the security really minimizes the effort in configuring security. Testing and monitoring tools are available and it's time to start planning how you can benefit from this combination. I think, Beth, this is your slide. So that does bring us to the end of the actual presentation and demo. So what we're going to do here is open it up to a question and answer. But before we do that, I just want to let everyone know the webinar will be posted at our website at trianglemicroworks.com. It will be on the homepage there. You will also get a follow-up email right after you sign off of the webinar with a link to a recorded version of this webinar. You'll also get a quick survey. It's only about five questions. We'd love to hear your, get your feedback on the webinar itself and what information that you'd like to receive. If you do respond to the survey, we will send you this presentation as well as the full question and answer spreadsheet. We got quite a few questions coming in today, so I think it'd be a great piece of information. So please respond to that survey and then certainly feel free to email sales at trianglemicroworks.com for any questions that you have or anything else we can help with. Or to request a download of a demonstration version of the KDC. Yep, absolutely. So we'd love to hear from you and stay tuned as we publish information about our next upcoming webinar, which we'll be scheduling sometime in August. So with that, anyone that wants to stay on, we have run out of time. So I understand anyone that's got to jump off, but anyone that stays on, we will start to go through the question and answers. So again, I'm going to open up the questions to all the speakers and anyone that is online with us that's been answering questions behind the scenes. So let me just take it from the top here. Like I said, we've got quite a few questions that come in and it looks like we've answered a lot of them online, which is great. Okay. What happens if an end device never gets a key? Will it not do its function? If it's got to communicate with other devices that have received their keys, it won't be able to do that. But except for communication issues, it will be able to do its function. Okay. Not all IEDs on the market have functionality to push and pull keys. What advancements required from an IED development aspect? I know there are several stack vendors actively working on getting this capability into their stack. Triangle Microworks is one of them. And I would suspect that once this capability is embedded in the Triangle stack, the OEMs of that stack will get that capability.
 and be able to put it into their products. Okay, next question. I have one other thing. Market gets driven by utilities requesting this capability. So if you want it, request it of your vendors. Okay, next question. If a group security key is accessed in two systems simultaneously, what will be the result? I got to honestly say I don't understand that question. Since it's a symmetric key and keys are not shared between groups, not sure how that would happen. And the encrypted channel for each group delivering the keys is different. So if they've got a clarification to the question, Beth, that would be great. Okay, so in that question just got submitted. So if you want to provide some additional information, we can go back to it. Next one that just came in. Can you provide an example use case for routable sample values? Routable sample values is really for synchro phasers. There have been, so you can actually embed the synchronized measurement information into routable sample values and deliver them in a standardized and secure mechanism. The, there've been a lot of utilities that have, whose IT staffs have raised the specter of how do you secure merging unit communications within a substation, which would be the layer two sample values. And this would allow you to authenticate those packets as well. Okay. Does this security cause a delay in either the RGOOS or RSV messages? Well, RGOOS and RSV have a requirement for a signature on them anyway. So of course, if you encrypt in the IED prior to publication, there is a delay. And I think Mark, you actually have some numbers, but it's, it's in the hundreds to 500 of microsecond range, if you decide to encrypt. Correct. Now, having said that, RSV and RGOOS have the ability to significantly expand the data set up to 64,000 bytes. So encrypting of 64,000 bytes would add a much more significant delay, but on a normal GOOS message, maybe with, you know, 30 or 40 or 60, even 64 data items, authentication takes only about 500 microseconds end to end, as well as the encryption would take about the same amount of time. Now, I don't know if this kind of answered that question a little bit too, but how much time delay does authentication and encryption add to end-to-end delivery of RGOOS messages? One millisecond. Or less. And that's from an implementation, by the way. Actual numbers. How do you send an ID? I think that's also with more...
 cryptocores coming out in new devices, that number is going to drop substantially, by the way. Sorry, Beth. That number is based on the calculations being done in a cryptocore. Okay. Go ahead, Beth. Okay, next question. How do you send an IEEE C37.118 sample data directly into it? It's got to be mapped into 61850 data objects, and there's an IEEE standard report on how to do that. Mark, do you know what the number is off the top of your head? It's a report, so it's not a standard, so it's not a C37 number. So there'll be a TR report in the IEEE world, and I do not know what that is off the end. But the report is freely available. Okay, next question. Do you use OpenSSL or WolfSSL? No. Okay, next question. Are we able to see the data transfer timings? Of what? The key delivery or routable GOOS? If that question, GOOS, you just responded. Do you want to see it right now, I guess, is the question. We aren't prepared to bring up Wireshark or the GOOS tracker in the lab right now to show that. I would point out that if a, assuming that there is GPS or synchronized timing between the two ends, timing can be measured by looking at the transmission time versus the arrival time of the message, and you would get your latency calculation that way. Okay, I think that answered the question. He responded, okay. Next question. In a wide area application with our GOOS, where should the KDC be located? In the control center or in one of the critical substations? One of the future capabilities of Garibaldi will be able to operate as part of a mesh network. So, you would be able to actually place it into the substation and it would be able to manage keys and receive keys from other substations or a centralized site as needed. So, it's a system design and deployment issue. It's not a technology issue. Okay, next question. Would RSV be a viable approach for line differential protection? And if yes, do you know of any deployments and where this is already up and running? So, I'll take that on, and the answer is yes, it can be, but the point I would make is that now makes your differential protection dependent on GPO. So, it's not a technology issue, but it's a viable approach for line differential protection. So, I'll take that on, and the answer is yes, it can be, but the point I would make is that now makes your differential protection dependent on GPO. So, I'll take that on, and the answer is yes, it can be, but the point I would make is that now makes your differential protection dependent on GPO.
 GPS. There are other implementations that work today without requiring GPS, and so it really becomes a philosophy question as to whether or not you want to have your differential dependent upon GPS. Okay, next question. Can mobile networks 4G connections be used as connection to public IP networks in Argus and RSV schemes? I don't see why not, Herb. I don't either. It needs to support UDP multicast. That's the requirement. Right. In theory, it should all work, but we have seen some issues with some routers on the market. 4G aside, some Ethernet routers have some implementation issues, so it should be tested, but it should work. Yeah, we've actually seen some problems with some firewalls with it, too. That's right, firewall issues. And the routers out there need to support the protocol-independent multicast, PIM. Yep. But the media itself should be able to support it. Okay, next question. Do you need redundant KDC centers for reliability? That's a question of utility policy. If you can bring up a KDC within 24 hours of failure, depending upon your key rotation time, probably not. I would suggest it, though. I would also point out that many utilities might have an availability requirement on the order of five-nines, and in order to meet that, you would have to have redundant KDCs. Yep. Okay, next question. Is the key delivery method standardized, or is it proprietary to Garibaldi? It's standardized. Internationally? Yep. Okay, next question. Doesn't KDC introduce a single point of failure for IED communication in the network? Not if you deploy redundant nodes. That's based on the previous question. Yep. One of the reasons KDA was developed was to address those issues. Okay, next question. If GOOS must be transferred at a maximum of four milliseconds, securing it would make it slower. What is your advice to make it secure and allowable for RGOOS and RSV? I think the four-millisecond requirement is for Layer 2. I'm not sure that the transmission latency, as the three or four milliseconds actually applies, would be impacted there. If you send more bytes, as Mark pointed out, it could be. You've also got that. That's also the protection class for GOOS, which probably isn't appropriate for RGOOS. But RGOOS still meets that protection, can meet that protection class, if the message is not terribly large.
 And you can constrain the network. Yes. If you're sending it over the open internet, for example, worst case, you can't prove that. This is true. I did it on a back-to-back network. But back-to-back, it's on the order of two milliseconds. Right. I'm just pointing out there's some variability in the network that comes into play there. Okay. Next question. Are there any plans to integrate KDC with cloud services, allowing Garibaldi to run on Azure or AWS, for example, rather than requiring a server computer in a control center or substation? It should be capable of running in an Azure cloud as is. We just haven't tested it there. Okay. Next question. Does Garibaldi support Layer 2 SV security as well? Is it only R-Goose, R-SV? Those switches, if you say to include Layer 2 Goose and Layer 2 SV, Garibaldi will manage the keys for those streams as well. Okay. Well, I think that wraps up all our questions. There were some early on that I believe did get answered online. But as I mentioned before, we will go through all the questions, provide answers, and send that out as part of the follow-up. So, you have all the questions and answers that were asked throughout the webinar today. So, again, I just would like to thank everyone for joining us today and hanging on for an extra 20 minutes to go through all the questions and answers. We certainly appreciate your time. Hopefully, you got a lot out of this. And we hope to see you on our next webinar in August. So, with that, thanks, everyone, and take care. Thanks.


Video ID: ycs5s9xUTUQ
Good morning, everyone. My name is Beth Capellas, and I am the Marketing Manager here at Triangle Microworks. I'd like to welcome everyone to our latest webinar, A Holistic Approach to Facilitate the IEC 62850 Process. This is a two-part webinar series. We will be covering part one today, specifically IEC 62850 Specification and Design. We're expecting a very large crowd this morning, so I'll go over some housekeeping items while everyone is still joining. The webinar is scheduled to be about an hour long. However, we typically get a large amount of questions, so we are happy to stay on longer to answer as many questions as possible. With that said, I'd like to encourage questions throughout the webinar. Since the audience is on mute, go ahead and submit your questions using the webinar menu under the section titled Questions. You can start typing them in as soon as you have them. Don't feel like you have to wait to the end. We will begin answering questions online, as we have several engineers monitoring the questions and responding to them directly. We will also stop a couple times throughout the webinar to read and answer questions, and then, of course, there will be some time at the end dedicated just to questions and answers. With that said, I'd like to introduce our speakers. First, we've got Joel Green from Triangle Microworks. Joel has worked with communication protocols for over 15 years, with the last six years in the power industry. He's the lead engineer for our IEC 62850 Development and Products at Triangle Microworks and is heavily involved with 62850 industry groups. He also works as the co-editor of IEC 62850 7-2. He is a member of the UCA Test Committee and Test Procedure Working Group and involved in the IEEE Standard Association. We also have with us Christoph Brunner, who is a utility industry professional and expert for over 30 years. He is the president of IT4Power in Switzerland and the organizer of working groups responsible for the core parts of the IEC 62850 specification. With that, I'd like to go ahead and hand it over to Joel to get started. Thanks, Beth. Good morning, everyone. Good afternoon, wherever you are in the world. I'm going to start with a quick overview of Triangle Microworks, who we are, what we do in general. Then we're going to move on to talking about the engineering process. Triangle Microworks has been around since 1994. We're located in Raleigh, North Carolina. We have customers all over the world, more than 70 customers. Our core business is source code libraries, which help equipment manufacturers develop devices without having to specialize in the various protocols. We also build tools on top of those that are a lot of help to electric utilities and system integrators. Our products are based on international standards. Beth mentioned a number of the groups we work with. Triangle believes in supporting these efforts and giving back to these groups to maintain these standards so that they continue to meet users' needs. We're very involved with the IEC 62850. The UCA International Users Group maintains the conformance test procedures for 62850. We're also very heavily involved in DNP, which is an IEEE standard, as well as Modbus and OPC. Being involved in these groups directly let us feedback and help our customers meet their needs. That's a brief overview of our protocols. As I mentioned, my core business is source code libraries. We write the stacks that implement these standards and protocols. On top of that, we build a number of simulation and testing tools. We'll see how those stack up here.
 in a moment. And we have a protocol gateway product that allows you to translate data between any of these systems. Our 61850 product line, we packaged into a convenient toolkit, includes the three major products that CL Navigator will look at today. Two other products we have that we'll look at in a later webinar, TestSuite Pro is our diagnostic client and test tool, diagnostic test tool, and Distributed Test Manager is primarily our simulator designed for helping test out system configurations. 61850 brings a lot of concepts to the design of power systems that are additional to what previous wireline protocols have brought. Some of that is the semantic data model, a very important piece of the pie that allows us to communicate what the data is in the devices in addition to the raw numbers. But it also standardizes the design process. And that's what we're going to talk about today. The design process is based around system configuration language, which allows tools and devices from different vendors to exchange information. And this is an overview of the whole process and how Triangle tools can help you be successful in these implementations. On the top line here is generally what we're going to cover in today's webinar. We start with the devices. Every device manufacturer selling a 61850 server needs to provide a file called an IED capabilities description file, which describes the data model and the services that that device provides. We'll see in a few minutes how our SCL navigator helps those vendors produce that file. That file and the others for the other devices in the system go into a system configuration tool. Triangle is partnered with Helix. Helix STS is the premier vendor independent system configuration tool. So we're happy to partner with them to do this piece. We'll see a bit of that a little bit later. Christoph's going to show us how that system configuration is done. Well, then when that system configuration file, the SCD, is produced, it is then consumed by the IED configuration tools for each of the different vendors. And this is a place where we can get into some troubles in the field. Unfortunately, there's sometimes disagreements in how the standard's interpreted, how SCL is consumed and produced. And so we see a lot of customers that have a lot of time and expense wasted in the field when the SCL file is transferred between two tools and the vendors disagree on some concept in the standard and how it's implemented. And that can cause a lot of wasted time. So we found it's very valuable to have a third party and independent tool that can verify that SCL file against the standard, not only with a schema check, but a model check and validate against a lot of other details in the standard to make sure things are right. And that's another key role of the SCL navigator is to verify those files are completely correct before you get into integrating them. Down here in this diagram, this section we'll be looking at in the next webinar. As I mentioned, our TestSuite Pro tool allows you to help verify this. And the DTM simulates it. So as you've got this design together from the SCD, we can simulate that in DTM, test the system configuration with TestSuite Pro, and then move on down into implementing it.
 with the real devices as those become available. So that in a nutshell is the 61850 engineering process. Christophe's gonna give us an overview of how of the system we're gonna be looking at today in just a moment. Beth, do we have any other questions before that? We don't have any questions to start with, so I will go ahead and switch it over to Christophe. Christophe, I'm changing into presenter. You should now have control. Yes, thank you, Beth, and welcome everybody as well from my side. Okay, so today, as Joelle mentioned, we will look into the different steps for a specification and design of a system. And in the June webinar, we will show you how to test the design that we made today using simulation and various tools. But before we start with the design, I would like to introduce you to our little application. Okay, so this slide shows the substation that we are building today together. We have a high-voltage substation with two line bays, a measurement bay, a transformer bay, and a busbar. Before we go into the steps of the specification, I would also like to, okay, the functionality that we have to do. So this slide gives an overview on the different functions that we assume for our use case. Line protection with different elements, reclosing, synchro check, and breaker failure, transformer and busbar protection, bay controller with control interface to all the switches and breakers, and also receiving the trip from the bay controllers. As a reminder on 61850, in 61850 functions or functional elements are represented by what we call logical nodes. And here we have a few of these logical nodes that we will use in our example today. The logical node for the trip logic, the PTRC, the recloser, switch control, XCBR. This is just a few examples. And as you will see, it's mainly with these logical nodes that we specify the required functionality when we do the specification. We will create SCL files, system configuration language as defined by 61850. An SCL file consists of multiple parts. We have the substation section that describes the functionality. That's the part we see up here. We have for each of the ID, we will have an element in, and we have one element in the design or in the specification and design, which is the communication part that describes how the system is configured. Okay, with that, let's now get into the specification stage where we use the Helix-SCS as a specification tool. And we produce the variant called system specification description file, SSD, a variant of the SCL files defined in a standard. During this phase, we are mainly filling in this first part of the SCL file, which describes the substation section or the functional part of the system. The specification can be split in three parts. First part, we do the one-line diagram with the process hierarchy, the substation topology. Then we do the functional requirements. We create functions and we specify logical nodes as functional elements that we require. And last, optionally, we can also allocate functions to virtual IDs. With that, we can...
 express how our system should be structured and implemented with various IDs. The slide here is the initial screen. When you start up the Halinks STS tool, it offers you a couple of tools that you have. And in the first step, we will now use the single line tool that allows us to create a single line diagram. When you open, you have an empty screen with a palette with various equipment. And then we can create our substation. We call them New Orleans here. We can create our voltage level. We call it D1. We then can add a bay, call it line 1. And then we can add the conducting equipment that we use in our bay, like circuit breakers, disconnecting switches, voltage transformer, current transformers, and the line. And at the end, we need to connect everything electrically. And with that, we have drawn the single line diagram of our bay. The next step, we have to add the busbar. We connect it as well to the busbar. So that's, in a very fast overview, the way how the one line diagram is drawn. And if you now want to see what we already have produced, we select from the tool, export the file. We include a substation section. We export an SSD file. And then our tool provides us a little navigation window where we can open. We have seen the file that here has been created. We can open it either with an SGL model editor, which shows us now the model that we have created as a hierarchy. All these models are hierarchical. We have the substation that we created, the voltage level, the bay. We have the various conducting equipment, like circuit breakers, like switches. So we have the various conducting equipment. So that's the hierarchical model view. But we also can look at the same in what we call an XML view. So here, you see the XML file that has been created in this first step. So with that, the one line diagram is defined. We also have with the terminals and connectivity nodes, we also define the connectivity. The next step is to define the functional requirements. And as mentioned, we create functions and then we add logical nodes. The functional requirements we define with a function editor that we can open by double clicking on the bay. And then we get a view like the following, where we have our conducting equipment. And what we see, the tool automatically, while we created the single line, already created a series of logical nodes and functions that are related to the conducting equipment. As an example, for the circuit breaker, it created a logical node XCVR, the switch control, and the interlocking function. But of course, we need to add more functions. And for that, the tools provides us the possibility to create a new function. We can call it UC1 control. Or we can have another function with measurement. And then we can add logical nodes into this function. And with that way, basically, we define our functional requirements expressed in logical nodes. The last step of the specification is the option to allocate now the functions to virtual IDs. And this is shown here. So the tool allows us to create a virtual ID. We can give him a name as a role. And we then can associate functions that we have. We can assign them to the virtual ID. So in that case, all these.
 four functions have been assigned to the bay control unit. The result of our application that I introduced in the beginning for one bay is presented here. So we have the bay control unit that implements the measurement function, the control of the three switches, we have switch control unit that implements the interfaces to the three switches, so we assume a fully digital substation with merging units and switch control units, we have the merging units implementing the voltage and current transformers, and we have our protection device with all the protection functions, the reclosing function, the synchro check and the breaker failure function. And if we blend this in our hierarchical overview, we see here now a few of the elements we see the voltage level, on top we see the measurement function with the logical node measurement unit as part of that function, we see breaker control with the switch control and the interlocking logical node, we see the protection function that we created with a differential element, two distance element, the distance that the scheme logic and the trip logic, we also see as an example down here the conducting equipment of the breaker with the logical node XCBR being the breaker interface, and we also see a conducting equipment transformer with sub-equipment per face and here we see a logical node TCTR representing the interface of the phase A. And with that we have created our specification and hand back to Beth if there are any questions before Joel will continue and create the IED specific SCL file. We do have a couple of questions that have come in, let me go ahead and read the first one, how logical node XCBR represents all the properties of breakers if only two on-off status and commands are connected to the IED? If only, can you repeat if only two on-off status? Well the logical node XCBR basically can represent as much as it's wired to the IED, the minimum is typically the position so the contacts for open and for close that are wired, but in many cases you may want to wire additional information like an operating capability that a breaker has depending on this mechanical charging, how many open close cycles can be done. If this is wired as well to the IED we can represent it if only the position is wired this is the minimum information that we then represent. Okay and I do have another question for you Christoph, why is there a need, why there are a need of two separate logical nodes for breakers for example XCBR and CSWI? Okay the XCBR as mentioned is representing the interface so that's the pure image of the contacts that you would get in a conventional system from the breaker, the CSWI is the control function as such so that's the the logic the software that is performing as an example interlocking check to figure out if it's allowed to operate the breaker that is probably performing a single check if we need a single check so that's really the software function that is executing the control the XCBR is purely representing the interface signals. Okay and I think we'll do one more question before we hand it back over to Joel, what is the
 of this virtual device? The virtual device is useful as an add-on to the specification is that the user can express the requirements which functions he wants to have implemented in what device. Sometimes you may have constraints that you want as an example do the reclosing in in the protection or you want to do in a bay controller so with that you can express which functions will be realized by what kind of devices so if you want to go to that level that's an option. We will see in the next step when we do the system design this also helps them to easily continue with design by mapping the virtual IDs to the real IDs but that we will see in in the third part of this presentation. Okay and with that Joel actually I have a question for you it's going to segue into your next section of the presentation. Is there any tool available which can compare the two different versions of CID files? Yes there is and we're actually going to get back to that in my next section in a few minutes we'll talk about how to compare those files after we put together the system file. That's all right. Okay yeah and with that Joel you can take it. Thanks Beth so we're going to go through as I mentioned in the introduction sort of the starting point the lowest part of all this we've got devices in the field that build up models to to communicate data from the servers. It takes a number of those to build a system so we're going to we're going to show how a device vendor is going to build up that ID capabilities description file for one of the bank controllers that we're going to use here. This is Triangle's SCL Navigator product. The first use case of this is to build this ICD file. So we'll start here and we're going to build a new file and in this case we're going to build a new file and in this case we're going to build a Edition 2 file which is a common point in the market right now. Edition 2.1 the first amendment to this has just been released. We'll start to see those devices shortly but we're going to go ahead with Edition 2 and so I've got to give it a file name and then I've got a little dialogue here that lets me set some basic information for the file. We've got an identifier that's a token text string vendor information and version number. In 61850 analog values can be represented either as a floating point number or as an integer with a scale value configuration to go along with it. There's a couple cases where the integer representation is useful. First of all there are small devices processors that don't do floating point math. Those can be represented this way. Sample values oftentimes uses an integer representation to meet performance needs. But in those cases we generally want to use a float. If we put both in the model the server has to keep the data consistent that's usually not worthwhile. So this is going to set the default in this file to use floating point numbers. We can override that at any level but that keeps us from having to make that decision most of the time here. So with that we're presented with a with a basic template to start our device with. We've got the IED level, server level, logical device level and a server has to have a logical device and a logical device has some information about the device itself presented in LLM0. So that always has to be there. And the LLPHD logical node gives us the physical description of the IED itself. So we have one of those in an IED. So these are all the requirements that we're going to use.
 requirements that any server has to start with that we pre-populate here. And then to build out our Bay controller, we need to add the other parts of the data model. So what we provide in this tool is this LN library here, if I click that, I get a toolbar on the left-hand side that shows me the logical nodes from the various parts of the standard. Standards we're using today come out of 7.4. We've got a couple other standards here, some DER, and some technical reports that provide some additional logical nodes, but we're sticking with 7.1 today. Interesting question came up there at the last break about modeling a circuit breaker. It's true that we need an XCBR and a CSWI to model a circuit breaker, and that's what's going on in our Bay controller here, so we're going to add those. So I open up my C group, and the logical nodes are grouped by the first letter, and C is my control group. So there's my C switch, and I just take that and drag it over here and drop it on my logic device. Now, this panel in the middle I just dropped it on is a navigation pane, and it allows me to jump to any logical node in the file quickly and easily. One of the features we add here to help you find your way around the file is why the product's called Navigator. So when I drop that on the logical device, I get this logical node editor window, and it comes up. Now, I don't have any CSWIs in this model yet, so my only option here is to define one. If I already had one, I could use a duplicate of that template and keep going, but I don't, so we have to define our first one. For every logical node, we have a prefix, which is a text string defined by the system designer. In this case, we're using QA1, not QA exclamation, to indicate the line that this breaker is attached to. The instance value is added to the end of the logical node name, and Navigator tracks that for us automatically. We can override it if we need to, but it generally manages that. So if I click Next, then I get a list of all the data objects that are available in this logical node, and the mandatory ones, according to the standard, like here, the behavior, are pre-selected for us, so we don't have to worry about that. Let's go through here and see if there's anything else I need in this model. In control, 61850 provides some testing functionality, test modes, and blocking modes, and those can only be activated if I've had the mod control in the model, which ends up setting the behavior, but to control it at this level, I need the mod, so we'll add that. We can add some nrefs here, so I can track where some of my inputs are coming from. We have some other fields here. Our local mode, we're not going to put this time. Sometimes that's necessary. This is a three-phase breaker, so I have the one position, a mandatory device. We're not going to add the separate three-phase positions in this case. We just got the one, but we could put those here. So that's what we need for this model. I need to go back up to my mod, though, because by default, an ENC here shows up as status-only. Status-only means a control that can't be controlled, so that doesn't help me very much. If I go into this column, this shows me the type template of the CDC, the data objects, common data class.
 that's built from. I can select that and I get the next level of menu just like I did at the logical node level. I can come in here and select the data attributes that make up this data object. In this case, I can select any of the ones that have already been defined in this file. This is the default that I started with. I can select this one or I can come in here and create an entirely new one. And we just give it some name and here I can select the control model. 61850 has four control models in addition to the status only. For ones like mode where it's an internal function, I'm not turning anybody's lights off. It's relatively safe. Direct with normal is a reasonable choice for that. So I can come down in here and add any of these attributes if I like. It's already selected not only the mandatory ones, but the ones that are appropriate based on the control type I selected. But if I want to come down in here, I can add my offer timeout. It's always a good thing to do. I don't need an SPO timeout because I've gone with direct control. I can put in some of my status information, be able to track what's happening this thing, and I gave it a new name. So that's all all good there. I finished that and now my mod has the CDC type that I just created. So there I'm finished with my LN. There's my CSWI, came up there. And as the caller pointed out, we need to also need an XCBR down here in the X group to model both sides of the functionality of this device. Again, I just drag it to my logical device. I give it the same prefix here so that we know that it's the same device on the same line. And I go through the same process to decide what I want in this device. So I can go back in here as well and set the mode in this logical node. And I want my mode to be controllable. Since I already defined how that's going to work, I can simply select the template that I defined last time. And now I can reuse that, which is of course a goal of 61850 to be able to reuse designs and reduce engineering work. So otherwise, this guy is what I need for this model. So now I've got my breaker modeled, both the XCBR and the CSWI. I'm going to go look through here from here on out. It's really a repeat of that process to add the other logical node. So we're just gonna go through a couple other features of the tool here. SCL has built up a data type template. So again, we get a lot of reuse of design. Navigator gives me some features here to allow me to move up and down this model and find out what I need. So at any point, I can come in here and select navigate to. In this case, I get navigate to template. And I can come down here and I can expand my logical node type and it shows me all my DO types. And from there, I can also navigate to my template and see what that DO looks like and I can expand that out. And from each of these, I can look at all my siblings. I can add data that I'm missing here and I can come back up and I can look at all my siblings and I can add data that I'm missing here.
 navigate to the usage at each of these DO types. So as I go back up, again, reuse of design. We're reusing these templates. So if I think I need to change this thing, I can easily see every place that it's going to be affected in my model. So I'm going to come back up. I'm going to come back up to my usage here and get back up to the top. So in these, we've just got the one line representing the LN, points to the data type templates. I can build that all up. Well, sometimes I need some instance data described in my SEL file to tell me details of what this unit is. And that requires building out that whole structure from the data type templates into DOIs and DAIs. Very tedious to do with a text editor. So what I can do here in Navigator is I come in here and right click and I say add instance data. And that builds out the whole tree for me automatically. As I mentioned, I've got tools in here to expand and collapse things at the various levels for where I need to be. So I'm going to expand all my children. So with one click, I just added all this instance data so that I can come in here and add the information I need into here. So if I come down and look at my position, I've got an STVAL in my position. And one of the things that we often need on a status value or measure and it's an S address, a short address, which is a token that my software in my server can use to identify a location in the device itself, actually. So I'm going to come in here and I can give it, this is an arbitrary string that only my server has to understand so I can put whatever I want to in there. Just made up that name. All right, that's all I need in here for now. So I come back up and again with one click, I delete unused instance data. And it gets rid of everything that's not necessary there. It's important in SCL to collapse these files as much as possible. They can get very large. So we need to minimize the size of the file everywhere we can. Along that line, some of these data type templates end up getting duplicated, particularly at the later stages when you're merging devices into a system configuration file. So we have a tool to clean all those up. It automatically compares the templates and finds duplicates and removes those. So we've got a nice tool there for that. Then when we're trying to figure out, when we're trying to visualize what this model looks like online, it's very difficult to do from the SCL because of the way the templates are constructed. So we have a model view tool here. And with that, I can quickly build out this whole device and see what this tree looks like online as a client would see it in the server. Again, we have a navigation pane to help me jump to the pieces I need in here. And I can quickly visualize what's going on in the device. So that's how we build out an ICD file. As I mentioned, just a few more clicks to repeat dragging logical nodes and set those up here. And I end up with this file. This is my basic Bay controller, which I'm going to hand off to Christoph as he gets back into the system configuration section after we see if Beth has any questions. Yep, we've got a lot of questions coming in. So let me just grab a couple of them. First one, what's the usage of the prefix field defined in the logical nodes tags? The prefix field is a.
 It's fairly arbitrary string, of course, there are limits on the length of it and the character set that's used. But it's used by the designer to indicate what part of the power system that LN is attached to. It's a token. Okay. Can we convert SLD to SCL files automatically using any of your tools or source code? SLD files are not in a standard format. Maybe Christoph, do you want to jump in here? So it's very difficult to convert those because there's not a standardized format. It's a, they're a drawing basically. There's a lot of work to try to ease that process. Christoph or Jorg, do you want to add anything to that? No, as you said, usually there is no standard format for the way how you draw your single line diagram. If you're not using a standard tool like a 650 tool like I have shown before, if you have a single line drawing from AutoCAD or whatever, it's not that straightforward to convert to it. Okay, next question. How many RCBs can be created in one IED? That's a function of the device itself. In the ICD file that I just presented, there's a services section. I believe I'm still showing my screen back. Yes. So in the services section here, the comp report control, the IED vendor has to specify based on the resources, memory essentially available in the device, how many it can support. It's purely a function of the device itself. Okay, let me give you one more, then we'll switch back over to Christoph. Usage of prefix in SEL file, is that required or optional? I'm sorry, is an SEL file required? Is usage of prefixes in the SEL file required or optional? It's required. Well, the LM prefix is optional, but when you need them, when you have multiple ones, you need to put them. Yeah. I was about to backtrack there. According to the scheme and the standard, it's not strictly mandatory, but I think it would be, the system would get confusing in a hurry without them. Do you agree with that, Christoph? Well, I mean, I didn't put a prefix for the MMXU before, or for the Reclo, yeah, for the MMXU, I didn't put one, because it's... It depends on how many... Yeah. Okay. And with that, Christoph, I've given you the controls back again, if you want to continue on. Yes. So welcome back. I hope you see the screen again. So now that we have done the specification, we also have the ID files, the ICD files. Now we can go into the system design. So the system design, what we are doing here, we import the file that we created with the specification. We import the ICD file that Joel created, as well as other ICD files for protection for all the devices we need. We import that into the system configuration tool, and then we do the complete design and create, at the end, what we call the SCD file, the System Configuration Description file. So what is done?
 The specification that we import, we have our substation section in our SCL file. From the ICD that we import, we create as many instances as we need that device in our system. So if you have two feeders, you need the bake controller for the feeder twice. So we create two instances of that. And during the design of the system, we also populate the communication section with all the information related to the networking, like IP addresses. So this is an overview on the different steps we have to do. We first have to define our protection and control schemes, the single flow that we need for our protection and control schemes. We then have to create the aspect of the system diagram, the communication network, the ID instances with the connections to the network, and the communication parameters. Then we have to implement our functions, which basically means we have to map the logical nodes from the specification that we created onto the logical nodes in the IDs that we have in our system. So we need to say which ID is the interface to the breaker on line one, which ID provides the interface to the breaker on the line two, and which switch logical node is the interface of the line disconnector, and which one is the interface of the busbar disconnector. So all that needs to be specified. And then at the end, we have to finalize the communication, which means we have to design a single flow for reporting, and we also have to create a control box. So the first step, define schemes, and we are still using the Halinx SDS tool for that. The first thing before we can start drawing a protection or control scheme, we need to configure signals. So we need to identify what signals are we needing. So we can right click on a logical node, we can select configure signals, and then we get the list of all the available 6150 objects, and then we can add those that we need for our application so that we don't need to select always from the whole set of signals. We can more or less add what is required for our applications. We then do our application design, and that's another tool in the Halinx SDS, which is called applications. And in that tool, we can create a scheme, an application, in the same way like you would do in a conventional system, a drawing of wires between the devices. The only difference is now we are drawing virtual wires between the different functions. So the first thing is we create a new Goose application, and as a result, we can do that. We can then open the application editor, and we can create the application control for the line 1. Once you have created the application, now we need to select the functions that are participating. We can start selecting a sending function. When we select from the palette sending function, we get a list of all our functions that we have in our specification, in our design. We can select them, and we can add them to the drawing panel. So now we have selected here a sending function, which was the QA1 control. We also have selected a receiving function, which was the break-in interface. And now we can select from the sending functions signals, and we get the list of all the signals that are available from that function. And in our case, we select the operate open.
 at the operate close as the two signals that need to be sent over to the break interface as the receiving function. So we basically wire the sending function to receiving function. And we have a wire for each of the signals, which is 650 data objects that are sent. And we can indicate to which logical node in the receiving function this signal has to go. This picture shows now the overview of our design again, what we need to do for the control. So we have the breaker interface with the breaker logical node that receives the operate open and operate close from the switch control. It also receives a retrip to opt in from the breaker failure function. It receives the trip from the protection function. The single check sends a release to the control function. So the function can both be a sending function, but it can also be a receiving function. The reclosing sends a reclose request to the control function. And the switch position is sent to all those various functions that are interested in the switch position. And with that, we have designed our application. Not yet anything with 61850, except the signals. You will see later on how out of that, whose control box will be created. But first, we need to create our devices. So we need to do the system diagram. And for that, we import files. And in this case, we import the ICD file. So the ICD file that Joel has created will now be imported. And then on the system diagram, we can place the device. We can give it a name. With that, we create an instance. So we have selected the device from the palette, have dragged it over here. We have then selected a name to give it a unique name. We then connect it with the connector subnet. We connect it to a subnet that we have created, represented here by a switch. And we also assign an IP address. At the end of that step, we have our system diagram with an HMI, with a protection unit, the switch control unit, a bay control unit, all then connected to the same network with various IP addresses. Now that we have all the devices in the system, we go to the next step. And now we have to link or connect our devices to our specification that we made earlier. As we made virtual IDs, we can do that in two steps. We can first map the device to the virtual ID, the real device, and then we have to map the logical nodes. So the first thing from our specification, we can right click again here on our virtual device that we call bay control unit. We say we want to implement that. And then when we select implement, we get a list of the devices that are in our system that we created before, the real devices. And in that case, we say we want to implement that with line one bay controller. And if you click OK, we then have now not anymore an indication virtual. Now we have here the name of the device that we have selected. And here we have the type of the ID that was in the ID, in the ICD file. And we see this symbol here that this device is now implemented. This virtual device is implemented. That's the first.
 step. The next step now we need to tell which logical nodes this device may have many logical nodes we need to tell which ones to use to implement our specification. We see here this chain symbol which is not yet closed that means those logical nodes from the specification didn't yet find a corresponding logical node in the VCU and for that we double click on this symbol to open another editor that allows us to map functions so we can select down here this gives us the list of all the logical nodes that are defined in the ID. We see there is an MMXU and by double click by clicking there we can say this MMXU shall be used to implement what from our specification and with that we can map all the logical nodes we require and at the end when we click okay we will get all these symbols linked so here we see all the different logical nodes from the IDs and what we use them in our design but what functionality but logical node they realize from the specification and at the end we see all the logical nodes linked. This is an overview now everything has been implemented all the logical nodes have been linked except one here that is not yet linked and what we also can see in some cases when somebody is not linked correctly it indicates us if there is some problems. Once everything is linked we have to finalize our design. The first thing we need to do is to define the single flow for reporting so the MMS based reports that are sent as an example to the HMI. This we can do in a function editor on the substation layer level that when we double click here we get a function editor where we can place a virtual HMI where we can add an HMI function with a logical node IHMI we can map that then to our HMI and then we can select here on this logical node MMS reporting inputs and this gives us the possibility to select from all data objects and data attributes available in our system which ones we want to use to report to the HMI and then we click okay and the last step is fully automatic in this tool it's the magic button that we can select which sets the communication settings from specification we can select if we want to configure all IDs all report sample values so it's taking the specifications we made for the schemes with the wires that we connected between the sending and receiving function it considers the inputs that we configure to the HMI and then it generates automatically all the control blocks we need and we can verify that in the communication editor which is another tool and then we see as an example here the ID created a goose control block here in the bay controller with those signals that are put in the goose message it's ended created another goose control block in the switch control unit we also see here who is the receiver of the signals we also configure the extra if that is needed so everything is configured automatically in the background and with that the system design has been made the STD file can be exported and Joel will then
 verify if everything was taken over correctly from the ICD file or not and with that I hand back to Bess if there are any questions. We have lots of questions coming in so I will go over just one or two of them because we are running short on time but as I mentioned earlier we will stay on after the webinar is over to answer as many questions and answers as possible as long as everyone wants to stay on they can definitely hang on. Okay so Christoph will you be able to import vendor-specific ICD files when adding IEDs to a system diagram? The import in fact is done before and I probably can move back. The import is done in the workspace of the tool and once I have imported the IEDs I see all the vendor-specific IEDs down here listed and then I can simply drag them over and with that I create an instance of them. How will we come to know what IP address is assigned to which device? Again that's done when I connect the device to a subnetwork I can define the IP address we see here on the next slide I think we see multiple ones so each of these connections here has as a property an IP address which is the IP address that the IED is connected to that subnet. An IED may have multiple communication ports and it can be connected to different subnets then for each of the subnet we will provide an IP address as we want to use it. Okay and for the sake of time I'm going to switch back over to Joel and then we can circle back to the other questions as they're coming in. Okay Joel you should have control again. Thanks Beth so let's go look and see if Christoph has done a good job here. As I mentioned in the in the overview the second use case for the SEL Navigator tool is to verify SEL files at every point in the engineering process as they're transferred from from one tool to another. So we can catch any critical bugs at that point when it's cheaper and easier. So we've loaded the STD file that Christoph built with the eLinks product. At this point we've got a collapse down to where we're seeing the IED names. Again we've got the navigation panel over here so we can jump into whatever point of the file we want. What we're looking at at the moment is our verify tool. We're going to come up here and we're going to run SEL verify and this is going to produce us a list of any issues in the file. It starts with a schema validation which is an XML schema SEL defines. That's good for token validation input validation so forth. Syntax of the file not nearly enough to see if the file is useful but it's a first cut. So we've got a we've got a schema validation error here. So if we jump down into this we get the line number where we find the problem and a click will get us right to it. And this is telling me that I need a log name. And so if I look down here the editor gives us all the ideas in here in orange is log name and it's orange because we're required. And I should have a log right here. So I'm going to find out in a moment that I'm missing the log so I can go in here. And we're we're cheating a little.
 bit here. This is an SCD file. We can't really change device properties here, but if we have to get a job done today, we can come put the fix in and tell the folks at the other point in the process what needs to be done to make it work. I'm going to come into my own zero, and I'm going to add a log. And his name is generally going to be the L device name, but we're just going to give it a simple name here. And then I can come back to my log control and add that name and save my file. And this is a history item dialog that's going to try to encourage me to do the right thing and put a revision note in the file. As soon as I thought, as soon as I saved the file, it reruns the verifier for me automatically. And that schema error has gone away. The next section here is called information. These are not warnings. These are things Triangle has decided are helpful for you to know about the file. And so we provide a lot of statistics and background information here. There's nine servers and one client defined in the file. You built this file. Hopefully those are what you're expecting to find. We talk about how many GGIOs are here, because, well, that's really not the right way to do things. So we want to give a little information. And we go through each logical device and provide a good bit of statistics of what's going on in each device. Now that we get through the information section, and we can collapse all this in the tree to organize what we want to see, and we can sort by it up here. So we've got a set of warnings down in here. And what this is telling us is that we've got settings controls in a number of these devices, but there aren't any attributes in the device assigned to a setting group. And that's not wrong. The device has this capability. We didn't use it. That's okay. But it's worth noting so your engineer can go back and look at it and make sure it's what it should be, because it's not quite right, but not really wrong. So it looks like overall Christoph's done a fine job here, and we've got a file that should work very well. So the other critical component in comparing an SCD file is we want to make sure that we've got the files, the input files we expect to have in the final file. As I mentioned in the fixing that log, we should have sent that back to the device vendor, have that corrected in the device configuration, and gone back through what's called the round trip engineering process. And if those ICD files are updated as we're working through this, well, we want to make sure we have the final version of each of those in the final version of the SCD file. So to enable her to do that, as well as the question in the first break about comparing ICD files, we have a tool here called Model Compare that accomplishes both of those functions. And we can compare essentially any two SCL files. Now, of course, if the SCL files are not in any way related to each other, you're not going to get any useful information. But if the models are somewhat related, we can determine if there's any differences and describe any differences in the two. So here we've got our SCD file that we just came out of the system with. And here we have that ICD file for the bay controller that we started with.
 This tab tells us communication sections, this ICD file is a template file, so it doesn't have any communications information. So we're expecting all that to be showing errors. In the model, object model view, we can come in here and pick any of these we want to compare to. Now, this is the template for the bay controller, so it only makes sense to compare these two bay controller files, and we'll pick one of those. And we'll look through that. And here we can expand this out and see the whole tree, where there are problems or where there are things that look okay. And over here on the right, we have a report, just the differences. And what we see here are largely things we expect to see, LGOS logical nodes are reporting information on goose subscriptions, and those get instantiated in the system configuration tools. So we don't expect to find those in the ICD file. That's fine. Our control blocks get instantiated at the system configuration level. Again, we don't expect to find any of these things in the ICD file. So everything looks very good there. That quickly lets us show that this file is using the version of the ICD file that we expect. And in a similar way, we could compare those two CID files with the caller asked about earlier. So that's the gist of how to check your files and see that they're correct and what you're expecting as you move through the engineering process. Beth, do you want to go back and see if we have any other questions at this point? We do have quite a few questions. However, we are at time. So what I would suggest is just to do a quick wrap up, and then we certainly are going to leave the line open and we'll stay on as long as someone wants to stay on with us to go through all the questions we have here. So with that, Joel, I don't know if you do want to do a quick wrap up or you want me just to go to the next part, my last part from here? Yeah. Yeah, well, I think, well, you were going to go through the questions and talk about the next webinar, I believe. Yep. So for everyone that's not able to stay on while we keep the line open, we will be sending out additional information about the products that you've seen here today, both SEL Navigator and Key Links. You'll also get a short survey once you sign off of this webinar. So we really appreciate anyone that can respond to the surveys. Let us know what you thought about this webinar, what information you want, additional information you want to see. And then our follow-up webinar to this will be, the date for that will probably be announced in about two weeks, but we are planning to have it in June. And that really is going to cover the testing and the simulation of the design of what you saw here today. So please stay tuned for more information on that upcoming webinar. And as I mentioned, it will be sometime in June. So with that, anyone, we still have a lot of people on the line. So anyone that wants to stay on, we're going to leave the line open and just start to go through the list of questions. So feel free to continue to ask questions and we'll just keep going through them until we're done with the list. Okay. So with that, I'm just going to start with the last question that we were asked and then Joel or Christophe, between the two of you, you guys can jump in and answer the question. How do services compare with actual IED when we create user specifications? How do we check this specification with manufacturer IED services capability? Can you...
 Mr. Joel, I still have a question. Can you read again? I didn't get to the beginning. It's two questions. How do services compare with actual IEDs when we create using specifications? Second question to that, how do we check the specification with manufacturer IED services capability? OK, so in principle, as part of the specification, we typically limit the specification to a function specification. But of course, we could also add specifications of required services, so the service section that Joel has shown, where we see as an example how many reports are supported. We are currently the working group discussing some extensions to the standards that would allow to create what we call an ISD or IED specification description file. And with that, we could express all the service requirements. Currently, that's not yet directly available in the standard as such. Of course, you can use the approach of a virtual IED to do something similar. Then the tool basically will need to compare the required services against what is offered from the IED. But what the tool is doing in any case, and I think that's even more important, when the Helix tool at the end, as we have seen, is creating these automatically the report control blocks and the data sets, it is checking the service section to see what is allowed. Because in some cases, you may have IEDs that have predefined data sets that it cannot configure or it has predefined control blocks. And then the tool needs to adapt to that. By default, when you have fully flexible IED, the tool can just start from scratch and create those. But if there are already some there and you are limiting the way how you can modify them, then the tool needs to consider that. And then it has to do based on the service section of the device. Not sure if Jörg wants to add something or Joel. I don't know. I think you covered it pretty well. That gets to a common question is how to do it. It's a procurement question and verifying the devices on the market provide the functionality that's needed. And a lot of that gets into there's a lot of work on the basic application profiles going on to try to address those questions. OK. We move on to the next question. Can we do engineering of mixed edition using system specifications and do all whatever is shown today? I would say yes with some caveats. Christoph? Yes. There is a part six of 6.850 that defines the system configuration language has an annex that defines some constraints and how mixed systems shall be working. As one condition, it is required that the system tool shall be of the highest edition. So if I have edition one, edition two, and edition 2.1 devices, it is assumed that the system tool supports edition 2.1. And then we have, in some cases, what we call downgrading. So the tool is able to export. So the whole design will be made on the highest edition. But when importing all the files, the tool needs to upgrade these devices depending on the model that may be changes. And when exporting file, it may be required that we export not only edition 2.1.
 2.1 file, but maybe also an Edition 1 file. And that's basically supported by the tools. All right, next question. Does the config revision for each IAD come from its corresponding ICD file? And if so, every time a new ICD version is released, does this need to be updated? Yes, however, there are several config revisions throughout the file. Does Control Box carry one to indicate if data sets have changed? But at the IAD level, yes. Yeah, the configuration revision is related, as an example, to Control Box. When the Helix tool is modifying them, so it typically increases the revision of the configuration. The model, the IAD model revision, OK, if you change the IAD model, there is another information, which is the revision of the model. That, of course, when you import an IAD with an updated model, you need to consider that as well. Doesn't necessarily require that you every time recreate everything. But I mean, if you have a model change in the IAD, then apparently you did it because you need these additional signals. I'm not sure if the config revision was referring to those. A pure software revision that doesn't affect the model as such of an IAD, that the system tool doesn't really care about. And that would show up in the software revision information. Yes. It's not configuration. OK. Next question. Can you use the SEL Verify on any SEL file created from different vendors? Yes, that's absolutely the reason for, as I mentioned, to sort out differences in opinion between different vendors. The value of having a third party tool to objectively identify any file against the standard. We found a great value to a lot of customers. OK. Next question. If you build the HMI logical node, where will this reside in an actual implementation? What IAD? Well, the HMI logical node in the substation section typically will reside probably at the station level. But then the HMI itself is, at the end of the day, also an IAD in the system. Like every other IAD, it's probably an IAD that has only a client. But it still appears in an IAD section as an IAD. And the HMI that is part of the specification will refer to that HMI logical node in the IAD. OK. Do we get SEL ICD files when we buy a device? Or can we download them from the device itself? This depends on the vendor. Some vendors, you can download the ICD files. That's mainly for devices that have not necessarily flexibility. If you have a multifunctional IAD that you can configure with the vendor tool, whether it does what kind of protection functions it does, then you may create it with the vendor tool when you configure it to your specific purpose. OK. So the vendor has to provide this file to you. But the method by which it's just shown here is not available.
 distributed is not defined by the standard some devices have FTP access directly on the device you can get it Some you download from the vendors website and as Christophe mentioned some you produce to some extent on the flexible devices Okay, here's more of a generalized question as an end client customer We have never come across these engineering processes who and how these tools could be utilized if at all we want to do this for our future projects One more time, please As an end client customer we have never never come across these engineering processes Who and how are these tools could be used? for our future projects Basically a general question on this engineering process I still didn't get the last part who and how can these tools work? Yeah And how do we use process for future projects? Hello Yes So generally Utilities can use this to produce the SSD file the system specification which they hand over to the system integrator which then at the end gives back the SCD and Then it can also be used to compare the results of the engineering against the specification The system integrator can be the utility itself or a third party or a vendor or whatever Yep Okay, Joe, do you want to add anything or is that it I did want to add I'd like to introduce your router Our partner from from Helix just spoke there. I think we forgot to introduce him York's been working with the Helix STS for many years. And this is one of the SCL experts at 16850 Okay The next question, how did these tools become useful to a utility customer engineering when usually we get an SCD as an end product? Well, I think that's exactly what The same answer like you're provided before I mean if the utility customer gets the SSD file They will not really use the SCD file. They will not really use it. They may use it to do the specification They may use it when they later want to maintain the system If they have contracted somebody to do the design, but later on you want to maintain it yourself Then this tool may be useful as well. I think today a lot of utilities are really Aiming to improve their purchase and Specification process so their collaboration with the with the suppliers. They want to more precisely specify their system in order to get similar solutions for similar substations For maintenance and also for quality and here the 16850 process offers the opportunity to make a formalized standardized specification instead of Excel based or word-based POSA specification document Is there available standard libraries from IEDs such as ABB and Siemens
 Standard libraries from IEDs? What does that mean? Well, an answer that I have here from Jim Coats, who's online answering these questions is, the vendors usually allow downloading the latest versions of ICD files from their website or have configuration tools that allows creating an ICD file consistent with the configuration of the IED. Okay, so that would be the same question we had before. How do we get the ICD file? Next question. How can it be indicated where the subscription can take place in the ICD? Okay, there is two aspects of that. I mean, the subscription as such is configured in the ICD file. We didn't look into that in detail, but we configure the IEDs that are supposed to receive the goose message. And in the IED, we can configure with an xref the signals that we receive on the particular logical nodes. And then there is the aspect of the live system where we have these logical node LGOs for the goose supervision that was introduced in the second edition of the standard. And there is a status indicating that the IED is currently receiving or subscribing the message. So with that, you can verify that everything is okay, that the IED really sees the message and is subscribed to it. And there we can also see when we talk about testing, but that's another topic, if it receives the simulated one or the real one. So that we can see in the logical node LGOs. Okay. Next question. Is there a tool to test the report reservation functionality in 621850? TestSuite Pro has features that test reporting as well as all the services in 621850. It can definitely be used to test the reservation. Okay. Is there a way to easily merge ICD files after comparing them? The short answer is no. We allow the comparison of those files so we can see the difference. Those need to be routed back into the engineering process and the design of the ICD file. The merging is basically done in the system tool. I mean, if something has changed, Navigator allows us to figure out the differences. But we need to go back to the system tool and we need to go back to the system tool. If something has changed, Navigator allows us to figure out the differences. But we need to go back to the system tool to have the right file there. So the system tool when importing an updated ICD file for an ID it has already imported earlier, it will do a merge. I understood the question to be about doing a difference between revisions of an ICD file. Maybe I misunderstood. Okay. Next question. When you change only one device in Healings, what happens with the other devices? Must I update all the devices? That depends on the consequences. If I change in one device, a signal that I add to a goose control block.
 I probably add that because there is another one that needs that signal so in that case yes I need to to reload the devices that are subscribing to the school's control book but there may be other changes that I do on the device that don't affect any other devices so that's still the responsibility of the system designer to know what he is doing and to know if he does a change what other IDs are affected by that change because they somehow depend on that change so if if you if you update in healings communication link then healings proposes you to regenerate the communication configuration for every concerned ID and indicates in a lifecycle support that you have to reload the set of IDs you have to reload to make this consistently happen so you do not have to reload all the IDs but just IDs that have changed okay does every vendor support configuration of IDs via SEL and is it mandatory 61 I 50 devices yes no they have to provide a file that represents the configuration the device the device can be fixed doesn't necessarily have to consume the ICD file does triangle have any tools to test secure MMS and goose protocols if yes please explain a little bit about that yes both DTM and test suite Pro allow testing of secure MMS okay well SEL navigator verify all test conditions for ICD based on conformance well the conformance is testing at the SEL level testing the files correct according to standard which is what the verify tool does I would say yes it will verify that it's your conformant okay I've got a very specific question here let me try to read it as accurately as possible how do I find target points where we should place input X ref and input sections in an SED file in the IAD subscriber I'm not sure if I'm reading those specs correctly can you read yes and and you should be able to see it Christophe on the list of questions if you've got that out in front of you it's the third one from the bottom at the bottom how to find target we should place in point at input X refs and put sections in SED files in the IAD subscriber okay where we should place input X ref in SED IAD subscriber okay that's there is the wrong answer behind that in principle the standard supports multiple options and this has been clarified a lot with addition 2.1 of the standards because addition 2 and 1 were not
 not yet so clear there. So in principle, there's two options. One option is what we call the later binding. When the ICD file from a vendor already provides inputs that have not yet as inputs with extracts, but the extract itself has not yet any binding to a source, but it already has a placeholder for an input. And the recommendation is that in the description or in the source address, we have a text that is more or less understandable for the system engineer what kind of input that is. Is it a blocking signal? Is it an initiate of a break or fail function or whatever? In that case, the system tool, when it creates the connection, needs to connect that to the right point. If the vendor ID doesn't support the later binding, then typically, in the ICD file, we don't have any inputs and extracts. And then the system tool, according to edition 2.1, can connect it to any logical node where it needs it. So in principle, a trip signal will be connected to the XCBR. So the system tool will add an extract there. And then the IED tool needs, in the post-processing, take that and connect it to its right internal variable. That's basically the two main options as they have been now defined in 2.1. I'm not sure if Jörg wants to add on that. No, that is fine. That is fine. That's fine. OK. OK. Yep. Next question. But the relay may not be able to support subscriptions to all data attributes. There may be limitations on type, total numbers. How can all these issues be indicated to the SCT in the ICD? I think the number can be limited in the service section. For the type, normally, the relay should be able to subscribe to any type if there is a compatible logical node later on to use it. If this is not the case, then there is the option to use, as Christoph has mentioned, the later binding and not allow additional extracts. With this, the receiver can define exactly which data and which data type he is receiving. OK. Can Helinks monitor as a packet? Does Helinks have a packet tracer? And can monitor how fast the IEC data packets are received? And also, how big is the data being transmitted? So Helinks has no online connection. It is only for engineering. But I think there may be some tools from Triangle on this. You're really getting more into the realm of Test Suite Pro to monitor the online traffic. So we monitor those connections there, give some information about the traffic, which we'll go through in the next webinar on how to test those. OK. Well, with that, oh, go ahead. No, it's OK. I was going to say, with that, we've come to our last question. I think we've gotten through everyone's questions and answers. I really appreciate everyone that has stayed on with us for the extended amount of time. A good amount of you did stay on. So that's good to see. If you continue to have questions, please feel free to email me directly.
 Everyone should have my email address in the follow-up email that you get once you sign off of this webinar, or you can also always email sales at trianglemicroworks.com. We will also be sending out a recording to this webinar, as well as posting the recording on our website, trianglemicroworks.com, and it will include this question and answer session. And you will also get some follow-up information from me on the tools you've seen today, as well as the printout or the PDF of the questions and answers that we went over here today. We hope to see you on part two. We'll be announcing that date in the next couple of weeks, it will be in June, and we hope to see you there. So thanks again, everyone, for your time. Take care. Bye-bye.


Video ID: Wy-Sn-XT3xU
Hello, everyone, and welcome. My name is Beth Capellas, and I'm the sales and support manager here at Triangle Microworks. I'd like to welcome everyone to our latest webinar, Charting a Course to a Digital Substation, How to Integrate IEC 6250 in Your Existing Solutions. I'd like to mention that this is a three-part series. Today's webinar is Part 1, An Introduction and Comparison to DMP3 and IEC 61850. We'll talk a little bit about Part 2 and 3 later in the webinar. However, you can always find out more information and details on our website at trianglemicroworks.com under the Training tab. Before we get started, I'd like to go over some housekeeping items while everyone is still joining. The webinar is scheduled to be one hour long. We have a lot of material to cover today, but we will try to take at least two breaks to answer questions. The audience is on mute, so please submit your questions online using the webinar toolbar under the section titled Questions. You can submit these questions as soon as you have them. Please don't wait until the end. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly. We're also happy to stay on after the webinar is over and answer as many questions as we can. So with that, I'd like to introduce the folks behind the scenes answering those questions. Today we have with us Greg Godolewski, who is our engineering manager here at Triangle Microworks. And we also have Dave Kuchenauer, our lead developer for Distributed Test Manager, also known as DTM, which is one of the tools that you'll be seeing here today. Next, I'd like to introduce our speakers. First, we have Christoph Brunner, president of IT4Power. Christoph is a well-known utility industry professional and 6250 expert with over 30 years of industry experience. Christoph has been involved in the development of the standards since the beginning. He is convener of Working Group 10 and member of Working Group 17, 18, and 19 of IEC TC57. He's an IEEE fellow and a member of several IEEE working groups, as well as a member of the PSRC main committee and subcommittee H. Next, we have Jackson Moore, application engineer at Triangle Microworks. Jackson received bachelor's of science degrees in both electrical engineering and computer engineering from North Carolina State University and has a background in power system engineering. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer, where he designed and developed load management control systems for multisource microgrids, ranging anywhere from one megawatt to 30 megawatts. In his current role of application engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve unique and complex challenges our clients face. So now that you know everyone here, I'd like to give you a little bit of background on Triangle Microworks. We were established in 1994 and have over 28 years of communication protocol experience. We're headquartered in Raleigh, North Carolina, and have customers in over 70 countries. We work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs in development time.
 Code library also includes support for your implementation and any issues that may arise in the field. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They are designed to troubleshoot, simulate, and automate testing for both a single device or a complete substation. Lastly, we also provide a SCADA data gateway, supported on both Linux and Windows, for protocol translation and data concentration with standard SCADA protocols as well as OPC. It's available as software only or pre-installed as hardware. We are also heavily involved in the standard committees that support SCADA protocols like UCA group for 620.50 and DMP user group and technical committees. Through our participation, we help define the protocols, stay up to date on the changes taking place, and ultimately provide feedback based on our customers' needs as the protocols continue to evolve. So with that, I would like to go ahead and hand it over to Christophe, who's going to kick off the webinar for us. So Christophe, I'm going to head and make you presenter. Thank you, Beth. Welcome everybody from my side. Okay, I hope you can see my screen now. Okay. So in this introduction of our first webinar of this three-webinar series, I would like to introduce IC61850. As many of you are familiar with DMP3 or other SCADA protocols, I will use DMP3 as a starting point and explain what is similar in IC61850. The graphic here shows a substation like we know it for a long time. We have what we call station-level devices like the gateway and the local HMI, and we have microprocessor-based bay-level devices like protection relays. Communication between the devices and the HMI is using SCADA protocols. The interface to the process is wired. Also wired is typically the interaction signal flow that is required between the devices to implement protection schemes. In this classical world, information that the device could provide was declared through a point list. The SCADA gateway probably or may remap some of those points for the communication to the control center. Here we see one of these classical point lists. When we talk about design, we mainly had two things that needed to be done in this classical world. We had to assign communication addresses for the different devices for the SCADA protocols. With regard to the applications, we more or less have to draw wire diagrams, wires exchanged between the different equipments to implement the applications. Those wires were relayed and also physical wires with binary signals or analog signals. In the second part of this, in the second webinar of this series, we will see how the design will be done with 61850. But for today, let's focus on the protocol. We will start now the introduction of 61850 on the protocol level. Again, starting from what we know from DMP3. DMP3 is used from control center through data concentrators down to devices. Within a substation, it's typically used for what we call sometimes vertical communication between IDs and the gateway local HMI.
 DMP3 provides control of process equipment and event-driven data acquisition with quality flags. Similar functionality like what we just discussed on DMP3 is supported in IEC 61850 by what we call there the client-server communication using MMS. But IEC 61850 adds two protocols on top of that MMS or in addition to that MMS communication, which is the virtual communication, 61850 adds two protocols that are typically used for communication between the devices and that are a key for the digital substation. One of them is what we know as GOOS, or generic object-oriented system event. The other one is the transmission of sampled values. We'll introduce those two communication concepts later in this introduction. Getting back, as already mentioned, a lot of the functionality that we have in DMP we also find in 61850. So I have a lot of functionality that is equivalent between these two protocols. But in some cases, we use different terminology. That's why I try to compare them a little bit here to help you find the related things that you know from DMP in 61850. So while 61850 uses the terminology client-server communication for the vertical communication, in DMP3, we typically talk about master outstation communication. But also in DMP3, we have the possibility that multiple masters can access the same outstation or that the same outstation can also be accessed by multiple masters. When we talk about the communication services in DMP3, we typically call it function codes, what is identifying the purpose of a message. And we have messages like read, write, select, or operate. In IC61850, we call those services. Service names identify the purpose of the message. But we have similar services, like we have a get data, which corresponds to the read. We have a set data service. We have select this value, which is the select service. And we have operate. So the services we have are very similar. They have just a little bit different names that we call them services. Yeah. When we talk about information in DMP3, the information is organized in points, like in all the SCADA protocols. And the information is identified with a point index number. In 61850, we call the information data object. And the identification of a data object is with a string. We'll explain that on this next slide. So somewhere here, we have a 61850 device. The data model is hierarchical. Somewhere in here, we have a data object. In this case, the data object is the position. And the identification of that point would be through a hierarchical path created down from the hierarchy. Now, what is the hierarchy? On top of the data objects, we have logical nodes. In this example, we have a logical node called XCBR, or circuit breaker. We will talk about logical nodes a little bit later. And on top of that, we have logical devices. So all that's what we find within an IED. Beyond the data object, we have
 then the data attributes, that's where we have the values. So looking at the naming again, we not only can identify this data point, but we also can identify the other elements. So as an example, we can identify a logical node. We also can identify the logical device. And as you have seen, the data point itself is really identified through a hierarchical path. OK, now again, some more common characteristics that we have between DMP3 and IC61850. Both protocols have quality indication to indicate the quality of the data. DMP3 has various flags indicating if the data is valid and reasons why it's not. 61850 in a similar way, but it has an explicit validity flag and additional flags indicating the reasons why a value could not be valid. With regard to timestamping, both protocols use timestamps. Both require the timestamp to be in UTC. For those in both protocols, it's possible to indicate local time zone. With regard to control, again, both protocols support direct operate and select before operate. One of the differences than in the IC61850, the status information is directly associated to the control point. So it's the same data object where we do the control and where we get the status back. In SCADA protocols like DMP3, we typically have two independent points, the binary input for the status information and a binary output for the control point where we do the control. With regard to event-based communication, again, both protocols support event-based communication. In DMP3, this is called report by exception, which is supported through the use of unsolicited messaging and event class polling. In 61850, the similar functionality is called a reporting. Similar to the unsolicited communication, once reporting is enabled, the server will send events without a client request. DMP organizes the event data into class 1, 2, and 3. In IC61850, the event data is organized through the concept of a data set, which defines the data scope. And as well, we have a control block that defines various constraints for the event-driven communication in 61850. Most characteristics of event-based communication in DMP3 are predefined and documented through the DMP3 XML device profile. In IC61850, those are attributes of the report control block. And in some cases, they can be changed even in a live system. Both protocols support general interrogation. Both support cyclic reporting. Both protocols support buffering of events. IC61850 has, as well, an unbuffered reporting model, while DMP3 has a concept of reporting only the most recent change, where we don't have exactly a similar concept in 61850. There, we use more the unbuffered reporting, where we don't need the events. OK, now let's have a closer look on how that reporting works in IC61850. And in the first step, I would like to introduce the IC61850.
 250 data set. A data set is basically can be seen as a reference to a couple of data points or data objects that we find in our system. So in this example, we created just one data set where we can find all the positions from the different switches we find in that reader. Then we now want to configure a report where whenever one of these switches changes, we will get a message with an updated position. So whenever we want to do that, we now have our data set. And now we need what we call a report control block. And the report control block on one side points to our data set. So with that, we know the behavior described in this report control block for event-based communication is applied to all these data objects. And then we also can configure additional information of the behavior mainly, like the trigger options, but what conditions should we send a new event, as well as the identifier of the message. And based on that, we will then have report messages going out. Now, how does the mechanism look? So we have our server and our client. The client may set up, and he has to enable the report. And once the report is enabled, and whenever we have an event happening, a report message will be sent from the server to the client. And this continues with each event, a new report will be sent. Now, what is an event in IC61850? An event can be a data change. In that case, the data that has changed will be packed into the message, and then send it as a report. But we also consider integrity period expired. So what we are using for cyclic reporting, that's another kind of an event. Also, general interrogation, if we issue a general interrogation, then we also will send the report. While for data change, only the information that has really changed will be put in the message. Integrity period and general interrogation will always report the complete data set that is associated. As already mentioned, we have both buffered and unbuffered reports. With buffered reports, events are buffered for retransmission on request following a loss of communication. With unbuffered reports, events may be lost during communication interruption. This retransmission, when we reconnect, the client can request to continue sending the events that he didn't receive, but he can also tell to purge the buffer and just send new events. So much about reporting. Now, as I mentioned before, IC61850 added two protocols to the MMS, which is the vertical communication. And one of this is the GOOS. GOOS basically is intended as communication between devices to implement protection and control schemes and more or less replace physical wires that we had in the classical substation. We had binary outputs where we routed signals. Then we went on binary inputs from the other delays, as an example, blocking signals. Now, the communication mechanism called GOOS, or generic object-oriented system event, can basically replace these wires. How does this work?
 So again, let's assume we have our data set with all the positions. At a certain moment, these positions have specific value, we call that a state, and based on that, the message is sent over the network with the state in, and we also have a state number that changes whenever something changes. A subscriber that is listening is not necessarily required to receive all the data, he just, in this case, needs these two information, so when he receives the message, he can check these values, store them locally, and with that he knows now more or less the value there. Now if something changes, so let's assume that data changes from off to on, that means we now have a new state, which will increase the state number here, and the new value will be in, and again, that new message is published out, the subscriber notices that there is a new state number, and as a consequence, he can recheck the information he needs, he will identify that this has still not changed, but then he can look and he will see that this value has changed, so he always needs to read the values he is interested in, and then he can identify what has changed. Now that's the basic concept. In order to make that reliable about our Ethernet technology, we introduce certain repetitions. As a starting point, when we don't have a change of the state, instead of just sending that message once, that message will be repeated with a predetermined time, which is typically configurable, until the next event. So as an example, you will have every one second a repetition of the same frame. This can be used by the subscriber to detect that the publisher is still alive and still publishing messages, so the information he has is still valid. Now next, when we have an event, that means when an information changes, the publisher will immediately send out a new message, independent of this timing, will immediately send a new message, and then it will repeat the messages with a much higher frequency in order to care for communication loss and still fulfilling timing requirements. At once, having repeated it fast, there's a process in that it slows down, and after a while, we will get back to the normal maximum repetition time that we have in the steady state when nothing is going on. So with that, on one side, we ensure a fast reaction when something changes, but then we also ensure the supervision of the publisher without using too much bandwidth on the network. The content for Goose is configured similar to the reporting. We have as well the data set, instead of having a report control block, we have a Goose control block, but besides that, everything is more or less the same. Now let's have a look at the sampled values. Sampled values are typically used when we introduce what we mostly call merging units, which is an analog-digit conversion directly at the source where we have our current and voltage sensors. In that case, instead of having an analog signal going to the devices that need to process this, and each of these devices having an analog-digital conversion, we only need one analog-digital conversion, and then we send...
 out a stream of sampled values over the network. The mechanism behind works as follows. We have at a certain moment we sample the values and they get an identifier. So when we have different sources they get an identifier when they have been sampled and then they can get delayed over the communication because there may be some jitter in the communication delay. The subscriber on his side just needs to wait until he gets a set of samples with the same number and then he knows that those have been taken at the same time so they are correlated to each other and he can use them for his algorithmic processing. Of course the sampling here needs to be time synchronized. That's why we know then that those have all been taken at the same time. Okay, so much about the protocol level DMP3 and IC61850. Now let's compare them at more the functional level. At functional level DMP3 defines various types for the data points. 61850 has a similar concept but there we call it common data classes. Common data classes in 61850 however do not only include operational process information like typically in the SCADA protocols but here we also include other attributes like the scale factor, dead bands, control output pulse configuration and so on. Additionally some of our common data classes can be hierarchical structures. As an example there is one CDC defined that combines all the three phases phase measured values into one type. As already mentioned earlier in 61850 we call the data points data object and that's one of the differences. In 61850 we have standardized the names of these data objects and we have given semantic meanings to the data objects. Data objects that belong to the same functional element they are grouped in logical nodes. So the next slide tries to compare that so what we see above is a table that shows an extract from a DMP3 device. So somewhere we need to know that in this device the point and in this case it's binary input point with the number six is associated to the breaker open compact point with the index number seven is associated to the breaker closed compact. In IC61850 this same information will be defined in a standardized data object called POS and this will be placed in a standardized logical node called XCBR. So we know when we have to search for a position that we always have to look a position of a breaker into the XCBR we need to look for the data object POS for position and there the status value STVAL will indicate the position. In the second part of this webinar we will see that in a live demonstration given by Jackson. So when we look at logical nodes again as mentioned in 61850 the logical node is one of the key elements of the data model and it's a container for function related data. We have here an example of a switch we see different data objects the type which is a reference to the common data class we also see indications that information some of them is mandatory other ones is optional. We also support the position the possibility to extend the the model with
 vendor or user-specific data, but we will not have time to go in all these details. Data objects and data attributes, as mentioned, data objects are based on common data classes that are as well standardized, in this case, a double point control. In a common data class, we have attributes like the real operational attributes, status value, quality, timestamp. But we might also find configuration attributes like pulse configuration. We can configure the output pulse. The control model, if it's a controllable point, are we supporting direct control or control with select before operate? If you have select before operate, configuration parameters like timeout. So all this information we will find in the same data object. Of course, typically, for reporting of the events, we are limiting it to the operational part, but the other information is as well available. We also support substitution or blocking of information. OK, the last point I would like to mention is the device profile or the corresponding element in 6.1.850. So a DNP3 device supports a device profile which describes capabilities of a device, including a point list. In IC6.1.850, in a similar way, we require that the device provides a PIX, Protocol Implementation Conformance Statement, and MIX, Model Implementation Conformance Statement. So PIX corresponds to the capabilities, the MIX to the point list, which is, in this case, the 6.1.850 data model. DNP3 device profile exists as an XML device profile. In principle, they are required to be provided in XML. In a similar way, in IC6.1.850, as part of our System Configuration Language, or SCL, we support the ICD file, which is called ID Device Capability Description. How does this look? An ICD file, you basically see here. It's an extract, and that will be a topic that we discuss further in the second webinar of this series. But we have a service section, which provides the capabilities. And then we have a data model section, which provides our information model, the data model. And we have a template section, where we have the data types that we are using in that device. Now, before I hand over to Jackson, I would like to summarize again and explain a little bit how Goose is replacing wires. So when we talk about an application, we may have in 6.1, we may have multiple devices. The yellow boxes here are devices. In the devices, we have data, which is grouped in logical nodes in 6.1.8.15. And for some applications, some of this information is required in other devices. So you may have, as an example, a signal from here required to be used for this functionality in this other device. So we may have all kinds of signals flowing between the devices. In a conventional world, these signals that go out here would be mapped on physical outputs. They would be packed in a mass message, and then would then go on physical inputs into the devices that have to use these signals. In 6.1.8.50, we basically.
 create messages behind. We assume virtual outputs instead of physical outputs. We pack them in a Goose message, and we send over the Goose message. And then instead of having physical inputs, we have now virtual inputs that we wire to our applications. And with that, I think before we go to the demo, Beth, we have some question and answer session, correct? Yep, we do have a couple of questions that have been coming in and we've been answering online. The one question I would like to put out there for you, Christoph, let me go ahead and read it off. 621A50 appears to be an IoT application, hub submodel. Is it using UDP under the hood or TCP? The second part to the question, if UDP, how is delivery guaranteed? OK, for 621A50, for the MMS part, so the client-server communication, we are using TCP IP beyond. Yes, that's correct. So it's the MMS, the manufacturing message system that is on top of TCP IP, and then on Ethernet. So there we are using TCP. For Goose, we are directly using Ethernet communication. So it's more or less using specific Ether types. So we are not using neither IP nor UDP there. There is a mechanism, like a routable Goose, where we will use UDP. But for Goose, the delivery mechanism, the delivery guarantee is provided by the repetition, as we have explained. But yes, for the virtual communication, which is MMS-based, the TCP IP guarantees the delivery. I hope that did answer the question. OK, thanks, Christoph. Since we are a little tight on time, I'm going to go ahead and hand it over to Jackson to start his part of the webinar. Jackson, I'm going to make you presenter now. All right. OK, you should have the screen. All right, I just want to make sure you can see my correct screen. There we go. All right, sorry about that. All right, so thank you, Beth, for the introduction. And thank you, Christoph, for your presentation outlining some of the similarities and differences between DNP3 and 61850, as well as just kind of a basic overview of what the two protocols are. So now moving into the second part of our webinar, we'll actually be doing a kind of live demonstration using a couple of our different tools. So the first tool that you see up on my screen now is DTM, or Distributed Test Manager. DTM is our flagship multi-protocol test and simulation tool. And today, I'm going to be using it to simulate devices in two different protocols, DNP3 and 61850, that you see over here on the left in our workspace tree. Now, to create this DNP master that you see up here at the top and the DNP outstation that you see right here below, I just right-click, Manage, Add. And then you can see the different protocols that are supported in the tool, which include DNP3, ICCP, 6870, 101, and 104, as well as 61850 and Modbus. So I went ahead and did this at a time. I created, as I mentioned, a DNP outstation and master. And that's what you see over here on the right, the data map for a DNP3 outstation. As Christophe mentioned, a DNP3 data map is made up of a couple different parts. So first, we're going to have our data type. In this case, I have some binary outputs. I'm sorry, inputs, double-bit inputs, binary output statuses, as well as analog inputs. In the next column over, you'll see the index. So the index combined with the data type.
 type make up the address of an individual data point. And at that address, we'll find a value, a quality, and a timestamp. Value being the payload, and then quality and timestamp being associated as metadata with that payload. And then finally, the last part of a data map is the ever-important description. And this is what we use as humans to know what this data point actually does. In this case, I've made descriptions and data points that might be used in a Bay controller that you might find in a substation, controlling maybe a breaker and two switches. In that vein, I've named this first point enable open breaker QA1. So for a breaker with the name of QA1, this would enable the open. That might be part of an interlocking scheme. And below that, an enable close. Down below in my double bits, I have the positions. So again, double bits, these could be on, off, intermediate, or indeterminate. And then operates to actually open these breakers. And then finally, down here at the bottom, I've created a couple of analog inputs that might be my phase voltages or currents. Now, the last thing I want to point out before we take a look over at a DNP3 master is class. So as Christoph mentioned, DNP3 has the option to make classes for or to assign data to different classes. So let me expand out my DNP outstation and just edit that database. And you can see I've done this ahead of time, but I went ahead and assigned my analog inputs up here at the top to be part of class 2, whereas my binary inputs and other objects I assigned to be class 1. And again, that's visible over here on the pane in the right. Again, these are just different buckets or categories that I can assign my data to. But in order to see how that's really useful, I'll need to swap over to my DNP3 master. All right, the first thing you'll notice is that it looks pretty much the same. And that is by design. Your database that exists on your DNP3 outstation is going to match that one that is on your master. But the way that you create these two databases is fairly different. So first, I'm going to drag them down side by side so that you can see them next to each other. So on an outstation, to create this data map, it's either done by the manufacturer or vendor when the device is made, and then provided to the customer in the form of maybe a PDF or XML DNP device profile or possibly a spreadsheet. But it essentially just is what it is. It's on the device. On some devices, you might have a user configurable DNP3 map, in which case it's up to the user to provide that documentation to whoever is going to be configuring the master. On the master, we must take that documentation, be it a PDF, Excel sheet, or even a DNP XML device profile, which is the machine-readable method, and then import that or create the corresponding points on the master. And I've gone ahead and done that ahead of time. So now I've got kind of a mirror of the database, one that exists on my outstation and one that exists on my master. But if I change a value, let's say I change this binary input from off to on, I don't see that change reflected in my master, which brings me to the next step in our presentation, which is talking about the different tools that we have in our DNP3 toolbox to transfer data from an outstation to a master. So the first of these is the integrity data poll. So an integrity data poll is initiated by the master and it's a request that goes out to the outstation and says, give me everything you have. So every data point, every value, every quality, and every timestamp for.
 every data point in the database will be sent. This is considered a fairly heavy weight on the wire request because, as I mentioned, everything is sent. But I'll go ahead and just execute that command. And as I do, sure enough, I see that my binary input up here at the top on my master has updated. It went out, it sent a request to my outstation for all data, and then all data was sent back. If we had a little bit more time today, we could look at this in a protocol analyzer and see that not just this was included, but values, quality, and timestamp for every other data point was included as well. And this actually brings me to our next tool, our DNP3 toolbox, which is RBE, or Report By Exception. So this is still a data poll. And by that, I mean it is still initiated by the master. But in this case, the master asks for only data that has changed, so since the last time a data poll occurred. Additionally, you can specify what class you want that for. As I mentioned earlier, classes are three different buckets or groups that we can assign our data to. In this case, I've configured this report by exception data poll to only ask for data that's part of class two events. As I mentioned, I assigned my analog inputs down here at the bottom, which contain my voltages and currents, to class two. So I'll just put in some values here. And then I'll change this top one back to off and this one below it back to on. Now, as I execute this report by exception data poll on my master, it'll go out, ask for only the changes to class two. What that means is we'll see the update to the analog inputs over here on my master. But I will not see the updates that I made to my binary inputs, as they're part of class one. So I'll go ahead and execute that command. And sure enough, I can see over here my analog inputs came through, the 1, 1, 2, 2, and 3, 3 that I typed in on my outstation, whereas my binary inputs have not yet changed. And finally, the last EMP3 tool in our toolbox that I want to talk about is unsolicited messages. So unsolicited messages, as opposed to the two polling events that we talked about earlier, integrity and report by exception, unsolicited are initiated by the outstation. So once enabled, the outstation will send events automatically up to the master as they occur. In this case, I've set a five-second buffer window. And that will begin counting down as the first event occurs. So once I make a change, let's say I change this binary input to 1, it will begin a five-second countdown. And any other changes that I make within that five-second window will be packaged up as the timer expires and sent up to the master. So this is adjustable and can be configured by the user. But the idea is that it's a little bit less traffic on the wire. It's a little bit more lightweight. And you can package things together in that method. So I'll go ahead and enable unsolicited messages. Immediately, we see that the changes that I made before to my binary inputs have already come through. And then I'll just go ahead and make some more changes. So let's say, and if I do this quick within that five-second window, then we'll see they all three come in together as they update. And again, unsolicited, I can just do this up at the top and change these guys. And then in just a moment, we'll see, sure enough, a message was sent from my outstation up to my master reflecting those events or changes which occurred. With that, I'm going to go ahead and switch over to 61.850 and point out some of the changes that I made. So I'm going to go ahead and switch over to 61.850. And I'm going to go ahead and switch over to 61.850.
 of the similarities and differences that we see there. So, as Christophe mentioned, 61850 is kind of three communication protocols rolled into one, as well as a whole engineering process, which will be the topic of our next webinar. And at the center of these three different communication protocols and engineering process is the data model, which is what we see here. Right now, it's just one row, and that's because the 61850 data model is hierarchical. It's one of the signature features of the protocol. So, at this top level, we see where it says Bay Controller 1. So, this is my physical device, or IED. As I expand that out, I see three different logical devices at the next tier. So, if we can imagine a protective relay that you might have in a substation, that is represented by this top rung or trunk. But as we go down, the first three branches are the logical devices in that. And in this case, they've been used to break up the different functions, logically, that this device might perform. So, those would be control, measurement, and protection. As I expand out one of these logical devices, I'll see different logical nodes. So, this is what Christophe mentioned earlier in his presentation. These logical nodes are different data objects, or collections of data objects, that are associated with different functions. For example, QA1 XCBR. If you're not familiar with 61850, this might look like alphabet soup. But once you get to know the protocol, it becomes second nature. So, QA1, that might sound familiar because that's what we named our breaker. And then XCBR, X indicates interface in 61850 parlance. And then CBR stands for circuit breaker. So, this is our interface to circuit breaker with the name of QA1. Contained within that are several data objects. The one that I want to highlight is PAUSE, which is short for position. And as I expand that out, I can see an ST vowel. So, this, as a double bit, has four possible states, intermediate, off, on, and bad. So, if this is sounding familiar, I want to drop down our DNP3 device that we looked at earlier. This is roughly analogous to our breaker position, QA1, that we saw in DNP3. But instead of having this flat data map, we have it in this hierarchical predefined data model. So, while that breaker position corresponds to that position ST vowel, I do want to point out our analog values would also be contained in here. If I navigate down to my measurement logical device and open up my MMXU or measurement logical node, I'll see that I have phase voltage and amperage. If I expand these out, I can see phase A, B, and C for both current and voltage. And again, expanding that out a couple times, eventually I can get all the way down to the magnitude of my phase A current. So, this value would be equivalent to my phase A current they had over here in my DNP3 map. Now, one thing that I do want to point out before we move on, we talked about in DNP3 how metadata is associated with a payload. Effectively, this on value that we have for the binary input has a quality and timestamp that are associated with it. As Christophe mentioned, things are a little bit different in 61850. If I go back to that position value that we looked at earlier for our breaker, we'll see that we have an ST vowel, and then below it we have Q, which stands for quality, and then a T, which is our timestamp. Now, from a data object level, which is what we have here at position, these three are all three.
 three together. But I could address just one of these data attributes, for example, the pause.sdval or the position of the breaker individually, and send that or read that in a message and not necessarily receive a quality and a timestamp. A small distinction, but we'll see where that comes into play here in just a moment. Now, before I go over to my 61850 client, I do want to point out how we're going to categorize our data that's going to be sent. So for DNP3, we had classes, and we had a limit of three classes that we could place our data into. For 61850, the equivalent concept might be called report control blocks or goose control blocks, as Christophe mentioned. So to show those, I'll expand out my logical node, lln0, and then we'll open up the report control section. Within that, I see brcbstat, so that's buffered report control block stat, and mesh for measurement. And these names are ones that we gave just to kind of categorize what or clarify what sort of data is in that report control block. Within that report control block is a data set. Again, you can have as many data sets as you want and as many report control blocks as you want, the only limiting factor being the device that you're actually working with. There's no limit per the standard. But as I locate that data set, I can see the different data objects, in this case, that have been placed within that data set. Roughly equivalent to assigning all of these things to maybe class one, although, again, you could have as many data sets as you wanted. And in just a moment, we'll get to see what this report looks like in action. But for now, just know that if any of these items changed per the trigger options that we have set, then it would cause a report to be sent out to a client. In much the same way, we have our goose control blocks seen up here, which have data sets as well. As I locate those data sets, I can see the data attributes which make up my data set of my goose control block. As we started out by saying, there's really three protocols in one that make up 61 and 50. And these are two of them, the first being report control blocks that we looked at a moment ago, and the second being goose. But to see these in action, I'll need to open up a 61.850 client. So let me go ahead and do that. And I'll drag that down just so we can see it side by side. So just like in DNP3, where we see kind of a mirror image between our master and outstation, we see the same mirror image in our 61.850 client and our 61.850 server. So the client over here on the right has the same data model as I have configured it for my server. In this case, instead of using a spreadsheet or a PDF, I did use the SEL file or system configuration language in order to create my client. And we'll talk a little bit more about SEL during our next webinar on 61.850 engineering process. But as I expand down my control logical node into my XCBR and take a look at that same position value, then I'll pull it up on my server as well. I want to show that if I make a change on my server, by default, it will not come through up to my client. Even though we saw that this position value was part of a data set that was part of a report control block, I must first enable that report control block in order to receive the value. I could do a one-time read just by right-click and hit Read. But if I want to receive that value automatically, maybe kind of similar to unsolicited messages that we saw in.
 in DNP3, then I would come up here to my LLN0, expand the Report Control Block section, and then just right-click and enable the Report Control Block. So now I have enabled it. I've reserved it for myself as a client. And any time any of those values change that are in the data set associated with this Report Control Block, I'll see it over here on my server. I'm sorry, on my client. Expanding back out to that position value, now if I make a change on my server, I can just change it back to on, and immediately we'll see that change reflected up here on my client. So as I mentioned at the beginning, we're going to be looking at two tools. Next, I'm going to be pulling up TestSuite Pro. And that's so that we can take a closer look at what's happening under the hood in these Report Control Blocks. And then after that, we'll take a look at another one of the protocols that makes up 61850, and that's Goose Messaging. And TestSuite Pro is what you see up on my screen now. So while DTM is multi-protocol and focused on simulation, TestSuite Pro is more of a testing focus, visualization, and analysis of protocols. And it's specific to 61850. So on this first system status screen, you'll see where I've connected to that 61850 device, that Bay controller. And let me just pull DTM over here. As I move down into the advanced client, we'll see a very similar view to what we saw in DTM. These are the three different logical devices which make up my IED. And then as I drill down further, I can see the different logical nodes that are contained within it. If I wanted to, I could go ahead and execute a read on maybe this position value, but that's really not the magic of 61850. It's really in the report control box. So I want to enable this first report control block. Before I do that, I need to disable it in DTM on the other client that I have. So bear with me as I do that. All right, now I can enable it over here. And if I come back down to my XCBR position value that I looked at before, currently set to on, now I'm making changes off screen over in DTM, but I'm just going to change that value from on to off. And just like we saw before, it's going to come through here into my test tool. But as I promised, I want to take a little bit closer look at what's actually coming across the wire. In order to do that, I'll be using one of the tools within TestView Pro named Report Viewer. So let me start out by beginning a capture, and then I'll go ahead and make a change again over in DTM off screen, and that will trigger a report to be sent, which we see here on the right. So this is a report that just came in, and the first thing I want to point out is the reason for the report. It's a data change, and we'll take a closer look in a minute at what the different reasons or options are for why a report could be sent. But in this case, it's because a value changed, and it was the on value, or off changing to on for the breaker position. I'll go ahead and issue a second change by changing that breaker back to off or open, and we'll see that a second message comes in. I can select both of them, and it'll highlight the changes that occurred. In this case, it was the breaker changing from on to off, and then the timestamp updated as well. Now, one thing that I want to point out is that in this data change event for an MMS report, only the data object which changed is sent across the wire. Not all of the data.
 that makes up the data set. However, there are other methods or triggers that you can have for a MMS report. For example, if I expand out my trigger options field on my report, we'll see that I have Integrity Poll. Now Integrity Poll, the name sounds familiar, we spoke about it earlier for DNP3. Integrity Poll requests all data on DNP3 and requests the data at periodic intervals. It's typically sent from the master to the outstation. Similar in 61850, although instead of being a request from a client to a server, it's actually configured on the server and then the server will periodically send that data out. But just like an Integrity Poll in DNP3, in 61850, all data in the data set will be sent at that interval, not just the data that's changed. So I have that trigger option set up and then down here I see this INTGPD, that stands for Integrity Period, and so I'll set that to 10,000. This is a millisecond's value, so 10,000 will represent 10 seconds. And then I'll go ahead and enable that report. Immediately you'll see Integrity Data Polls begin to come in to our report viewer. As I click them, you'll notice that there's a lot more data present. This is because, as I mentioned, an Integrity Poll sends all data every time it is initiated. And finally, before I turn it back over to Beth for Q&A, I want to take a look at Goose Messaging. So I know we keep harping on it, but I think of 61850 as three protocols in one, the first one that we just looked at being report control blocks, and the second one being Goose Messaging. So much like report control blocks, Goose Messages rely on data sets, as I mentioned, and if I locate the data set for this report control block, for this Goose control block, excuse me, I can see that the same breaker position value is contained in the data set. This means that if I make a change to this value and I've subscribed to this Goose control block, then I'll receive that message. And I'll extremely receive it very quickly. To reiterate what Christophe said earlier and one of the questions that we received during the break, Goose is the high-speed messaging system that's meant to replace wires, and it typically goes from server to server, whereas MMS or report control blocks is still speedy but not as fast, not in the millisecond range, and is typically used for vertical communications, which go from a server ID, say a protective relay, up to a SCADA system. And with that, I'm going to go ahead and subscribe to this Goose control block, and then I'll switch over to my Goose viewer tool in the pane over to the right and click Start to begin recording. So as Christophe mentioned, there's a sequence number and state number. The state number increments every time something in that data set changes, and then sequent number refers to the number of retransmissions that occur. So this is the initial retransmission that I captured when I began recording, and I can see the current values of all of the data attributes within my Goose control block. But if I come back over off-screen to TTM and change that value from off back to on, immediately I'll see a new Goose message come in with a reset sequence number back to zero, indicating that this is not a retransmission but an original transmission, and then the state number will increment as well as a result of the change of the value. And if I highlight these two messages and then scroll over to the right, I can see that highlighted in our tool is what changed between the two, and that's going to be this position value that I just swapped over in TTM.
 And with that, I think we're running out of time, so I'm going to go ahead and turn it back over to Beth, who's going to handle our Q&A session. Thank you. Thanks, Jackson. Yes, we are coming up to the hour, so I think what I'd like to do is I'm going to take the screen back from you and just go over some final points, and then we can switch over to the remainder of the Q&A. As I mentioned earlier, anyone that would like to stay on longer, we're happy to stay on and continue to answer questions, so please feel free to continue to submit them. With that, just to wrap up, so the webinar will be posted to our website, trianglemicroorgs.com, in a couple of days. You'll see it right there on the homepage. A follow-up email will also be sent out that will include a link to this recording. And when you do leave the webinar, you will be prompted to answer a survey. It's only about five questions, takes about two minutes. We really appreciate any feedback that you have on what you've seen here today and really any thoughts. You can even, you know, continue to submit questions via that survey, and we will respond to those. We mentioned that this is a three-part series. Today was part one. Part two is IEC 62050 SCL and the engineering process. That will be hosted on May 4th at 10 a.m. Eastern time. And then part three, strategies for integration of DMP3 and IEC 62050. That will be hosted on May 24th at 10 a.m. Eastern time. So again, you can go to our website under training tabs and see, get the links to sign up for each one of those webinars today. In fact, I've noticed that a lot of folks have signed up for this one have already signed up for part two and part three. That's great. Get it on your calendar now. If you haven't, please go out there and go ahead and sign up. And then of course, if you have any questions, you can always just email sales at trianglemicroworks.com. So with that, I'm going to switch it back over to Q&A. We did have a lot of questions come in, in the last 15, 20 minutes, and a lot of them have already been answered online, but I'll go ahead and just kind of take it from the top for the benefit of the whole audience. So I'll throw this out to you, Christophe. Is SLAVE mastered the same as client and server? So the question is for me, should I, they have different answers being provided. I mean, my short answer was when we talk about DMP3 over Ethernet, we can basically assume a similar mechanism. So in that case, SLAVE, the master outstation that we have in DMP are really comparable to SLAVE and to client server that we have in 6.1 and 6.50 as a short answer. Master SLAVE is really coming from the time when we had serial links, where there was also a need to have a media access control. So not just every device could start talking, but with an Ethernet network, we don't really need that. So that's why master SLAVE can be compared with client server. Okay. Christophe, I'm going to keep the focus on you with this next question as well. I see that a couple of folks did answer this online, but you did as well. As far as I know, IEC 6.1.850 comprises not only protocols, MMS, Goose, and sample values, but also common routines for exchanging interface files, ICD, CIDs, et cetera. Does DMP3 provide similar functionality? As it has been answered, I don't think, I mean, DMP3 supports the XML device profile, which corresponds to an ICD file in 6.1.850.
 and we will talk about in the webinar too in more detail about the engineering process. But in DMP we don't have behind an engineering process defined like it's very like in 62050 very defined system tools that can impose this ICD file and use it then to configure the whole system. That's not that's not available in DMP. But the whole engineering process we will talk next webinar. Okay next question there is a mechanism how DMP3 slave devices can get information for other DMP3 devices not hardwired just like goose device devices interchange data excuse me data. So I guess the question is is there a mechanism on how DMP slave devices can get information? Should I answer or Jackson? I can answer. I mean there is no standard mechanisms in DMP3 to exchange directly information between devices as it has been stated in one of the responses. We have systems that where users are using goose together with DMP3 to perform that functionality. In other cases it's still wired and we will discuss this a little bit in the webinar three the possibility to use goose in parallel with DMP3. Okay so next question I'm not sure if this is for Christoph or Jackson so either one of you can take this. The databases these programs are using must be different from the one the IEDs contain the binary information. Is that correct? I'm not sure that I understand totally but in general I would say the database that we're using in DTM to represent a device could absolutely be identical to what you have in a physical device. Let's say you had the documentation that defined the database for a real DMP3 device be that a spreadsheet PDF or a XML device profile you could create a mirror image of that or a simulation a copy whatever you want to say in using DTM. It's just a matter of addresses point types and then really descriptions are secondary and there's some other settings parameters that you have access to as well in DTM. I showed class but there's many others as well. I think there is a follow-on question on that that just popped in. Mm-hmm I did just see that yeah I can go ahead and read that one. What about the way the IEDs store? Further down please. Oh further down? Anyway my actual question is. Okay my actual question is how would you carry out an integration of the IEC 62050 protocol between two substations one more modern with the protocol already integrated between relays and PLCs and one a bit older using less older methods of communication and control? So that's actually a large topic of webinar 3. Christophe if you have more to add then then please do but there's certainly methods of upgrading your existing substation to 61850 or integrating in a substation which uses older protocols for example DNP 3 or 101 104 or something like that. You could use data concentrators or you could use some of the engineering processes that we'll be talking about during webinar number 3. Christophe do you have anything to add on to that? Yeah and I'm depending on how to understand the question. I mean let's assume
 One substation has still, as he says, less modern methods of communication. Yeah, you could have a gateway somewhere that goes from the whatever communication you have to 61850. And then, yeah. We do provide a SCADA data gateway. You can see more information about that on our website, but that does convert between all of these different protocols that we've been discussing today and many more. Okay, thanks. Next question. What is the motivation behind IEC 61850 and how it is enabling a digital substation in a way DMP3 can not? I'll give that one to you, Christoph. I think one of the, okay, one of the key elements is like, it has been answered already, the engineering process that we will discuss in the next webinar. That's one key feature. And the other features that really make it, well, it always depends on what you call a digital substation, but commonly we understand digital substations where we have merging units that do the directed analog to digital conversion. And again, for that, we have the sampled value service that is something that we cannot directly use in DMP. But in principle, as we mentioned before, you could do a combination of using DMP3 with schools and sampled values would probably also allow you to build a digital substation. But I think the key differentiator is really also the engineering parts that we will discuss in webinar two. Okay, next question. Can I select more than one data attribute on one data point in 61850? Short answer is yes. There are communication service that we can, we didn't enter in all the details, but we have a classification of data attributes as with function constraints, like function constraints for status attributes, for configuration attributes. And one way to access more than one data attribute is that I can select all the, or access all the attributes of function constraint ST as an example, status information from a data object position. I have this possibility. I can also, from an addressing perspective, select even multiple data points, multiple data objects by addressing the logical node. Okay, next question. Christophe, this one I think is for you. What's the most used for integration nowadays, DMP3 or IEC 61850 in substations? Sorry, can you repeat it? Sure. What is most used for integration these days, DMP3 or 61850 in substations? Well, it depends a little bit on the areas. In some countries, they are using almost only 61850. I mean, in Europe, in many places. Okay, we anyway didn't use DMP3 in Europe. There was the corresponding thing 6875-101-104. That was very popular. But in many countries, the utility are now completely applying 61850 there. In North America, DMP is still much more used, but 61850 usage is growing. And mainly in the context of digital substations, where they include as well, sampled values and these communications. Okay, next question. And Jackson, you've already touched on this. So maybe we'll just reiterate some of the points you already mentioned.
 take this one. Is it possible to convert protocols between DMP3 and 6.2.850? Yes, absolutely. There are several different ways to do that. You can, using a DMP XML device profile, define those mappings. We'll be talking a little bit about that during webinar three, but you could also just simply do it in a gateway or protocol converter, such as our SCADA data gateway. You would need an ICD file, an SCL file, which describes your 6.2.850 device, but the answer is yes, absolutely. You can do that in both directions, from DMP3 to 6.2.850 or vice versa. Thanks. Okay, next question, Christoph. I believe this one is for you. For MMS part, how many subscribers can connect with the publisher at one time? Unlimited subscribers can receive the message from the publisher. For MMS, it's one-to-one. Sorry, I missed the first part of the question, because I was typing in an answer to another question. MMS is not a publisher subscriber. MMS is a client-server, and there is always an association between one client and one server, yes. You could create multiple report control blocks, which share the same data set, in which case, you could have multiple. I suppose the limit is defined by the specific device that you're working with, in terms of the number of report control blocks that it would support. Okay, well, I think we have covered most of the questions, if they have not already been answered online. So, with that, I want to thank everyone for spending the time with us today. Again, I encourage everyone to go to our website under the training tab and sign up for part two and part three to get those on your calendar, and I appreciate you staying on even longer, past the hour, so we can wrap up with the Q&A. Once you sign off here, you'll get prompted with the survey. We really appreciate if you can fill that out and give us your feedback, and we look forward to seeing you next time. Thank you, everyone.


Video ID: SfuZQaSKsDI
Now let's take a look at the application layer. The DNP3 application layer provides features required for data transfer and processing, control operations, and retrieval of data. The data transfer and processing features include time synchronization, time-stamped events, the ability to freeze and clear counters, and data groups and classes. DNP3 supports 2-pass or select-before-operate control operations. Data retrieval can be accomplished via polling, either for all data, specific types of data, or data changes, as well as unsolicited responses. An application layer message consists of the application layer header, possibly followed by one or more object data header and data pairs. The object header and data pairs may occur multiple times in a single message. The application layer header varies depending on whether the message is a request or response. In DNP3, the master always sends requests, and the outstation always sends responses. Note that an unsolicited message from the outstation is called an unsolicited response. Both the request and response header contain an application control field, which varies slightly depending on whether the message is a request or a response. The request and response headers also contain a function code, which we will cover in the next few slides. The response header also includes internal indication, or IIN bits, which will also be covered in future slides. The application layer header includes first and final bits, and a sequence number for reassembling fragments into a single message. In a response, the outstation may set the confirm bit to request confirmation of the message, and in particular, the events within the message. It may also set the unsolicited bit to indicate this is an unsolicited response. The unsolicited bit can be set in a request header when confirming an unsolicited response. The confirm bit should never be set in the request header. This table outlines function codes used for request messages. Supported function codes include confirm, read, write, control operations, freeze operations, initialize data and initialize application, start and stop application, save configuration, enable or disable unsolicited responses, assign class, delay measurement and record current time, file transfer operations, activate configuration, and authentication operations. Note that unlike some protocols, the write function code is generally not used for control operations in DNP. A separate set of function codes are used for control operations. The write function code is primarily used to clear the restart indication from an outstation or to write the time for an outstation. It can also be used to download configuration or files to an outstation. Function codes for the control operations, freeze operations, file transfer, and authentication blocks will be discussed in the next few slides. DNP3 classes were discussed earlier. Click on assign class to go back and review those slides if desired. Delay measurement and record current time are discussed in future slides. To go directly to these slides, click on delay measurement or record current time in the table. DNP supports traditional one-pass control operations as well as more secure two-pass operations. The two-pass operations require two control commands. The master first sends a select, function code 3. Once the select is confirmed by the outstation, the master can send the operate, function code 4. For one-pass operations, the master may send a direct operate request, function code 5, or a direct operate no response request, function code 6. Control operations will be covered in more detail in future slides. To jump directly to those slides, click on control operations in the title of this slide. DNP also supports frozen counters. Frozen counters report a counter value from a given...
 instant in time. When the counters are frozen, the current point value is copied into a separate freeze or holding buffer or register, while the running counter continues to accumulate new values. The freeze operation function codes allow the master station to indicate when the counter should be frozen. Function code 7 instructs the outstation to freeze the counter upon receipt of the command. Function code 8 is similar, but also instructs the outstation not to send a response. This command is useful for broadcast requests. Function codes 9 and 10, or 9 and A in hex, freeze the counter and simultaneously set the running counter back to zero. Function code 10 indicates the outstation should not send a response. Once again, this is useful for broadcast operations. Function codes 11 and 12, or B and C in hex, instruct the outstation to freeze the specified data points at the specified time or time interval. Use of this function code is beyond subset level 4, and it is rarely implemented. However, it is permissible for an outstation to be programmed to freeze the counters at specified times. More details on frozen counter operations will be covered in future slides. If you wish, you may click on freeze operations in the title bar to jump directly to those slides. Function codes 25 to 30, or 19 to 1E in hex, are used for file transfer operations. Available file operations include open file, close file, delete file, get file info, authenticate file, and abort file. Note that file transfer operations are always initiated by the master station. Function codes 32 to 34, or 20 to 22 in hex, are used for DNP secure authentication. These function codes include authentication request, authentication reply, and authentication error, no reply. DNP security is covered in more detail in future slides. If you wish to go directly to that section, click on secure authentication in the title bar. Response function codes are limited to response, unsolicited response, and authentication functions. Function codes 131 and 132, or 83 and 84 hex, are used for DNP secure authentication. These function codes include authentication challenge and unsolicited authentication challenge. DNP security is covered in more detail in future slides. To go directly to that section now, click on secure authentication in the title bar. The application response header includes internal indication, or IIN bits. These bits indicate conditions at the outstation. The IIN bits are reported in two octets, usually referred to as IIN1 and IIN2. The bad function, object unknown, and parameter error bits indicate errors while processing the message. The buffer overflow bit indicates that the outstation's event buffer has overflowed, and some event data may have been lost. Setting this bit will usually trigger an integrity pull from the master in order to resynchronize its database. This bit should not be set based on any other buffers in the outstation. The IIN bits also provide indications that the requested function is already executing, or that the outstation has detected a bad configuration, although the latter is currently rarely used. The all stations bit is set in the first response following receipt of a broadcast message. This provides feedback to the master station to let it know that the outstation received a broadcast message. The class1, class2, and class3 events bits indicate that the outstation has event data belonging to the specified class. Setting one or more of these bits will typically trigger an event pull of the corresponding class from the master. The need time bit can be set by the outstation to request a time synchronization message from the master. The local indication bit is set when one or more
 of the outstation's digital output points are in the local state. That is, they're not accessible via the DNP protocol. The device trouble indication is set when an abnormal condition exists in the outstation. The conditions that may set this bit should be specified in the outstation's device profile document. The device trouble indication should only be set if the condition cannot be described using one of the other IIN bits. In practice, this indication is rarely used. The restart indication bit is set when the outstation restarts. It remains set until the master explicitly writes a zero into this bit of the internal indications object in the outstation. The object header consists of the object group, object variation, qualifier, and range fields. These fields are described in detail in Volume 6, Data Object Layer, of the DNP3 specification. The following slides provide an overview of these fields. This table indicates the object groups supported by DNP3. Not all object groups in each range are used, allowing for additional object groups in the future. For example, double-bit binary inputs were only recently defined. We will discuss object groups in more detail after we discuss variations and data quality indicators. This chart shows the object groups that are currently defined for DNP3. Each object group supports one or more variations. The supported variations can vary depending on the object group. The supported variations for each object group are defined in Volume 6, the Data Object Library, of the DNP3 specification. Variations provide options for how data are reported. Options may include with or without timestamps, with or without data quality flags, or, for example, the size of analog data. The master station may request a specific variation or the default variation. If the master requests the default variation, the slave will respond with its configured default variation. The default variation is defined in the outstation. Depending on the manufacturer, this may be a user-configurable option or it may be hard-coded. The outstation reports the default variation when the master specifies the default variation or variation 0 in a read request. For example, if the outstation's default variation for object 1 is 1, then if the master reads the default variation or object 1 variation 0, the outstation will respond with variation 1. The default variation is also used if no variation is specified in the request. For example, no variation is specified in class reads or unsolicited responses. So, for example, if the outstation's default variation of object 2 is variation 3, when the master requests a class 1-2-3 read, the outstation will send its digital input events with variation 3. Similarly, if the outstation sends unsolicited events, it will use variation 3 for events of object 2. Note that the outstation never reports variation 0 in a response. Timestamps are typically used on event data to record the time of the event. Timestamps may also be used on some types of static data, such as frozen counters with time of freeze. Variations with timestamps are not available for other static data types, for example, binary inputs and analog inputs. Digital input change events often use variation 3, binary input change with relative time. With this variation, the response includes object 51, common time of occurrence, or CTO. The CTO is a 48-bit timestamp representing milliseconds since January 1, 1970. Each binary input change event includes a 16-bit offset representing the number of milliseconds since the CTO.
 The time of the event is determined by adding the offset to the CTO. This technique saves bandwidth when many events occur at approximately the same time. Most DNP data types support variations that report data quality indicators, which are sometimes referred to as flags, to indicate the quality of the data point. All of the flags are described in Part 1 of Volume 6 of the DNP3 Object Library. DNP provides a standard set of flags that include Online, Restart, Communications Lost, Remote Forced, Local Forced, Chatter Filter, Overrange, Discontinuity, and Reference Error. Not all of these flags may apply to all data types. The Online, Restart, Communications Lost, Remote Forced, and Local Forced flags are common to all object groups that contain flags. The remaining flags are specific to particular data types. Also, the meaning of the flags may vary depending on whether the reporting device is an originating or non-originating device. We'll discuss originating versus non-originating devices and the meaning of the various flags in the next few slides. An originating device gathers field data or issues controls directly to the field. A typical IED or relay is an originating device. A non-originating device obtains input data or issues control commands via a communications link. A data concentrator or a protocol converter that converts data to DNP is an example of a non-originating device. A reporting device is a device that acts as a DNP3 outstation, sending messages to an upstream device. As shown in this diagram, data may reach the master via one or more data concentrator devices. In this case, each device in the chain, except for the master, is considered a reporting device. The meaning of terms upstream and downstream are also shown in this diagram. The online flag is normally set for good quality data. If this flag is clear, then the point is inactive or disabled. For example, it may be powered down or faulty. For input points, if this flag is clear, the point is unable to obtain field data. For output objects, if this flag is clear, then the point may not be observable or may not be controllable. Any commands sent to this point may fail. A non-originating device, such as a data concentrator, may optionally set this flag if communications are lost to the originating device. However, in this case, it must also set the communications lost flag. The restart flag indicates that data has not been updated from the field since the device reset. For inputs, originating devices must set this bit immediately upon restarting and keep the bit set until the point has been updated in its database. Non-originating devices must set this bit immediately upon restarting and keep the bit set until it is overwritten by collecting data from a reporting device and the reported value does not have the restart flag set. For outputs, originating devices only set the restart bit while the device is restarting. It is cleared when the device is available to accept commands, regardless of whether a control has been sent to the output object. For output points for non-originating devices, the restart bit remains set until it is overwritten by collecting output status information from a reporting device and that data do not have the restart flag set. The communication lost or com lost flag indicates that a communication failure has occurred in the path between the originating device and the reporting device. This flag is set to indicate that communication has been lost with the adjacent downstream device. The flag is also propagated by non-originating devices if it is set in the received data. This would indicate that communication has been lost.
 that the communications with the originating device was lost somewhere downstream. The flag is only set by non-originating devices. If it is set, the data value is the last value available before communications were lost. The comlost flag is cleared when data are received from the adjacent downstream device, and the comlost bit is not set in the data from that device. The remote forced flag indicates the data value is overridden in a downstream device. This flag can only be set by non-originating devices. The remote forced flag is set when an overridden value is received. In other words, if either the remote forced or the local forced flags are set in an object received from a downstream device, the remote forced will be set in the object when it is reported to the upstream device. Note that for output points, this flag indicates that the value might not correspond to the state of the output point. This flag does not indicate that a control was issued locally or remotely to the output point to set it in the reported state. The local forced flag is set to indicate that the data value is overridden in the device. This condition may be because the device is operating in diagnostic mode or due to human intervention. If the value is forced in a non-originating device and is overridden in a downstream device, then the non-originating device sets both the remote forced and the local forced flags. Note that for output points, this flag indicates that the value might not correspond to the state of the output point. This flag does not indicate that a control was issued locally or remotely to the output point to set it to the reported state. The chatter filter only applies to single-bit and double-bit input object groups. The chatter filter flag indicates that the binary data value is changing between states at a rate high enough to activate a chatter filter. Note that the binary data value reported does not necessarily represent the actual state of the input because the chatter filter may clamp the reported value to a single state while the filter is active. The originating device sets the chatter filter flag while a binary output is chattering. When the input becomes stable, the originating device clears the flag and reports the current state. The originating device creates an event when the chatter filter is either set or cleared. No events are generated for the binary input while the chatter filter bit is set. The rollover flag only applies to counter object groups. Note that the use of this flag is obsolete and should not be used in new designs. There is no direct mechanism in DNP for an outstation to report the value at which a counter will rollover. The rollover value is shown in the device profile document in section 3.4.9. The XML-based device profile could be obtained via file transfer in red by the master, however. This flag was deprecated because the master cannot determine how many times a rollover has occurred. Outstations should not set the rollover flag and master stations must ignore the rollover flag. If poll-to-data reporting is used, the master is responsible for polling the counter at a frequency that will allow detection of rollover conditions. The overrange flag is set to indicate that a data object's true value exceeds the range that can be measured or reported. This flag only applies to analog input object groups. When this flag is set, the data value should be the full-scale, maximum or minimum value. Suppose, for example, that a device uses a 12-bit analog-to-digital or A-to-D converter. The master may request data from this device in a specified format, such as 16 or 32-bit integers. If the master requests a particular object variation and the value of the data point is within the range for that variation, then the outstation can report the current value without modification within the requested variation. For data values stored in a device in a lower resolution than requested, for example,
 Data obtained with a 12-bit A-to-D converter being returned in response to a request for 16-bit data. Simple sign extension may be required. Since analog values are signed, sign extension is not considered a modification of the value. If the master requests a 16-bit variation and the value is outside the reportable range, that is, minus 32768 to 32767, then the outstation reports minus 32768 for negative values or 32767 for positive values and sets the overrange flag. Similarly, if the master requests 32-bit data and the measured value is not within the range of a signed 32-bit integer, the outstation reports full-scale negative or positive value and sets the overrange flag. If the input exceeds the value that can be measured by the hardware in the outstation, the outstation sets the overrange flag. For example, if the substation has a 12-bit A-to-D converter and the measured value exceeds the full-scale range or 2047, the outstation would report 2047 and report the overrange flag. Complete details for this flag are provided in Volume 6, Part 1 of the Object Library Basics of the DNP Specification. The discontinuity flag only applies to counter object groups. If it is set, the reported value cannot be compared against the prior value to determine the correct count difference. Resetting a counter via the freeze and clear commands is considered a normal operation. These operations shall not cause the discontinuity flag to be set. Once set, the discontinuity flag is cleared after a new value is acquired and transmitted to the upstream device. The reference error flag applies only to analog input object groups. If set, this flag indicates that the measurement process has determined that the object's data value may not have the expected level of accuracy. For example, the reference signal used in the analog to digital conversion process is out of limits or a calculated value has been contaminated with noise. The state flag only applies to binary input and binary output object groups. This flag does not flag an exception per se. The state occupies a position in the flag byte in order to reduce the number of octets transmitted for digital input and digital output objects when they are reported with flags. The state indicates the current state of a single-bit binary input or output point. Bandwidth can be minimized by not reporting flags. However, the DNP3 Specification specifies that if flags are non-nominal, the outstation must report a variation with flags. In other words, if any flags other than the online flag are set, the outstation must report a variation with flags. The outstation may choose to report the remaining values with flags, even if the flags are nominal. This option may reduce the message size and processing by avoiding repetitive switching between variations with or without flags. The qualifier field is described in Volume 6 of the Data Object Library. This table shows some more commonly used qualifier codes. Other qualifier codes are also supported in the DNP3 Specification. The range field is used in conjunction with the qualifier field to specify how data points are specified. The range field may contain start-stop indices, for example to specify points with index 0 through index 4, or a count of objects, for example 5 points starting with index 0. The range field is typically 0 to 4 bytes long. DNP3 objects are categorized into different types depending on whether the object contains a current value or static data, event data, control related information, other information, or attribute information. In the next several slides, we'll look at commonly used object groups including binary data, counters, and analog data.
 Binary inputs are used to represent single-bit read-only values. These inputs have on-off or true-false type indications. Binary input status values are reported as Object Group 1. Object Group 2 is used to report binary input change events. This table shows binary input support for Level 1, Level 2, Level 3, and Level 4 and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP Specification, and is provided here as a reference. Note that Level 2 adds support for reading the default variation, variation 0, and reading binary input change events. Level 3 adds start-stop indices support for static values. Double-bit binary inputs have two stable states, such as on and off, and an in-transit state. A fourth state may represent a condition that does not occur in normal operation. Motor-operated power line switches and motor-operated valves are two examples of devices that can benefit from reporting double-bit input points. These devices have three states, open, closed, and transitioning between states, either between open and closed or between closed and open. These devices may require several seconds to move from one position to the other. There is a possibility that something could happen that could cause the device to stall between states. Reporting the intermediate or transitioning state can be advantageous, as it potentially allows for detection of a problem with the transition. The double-bit binary input point type is also suitable for bistable devices, like switches, where the field wiring uses Form C contacts. Form C wiring uses two inputs. When a Form C device is at rest, one input is assertive and the other is not. When the contact moves between one input and the other, and during contact bouncing, an intermediate condition exists. If the Form C contact is a break-before-make type, then neither input is asserted during the intermediate state, and the indeterminate state indicates that both inputs are shorted together. If the Form C contact is a make-before-break type, then both inputs are asserted during the intermediate state, and the indeterminate state indicates open-circuit inputs or disconnected wiring. Double-bit inputs may be used for other devices that have up to four states. However, they should not be used to represent two uncorrelated single-bit inputs. This table shows double-bit binary input support for Level 1, Level 2, Level 3, Level 4, and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP3 Specification, and is provided here as a reference. Note that support for this object was added in Level 4. Binary control outputs are referred to as control relay output blocks, sometimes pronounced CROB, CROB, or CROB. CROB commands are sent via Object 12. Object 10 reads the current output state. Typically, a binary or double-bit input is used to provide feedback of the control operation. DNP3 supports several control types. Activation, which initiates an action on a single output. Complementary latch, which affects a single output and remains latched in either an active or non-active state. And complementary two-output model. The complementary two-output model has two virtual or physical outputs at a single index. These two outputs are named trip and close. Either trip or close is set active momentarily, that is, it is pulsed, based on which command is received. Examples of complementary two-output types include run-stop motor, trip-close breaker, raise-lower transformer tap, and open or close valve. Control relay output block commands are
 written to object group 12. The current output state is read from object group 10. This table shows CROB support for level 1, level 2, level 3, level 4, and commonly used variations and qualifiers that are beyond subset level 4 for object groups 10 and 11. Object group 12 is shown on the next slide. These charts are taken from volume 8, Interoperability of the DNP specification and is provided here as a reference. Note that level 2 adds support for reading the default variation, variation 0, and for reading binary input change events. Level 3 adds start-stop indices support for static values, and level 4 adds support for the binary output change object group. This chart is a continuation of the previous chart and shows CROB support for object 12. Counters report monotonically increasing unsigned integers. Examples of counter types include electric and water meters. These meters have counts that increment based on usage. DNP originally included a delta counter type. However, this object group has been deprecated in the specification and should no longer be used. DNP also supports frozen counters. A frozen counter shows the value as of a specific point in time. When the counters are frozen, the current point value is copied into a separate freeze or holding buffer or register. The freeze operation function codes allow the master station to indicate when the counters should be frozen. It is also permissible for an outstation to freeze the counters on a regular schedule, for example, every hour. Frozen counters facilitate reporting values from multiple outstations at the same instant in time. Instead of having to try to read each outstation at the desired interval, the counters can be frozen, either on a schedule or, for example, by a broadcast counter freeze request. The master can then read the frozen values at a more leisurely rate. In many systems, it is preferable to freeze the counters on a regular schedule so that freeze operations will continue to occur on schedule even if communications are interrupted or if the master goes offline. In this case, data may still be retrieved using frozen counter events when communications are restored. Although not specifically defined in the DNP3 specification, this mode of operation is defined as Mode A in Section 7.4.8, Selections from Transmission of Integrated Totals, of IEC 60870-5-101, Addition 2. Note that like delta counters, frozen delta counters have been deprecated and are no longer used. This table shows binary counter support for Level 1, Level 2, Level 3, Level 4, and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP specification, and is provided here as a reference. Note that Level 2 adds support for reading the default variation, Variation 0, and counter freeze function codes. Level 3 adds start-stop indices support. This table shows frozen binary counter support for Level 1, Level 2, Level 3, Level 4, and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP specification, and is provided here as a reference. Note that Level 2 adds support for reading the default variation, Variation 0, Level 3 adds start-stop indices support. This table shows binary counter event support for Level 1, Level 2, Level 3, Level 4, and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP specification, and is provided here as a reference. Note that Level 2 adds support for reading the default variation.
 Variation 0. Level 3 adds start-stop indices support. Finally, this table shows frozen binary counter event support for Level 1, Level 2, Level 3, Level 4, and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP Specification, and is provided here as a reference. Note that support for these objects is added in Level 3. Support for frozen counter events with time is added in Level 4. The analog input object group reports signed numeric values. While this object group typically reports physical values, such as voltage and current, the object group may also be used to report non-physical values, such as the result of computations. Note that floating point values should be represented by variations 4 and 5. Object group 100 has been deprecated and should no longer be used. DNP does not specify a deadband algorithm. Outstation vendors choose whether or not to implement deadbands, and if so, the deadband method to implement. Two commonly implemented deadband methods are fixed deadband and integrating deadband. With a fixed deadband, a new event is generated if the absolute value of the difference between the present value and the value that was most recently queued as an event exceeds the deadband. With an integrating deadband, the difference between the present value and the value that was most recently queued is integrated over time. An event is generated when the absolute value of the integral exceeds the deadband value. This table shows analog input support for Level 1, Level 2, Level 3, Level 4, and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP Specification, and is provided here as a reference. Note that Level 2 adds support for reading the default variation, variation 0. Level 3 adds start-stop indices support, and Level 4 adds support for short floating point values. Long floating point values are beyond subset Level 4. This table shows analog input event support for Level 1, Level 2, Level 3, Level 4, and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP Specification, and is provided here as a reference. Note that Level 2 adds support for reading the default variation, variation 0. Level 4 adds support for analog change events with time, and for short and long floating point analog change events without time. This table shows analog deadband support for Level 1, Level 2, Level 3, Level 4, and commonly used variations and qualifiers that are beyond subset Level 4. It is taken from Volume 8, Interoperability of the DNP Specification, and is provided here as a reference. Note that support for this object group begins at Level 3. Analog input points may optionally support frozen analog input values and frozen analog input events. These options are useful for reporting the analog input values from multiple outstations at the same instant in time. Typically, the analog input values are frozen at regular intervals that are appropriate for the system, although the freeze can be initiated by the master via a freeze command or by some local, internal, or external source. When a point is frozen, the current value is copied to a separate register or buffer. Freezing values allows the master to read values from the same point in time at a more leisurely pace. Note that frozen analog inputs are beyond subset Level 4 and are rarely used. The analog output object group provides an analog output signal that can be used to control physical or logical quantities. Generally, analog outputs have a continuous range of possible values, subject to quantization, and represent a physical amount. Analog output values
 are carried as signed values in DNP. Analog outputs are frequently used to set the desired operating level or position of a process or hardware mechanism. This type is often called a set point output in other protocols. Analog outputs can be physical devices, such as digital to analog conversion hardware, but they may also be virtual or pseudo points, such as software variables. Analog output control commands are written to object group 41. The current output value is read from object group 40. This table shows analog output support for level 1, level 2, level 3, level 4, and commonly used variations and qualifiers that are beyond subset level 4. It is taken from volume 8, Interoperability of the DNP Specification, and is provided here as a reference. Note that level 3 adds support for start-stop indices when reading the default variation, variation 0, 32-bit analog output status, and start-stop indices and all when reading variations 1 and 2. Level 4 adds short floating point support. This table shows analog output block support for level 1, level 2, level 3, level 4, and commonly used variations and qualifiers that are beyond subset level 4. It is taken from volume 8, Interoperability of the DNP Specification, and is provided here as a reference. Note that level 4 adds short floating point support. DNP3 supports 2-pass or select-before-operate control operations. With many traditional protocols, the master can only issue a command to execute a control operation. With DNP3, however, several control operations are available. For example, the master can use select-before-operate, direct-operate, or direct-operate with no acknowledgement. Direct-operate with no acknowledgement is useful for broadcast operations. Here is an example of a select-before-operate control operation. The master sends the select, and the outstation answers with the response. The select response exactly echoes the select request after the IIN bits at the application layer. The master then sends the operate request. The operate request must exactly match the select request at the application layer. The outstation then sends the response. Note that the operate must be the next request after a select. Otherwise, the outstation will cancel the request. This table shows support for the class object for level 1, level 2, level 3, level 4, and commonly used variations and qualifiers that are beyond subset level 4. It is taken from volume 8, Interoperability of the DNP Specification, and is provided here as a reference. Note that a sign class was added in subset level 3. Volume 7 of the DNP3 specification defines DNP3 operation on local area networks and wide area networks. Basically, DNP3 puts the entire stack on top of TCPIP. This specification became part of the standard in November of 1998. This specification also supports UDP or connectionless service. Time synchronization procedures are defined in volume 5 of the layer independent specification. The outstation may request a time sync via IIN. As of January 1, 2008, all times should be reported in UTC. DNP3 supports different time synchronization methods depending on the physical medium. For serial networks, it uses a delay measurement. For LANs, however, a different scheme is used. A Function Code 18 Record Time of Receipt of LastOctet in the current time message is sent to the slave. The slave compares the time sent by the master in a subsequent write with the last recorded time object and corrects the internal clock based on the calculation. Note that Function Code 18 mechanism is to be used only on
 on local area networks. It is not for use in wide area networks or any store and forward network. DNP3 security was approved at the 2007 DNP user group meeting. Until recently, the major threat was considered to be noise on the wire. In fact, CRC bytes were sometimes called security bytes in the protocol analyzers. Most security was provided by physical isolation of the network and a lack of common knowledge about the systems. Since moving toward more network solutions, however, security has now become a priority. DNP3 security is based on an exchange of security keys and a challenge response mechanism. The specification was approved at the user group meeting in February 2007, and an independent cryptographic review is set to begin soon. There are, however, a few open issues that the DNP technical committee is still considering. Let's look in more detail at the DNP secure authentication enhancement to the DNP protocol. It's a set of additional function codes, objects, rules, and definitions to be added to the standard protocol. As previously mentioned, it's contained within a supplement to volume two of the specification. It's designed to ensure the receiver of a message can verify that the message was sent by an authorized user and to verify that it was not modified in transit. Security experts would say this means it addresses the threats of spoofing, an attacker pretending to be an authorized user, and modification, an attacker changing the message in transit. In addition, the DNP secure authentication mechanism includes features that protect against replay. Replay occurs when an attacker captures messages being transmitted on an open link, for instance, a radio system, and plays them back at another time, attempting to cause havoc by making the system do something it wasn't supposed to do. To protect against such an attack, DNP secure authentication includes information in each message that changes each time a message is sent. DNP secure authentication can also help protect against the threat of repudiation, a valid user claiming not to have performed an incorrect or illegal operation. Although logging is not part of the specification, the authentication mechanism gives the remote device the necessary information to store a permanent log of who performed what operations. It's important to understand what DNP secure authentication does not do. The mechanism does not encrypt the message. This may surprise some people who assume that all electronic security involves scrambling the message. It was decided by the DNP technical committee on the advice of utilities that it was not necessary to protect the message against eavesdropping, an attacker reading what data was in the message. Encryption requires significant processing Encryption requires significant processing power on the part of the transmitting and receiving devices, and this cost was not considered worth the benefit. The mechanism also does not protect against traffic analysis, the ability of a tracker to determine what a utility is doing with a remote device by watching the pattern of the remote message traffic. Lastly, the mechanism does not protect against denial of service, where an attacker transmits so many valid or invalid messages to a remote device that it's made unavailable to valid users or brought to its knees. Denial of service attacks can rarely be addressed within the receiving device itself and are best dealt with by an intervening device like a proxy server or a firewall that can filter unwanted messages. DNP secure authentication was designed with the following principles in mind. Authentication only. It addresses authentication only, not encryption or other security measures, as already discussed. It does not rule out the possibility of such measures being added to DNP later or through the use of external measures, such as bump-in-the-wire link encryptors. It's application layer only. DNP3 must be used over a variety of different physical devices.
 networks, and may be bridged from one to the other, as in the case of a TCP IP terminal server or IP radio. Only authentication at the application layer will ensure end-to-end security. Application layer authentication also permits the possibility of protection against rogue applications that may be co-resident with the DNP3 application and attempt to use the DNP3 link without authorization. It's based on standards. DNP secure authentication follows the IEC 62351-5 specification, which also applies to the IEC 60870-5 family of protocols. IEC 62351-5 makes use of various ISO, IETF, and NIST standards so that the security technology used has been well proven. The only new portion is its application to DNP. It's bidirectional. Either the master or the outstation can authenticate messages. Challenge reply. It's based on the common security concept of challenge and response. The device performing the authentication challenges the sender by providing data that must be included in the authentication calculation. This makes spoofing much more difficult. It also means the receiving device also determines which messages must be authenticated. Pre-shared keys. The current version assumes the sender and receiver have previously shared a cryptographic key, that is a string of numbers, through some method other than DNP. The DNP technical committee is currently working on a method that will permit keys to be changed remotely and securely. Backwards tolerant. The specification attempts to make it possible for a non-secure device to identify that a secure device is using a portion of the protocol it does not recognize without an authentication message causing the non-secure device to fail. This principle is dependent on how well the DNP implementation on the non-secure device handles unexpected messages. Upgradable. Security technology is always changing, so the mechanism permits the sender and receiver to identify which algorithms, data, and key sizes that they are using and to change these in the future. The specification also requires that any implementation support a mode in which security is turned off on a per-device basis for upgrading purposes. Perfect forward secrecy. If a session key is compromised, this mechanism only puts data from that particular session at risk and does not permit an attacker to authenticate data in future sessions. And multiple users. It assumes that there may be multiple users of the system located at the site of the master. It provides a method to authenticate each of the users separately from each other and from the master itself. The intent of this principle is to permit the outstation to conclusively identify the individual user, not just the device, that transmits any protocol message. The DNP secure authentication mechanism is based on the concept of a cryptographic hash. A hash is a function like a cyclic redundancy check, or CRC, or a checksum, that when performed on a message, produces a much smaller string of numbers. This smaller hash value is very sensitive to changes in the message, but it is virtually impossible to determine the original message if all you have is the hash value. Let's assume that Alice is trying to send a message to Bob in a way that Bob can be sure the message is authentic. To make it work, Alice and Bob must have a previously shared key, that is a string of numbers like a password, that only the two of them know. Alice performs the hash function on the message she wants to read, concatenated with the key. This produces a small hash value. Alice sends the original message and the hash value to Bob. She does not send the key, because it could be seen by an attacker. However, the message is not encrypted in this case. The attacker can see what the message is doing, but as we shall see, cannot modify it or send the false message of the attacker's own.
 Bob receives the message. Since he already has a copy of the key, he can now duplicate Alice's calculation. He hashes the message and the key together to produce a hash value. If Bob's hash value matches the value that Alice transmitted with the message, he knows two things. First, the message has not been tampered with. If an attacker had tampered with the message, Bob's calculation would have been on a different message than Alice used, and therefore the hash value would have been different. The hash function is carefully designed so that without knowing the key, an attacker could not modify the message in such a way that would produce a correct hash. Secondly, Bob knows that the message came from Alice, or at least someone who knew the pre-shared key. Although the key was not transmitted on the link, it was intrinsic to the calculation, and without it, an attacker could not produce a matching hash value. Because of the way the hash works, it is nearly impossible to determine the key from the hash value. When a hash is used with a key in this manner, it is known as a Hashed Message Authentication Code, or HMAC. The hash function used for DNP secure authentication is defined by the National Institute of Science and Technology, the International Engineering Task Force, and the International Standards Organization. It is known as the Secure Hash Algorithm, or SHA. Two varieties of this algorithm, SHA-1 and SHA-256, may be used in DNP secure authentication. In practice, the core of the DNP secure authentication mechanism is three new function codes shown here. Almost every security message will use one of these function codes. This permits non-secure devices to clearly identify that the secure devices are using a feature that it does not support, and to raise a configuration error. Authentication can take place in either direction. Not only can an outstation verify that a control operation is authentic, for example, but a master can also verify that the data it is receiving from the outstation is authentic. The specification includes a list of which items are mandatory to be authenticated. For instance, controls must be authenticated, but it's optional whether data responses from the outstation are authenticated by the master. Each individual device has the choice of what must be authenticated for its proper operation. There are eight new objects to find for use with DNP security. Note that all the objects except those related to key exchange can be sent by either the master or the outstation. A challenge message is sent by a receiver to indicate that the previously transmitted message must be authenticated. The challenger supplies data to be included in the authentication calculation. A reply message contains the hash value to authenticate a message. An aggressive mode request and an HMAC object are included at the start and end of a DNP fragment, respectively, to authenticate a message without exchanging a challenge and reply. There are two types of keys that are used in DNP secure authentication. The pre-shared key already discussed, known as the update key, and a session key in each direction. The update key is used to periodically change the session key using the three session key messages. An optional error message can be used for debugging a secure system to determine why authentication is failing. Devices can stop sending error messages at any time to avoid denial of service attacks. Let's take another look at the select before operate example, only this time with DNP3 security enabled. As before, the master sends the select request. This time, however, the outstation sends an authentication challenge, and the master sends the authentication reply. The outstation then sends its response. Next, the master sends the operate request, which is once again met with an authentication challenge that requires an authentication reply, and then the outstation sends its response. The Triangle Microworks communication protocol test harness supports DNP3 security.
 security messages. This screenshot shows the protocol analyzer output with security messages highlighted. Welcome. This presentation is provided by Triangle Microworks as an introduction to using XML for DNP3 self-description and mapping to IEC 61850 object models. It also serves as an example of our online training tools. The device profile document was introduced in the original BASIC 4 specification. Over the next several years, the tech committee worked on subset definitions, the LAN WAN specs, certification procedures, and various other technical bulletins. In 2007, these were all rolled into the version 2 document set. This set expanded the number of parameters in the original device profile document and defined an XML schema for a machine-readable version of the device profile. Beginning in 2009, a DNP3 device is required to have its device profile available as an XML file. The steering committee has approved a stimulus plan to fund creation of the first several outstation DNP3 XML files. The Triangle Microworks ICD editor can also be used to define XML-based DNP3 profile documents. In this next section, we'll take a look at self-description using XML. This functionality is defined in the interoperability section of the DNP3 specification. Topics covered in this section include what is a DNP3 XML device profile, features of the DNP3 XML device profile, use cases, mapping DNP3 to IEC 61850 object models, and the use in the IEC 61850 engineering process. First, we'll look at what is a DNP3 XML device profile and some of its features. The XML device profile document is organized into four sections, configuration, points list, implementation table, and mapping to IEC 61850. The configuration section has four columns. The current value and configurable methods are not shown in this picture. Protocol capabilities indicates the option supported. The current value indicates which option is active. The next section, points list, describes the scaling in units and provides for a text description of the point. The implementation table describes which data types, function codes, and qualifier codes are supported. An IEC 61850 object model can be represented in the DNP3 XML device profile document by assigning each IEC 61850 data attribute to a configuration parameter, data point, or constant in the DNP3 XML file. An optional user data element is included in all main data structures defined in the DNP3 schema to allow user-specific data to be incorporated at various places within a DNP3 XML file. This is shown in the DNP3 XML instance document. Note that no outstation code changes are required to support the new DNP3 device profile in XML. The XML file is just a new file that's supplied on disk or by a web download. This format file can be applied to existing devices that were placed in operation years ago. It does not interfere with real-time communications and is good for small DNP implementations as well. A DNP3 file transfer is not required for any subset level. Generally, all the XML files would be stored in a central network location. If DNP3 file transfer is supported, then it could be used to read current values from the device or write new configuration parameters to the device. How are DNP3 XML files created and how are they used? The most common uses are shown in this diagram. The file is ideally generated by the IED configuration software. Otherwise, any XML editor could be used. If read directly from the IED with DNP3 file transfer, it would probably contain only the current values to reduce the file size. A separate capabilities file would contain.
 the Capabilities and Configurable Methods columns. This would typically be a generic file for each device provided by the vendor that does not change with each installation. System configuration tools might take one of these files as input and instantiate it with values for a specific installation. It could also qualify devices and ensure required features are supported for a specific application. A DNP3 master can use this file to set all communication parameters to connect to the device and display data as 61850 object models or DNP3 point maps with descriptions and other information. The DNP3 Users group provides an XSLT file that can be used to translate the DNP3 XML into HTML and viewed in a standard browser like Internet Explorer or Firefox. It can then create a PDF file or a printed copy. The XSLT translation to HTML is only in one direction, but all others could read or write the DNP3 XML file. For example, the DNP3 master could write the file to the IED to change some configuration parameters. The MS Word version that is in common use today is equivalent to just the HTML or paper version. The XML device profile can be used when mapping DNP3 to IEC 61850 object models. Standard models allow a consistent database interface in a multi-vendor product. The device profile will include fields for mapping DNP3 point values, flags, and timestamps to IEC 61850 data attributes. This picture shows the Triangle Microworks ICD editor. The left column shows an IEC 61850 ICD file, and the right column shows a DNP3 XML file. This example shows mapping an analog input, total watts, from DNP3 to the total watts magnitude data attribute. Once it is mapped, it appears in section 2 of the device profile document, and the ICD file is no longer necessary. The properties window shows the mappings. Note how a 61850 data attribute can be mapped to the timestamp, quality, or value on a DNP3 point. IEC 61850 configuration values, such as the multiplier or units, can be set to a constant, since this information does not need to be transmitted by a DNP3 data point. Standard models allow a consistent database interface in a multivendor product. Device profile will include fields for mapping DNP3 point values, flags, and timestamps to IEC 61850 data attributes. This picture shows the Triangle Microworks ICD editor. The left column shows an IEC 61850 ICD file, and the right column shows a DNP3 XML file. This example shows mapping an analog input, total watts, from DNP3 to the total watts magnitude data attribute. Once it is mapped, it appears in section 2 of the device profile document, and the ICD file is no longer necessary. The properties window shows the mappings. Note how a 61850 data attribute can be mapped to the timestamp, quality, or value on a DNP3 point. IEC 61850 configuration values, such as the multiplier or units, can be set to a constant, since this information does not need to be transmitted by a DNP3 data point. In IEC 61850, the CLN class is mandatory for all logical nodes. Attributes such as health, behavior, and mode were created as part of the IEC 61850 object model, and typically are not included in an existing DNP3 database. Therefore, this information is not needed. 
 will not be available through the real-time DNP3 data mapped to IEC 61850. If a DNP3 database is created specifically to be mapped to IEC 61850, then it may be designed to obtain 100% coverage. This diagram shows a typical substation network architecture. At the bottom, we see DNP3 and IEC 61850 devices sharing the same Ethernet connection in the substation RTU. The substation RTU is connected to the control center via DNP3 through a slow connection. The DNP3 XML device profile configures the mapping from IEC 61850 to DNP3 in the substation RTU. This same DNP3 XML device profile can be used by the control center to represent the data received via DNP3 as an IEC 61850 object model to the user or other applications running in the control center. The substation RTU can usually read DNP3, IEC 61850, and a range of other protocols, but if other devices in the substation need data from the DNP3 device, a protocol converter can be configured by DNP3 XML to translate from DNP to IEC 61850 instead of IEC 61850 to DNP3 as just described. It is generally not feasible to replace or upgrade all equipment in a substation at once. Thus, we need a strategy for migration that allows different protocols to coexist. If there are no IEC 61850 devices in the substation, you can still represent data as an IEC 61850 object model with DNP3 XML in the substation RTU and or the control center. If we want to display or organize the master database by IEC 61850 object model, in the past we would have used a protocol converter. DNP3 XML gives an equivalent solution without adding extra hardware to the system by providing the mapping as an extra layer in the software. Now let's look at how the XML device profile can be used in the IEC 61850 engineering process. What is an IEC 61850 SCL? SCL stands for Substation Configuration Language. It is an XML-based language for configuring substations and substation devices. It was designed for exchange of information between engineering tools and includes descriptions of device models, communication infrastructure, and relationships within the power system. How can DNP3 XML fit into the IEC 61850 substation configuration language or SCL engineering process? By combining generic IED files with substation templates and instantiating to obtain a central database for a specific substation. We can then split off a file to configure each IED or its PC-based configuration tool. The DNP Technical Committee is considering a proposal for a standard DNP private field in the ICD file. This picture shows the Triangle Microworks ICD editor, which includes a free component to create and edit DNP3 XML device profiles, shown on the right. On the left we see an IEC 61850 ICD file. According to the proposal, the entire DNP3 XML device profile would be linked or embedded within a private field in the ICD file. This is the IEC 61850 substation configuration language or SCL engineering process shown previously. Tools that understand DNP3 XML and IEC 60870-5 private tags can use or update the information in the DNP3 XML device profile. Others are required to pass it through unchanged. The Triangle Microworks ICD editor or other system configuration tool adds DNP3 data mapping information to the SCD file. Inside the substation, the IED configuration tool splits the contents out of the SCD into IED-specific CID files and uploads them to the IEDs. Note that the gateway IED will contain a private field containing a DNP3 XML device profile with the mapping from IEC 61850 object models to either the value, timestamp, and quality of the DNP3 data points or constants for configuration requests.
 parameters. Outside the substation, the SCD file can be used as an input for a control center specific engineering tool to translate the data communicated from the IED via DNP3 back to an IEC 61850 object model. In summary, the DNP3 XML device profile gives you everything the old paper document had, plus it's machine readable, so it enables creation of new DNP3 configuration tools. It can be used to create a DNP3 master that organizes data to user or other applications with IEC 61850 object models. Let's look at one of the more recently developed features of DNP3, datasets. In the following slides, we'll look at what datasets are, how they're described, and look at an example of a dataset. For those wanting a more in-depth look at datasets, we'll take the example and look at different ways of describing its structure, and we'll look in detail at the structure of the DNP3 objects associated with datasets. Before we go into details about datasets, it's useful to introduce some basic abbreviations and terminology that we will come across when using datasets. DAEL, a data element, is a field that contains data formatted according to a data type that we will look at in the next slide. NSPS, a namespace, is a reference to a set of names that are in some way related to each other. They're helpful to prevent confusion if a name such as Distribution Feeder appears in two or more namespaces. Namespaces are used to qualify the names used for dataset prototypes. Dataset prototypes are most often given generic names such as Fault, Well Pump, and Generator. Using a namespace reduces the confusion if two organizations want the same name for their prototype. With a namespace, the two fictitious organizations Worldwide Relaying Organization and Wind Power Association could define a DNP3 dataset prototype with the name of Distribution Feeder without concern for a user misinterpretation. Namespaces are intended to reflect the name of a standard organization, a standard specification, or a vendor, but that's not a requirement. Namespaces must be registered on the DNP website at www.dnp.org in order to assure unique namespaces. PTYP, a prototype, serves two main purposes. They allow vendors and third-party standards groups to predefine a subset of elements that must appear within a dataset. Secondly, they reduce the number of octets and messages transporting dataset descriptors when the same pattern would otherwise appear repeatedly. CTLV, a control value, is used to denote that the field contains a value that is to be used with a control output. CTLS, a control status, is used to denote that the field contains a value showing the status of a previous control output. And UUID, an acronym for Universally Unique Identifier, is a unique 16-octet string. For human readability, UUIDs are often expressed as an ASCII string that includes the hexadecimal characters, hyphens, optional opening and closing curly brackets, and spaces, as in the example shown. Continuing with abbreviations and terminology, visible strings are normally a set of ASCII characters. Octet strings are strings of 8-bit octets. The example shown here is expressed in hexadecimal notation, and hence the prefix 0x. UNCD, or Unicode, is a computing industry standard allowing computers to consistently represent and manipulate text expressed in most of the world's writing systems. The values are pretty self-explanatory. DNP3 time is a 48-bit value of time expressed as the number of milliseconds that have elapsed since January 1st, 1970. So just what are data sets? As the name suggests, they're just a collection, or in other words a set, of data elements. These data elements can be anything we like. They don't have to be data that is physically present at the outstation.
 A dataset can have almost as many elements as we like. Typical real-world datasets have about 10 elements. Elements are normally data elements, although other element types are allowed to assist in the definition of the dataset. For example, one element could be the dataset name. Another element could reference a dataset prototype, which we'll talk about later. Data elements are defined to be of a specific data type. The data types are predefined in the DNP3 application layer document. Examples of these data types are visible or ASCII strings, bit strings, integers, and floating point. A dataset is referenced by an identifier, that is, an integer index, in the same way as other DNP3 objects are identified, for example, analog inputs. The first dataset in an outstation has identifier 0, the next one identifier 1, and so forth. The first element of any dataset is its identifier. A dataset can optionally have a name, which can be used by the system as an identification or description aid. If it has a name, then this is always the second element of the dataset descriptor. Pictorially, what are datasets? They can be represented as a set or group of elements. The maximum number of elements allowed in a dataset is governed by just one rule. The dataset must be able to be carried by a DNP3 message using just one application layer fragment. So, the system's application layer fragment size dictates the maximum size of the dataset. In practice, this is not a limitation. An element in a dataset has to be defined as one of the following types. A data element, we'll talk more about this in a moment, the dataset name, the dataset identifier, a universally unique identifier or prototype used for referencing other common definitions from this dataset, a namespace, and a control value and control status elements used if the dataset is used with DNP3 control function codes, like select and operate. Each of the data elements must be defined as one of a specific data type. First, strings, that is, visible ASCII strings, bit strings, or octet strings. Second, numeric, including unsigned integer, signed integer, and single precision floating point. And finally, there are two other specific data types, unicode character field and time. Time is structured as the conventional 48-bit DNP3 time, the number of milliseconds since January 1, 1970. To achieve flexibility when using these data types, each data element is defined with its own length. This means that we can set up a dataset, for example, that has a mix of integer data elements, some sized as 8-bit integers, some as 16-bit integers, and others as 32-bit integers, for example. So how is a dataset described? The description of a dataset can be conveyed between the master station and the outstation, and vice versa, using object group 86, variation 1. Each element of the dataset is described by a dataset descriptor. The descriptor is made up of the length, pretty self-explanatory, the descriptor type, for example, identifier, name, or data element, the data type code. For data element descriptors, this defines the data type as, for example, integer, float, string, etc. The maximum data length. When the dataset is instantiated, the value put into this element must not exceed this length. For example, a data element of type unsigned integer, with maximum data length 2, can accommodate a value between 0 and 65535. And finally, the ancillary value. That's the value of this element, instantiated according to the descriptor type. For example, if the descriptor is of type identifier, then the ancillary value is the integer index of the dataset. So how is a dataset described? The description of a dataset can be conveyed between the master station and the outstation, and vice versa, using object group 86, variation 1. Each element of the dataset is described by a dataset descriptor. The descriptor is made up of the length, pretty self-explanatory, the descriptor type, for example,
 example, identifier, name, or data element. The data type code. For data element descriptors, this defines the data type as, for example, integer, float, string, et cetera. The maximum data length. When the data set is instantiated, the value put into this element must not exceed this length. For example, a data element of type unsigned integer with maximum data length 2 can accommodate a value between 0 and 65535. And finally, the ancillary value. That's the value of this element instantiated according to the descriptor type. For example, if the descriptor is of type identifier, then the ancillary value is the integer index of the data set. This slide shows a data set descriptor. The first three elements are all single octet entries. The ancillary value is optional, and thus the data set descriptor with no ancillary value will have a length of 3. The ancillary value is optional for the three descriptor types of DAEL, CTLV, and CTLS. If the ancillary value is used with these types, then it is used to give a name to the element. For example, a DAEL being used to show a trip current could have the optional name of trip current defined in its ancillary value. In all of these cases, the ancillary value is structured as a visible string. This table, split over this and the next slide, shows the formal definition of the data set descriptor codes. It is taken from part 2 of volume 2 of the standard DNP3 documentation set. In the documentation set, this appears as table 4-4. You may wish to pause the audio in order to study this table in more detail. This is the continuation of the formal definition of the data set descriptor codes. Once again, you may wish to pause the audio in order to study this table in more detail. This table shows the formal definition of the data type codes. It's taken from part 2 of volume 2 of the standard DNP3 documentation set. In the documentation set, this appears as table 4-3. It becomes clearer if we look at an example of a data set. Let's construct a data set that could be used by an outstation in an electrical distribution substation. The substation has a number of 600 amp distributions per volt. Let's look at an example of a data set that could be used by an outstation in an electrical distribution substation. The substation has a number of 600 amp distribution feeders. Each feeder has an auto-reclose breaker that attempts to reclose up to three times after a protection trip. After the third trip, the breaker locks out. The outstation has intelligent processing that just wants to report the reclose sequence to the master station after a fault scenario. So typical information we may want to put in the data set is the distribution bay identifier. The fault current at the first protection trip. The time of the first protection trip. The fault current at the second protection trip. The time of the second protection trip. The fault current at the final protection trip. And the time of the final protection trip, or lock-out. If this data set is the first data set to find in the outstation, then when the outstation sees the fault, the constructed data set would appear as shown. The first two elements of the instance of the data set are always the data set identifier and the time that the data set was instantiated. The instantiation as seen here is reported to the master station using object group 88, variation 1. This is a data set event and would be reported either as an unsolicited response or in response to a class poll. Looking in more detail, the data set descriptor for our example could look like the one shown in this table. Note that this table was split across several slides. You may want to pause the audio on each slide in order to study the table in more detail. This is a continuation of the table started on the previous slide. Once again, you may want to pause the audio in order to study this table in more detail. The previous slide showed a repeated part of the data set descriptor, the fault.
 current and fault time. We could set up a prototype definition to create these two elements. In order to identify the prototype definition, we need a UUID obtained from the software utility available at the DNP website in the Documents area of the website. Run the program once and it generates the next available UUID. We can see here that the third element is an optional namespace. The DNP website allows an organization to register a namespace, which can then be associated with all dataset prototype definitions from that organization. This table is a continuation of the previous slide, completing the dataset descriptor using prototypes. You may wish to pause the presentation in order to study this table in more detail. With the prototype defined, we can now use references to the prototype in our dataset descriptor. The first three elements of the descriptor remain the same as before, the identifier, the dataset name, and the bay identifier. The rest of the descriptor is replaced by three references to the prototype. When does the system exchange dataset prototypes? If prototype information has to be sent, then the master station will send the outstation details of the prototypes that it is using in a write request with group 85, variation 1. A master station may choose to read details of any prototypes that the outstation may be using in its outstation-defined datasets. The master station would issue a read request for group 85, variation 1, all points. Alternatively, the master station may choose to parse the outstation response of dataset descriptors and request details of any identified prototypes that the master does not recognize. In this case, the master station would selectively read the details using group 85, variation 1 requests with the relevant prototype identifier. We've seen that dataset descriptors are used to describe the contents of the dataset. An alternative is to use a prototype to describe the contents or part of the contents. Prototypes are used when we want to pre-publish the description of a group of data elements, often because the information needs to be shared across multiple users or vendors. Datasets and descriptors occur in pairs. Dataset prototypes are optional. The descriptor provides information needed by the receiving device to interpret the sequence and type of elements in the dataset. A dataset transfers values during normal run time. Its element ordering is as specified by the corresponding dataset descriptor. A dataset descriptor may reference one or more prototypes instead of including the descriptor elements from the prototype. When does the system exchange dataset instances? Once the master station knows the dataset descriptor and any prototype details, it would initialize itself by issuing a read request for all dataset instances, using a read request with group 87, variation 1, all points. Outstation-generated dataset events are returned to the master station in the same way as any other data point event, solicited via class 1, 2, or 3 read requests or as an unsolicited response from the outstation. The events are returned using object group 88, variation 1. Datasets have a characteristic that is used to determine the behavior of the outstation with a specific dataset. Details of the characteristics are transported using object group 86, variation 2. Dataset characteristics include RD, the dataset is readable, WR, the dataset is writable. This must be asserted if the master station is to write an instance of the dataset to the outstation. That is, use function code 2, write, with object group 87, variation 1. ST, the outstation maintains a static copy of the dataset. This characteristic needs to be asserted for the outstation to respond to a read request for object group 87, variation 1. EV, the outstation generates dataset events for this dataset, object group 88, variation 1 responses, and DF, which is set if the dataset is defined by the master station or clear if it's defined by the outstation.
 When does a system exchange data set characteristics? Strictly speaking, it's not mandatory for the master station to read details of the data set characteristics. However, some master stations prefer to have knowledge about the expected outstation behavior. So, in this case, the master station would read the data set characteristics. The master station then knows how the outstation should behave with requests to read data set instances and requests for data set events. If the master station performs a read request for object group 86 variation 2, then for each data set identifier's characteristics requested, the outstation builds a response with a single octet, formatted as shown here. There are occasions when a data set element is just a primitive DNP3 data point, for example, an analog input, binary input, etc. Where this is the case, then it can be desirable to correlate the data set element to the DNP3 database point. The correlation is transported using object group 86 variation 3. This object is a list of entries for every data set data element. For each data set entry, there is the object group number of the point, zeros used if the data element does not correlate a primitive point. Allowable group numbers are 1, binary input point, 3, double bit binary input point, 10, control output point, 20, counterpoint, 30, analog input point, and 41, analog output point. This object group number is followed by the point's index within that group. When does the system exchange point index attributes? Again, it's not mandated that the master station and outstation exchange details of point list attributes. In this case, the master station will interpret and process data set instances completely separately from its normal telemetry database. However, if the master station wishes to relate data set elements to its normal point list in its telemetry database, then the master station and outstation must exchange details of the point list attributes using group 86 variation 3. Object group 86 variation 3 is structured in a similar way as the other group 86 variations. The first entry is the data set identifier with the length and index number. Each point index attribute is made up of three fields. First is the size and octets of the following group number and index fields. Then comes a one octet group number, followed by the index, which can be up to four octets depending on the range of values of the index that need to be transported. Data sets are able to support the transport of output values from the master station to an end device. Normally, these functions are used with the standard DNP3 control function codes of select, operate, direct operate, and direct operate no acknowledge. Data sets used with control functions allow the outstation to send multiple values atomically to the end device, that is, to send them all together at the same instant in time. In the two stage control operations of select and operate, the same validation occurs for data sets as occurs for normal select, operate operations. When used with control functions, the outstation looks at the element types in the data set. CTLS tells the outstation what to do with the following CTLV elements, pass them to the end device or ignore them. In responses to select, operate, and direct operate commands, elements of this type represent a status code. The value is always set to an appropriate control status code as listed in Part 1 of the Data Object Library Specification. The master ignores the value in elements of this type in read and unsolicited responses, and the outstation ignores the value in elements of this type in write requests. CTLV represents the values themselves. The values in response to select, operate, and direct operate commands always mimic the values in the request. The master ignores the value in elements of this type in read and unsolicited responses, and the outstation ignores the value in elements of this type in write requests. DAEL elements may appear in data sets that also have CTLV and CTLS elements.
 However, DAEL elements are not intended to convey information in control requests and responses. The master and outstations ignore the value and elements of this type in select, operate, and direct operate requests and responses. The outstation must mimic whatever values the master places in the DAEL elements of the request when it sends the response, but otherwise disregards its value. It is mandatory to include at least one control status element in any Group 87 Variation 1 that is transmitted in a control command or response. A dataset may include more than one CTLS if not all of the CTLA values are outputted atomically. A CTLS applies to all of the CTLV elements that follow it until either the next CTLS element appears or the end of the dataset occurs. The term applied to a CTLS and its subordinate CTLV elements is a control group. This diagram shows a sample dataset having two control groups. The CTLS and CTLV elements are described in the same way as other elements, with the rule that the CTS element must use a single octet, unsigned integer dataset value. Let's wrap up the details of datasets with some final observations. Dataset identifiers are contiguous, starting at zero. If the outstation defines N datasets, then these have identifiers 0 to N-1. If the master station then defines further datasets, then these have identifiers from N up. Object Group 86 is used to describe the dataset. We've seen the use of Variations 1 and 2 to describe the dataset. Variation 3 allows us to map a data element in the dataset to a point in the outstation's database, for example, a binary input, analog input, and so forth. Object Group 87 is used to read and write static instances of datasets. Object Group 88 is used to generate dataset events. In addition to using dataset descriptors, the concept of prototypes is available. Prototypes serve two main purposes. They allow vendors and third-party standards groups to pre-define a subset of elements that must appear within a dataset. Secondly, they reduce the number of octets and messages transporting dataset descriptors when the same pattern would otherwise appear repeatedly. This table shows dataset support for commonly used variations and qualifiers that are beyond subset level 4. It's taken from volume 8, Interoperability of the DNP3 Specification, and is provided here as a reference. It's continued on the next slide. This is a continuation of the table showing dataset support for commonly used qualifiers and variations. Now let's look at one of the level 3 features of DNP3, file transfer. In the following slides, we'll look at how we perform file transfer with DNP3. It is very important to remember that DNP3 shows us how to transfer files between DNP devices. It does not tell us how to interpret the contents of the files. The file transfer process we use allows for sequential file transfer, that is, reading and writing sequential blocks of information. It does not permit random file access. Also remember that the original specification for file transfer using Object 70 Variation 1 was superseded in 2000 by an enhanced set of Object 70 variations and a set of new function codes, which will be described in the upcoming slides. There are a number of terms that we need to understand before we delve into the details of file transfer. First, let's look at the file handle. When a file is successfully opened, the device gives us a number to be used with all further operations on that file. When we no longer use the file name, we use this file handle. Devices may allow a number of files to be opened simultaneously, so the file handle is always a unique integer number. This example shows opening a file named Hourly Totals in a directory named History. The outstation returns a file handle with number 23, which the master station uses
 for all file operations until the file is closed. Continuing with our basic terminology, let's look at function codes. All DNP3 messages contain a function code. File transfer uses the two standard function codes of read and write and a set of function codes dedicated to file transfer operations. Open, close, and delete are pretty obvious and need no further explanation. Get file info is used to request the device to respond with details about the file. Delete is used if the device is implementing secure file access that requires usernames and passwords to allow access to the files. And abort is used to abort the current operation on an open file and to automatically perform a close of that file. The actual values of the function codes are shown here. Continuing with our basic terminology, let's look at the file operational mode. DNP3 file command operations contain a field named the operational mode. This is only pertinent when opening a file and specifies how the file contents are to be treated. Read specifies that the file contents are going to be read sequentially from the beginning of the file. Write specifies that the file contents are going to be written sequentially from the beginning of the file. Any data already in the file will be discarded. And append specifies that additional file contents are going to be written sequentially to the end of the current file contents. Continuing with our basic terminology, file permissions. This is an optional feature that allows the outstation to implement an access strategy based upon the concepts of just allowing an owner access, allowing a member of a group access, or allowing anyone or the world to have access. How to use the concept of owner, group, and world is not defined in DNP3 and must be agreed between the master station and the outstation. It could, for example, be based on the physical DNP3 address of the master station. Next, let's look at the block size. When data are written or read from a file, it is done in blocks. Each read response or write request carries one block of data. The maximum block size to be used is negotiated at the time the file is opened. The file open request carries a parameter defining the block size that the master station wants to use. The response from the outstation carries a parameter defining the maximum block size that the outstation can use. The value returned by the outstation must be equal or smaller to that requested by the master station. For read operations, it specifies the number of data octets that the outstation will return in response to a read request. For write operations, it specifies the maximum number of data octets that the outstation can accept in a write request from the master station. Now that we know some of the basic terms, let's continue by looking at how we use file transfer functions. To support open, close, delete, and abort functionality, we use two file transfer objects. The file command object, group 70 variation 3, and the file command status object, group 70 variation 4. In general, the master station issues a request using the file command object and the outstation replies with the file command status object. The fields in the object are used as follows. The filename is an ASCII string detailing the filename. If the outstation supports a hierarchical file structure, then the filename must be described with the complete path name and filename.
 for example, path backslash filename. The creation date is only used when opening a file with operational mode set to right. The field contains the date and time that the outstation should store as part of the file's attributes. When opening a file with other operational modes, the field is set to zero in the request and ignored by the outstation. Next, let's look at details of the permissions. This is only used when opening a file with operational mode set to right. The field contains the appropriate permissions that the outstation should store as part of the file's attributes. When opening a file with other operational modes, the field is set to zero in the request and ignored by the outstation. Next is the authentication key. If the file transfer is to use an authentication, then this field contains the authentication key that was negotiated during an authentication exchange. See slides below for details of this exchange. The file size represents the number of octets contained in the file. This is particularly important when opening a file so that both ends have the opportunity to allocate adequate buffers. During an open for write, the outstation may prepare its buffers appropriately. During an open for read, the file size must be zero and is ignored by the outstation. The operational mode is either read, write, or append when opening a file, or null when deleting a file. The maximum block size is negotiated during the open process. For reading a file, the master states the maximum number of octets it will accept in the file command object. The outstation must set a size, which is the lesser of the value in the master request or limitation in the outstation, into the maximum block size field in the response's file command status object. The outstation must not transfer more octets than the block size it returned in the response. The request identifier is a simple parameter whose value is arbitrarily assigned by the master station and sent in a request. The outstation device must remember this value and copy it into the request ID field of the corresponding response. This permits the master station to correlate responses with their requests when multiple files are simultaneously opened in the same device. The filename offset and filename size fields form a jump table, allowing direct access to the filename string at the end of the object. Offset is relative to the beginning of the object, and it's zero-based, excluding the header. That is, it contains the default value of 1A hex. The permissions field is a bit array indicating, when it's set to 1, that read is allowed, write is allowed, and execute is allowed for the three entities of owner, group, and world. A value set to hex 1FF indicates there are no access restrictions. This field must be set to zero for all requests other than opening a file for writing. The authentication key is expressed as a 16-bit unsigned integer obtained from the outstation during an earlier authentication message exchange. This is detailed in upcoming slides. The file size is specified as a count of the octets in the file and is expressed as a 16-bit unsigned integer. The operational mode and maximum block size are expressed as 16-bit integers. The file command status object is dual-purpose. It's used as a response to a file command object and also used as a request object when closing or aborting a file action. In this latter case, the response is also a file command status object. This can be seen in later slides where we show examples of typical file operations. The fields in the object are used as follows. The file handle is a numeric identifier of the file opened. Its value is arbitrary and provided by the outstation, but it must be unique to the outstation DNP address. It can be reused after a corresponding close is issued. Other file operations will reference this identifier. The file size is used to return the file size when a
 file has been opened for read, that is, when Operational Mode 1 is used in the FileCommand object. The value of this field is undefined in all other cases and must be 0. The MaximumBlockSize field is used to return the maximum block size that the outstation will return. This value should be less than or equal to the value set in the MaximumBlockSize field of the FileCommand object. For read operations, it specifies the maximum number of octets that the outstation will return in a file block in response to a read request. For write operations, it specifies the maximum number of octets that will be accepted by the outstation in a write request block. The RequestID field is returned in each FileCommand status object as the value of the RequestID field specified in the FileCommand object or FileDescriptor object for the corresponding file request. The Status field is used to return the status of the operation and must be set to 0 by the initiating station. The outstation reports any error conditions in this field. The actual status code values will be shown a few slides from now. Optional ASCII characters may be appended to the object after the status code and are a printable ASCII string that may be used to provide additional error information for any error condition. Here we see the details of the FileCommand status object. This slide and the next slide show the error codes returned in the status field of a FileControl status object. These error codes are also used in the status field of a FileTransport status object, which will be shown in a couple of slides. Feel free to use the pause button to study these codes in more detail. This slide shows additional error codes. Once again, feel free to use the pause button at the bottom of the screen to study the codes in more detail. Here we see a FileCommand object being sent to the outstation to open our history slash hourly totals file. We are asked to open the file for read operations and suggesting a maximum block size of 1K octets. For convenience, we've set up a request ID with value 1 so that we can match the resulting FileCommand status response. The outstation has responded with the status code of success, giving us a file handle of 0x99887766 for use with our subsequent read requests. It has told us that the file is sized to 64K octets, but that we can only request data in blocks of 512 octets rather than the 1K that we requested. Having seen the objects used to open a file, there are two objects used for file data transfer, the FileTransport object, group 70 variation 5, and the FileTransport status object, group 70 variation 6. The master station issues a request to read a block of data using the FileTransport object. The outstation response is also a FileTransport object, assuming the request is interpreted as a valid request. If the read request is not valid, then the outstation responds with the FileTransport status object, detailing the reason for rejecting the read request. The master station issues a request to a write block of data using the FileTransport object. The outstation response is a FileTransport status object, detailing the outcome of the write request. The fields in the object are used as follows. The FileHandle is the unique file handle that was returned in the FileControls status object when the file was opened. The BlockNumber field is an integer parameter plus a 1-bit last flag. The block number starts at 0 and increments by 1 for each block read or written. The size of the data portion is not relevant and need not be the same from block to block. It is limited by the maximum block size field passed in the FileCommand object during the open process. The last flag is set to denote the indicated block number as the last block, and cleared to denote the indicated block number as not the last block. The fields in the object are used as follows. The FileHandle is the unique file handle that was returned in the FileControls status object when the file was opened.
 The block number field is a 31-bit parameter plus a 1-bit last flag. The block number starts at 0 and increments by 1 for each block read or written. The size of the data portion is not relevant and need not be the same from block to block. It is limited by the maximum block size field passed in the file command object during the open process. The last flag is set to denote the indicated block number is the last block and cleared to denote the indicated block number is not the last block. The file transport status object contains a response to a file transport request when writing a block of data or when an error occurs while reading a block of data. The fields in the object are used as follows. The file handle is a unique file handle that was returned in the file control status object when the file was opened. The block number field is a 31-bit parameter plus a 1-bit last flag. The status indicates the outcome of the write request or an error code for a read request. Optional ASCII characters may be appended to the object after the status code and are a printable ASCII string that may be used to provide additional error information for any error condition. This slide shows the layout of the file transport status object. This example shows the master station requesting the first block of data from the history slash hour totals file that we opened a few slides back. The out station responds with the first block, 512 octets, of file data. In this example, the master station is writing data to a file that it has previously opened and was given the file handle 0x44332211. The master station is writing block number 31, the last block. The out station responds with a file transport status object showing a status code of success. In this example, the master station is closing the history backslash hour totals file. The out station responds with a status code of success. The out station may need time to process a file command or file transport object and to generate the response object. In this case, there is a danger that a timeout may occur at the master station. To obviate this, the out station can choose to send a null response and reply to the file command or file transport request. When the out station has processed the request, it then generates the relevant response object and has this available for the master station as an event object, that is, either in unsolicited mode or in response to an ensuing class request from the master station. The master station uses the authentication object to send the out station details of a username and password. The object used in the request contains a zero field authentication key. If the out station accepts the request, it returns an authentication object, defined as group 70 variation 2, with the username and password field set to null. It populates the authentication key field with a pseudorandom number that must be used by the master station in a subsequent file command object. If the out station does not accept the request, it returns a zero authentication key. This can still be used in subsequent file command requests, doing so as a local issue as to how it will be handled. A quoted example is that some files may need authentication and some not. In this case, the out station may accept a zero authentication key for those files that do not require authentication. This slide shows the fields in the authentication object. The fields in this object are pretty self-explanatory. In this example, a user with name Fred and password password is requesting an authentication key. The out station accepts the request and returns an authentication key of 485e hex. A master station can use a file descriptor object to obtain information about a file.
 file or directory. Using this request with a file descriptor containing a file name, that is a full name including any path details, obtains information about the file itself. Using this request with a file descriptor containing a directory name returns information about the directory. This slide shows the file descriptor fields. The file name offset and file name size fields are pretty self-explanatory. The file type field is set to 0 to indicate the references to a directory or set to 1 to indicate a simple file. This is only set in responses. In request messages, the field must be set to 0. The file size is set to 0 in requests. In responses, it contains the size of a simple file or the number of entries in the directory. The other fields we've seen before in other file transfer objects, the time of creation and permission fields are set to 0 in requests. Obtaining directory information is just like reading a file. The master station opens the directory using an open function with a file command object where the file name field contains the directory name. The outstation will return a file handle if it is able to find the directory. The master station then reads the first block of the open file. The outstation returns a block of data that actually contains details about the directory entries. The data is a set of file descriptor objects, one for each entry in the directory. The example shows a directory with two simple files, our hourly totals and daily totals files, and a subdirectory called archive data that contains four files. The file specification string completes the group 70 set. The object is just a string of octets containing a file name. The object is used to provide a file name to another DNP3 function code, such as activate configuration. More information about this is shown where we described the In summary, file transfer function codes and objects allow the master station to exchange potentially large blocks of data with an outstation. Typical uses include writing a configuration file to an outstation, reading a file of historical data values from an outstation, or reading a security log from an outstation. DNP3 supports file transfer, potentially with multiple files open at the same time. The master station can also be used to transfer files from the master station to the outstation. Potentially with multiple files open at the same time, but it does not specify how to interpret the contents of the file. This table shows objects, function codes, and qualifiers that are used with sequential file transfer. This table shows objects, function codes, and qualifiers that are used with sequential file transfer.


Video ID: lTwR8w8i1AM
Hello, everyone, and welcome to today's webinar. My name is Beth Capellas, and I'm the sales and support manager here at Triangle Microworks. Thank you for taking the time and joining us. Today we will be talking about establishing a vendor-independent specification process to support top-down engineering of IEC 62050 systems. Before we get started, I'd like to go over some housekeeping items while everyone is still joining. The webinar is scheduled to be 90 minutes long. The audience is on mute, so please submit your questions online using the webinar toolbar under the section titled Questions. You can submit questions as soon as you have them. We have several folks monitoring the questions and responding to them directly. We will answer questions at the end of the webinar and are happy to stay on longer if questions continue to come in. Next, I'd like to introduce our speakers. First, we have Thomas Sturckx, expert in virtual substations at ELEA Engineering. Thomas has worked at ELEA since 2014 with a focus on 62050 system engineering and modeling. He is also a member of 62050 Working Group 10 in C-Grade B568, as well as co-editor of 62050-90-30 and 7-6 Edition 2. Next, we have Yannick Tiaso, engineering project manager at Ulvexis, a Swiss company within Qantas. Yannick has over 15 years of experience known for designing innovative and robust solutions for data management, engineering of energy transport, and distribution networks in Europe. Yannick specializes in Helix SDS, a 62050 vendor independent system configuration tool, and serves as a key figure in training and user support. In addition to his role as product manager for customized client versions of Helix SDS, he oversees standardization projects for TSOs, DSOs, and integrators. Next, we have Jackson Moore, application engineer at Triangle Microworks. Jackson received degrees in electrical engineering and computer engineering from North Carolina State University and has a background in power system engineering. Prior to joining Triangle, Jackson worked as a microgrid system engineer where he aided in the design and development of control systems for multisource microgrids. In his role of application engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges facing the energy automation sector. Last, but certainly not least, we have Cedric Harrisburg, product manager of CiproTec Digital Twin at Siemens Global. Cedric joined Siemens in 2006. He is product lifecycle manager and senior key expert for engineering tools in energy automation. He is responsible for CiproTec Digital Twin, CiproTec tools, and 6150 engineering. He is also a member of TC57 Working Group 10. So now that you've met everyone, I'd like to go ahead and hand it over to Thomas to kick off the webinar. Thomas, with that, I'm going to make you presenter. Thank you. All right. Can you just confirm you see my screen? Yep, we can see everything. Okay. Thank you. So yeah, hello from my side. Great that you joined this webinar. So today we're going to speak.
 about engineering and more specific 61850 engineering and introducing all the work that we've done on it. And we are also going to demonstrate it with real demos of real tool prototypes. So it should be a very interesting webinar for all of you. We're going to start with a short introduction of the context, who we are, why we are doing this. Then I'm going to introduce the engineering process, speak about what we understand from it, how we think everything should be done. Then I'm going to shortly introduce all the new elements that we are working on in the 61850 standard and what the opportunities of all these new developments are. And then after that, we're going to focus on the demonstration itself. So we're going to build a real system or at least a part of it. We're going to build it together. We're going to go through each and every engineering process step and show you live in the tools how things are done. And then we're going to end with some conclusions and some next steps. Okay. So ALIA is a Belgian TSO. And together with 50 Hertz in Germany, it's part of the ALIA group. So two TSOs that together form the ALIA group. When it comes to engineering, why is this topic so important for us? We are facing strong grid investments in front of us. And in order to do that and deploy digital substations, move through the digital transition, we require a very strong framework to engineer all these systems. So we are really focusing on process efficiency, lifecycle management, being able to do an optimal traceability, handle changes, do a very good version management, and also interact in the most efficient way between system engineering and IED configurations. And very important as well, provide good quality of engineering at all stages of the process. A short history of the engineering topic. Yeah, already in 2012, ENSO-A, which is an organization grouping a lot of utilities, published a statement about engineering, about 650 engineering, and tried to push it forward there already. And I think I can say that finally, we have or we are being able to meet the statements or the requirements that they asked for in 2012. So that's very good news. Then in 2018, we worked together with some important key players of the market in a European funded project called DOSMOS. And that was where we funded or where we created the basics for the process in which we are right now, where we developed the ideas and took it to the next level. Then in 2020, working group 10 of 650 really started developing two technical reports that are related to engineering, 90-30 and 76. Last year, ELEA together with 13 other utilities published a common statement about engineering, where we try to communicate in a clear way
 way, how we want engineering to be done, where we made recommendations to all the stakeholders in the industry. So users, tool suppliers, IED suppliers. At this stage, we are finalizing our proof of concept project. So it's a project that we started last year to develop a proof of concept that demonstrates the process. And for that, we partnered up with the people here in the webinar. So Helix, Elvexis, Siemens, and Triangle Microworks. We tried to partner with industry leading players to give our proof of concept project as much as possible quality and demonstration purpose. And with all of that done, we are now preparing to do the next step and prepare to tender for what we call an engineering ecosystem where we want to go onto the market and get all the tools in order to support the process and deploy it now really within our company. That's the history. So now a few words about the common vision, the common statement that we made last year, and the process that we published there, which is now being demonstrated today in this webinar as well. So very important is that we identify five different steps in the process. You could call it a top-down process, but now here on this slide, the steps are presented from left to right. So the first step in the process is called system profiling. That's a new step for 61850. And it's really a very important step where a user is able to create this concept, define his concept by using what we call function and application templates. And these templates are being really the building blocks to build a system in all the next steps of the process. Once you combine these templates to a specification in the next step, you can then really start building a system and specifying a system or a specific IED. And with this specification then done, you can go to an IED supplier and ask him to prepare an IED configuration template based on your specification, documenting how he implemented that specification. And with these two elements available, you can then combine them in the system configuration step and really implement the system and create the final configuration to deploy that afterwards in the configuration of the IED. Within these steps, we also see opportunities to interface with other databases, with other data. For example, SIEM might give us in the specification step data which we can work with within the specification. For example, a single line data for when you specify a project. And when the system configuration is done and all the engineering data is grouped in the system configuration, that data might be made available as well again to SIEM or any other interface to be handled there as well. And very important to mention as well is that each step of the engineering process can be tested, can be validated externally with the help of testing and validation tools. Those are the main blocks of the process.
 And what we are doing at the level of the standard is taking all these steps and making it possible to execute them. And in order to do that, two technical reports are created, 90-30 and 76. And the image you see here, the flow you see here, actually indicates which elements are new to the process or new to the standard or are being updated by these two technical reports. And in the next slides, we will take a look at each and every one of them. But as you can see already, the updates we are doing are very important, are very large to create this framework. What is very important is that all the work that we are doing is to create an efficient workflow from top to bottom, from left to right, focusing on some specific elements like device-independent concepts. So being able to model a system without any device allocation of a function. So you can really create or model a system without knowing in which device a function or an application shall be implemented. So you can really do it in a technology-independent way. It's also a vendor-independent specification, because as we have seen, the specification will be implemented by an IED supplier within the interaction between specification and the IED template. We're focusing on process automation. If we already start from the very beginning of our process by working in an intelligent format, in the intelligent SEL format, it can allow us to win in efficiency in all the other steps and also execute some steps automatically. Change management will be much more easy to do, because as you will see in the next slides, we are investing heavily in creating unique identifiers, in creating traceability between templates and instances of these templates, between specification and configuration, etc. So all of these elements we try to put in place as well. And then the last one, but which is a very important one, especially for large utilities that have different teams working together in the same project, is that we try to create a framework that allows different teams to collaborate on the same project, collaborate on the same system, on the same SED file. And one of the slides will go deeper into detail. So I suggest we wait for more until we get to that slide. Let's start from the left, system profiling. So we are introducing in 90-30 and 76, two new file formats, FSD, ASD, FSD meaning function specification description and ASD, the application specification description, which allows you to build a system that consists of several or a template that consists of several functions interacting with each other. And on top of it, also specifying the logical behavior of the function or a series of these functions. So when FSD is a template of a single function, and ASD is a template of a single application combining different functions, adding logical behavior to it, adding data flow behavior to it.
 So you can really see the ASD more as a grouping of FSDs which interact then with each other and which get additional behavior specification on top. So as you can see here, there is no notion of a physical IED here. It's all allocation independent, it's just modeling of a system or of a specific application. Once these templates are done and you're building your system specification by combining different application templates into a system specification, then you can start building also IED specifications. And we're also introducing a new file type ISD, so IED Specification Description, which is now a formal way to specify an IED. And it contains, of course, the IED data model, but it also contains a process or a substation section that documents also things like behavior descriptions, might be logical as well, data flow interfaces that are expected from the IED. And it is the first step into formalizing the interaction between a user and an IED Because instead of giving him a large series of written documents to specify what you want from an IED, you can give him something now that is machine readable, that is something that he can process inside his own tool and which he can use then to create an IED capability description that takes the information from what you have given him and maps it onto his device. And that's where we come to the process ICD, which is a process ICD is a new concept that we introduced in 90-30 as well, which is actually a regular ICD, which was already known, but which is now extended with the mapping information of your specification. So it contains all the necessary elements to tell a user how the IED supplier has implemented what you have specified. If the data model that you specified is conform his device, if the values that you have specified are implemented in the device, and if the data flow placeholders that you have put in your specification are being implemented inside the device. So as you can see, this is a very efficient way of working. And this process ICD also allows you to automate the validation step on a user's side to validate what you have received from an IED supplier. And then the last one is related to collaboration. So as I said, now the purpose is that different teams working together on a project can now collaborate in a better way. Until now, we have also been challenged with the fact that an SED file can only be edited by one person at a time, meaning that in the different stages of your project, while the system is being handled by different persons and when they need to do modifications, or you organize it in a way that only one person can do these modifications, or you hand over the SED file between the hands of all these different participants. It has always been a very difficult process to manage. We are introducing what we call a system configuration collaboration file, which allows you to create from a main project, a fork project, which actually extracts one or more.
 multiple IEDs from that main project and manages them in a fork project. So you can now, with this file in place, transfer the engineering rights on an IED from a main project to a fork project. You can only do it once, but when you have done it, it allows a different team as the one that is handling the main project, that allows that team to manage these IEDs in a separate environment where they can do modifications and push them back to the main project after they have done that. So it's a very interesting concept to be used, for example, to exchange information between the engineering team and the commissioning team, or within the same system, engineer different system extensions by different teams. So there are many use cases in which this concept might be used. Now, let's introduce the demonstrator. So the system that we are going to build today, of course, we don't have the time to build a full substation, but we're going to focus actually on a single bay. And within this single bay, we're going to focus on two applications, two applications interacting with each other. First one is the circuit breaker, and the second one is the distance protection. And of course, as you know, a distance protection trips a circuit breaker, so they will be interacting with each other. And both of these applications are being implemented by four devices. First one is a PIU, which holds the interface of the circuit breaker. The second one is a protection or bay controller device, which performs control over the circuit breaker, but also hosts the distance protection function. And then we have an HMI and a gateway that, for the circuit breaker, issue controls and manage their positions. And for the distance protection, sends or receives alarms, exchanges alarms with the bay controller, the protection device. That's the system we're going to build. Few more words about the individual applications and how we have created them. What you see here on the slide is the application template of the circuit breaker application. And within this application template, we have what we call in 90-30 four allocation roles. First one is a PIU, then you have the bay controller, the HMI, and the gateway. And within each of these roles, we have identified the functions that are required inside that allocation role. So from top to bottom, there are, of course, inputs that wire the position of the circuit breaker, outputs that wire the trips, the open and closing commands. There are some alarms as well related to the circuit breaker that are entering the application through a wired signal. Then we have inside the bay controller, the interfacing, the controlling, the interlocking, of course, and attached to that interlocking, an interlocking equation. So this is an example of where we can attach the specification of a logical behavior to a function. And then, of course, the alarms are being sent to the HMI, to the gateway. Also, the controlling can be done from those two places. This application is a container on itself, but it also interacts with other functions from other applications. For example, for the interlocking, it receives some information.
 from the high voltage switches from the bay. There is additional interfacing with the control center with the HMI screens now. And on the other side, exactly the same. The alarms are being sent to the functions inside the applications of the HMI and the control center. You can do the same exercise for the distance protection. The distance protection is an application from another – it's a completely different type. It also has a binary input. In our case, it's just an example here to switch between the overreach and the underreach setting. There are some protection functions. There is a trip logic, which we have modeled in 61131, which really documents how the trips are being created from the operations of the different distance protection zones. We have a logic that does some things, which has attached to that function a specific 61131 logic. We have LEDs that are being activated. We have, of course, the tripping itself. And we have data flow with other functions. Again, same thing like in the circuit breaker. There is, again, interfaces on the left and on the right. Of course, the distance protection needs currents and voltages. It needs input from the teleprotection application. The auto recloser, of course, is interacting. And so, you see, again, this application, it's a container on its own, but it interacts also with other applications. All these indirections, we can already model them inside the application template by placing placeholders, what we call process resources, by placing placeholders in the application template itself to start creating the data flow once they get instantiated in the same system where these other functions are available. Coming back to the big blue box in the middle, the 61131 trip logic, just to show you what is behind it. So, this is a logic that comes from the concept that we have within ALIA, and that allows us now to document in the 61131 language how all the signals need to be treated and how need to be treated and how the trips and other signals are being created from it. So, this is really now an added value enabled by the behavior descriptions that can now be added to the or attached to the applications. When we have the circuit breaker application template, the distance protection application template, when we bring both of them now together in a system, these applications can interact with each other. Of course, the trip is going from the right to the left, from the tripping function from the distance protection to the circuit breaker interface, that this uses the trip and that forwards it to the binary outputs. And we also have the circuit breaker control function providing a signal to the logic inside the distance protection function. I think that's the signal that activates the switch on default zones, for example. And once these two applications are being put together, then they can be implemented into the different devices. So, the system that we will be building will consist of the four IEDs in which these applications are being implemented and between which now exists data flow in the type of wired interface.
 internal data flow as well, reporting GOOS or MMS control. So that's the demonstrator. That's the system that we're going to build today. The process that we will be demonstrating is exactly the same as the one I explained in the slides before by taking from the profile side, from the top of the process, taking both application templates, building inside the Helix tools, a system specification from it, building the individual IED specifications, then handing them over to Siemens in order to create the ICD files, and then afterwards combining both of them inside a system configuration, which can then be used to finalize the configuration of the IED. And Triangle will demonstrate how an individual application can be simulated and tested so that it can be validated and used inside the remaining steps of the process. And now we're going to hand over the screen to Yannick. But if you can do that. All right. Thank you, Thomas. Yannick, I'm going to go ahead and make you presenter. You should now have. OK. We're not quite seeing your screen yet. OK. There we go. Yes. No, you see my screen and you can hear me. Yes. Nice. OK. Let's let's talk about the profiling part and demonstrate the use case of the specification of an application. The profiling part is the first part introduced before by Thomas. OK. The technical reports 90-30 and 7-6 extend the SCL data model, or more specifically, the process section of the SCL data model with additional elements. And this element allow the specification of the IEC61-A5T system in an IED manufacturer independent way. The current slide show all some of this element are represented in Helix-STS. On the left part here, you can see the Bay template. And on the right part, the function diagram gives a static view of the function. Here, for example, the logical node, the data attributes, the allocation role and the behavior descriptor. The function can be allocated either on the level like this here or on equipment level related to an equipment of the Bay template. The allocation role allow to group function and assign ID role to it. And this grouping does not yet require a manufacturer specific physical ID. It means we can really do this in the top-down approach without the knowledge of the manufacturer ID. The behavior descriptor are the elements that contain the logic definition. Thomas has just introduced this just before. You can define this logic with textual descriptor, but you can also define this logic and express it in 61-131.
 one. The data exchange is an important part of the specification and can be also done with in physical ID agnostic way. The content of an application can be stored with the new standardized file, the ASD file, the application specification description file, as described by Thomas just before. And in this slide, you see how it's reflected in a link status. And you can see here, the action, the application is really defined with exchange between function. And this exchange use some specified services. And that means during the specification, you can already decide or specify what kind of service you will use to make a data exchange. Here, you can see some some goose message, you can also see reporting. But now with the 90 dash 30 extension, you can also specify internal connection inside the same ID or wiring connection. And at this place, when you when you define the application, and as Thomas explained just before, you you can already define process resource, which is the the pink element here. And this process resource act as placeholder for function not yet available in the context of this application. For example, this this is the the application of the circuit breaker and you you expect signal from the from the switches from the also the disconnector switches. And these are not part of the application and it's represented with process resource. Later in the in this demo, we will show you all this process resource are resolved when we combine multiple application inside the inside the project. And this slide show you the step to define an application. First, you design the function, as well as the signal and the inputs necessary for this application. The signal or the output signal and input or the input signal, basically. You can add behavior descriptor as expressed before, to describe in textual format, but also in 61131, the complete internal behavior of your application. You use the drawing feature of any success as shown here to to describe this this exchange based on services. And then you can export this application to an ASD file. And this ASD file can be used to to create to create your specification of your substation. It can also be used to exchange with other tools in a standardised format. Now, I will show you this in the tools directly. I have to project here. The first one is the profiling project. And in this profiling projects in the single line drawing, I have defined a template substation with a template voltage level and template bay. And in this bay, we have
 So in this bay, we have several functions defined. And also, the signal for each logical node output signal are also predefined here. And also, the input signal. And this output signal and input signal are also represented in the extension of the 90-13 with DOS and DAS. And the inputs are defined with source ref elements. I will show you also in this profiling project the application. We have the CB application, which is described in the PowerPoint. But now we have also the P21 application, the distance protection application. And you can see here, this application is a little bit more complex. We have a lot of internal connection inside the protection between several sub-functions. And we have a lot of pink function, which is the process resource, which represents the process resource. And for example, here, you can see that the trip main protection signal will be sent to this process resource, to the circuit breaker. And this process resource will be later in the project resolved and replaced by a function of the circuit breaker application. And we have another process resource on the bottom, which gives the operation clause, the general signal, which is sending this case by the PIU, which handles the breaker, to the protection ID. Now, as soon as you have defined the exchange like this, you can then export the file. And to export the file, you just right-click on the application itself and use the Export button. And this will generate on the output folder a file called Yes, it's the correct one. We'll generate a file, and we'll just quickly show you what is inside this file. It's an SCL file, a normal SCL file, but with the adding of the 6-100, which is the namespace for the 90-37-6 technical report, where you have all the process resource, you have the application, and all these kind of things. You have the behavior descriptor. Here, you have all the logical node, the specification with source ref, DOS, DS, and everything coming from the 90-30 is now part of this file, which is now exchangeable between all the tools. Then, if I come back to the PowerPoint, OK, we have now passed through the first step, the profiling phase. We have specified application. In this case, we have specified the CB application and the P21 application. And we have stored this application on ASD file. Or, as it is the case in Headings SPS right now, we have stored this application in the internal library. We have also probably stored in the SSD file or inside the internal library some BAY templates. Then, we are ready to move to the second step, the specification of your substation base, based on this file.
 And when we have done this step, the substation specification, we can then export some specification file. We can export the ISD file, which is the ID Specification Description file, and which can be used later by the manufacturer tools, the ICT tools of the manufacturer. We can also export SSD file, which represents the complete substation. And this file can be exchanged in our demonstrator today with Triangle for the testing tools. And we can also export an SSD file for the system configuration tools, which in our case is also a LinkSS test. Then this file will just show you what I have already explained just before. On the P21 application, we have some process resources that represent the sending of the TRIP signal to the PIU, which is on the profiling phase represented with the process resources. And in the project, as soon as we have instantiated the two applications together, we will resolve the process resource. And by the resolving of the process resource, this signal will now go to the function of the PIU, which will handle the TRIP. On this slide, you see also on this step, the specification step. We will generate the ISD file for the manufacturer. And these are the steps required to generate this file. First, we need to define the allocation of each function. Then we set the allocation of the function of each function. Then we set the allocation of all. Then we need to generate, to craft the specification ID. And this will generate the data model of the specification. And then we can later export the ISD file. At this stage, we can also export the SSD file for the testing tools. Now, I will show you this in the project. I have the second project, which is the specific mission project. In this project, it's normal template, template, template. But it's the real substation that I would like to specify. In this case, it's the Voxel substation with only one bear. And in this project, I have instantiated my two ISD file, the CB application and the P21 application. And I can show you in the P21 application that these two process resources now resolve and implemented with a function coming from the CB, the PIU for the CB application. And also, the closing of the breaker is also resolved here in this project. And then if we come back to the single line, we can then create the data model of the specification ID. Here, we just create like this.
 and I will create the same for the protection and then if I show you the system diagram here we have now two specification ID one for the PIO and one for the protection and we can then export the specification that will be shared with Siemens here I export an ISD file and I export also the ISD file for the protection and this will generate this two file here and if I quickly show you what what is inside the file as Thomas said just before it's really a nice ISD file and in the ISD file you have the substation section where you have all the information coming from the 90-30 technical report with all the source wave all the the DOS and DAS that are used for the for the specification of this ID then you can give this file to the ID manufacturer and it will come back with the process ICD file that resolves your specification at this stage you can also export a system specification file of course if you use you export a system specifications file you need to have the substation section because every extension of the 90-30 is inside the substation section and also now in this file you have the substation section with all the information of the 90-30 and 7-6 that will be used by Triangle in the rest of this demonstrator to demonstrate the testing part then I think I need to pass the end to Jackson okay thanks Yannick Jackson I'll go ahead and make you presenter all right and you should have the screen okay all right thank you Yannick and thank you Beth as mentioned my name is Jackson Moore application engineer here with Triangle Microworks and today I'll be introducing step three in this top-down specification process which is system simulation and engineering validation shown here highlighted in blue now the goal of this step is to take the design that's been specified and to make sure that it actually works as intended so this can be an iterative process by that I mean that the testing can occur as the design is still being developed for example in E-Links as Yannick showed us or can be done once your your design is complete so that would be you've produced both your ASDs and your SSD but what's really unique here is how early on in the design process we're able to perform the simulation and validation in this case before we've even chosen the IEDs meaning that any errors that have been identified at the stage are comparatively much cheaper and easier to fix than they would be down the line so as part of this step we see several lines representing different SCL files that are flowing into this yellow box here at the bottom this yellow box represents the simulation and validation tool which I'll be demonstrating in a moment using DTM so there are two types of files that are coming into this simulation and testing box and those are going to be ASDs and SSDs which we've been speaking a lot about Yannick just explained those as well as Thomas in detail a few months ago
 So the CB ASD over here on the left, this would represent a circuit breaker application whereas the P21, P standing for protection and 21 being the ANSI code for distance protection, would specify a distance protection application. So the other file that could be passed in would be this SSD down here. As we said moments ago, SSD can instantiate these ASDs that we've already created, assign roles and allow you to specify an entire bay or an entire system which can now be simulated at this specification stage. So today I'll actually be simulating and validating the P21 ASD and in order to do this, the first step I'm going to need to do will be importing. So what you see up on my screen now is actually a screenshot of the import dialog box that we'll be looking at in just a few moments and I'll use it to import the distance protection application specification. And what I really want to drive home here on the screen is that I am beginning with an SCL file that was produced as a part of this specification process. And then all of the visualization, simulation and testing that I'll be doing afterwards is all coming from this one file. So you'll notice that there's different sections of this important dialog box and that these roughly correspond to what you might see if you were to open up a typical SCL file. So up here at the top we have the communication section, below that the substation section and then the IED section down here below. Now in ASD, the one that we're going to be working with today will only contain the substation section so that's really the only one that's going to be relevant. But as I mentioned, this is really applicable to both SSDs and SCDs, assuming that these were produced with the top-down engineering process that we're talking about today. Anything else would apply to those as well. So once I bring in that ASD file, I'll be able to visualize what's inside of it. I really think of ASDs as kind of containing two crucial parts, the first being a description of the data flow and the second one being a description of the behavior of that application. And what we see here is a visualization of that first part, the data flow. So for the distance protection application specifically, that's what we're looking at. You'll see the inputs over here on the left, this is what we refer to as process resources. Same thing over here on the right, these are outputs. Inputs are flowing into functions and within that sub-functions and finally LNs down at the bottom. And then we also have this P21 here in the middle, this is going to be our behavior description. And this is described in IEC 61131. So if you're not familiar, IEC 61131 is, it's really a family of programming languages that's popular in the control and automation industry. Variants of it are ubiquitous and programmable logic controllers or PLC. So as part of this specification process, a utility may describe the desired behavior of a specific application using this language. And this really serves to both remove ambiguity from the purchasing process, but also the benefit of allowing for enhanced levels of validation and simulation, which is what we'll be looking at in just a moment. So if we kind of zoom in and look at what's inside of that box there labeled P21, this is a peek inside the IEC 61131 logic. So we can see that this is written using the function block diagram variant of 61131. So this is going to be a graphical programming language that closely resembles the logic diagrams that paper specifications have been based off for some time. So much like the application visualization that we looked at.
 a moment ago, the inputs are coming into the left and the outputs are coming out on the right. But instead of flowing through somewhat ambiguous functions like we saw at the application level, here it's very specific elementary operators like ands and ors. So we really have a very clear picture of how our logic is operating and we can actually simulate it and run it. In order to do that, there's a couple of different ways that we can interact with our application. So the most basic method would really just be to come in here and manipulate a specific input and then we'll be able to watch outputs as they're updated. We can manipulate multiple inputs simultaneously and make sure that our outputs all update as expected. Now if I do want to update, let's say multiple inputs, and I know that for this given combination of inputs I should have a predefined set of outputs, we do have an integrated test sequencer that allows for that. So basically one row in this test sequencer would have a predefined value for every input that's flowing into my application and an expected value for every output. And in that way we can validate and make sure that our application that we've designed actually works as we expect and as we intend. All right, now I'm going to pivot over to actually seeing the application so we can see it in action now that I've kind of given you that background and description. So what you see upon my screen now is a, I guess, a pre-release version of the tool that's not yet available, but this is what we're using to do the design, the demo today. It's DTM or Distributed Test Manager. So the first thing I do, as I mentioned before, is actually import the test is actually import that ASD. So I come up here, go to the import substation wizard, and I will select that ASD file, that application specification description that was produced as part of the specification process. As I mentioned, the only section that'll be relevant for me is going to be the substation section because that's where all the information is contained. I would want to make sure that I check the box to allow 6-100 elements because those are going to be crucial in order for the simulation to operate and those data flows to be defined. From there, once I import the ASD, I'm going to see this reflection over here on the left-hand side in the workspace tree that corresponds to the substation section. So up here at the top, I would have a voltage level, I'm sorry, a substation, in this case, just called template, voltage level, and then a bay. And then within that, I'll see the different functions which may correspond to FSDs or functional specification descriptions that were combined into the ASD. So if you were to open up the actual STL file, the XML, you would see a hierarchy that looks quite similar to this. This is really a direct reflection of the STL. And then I think it would be fair to say that this diagram that we see here is a direct reflection of this tree over here. So for example, the teleprotect function that we see here, if we expand it out, we see several sub-functions within it. And then within those sub-functions, we see LNs. If I come over here to the teleprotect function within my application visualizer, I can expand out the function and within it, I can see the sub-functions and then finally the LNs within that. I can see how my process resources or inputs are being passed into those functions and outputs are coming out. And in this case, being passed up into our P21 XML. So as I said before during the slides, this right here is actually the behavior description. So included in the
 The ASD file that I imported was a reference to this 61131 file. So it's like a local file path. When I import the file, I go and look for that 61131 file and bring it in. Taking a peek inside of that 61131 file, here's that function block diagram that I described before. So this is something that describes really the exact behavior for a specific distance protection algorithm. As I said before, it's made up of really inputs, outputs, and what I call elementary operators, but ANDs, ORs, timer on delays. If you're familiar with PLC logic or controls on automation programming, hopefully it should look pretty familiar. Now I can interact with this in several ways. As I said, I could come down here at the IEC 61131 level and begin to manipulate these inputs and outputs. I could edit my logic if I needed to and test and validate here. Or I can do this back up one level at the application level. So here, because inputs are flowing into that 61131, I simply have to manipulate one of these process resources or inputs on the left-hand side. And I'll be able to see how my application actually works. So I'll go ahead and just set true for one of these distance protection operate signals. And when I hit apply, we'll see some red flash up on the screen as that signal changes to true, flows through a couple functions, ultimately into my behavior description, and activates several outputs over here on the right. That is reflected, of course, in the output tree over here as well. Now if I am worried that it was too fast and I missed it, I can click on one of the values. So for example, this true. And I can see that some true to false toggles have happened as I've been playing around with this project throughout the day and have the exact timestamps. I can also set up a capture and playback. And again, if tight timings are an important part of my application working correctly, then I can capture that and play it back to see what happened. Now because it can become tedious to test complex schemes repeatedly, especially as I'm making changes, there's also the opportunity to use the test sequencer. So the test sequencer really does kind of a similar thing to what I did here, where I selected a specific input set of value for that. But instead, it defines inputs for, or values, I should say, for every one of these inputs and expected values for every one of these outputs. Effectively, it sets the values for these inputs, wastes a short period of time in the monitors to make sure that these outputs are correctly set. And that is, it's done using a table interface. You can define these tests using Excel, allowing you to rapidly scale up and perform many tests very quickly. You can, of course, use other scripting methods to define more complicated tests. And in the near future, you'll be able to do that using a graphical method as well. So with that, I hope I've demonstrated how we've been able to both validate and test and simulate our applications. As I mentioned, this could also apply to our SSD as well. And then later on in the engineering process, once we complete specification, our SCD. Okay, thanks, Jackson. With that, I believe we're handing it back over to Janik. I'm sorry, to who? Oh, Cedric, sorry. All right, Cedric, you should be able to unmute yourself. All right, Cedric, you should have this.
 There you go, we can see your screen. Okay, great. So now let's go to the fourth step of the process introduced by Thomas. We had before the ICD specification file generated by Yannick. And with the same tools, I will show you how to generate the ICD file and the process ICD file that has been introduced as part of that project. So there's a workflow that I will show is how to read the specification file for both devices, the protection device, as well as the merging unit, to map it to the signals provided by default by the manufacturer product and how to deliver the process ICD file. So process ICD file, we have the particularly including the specific mapping as well as the deviations, if any deviations are found in the product that some logical nodes or some names cannot be realized in the vendor tool. To show you that, I will jump into the Dixie software. Dixie software is our IED configuration tool used to configure all CProtect5 devices. So in that case, our system new device merging it as well as the 7S87, which is the distance protection device commonly used at Ilya. So the standard device configurations here exist in our library without any specific features and specific customer configurations yet. So why we have selected those devices is because the system new device type is the best fit for merging unit application and the 7S87 also is one of the best fit for supporting distance protection function which is used in the prototype team. So the Dixie software is providing multiple functionalities to support the engineering of top-down engineering with typical. Today we will focus on the one using 16x50. So as mentioned earlier, every device from manufacturer is coming with its default data model. Why? Because the 16x50 standard is standardizing logical nodes but it's not standardizing the structure with logical devices or the prefix and suffix names used in the protection functions. So here you see for the distance protection device from the Siemens CProtect5 family, by default it is modeled with a logical device called LM1DistanceProt and containing a certain number of zones by default. Now we will see in the specification generated earlier how to map the expected data model with the one provided by the specification. So for that, we just need to import first that new format introduced in the ISP specification. We have both for the merging unit and for the protection device. Let's start with the merging unit. The merging unit that is expected to be delivered at the end of the engine process is containing those logical devices, the CP interface, the CC interface, the DC interface and so on.
 And we'll do the engineering starting from the substation section. So for that let's go to the circuit breaker structure, where we find the most important functionality of the merging unit here, and being the position of the circuit breaker. You get the device called template in the specification, you just map it to the device from the Siemens library. And by default, some magic is happening, that is recognizing that our default device is having also an XCBR, but not in the logical device called CBCB interface as required with specification but in the logical device called CB1. It is finding it, if it wouldn't find it, we have the possibility to select other logical devices that better match the specification. But here, this is a perfect fit. And you see that the majority of the required data objects are already provided in the device. Some are not there yet. This is especially the case for signals that will be received via GOOS from the protection device. For that we have the possibility to create external signals, I mean via GOOS, by creating them with that blue icon. The signal is now created and the match is found. I iterate the same for the other signal. And at the end, we end up with the required GOOS inputs for the device. And in case of a data object required but not modeled that way by default in the device, we have the possibility to select a better match in our device to meet the specification. Doing that, every signal from the specification is mapped one by one between specification and the default data model of the device. This process has to be repeated for every application. So if it's implication of the CT interface, now it's about finding where the TCTR applications are located. And we iterate that process for every element. So CT, the BT, as well as the other signals available in the device, such as the time synchronization signal also required in the application and now matched by the device. Once that mapping is done for every signal, so either the signal exists by default and you just want one mapping, or it's about creating the required functionalities and signals into the device, we can export that process ICD file. With a new option containing now the process ICDs that will inform what are the data supported by the device, how was the original name in our output by default, and how is the signal in the specification. So we can do it that way to keep exactly the same names in our device by default, or if the purpose from the utilities to go towards a unified data model and more interchangeability of no deviations at all, we have the possibility to import the data model changes into the device to take over
 all the logical devices logical nodes and prefixes and suffixes into our data model this is what we do now to import that model and we will see that now that merging unit and device is containing exactly the name as required the circuit breaker is called cbxcbr1 the hms1 was called hms0 by default has been changed and therefore reducing or avoiding as much as possible the deviations between the specified data model so now that the device is exactly or as close as possible from the specification I export again the icd file as process icd and I hand that over to the next step of the application the same process can be repeated then for the protection device for the protection device I just select the protection icd file and for the protection icd file same procedure I have there a distance protection function with a first zone I select the device where that function is realized and that functionality is realized as stated in the distance protection function I map all the signals the same way add up the data model again and export the process icd file that I can hand over to Yannick for the next step of the presentation okay thanks Cedric Yannick I'm gonna go ahead and make you presenter okay you should have the screen yes can you see my screen yes okay let's move on the next step and the final step for me today where we receive the feedback file from the id manufacturer the process icd file just just mentioned by Cedric we will use the process icd file provided by all the manufacturer in the substation and all the 90-30 information contained on it to implement our specification finally we will be able to generate an scd file which contains the complete substation configuration the control block the data set the x12 and all this that fits our specification and the id manufacturer id this file can then be exported again to the manufacturer to to be loaded in the id itself and these are the slides showed you just the process of the importation of the process icd file I will demonstrate this in the tools in couple of minutes but basically you import the process icd file from the manufacturer you create an instance of the of this id because it's the process icd it's still a template then you need to create an instance you use this instance in the the allocation role you implement one allocation role with the with the instance and then the system automatize for you the implementation of the logical node based on the information stored in the process icd file and this information is
 in the process ICD file with all the steps that was done just before by Cedric. And the system will also automatize the usage of the letter binding because the information of the letter binding that was demonstrated just before also by Cedric is also stored on the process ICD file. Then we can automatically implement the source ref element which was the specification inputs that we create just in the substation specification part, the previous part. And the system gives you also information if in this case, no more the case with the latest file, but in one file we have one signal which was not mapped in the process ICD and then the system gives you also information that this signal is, for example, missing in the process ICD, then you have some feedback. As soon as you have implemented everything, that means all the LNode is implemented with logical node from ID, all the DOS, the data object specification and data attribute specification are implemented, every input, every source ref is resolved with xref of the ID, all the process resource is resolved, you can then generate the communication that will build and or configure data set and control block and then will create xref or complete pre-existing xref in the ID and you see here in the dialogue and we will see this later in the demo that we provide a complete log dialogue which allow you to understand what's happening. It's all the step but I will show you all this step in a few minutes. Finally, you can then export the ICD file for the manufacturer. Then I will go on the demo. We have still the same project, the project we use to export the ISD file and the first step is to import the process ICD file, then I go on the import button here and I have received my two process ICD from Siemens, one is for the merging unit for the PIU and one is for the protection. I will import the merging unit and I will import the second one, the protection. Okay, then as soon as I have imported these two files, if I go on the system diagram here. Sorry, I need to restart the system diagram, I think it was already open. Demo effect and here on the pallet, you see the both process ICD file, one for the merging unit and one for the protection. I can then create instance of this template. I will name it, this one is the merging unit, I will name it Siemens PIU and I will create an instance of the protection and I will name it Siemens protection. I would then just connect this to ID to the subnet, just
 to make it proper, I'm sorry. Okay, then if I come back to my bear, it's currently implemented with my specification ID, I will virtualize everything. It's no more virtual and I will implement the PIO in this bear with the Siemens PIO. And I will implement the protection with the Siemens protection. And when I implement my specification with a process ICD-5, with an ID generated by your process ICD-5, the tools will read the information, the 19-30 and 7-6-100 namespace information inside the process ICD-5 and will automatically see, I don't know if you see on the screen, but on each logical node here, we have, if we are on the virtual step, we have an open icon. And when we are implemented, we have now a closed icon. That means this logical node is now implemented and it's implemented with the information given by the process ICD-5, given by the job done by Cedric just before. And then we can also, I can also show you, for example, in the PIO here, you see all the structure defined in the specification and the structure where it is implemented. And if I see you, this example here, for example, we have all the logical node in the PIO global is implemented within a logical device PIO global. In this ID, except that this one, because probably in this process ICD-5, the step done by Cedric just before, where it's defined the same logical device structure was not done for this one. Then you can quickly also see if your specification and your implementation is fitting one-to-one. As I said before, we have on, for example, on the XCBR here, we have some inputs predefined. And here, for example, two of these inputs, the open, close and operation close are resolved with real xref in the ID. This was done by Cedric just before. It has created some xref for these two specification input to source ref, and we can see here that these both xref are now correctly implemented. These both source refs are now correctly implemented with xref from the ID. There was some other one maybe created just after the exchange of the file, which are still open. Then when we do all this job, we can then generate the communication. Just before generate the communication, I will show you the communication editor where you will see, okay, we have two ID, the Siemens PIU and protection, but we have no data set in this ID. If I show you the matrix of GOOS, we have no GOOS message and we have no subscription to GOOS. If I go also in the input representation of our SCD file, for example, in the PIU, we have
 28x12 predefined, but this subscription is not currently connected to a publisher whose control block. Then I will generate the communication based on the information in our specification or the exchange that we have defined in the application. We will generate communications. That means we will generate the data set, the control block, and generate or use the XREF. And we click on Process here. And we will see a complete log. Here you see a red cross. Red cross is not something that doesn't work. Maybe there is some error. And we will go inside the error and see why there is some error. Here you can quickly see that there is some function in our specification which is using the communication, but it is currently not assigned to a device. And currently, all these functions are the process resource, which are not fully resolved in our example. Because in our example, we have instantiated only the P21 application and the CB application. But for example, we don't have instantiated the VT application and the CT application. And of course, the P21 application is pending on some CT and VT signal, which are not yet resolved. And that is the reason why we have this error. But if I show you the result, for example, for the Sinance BIU, we publish some goals. If I go a little bit down in this dialogue, you will see we have some application. We have the CB application, which extends some goals from the Sinance BIU. And in this application, we will extend the position of the breaker, for example. And we will implement this. And to implement this, we have created a control block, the GCBF1. And in the GCBF1, we are inside the data set. We have generated the data set in the CD file. And inside the data set, we have put the position. And we have prepared the subscription in the protection ID. And for the Sinance BIU, we also subscribe to some goals. And for example, here you see that we reuse two existing XREF, the operation close and the operation open, which was information in the process ICD file that we have used. And for some other one, we just create the XREF because the XREF was not predefined in the process ICD. And then if I show you now the communication editor again, and if we go on the data set, then you see that for the Sinance BIU, we have now some goose control block created with the position of the breaker. And on the protection, you have some other goose control block with some signal, for example, the trip, which is coming for the P21 application, and which go to the BIU. If I show you the goose matrix now, we have some goose messages, which are published and subscribed by the author ID. And see this with the yes here. And if we go on the inputs side for the BIU,
 just before we have 28, but now we have two other XREF defined for this ID because it was not predefined in the process ICD and we have reused the XREF for the OP-CLOSE here and the XREF for the OP-OPEN. Here you see there is an error, probably I have made a mistake somewhere, but that's it. And then we can export. Finally, we can export the SCD file. To export the SCD file, you use the SCD file here. You can select the edition, edition one to 2.1. In this case, we will use 2.1. And we can then export the SCD file and the SCD file is now available on the bottom here and can be sent to Siemens in this case for the next step. And that's it. Okay, thank you, Yannick. And I believe I switched back to Cedric. All right. Make your presenter and you should have the screen. So now let's come to the last part of the presentation. What to do with that SCD file containing the data flow of the signals. So this is something that is common on the market. It's nothing new in that project that IIT configuration tools can import SCD files from different system configuration tools. At Siemens, we're having a system configuration tool, but we can also import SCD files coming from Yannick. So I will show how to import that SCD file and then generate the configuration and give you a sneak preview of testing possibilities of that configuration, yeah, either in a real device or today in a virtual device that is easy to show in such a virtual meeting using our simple XG-TelTwin. So for that, let's go to, again, RTC5 software. Remember earlier when we were importing those ISD file to generate the device configuration. Remember that we were having options to import the data model. And earlier I imported only the logical device, which you can know that object structure. Now it is about importing the communication flow, the report, the goods and sample values, or if it will be used, it could also import the protection parameters model test 16x50. So we can do either the import, yeah, that menu or using our 16x50 station, selecting the file to be imported and select the file provided earlier by Yannick. I've already imported it earlier. So if I show that into our own system configuration tool, you can see that, for example, the goods configuration has been imported, what is sent from the TIU, so the merging unit and device to the protection device, or what the protection device is sending back to the merging unit. So the import of ISD is just a single click. The validation can be done in our own system configuration tool or can be done then with a real device.
 or with a virtual copy of the device. This is what I will show you now. So for that, when the configuration engineering is finished to go to the simulation step, we can export individual devices or multiple devices in a so-called simulation format. And the simulation format is then something that we can simulate into our Simulacron twin where every Simulacron 5 device, no matter if it's merging into a protection device, distance protection, or any other overcome protection can be simulated. And here I'm simulating currently two devices or merging as we've been talking about at the beginning and the protection device. Both devices are running at the same time. And if I inject voltages and currents at the merging unit that have been transmitted via sample values to the protection device, you see that the values are received as a protection device. Here, this is a front display of our 7SA87 where we have the navigation possibilities that you know from the real device. And to validate that the values are transmitted, I can see in the display, if I increase the secondary values for the current, it's reflected the primary values are getting higher. And as some overcome protection has been configured, here it is showing, for example, that the sample values are transmitted and the threshold used for the overcome protection is correct. And that way I can test various protection applications. We have also the possibility to connect to that environment, to connect via a 16x50 client to verify the data model. And here to see, for example, that the model of the CT inputs with a required name, phase A, CCTR1, 2, 3, or the neutral has been taken over into the device or the name of the circuit breaker that was required to be CDXCDR1 is available in the environment, currently simulated with a position 1, 0. If I just simulate now a position open or a circuit breaker, it's switching from 1, 0 to 0, 1. So that way can validate the data model implemented in the device. We can validate the data generated by the device and the communication possibilities of transmitting data between emerging heat through a protection device. And with that, I'd like to hand over to Tomas to conclude our webinar and go to the Q&A. Okay, thanks, Cedric. Tomas, I'll go ahead and make you a presenter again to wrap it up. Yeah. Okay, so first of all, thank you to all the other presenters for giving a very clear presentation of the process and demonstrating many of the benefits that we can easily identify. I'm going to conclude with some of the...
 most important ones. First of all, user concept and the specification is now from the beginning of the process translated in a machine-readable format. We can specify many things, so data flow values, logical behavior specification, independent from the implementation afterwards. All of that can be simulated and tested before it gets implemented. This is one of the very big advantages is that you can now test what you have engineered before it gets implemented in a device, so many of potential errors can now be identified much earlier in the engineering process. Also, with all of these data in place in SEL, we can think about opportunities for creating automatically the documentation of the system based on that data, and not necessarily in diagrams, but more in a dynamic way where somebody can browse through the system based on all the SEL data that is available. As we have seen, the interaction with the IED configuration tool is being standardized, is being optimized, which results in less interpretation errors because the IED supplier does not need to dig anymore into many documents which a user provides him. He gets a machine-readable file which he can automatically interpretate in his tools and document what he thinks his device need to do in the same way, in the same type of file. There's more automation in the overall interaction, more quality, and also an improved interchangeability. When a user builds an ISD file, he does not build it specifically for a specific IED supplier. In general, it should be a generic IED specification which a user can then give to multiple IED suppliers and see from their responses what is the best quality offer for them. The system configuration, now if many of the engineering steps are being enhanced and additional effort is being put in place in the earlier steps, of course, a system configuration step can profit from that and more automatically create the system configuration. If we compare what we consider now as a top-down engineering process with a classical bottom-up process where you start from the capabilities of the IED and build your system with taking that into account, we believe that there is a much higher process efficiency here and an improved engineering lifecycle management possible because your concept from the beginning, again, is already machine processable, contains the necessary identifiers which you can trace down up to the system configuration. Now, what are the next steps? We're almost ready with 90-30 and 76. I've seen some of the questions also asking when these documents are becoming available. Currently, they are in the draft stage. They have been circulating inside Working Group 10 and now we're really in the very final stage before sending out the DTR. Probably somewhere, I would say, beginning of 2024, things will be circulating and then it's in the process of publication.
 somewhere I would say mid-2024 by the latest these technical reports will be published. They will also include SEL examples. So there will be SEL files attached to it. There will be the new namespace extension. So everything should be there. We try to explain things as good as possible. So that's for 2024. We also within ELEA now will be starting a software project to take the next step to get the tools for a final solution in order to be ready to do first projects in 2029-2030. But first of all we plan to start developing in 2025 in order to start developing our profile in 2026. Probably we will continue to do some piloting, some improvements just to get things ready. But that's more or less the time frame we are looking at. So 2024 start, 2026 starting to create function application templates, creating typicals, ID specifications. So we are ready for 2027 when we are going onto the market to purchase devices. And then normally around 2029-2030 our first project should be starting. That's it. I want to thank you all for listening. If there is some time, if you still have some time we can take a couple of minutes to address some of the most important questions. And also thank you to Christoph and Andreas for answering. I see we have a lot of questions so that's very good. I'm very happy with that. Let's take some of the ones that you have given a star so we can take a look at the most important ones. So yes, 90-3076 will be coming out as soon as possible. So I would say by the latest mid-2024 they shall be published. That's another one related to the publication. Yes, so indeed the signals, specifying the service of the signal. Indeed when you are specifying data flow you have the opportunity to specify the service of that data flow. But it's an optional thing. If you don't know yet where it shall be implemented in, if the sender and the receiver shall be implemented in the same device or not, you're going to have to implement it. You're not forced to specify the service, you can still keep things open. And inside 90-30 we give some recommendations on how to handle the service when functions are being implemented by the same or by different devices based on the service that was specified. So that's the thing that should be covered. So of course we are bringing this forward to working group 10. It was actually the biggest purpose of our strategy was not to specify something or create something and then push it to the standard. No, the goal was to develop together with the standard first the standard and then the solutions. And I think we are doing that. Okay IED name is
 template in SEL? Yeah, indeed. So if your template, if your SEL file is a template, ICD or ISD, ISD in the case it is a template of an IED specification, ICD if it's a template of an IED capability description, then the rule is to name the IED template in SEL. Then here a question about the ISDs indeed. Yeah, so the definition of the ISD is that it specifies a specific IED configuration with a specific IED data model. And of course, you can create as many ISD files as IEDs that you need. But while you specify it as a template, the IED is to reuse it in projects and instantiate it in projects there. So the goal is not to do for every project the same process, but to specify your IEDs in the beginning or in the asynchronous stage of the engineering where you create the library of ICDs, which you can then instantiate into different projects. What we do at Alia as well is create what we call cluster IED configurations. And a cluster means that it is an IED configuration, which you can use for several type of implementations, for several type of day typicals or several type of, yeah, depending on the context of the implementation. So there are many strategies to tackle this problem and limiting the amount of ISDs you need to manage and the amount of interfacing with the IED supplier. Okay, so this is about the mapping of every... Yeah, I think the answer I just gave on the previous question applies here as well. Okay. Yeah, and I think that's here replacing an existing IED vendor with a different one. I think that's one of the biggest purposes of this process is that these kind of things become more easy. And as I said, an ISD is vendor independent and you can easily give it to multiple IED suppliers and get multiple offers back. And by documenting the mapping of your specification onto the device, it makes it then in the system configuration more easy to replace an existing device with a new one, because it's based on the same ISD file originally. And because of the mapping information, because you give already in the ISD file, the information to the system configuration to how we need to implement that device onto the system. So it's something that should become more easy now with this process in place and with the new enhancements of the standard. Yes, the webinar is going to be available. All participants will receive a download link if I'm not mistaken, but perhaps you can elaborate on that. Yeah, so everyone that attended today will receive a follow-up email with a link to the recording. Also, once you leave the webinar, you will be presented with a survey. We really appreciate if you took about two minutes, three minutes to answer that survey and give us your feedback. And that's, that will also...
 include a link to the download as well. Okay, thank you, Beth. So, as Beth said, yeah, you will receive a survey. We will ask just a very few questions because we want to get a little bit of ideas of how interested people are in this process, how happy they are with what they have seen and how they think this will move forward inside your own organization. So, please help us with with that and please fill in the survey. And in general, I would like to thank you all. I think we are done. Beth, do you want to say something? I think we're ready to wrap up. I think we've addressed all the questions. And again, thanks everyone for attending today. Yeah, thank you and have a nice day or a nice evening. Take care. Bye-bye. Bye.


Video ID: iG5FWUjwOVE
My name is Beth Capellas, and I am the marketing manager here at Triangle Microworks. I'd like to welcome everyone to our latest webinar, Reduce Travel and Increase Productivity with SCADA Simulation Tools. We are expecting a very large crowd this morning, so we're going to give it another minute or two to let everyone join, so just sit tight, and we will begin momentarily. Okay, we still have a lot of people joining, but I would like to just start by going over a couple of housekeeping items while everyone continues to join. The webinar is scheduled to be one hour long, however, we typically get a very large amount of questions, so we are happy to stay on longer to answer as many questions as possible. With that said, I'd like to encourage questions throughout the webinar. If the audience is on mute, please submit your questions using the webinar menu under the section titled questions. You can start asking questions as soon as you have them. Don't feel like you have to wait until the end. We will begin trying to answer questions online. I do have a couple of engineers that are monitoring the questions coming in, and hopefully we'll be able to respond online as soon as possible. We will also stop a couple of times throughout the webinar to read and answer questions, and of course, there will be some time at the end that we will dedicate specifically to questions and answers, and again, we'll stay on past the hour timeframe to answer all the questions that come in. So with that said, I'd like to introduce our speaker, Matt Green. Matt is a senior applications engineer and has been with Triangle for over five years. He is also one of the main testers and an expert on distributed test manager. This is the tool that you'll be seeing here today. So with that, I'd like to go ahead and hand it over to Matt. Hello, everyone. As Beth said, my name is Matt Green, and I'm a senior applications engineer here at Triangle Microworks. I figured we'd start by talking a little bit about Triangle Microworks. We were established in 1994 in Raleigh, North Carolina, and we have been working in the protocol communication fields ever since. We have customers all around the world, including equipment manufacturers, system integrators, and electrical utilities. So we have a wide variety of products that we sell, including source code libraries for 60MP3, 61850, 6870101, 102, 103, and 104, TASI-2 or ICCP, and Modbus. You may also have heard about us, about our SCADA data gateway, which is our gateway application for protocol data mapping and concentrating. And then we also sell a collection of testing tools, like the 61850, TestSuite Pro, DTM, TestHarness, and Iron. As you all know, COVID-19 has really changed the landscape of how we're all doing business. These changes include forcing us to distance ourselves from our normal work environments, shipment cancellations or delays from our hardware vendors, and restrictions on our ability to travel. As a result, we've had to adapt new ways to get our work done without letting our productivity suffer. This is especially important since we are a critical industry that everyone relies on. So today we're going to be presenting our tool DTM to help maintain this productivity through this and all future challenges. DTM is a system-level simulation tool used for simulating devices and running tests on entire systems of devices.
 One of the largest concerns in our industry is that our deadlines will be missed and projects will be delayed due to unforeseen circumstances such as we are experiencing now. Whether this is a shipment delay from our hardware vendors or limited time with the hardware that we do have, not being able to use the hardware that you need is a big problem. This is where DTM comes in. DTM can be used to simulate these missing pieces of equipment and allow your project to progress. Another large concern in the industry is our restrictions on travel. Many of our customers work from the design phase of a system through its deployment and maintenance. This requires them to travel to their installed system and work through a series of test cases. DTM can be used to simulate this system and have the test cases run across the simulated system. This will allow you to discover problems earlier on in the process and help reduce risks during your deployment. This will also help your team become more familiar with these tests allowing them to return home quicker when they are required to travel. So let's take a look at DTM. So DTM stands for Distributive Test Manager. It allows for large networks of protocol devices to be simulated. This simulation can be done on one Windows machine or distributed across multiple computers to spread out the load and generate more realistic network traffic. I have already preloaded one of our finished workspaces to show an example of what a simulated system would look like once it is configured. But after we show that off, we will take a look at how to create and configure a workspace like this. On the left hand side here, you will see the workspace section. This is where everything that you create in DTM will be housed. This includes all of your protocol devices, any of your displays, your test managers, and any of your scripts. DTM can simulate both the controlling and controlled devices. This includes clients and masters, slaves, outstations, and servers. It supports DNP3, ICCP, TASI2, IEC 6870-101 and 104, 61850, and Modbus. In the middle here, you will see a single line diagram of the network that we are simulating in DTM. This is all placed on an in-site display, which could also be used to simulate an HMI or a SCADA display, a faceplate to an actual device, and actually has multiple other uses as well. The IEDs simulated in DTM are monitoring the electrical equipment in this diagram and their databases will reflect the value that are discerned by the state changes of these devices. We've also added a few buttons and checkboxes here to allow us to trigger events inside the system. And then on the far right hand side here, you will see our test manager. This allows for manual and automated testing of your network. Each row here is a single test and the results can be seen in the results column. The tests are defined inside of a javascript that you can see once you click the edit test script button. The test being configured here is really determined on what type of tests that you are doing and what your devices under test are. So I'm going to jump back to the PowerPoint now for a bit to explain a few test cases that DTM was designed for. This diagram illustrates the
 typical communication paths in a generic substation containing IEDs, potential PLCs, an internal HMI, a data concentrator, and your SCADA system. We will use this diagram to illustrate how DTM can automatically test each of these devices by simulating the other devices around it. The color of the box determines whether the device is simulated in DTM or the device is under test. A gray box indicates that the device is not required for this test scenario. In this test, we are simulating the inputs to the HMI and monitoring the responses from the HMI to make sure it is displaying the data properly. You can have a user simulate the data changes down here on your simulated devices and then check for the response at the HMI and report back to DTM for a pass or fail. Alternatively, you could feed a sequence of events log from your HMI back into DTM and then DTM can automatically verify that the HMI was displaying the proper data. We can also test a data concentrator or gateway. You can do this by simulating the same IEDs and PLCs in the downstream data, but also simulating the SCADA upstream that the data concentrator reports to. In this situation, since DTM is able to simulate both sides of the data concentrator, we are able to automatically change data downstream and verify the proper data is changed upstream. This allows us to do an automated test of the entire data concentrator. So we actually have an example of this in DTM, so let's jump back over there and take a look at that. So let's take a little bit closer look at this workspace section I was talking about before. So in this workspace that I have created, we are simulating the substation or IEDs in the network, the HMI and data concentrator, and then the SCADA. These are all folders, and the devices simulated are inside. So for the data concentrator test, we would go to the HMI folder, right click, manage, disable devices. That would disable all of the data concentrator inside of the HMI here, allowing us to plug in, say, our SCADA data gateway and run the automated tests on. We'll still be simulating the downstream IEDs and the upstream SCADA. So we'll look at this status test, which is what our automated gateway test is, and that would bring up this test manager here. And what you'll see is a variety of tests here that will go through and test all the inputs to the data concentrator and match that to the outputs. So what will happen is if I click this test, it will run through and change the value on the downstream device. Alternatively, you can go through and run these extra tests. So what's happening, if you look here at the description, is that the source of the data is this CSWI general. We change that automatically downstream. The value is then passed up to our SCADA data gateway, which is then mapped to a MD&P device, and then that is being pushed up to our SCADA system at the top. So once we have both forms of data, we can compare them together and see that the results passed and took about 100 milliseconds. And of course, you can go through and select multiple tests, run all
 the selected ones, run all of your tests, and have it all done automatically. After you have your test results that you are happy with, you can go down to export results and save that off as a text file, HTML, CSV, and XML. This will allow you to see test results and group all of your individual tests together. Let's look at a few other examples of testing. This is our SCADA point test. This is very similar to the data concentrator test, but now that we have verified that the data concentrator is working properly, we have now added in the physical SCADA system. This way we can test that the SCADA is displaying what is necessary being passed up from the data concentrator. An alternative test for this would be if we just simulated the data concentrator in DTM, everything below would no longer be necessary to simulate in DTM as we just really care about the outputs from the data concentrator up to the SCADA. In this scenario, it is very similar to what we were doing with the data concentrator. We are simulating the source of all of the PLC's inputs and the recipients of the PLC's outputs. This way we can determine that the PLC is behaving properly in the system. In this testing scenario, we are simulating everything around a single specific IED. This will allow us to check that everything from basic protocol communication to making sure device behaviors are performing properly for all inputs in the system. Now that you have gone through and tested each of your devices, we are able to build up a full network. What you can do is you can go in and disable simulated devices one by one and then replace them with physical devices. During this process, you can monitor the health of your network and run any of your automated or manual tests as you swap out devices. This should allow you to pinpoint which devices may not be behaving properly with other real devices. This is the last example I will be showing off today. In this situation, we are simulating the IED's and the PLC's in the network. Everything external facing is now physical, which allows people to run through practice scenarios on backup systems without any risk of affecting your actual IED's. This provides a perfect environment for operator training and for building user confidence when running actual tests in a network. Let's go back to DTM now. I am going to show off an IED test or a system wide test. That would be stored here under the substation testing, which is the test manager I have in this separate tab. In this situation, we have defined six different tests. We have a recloser situation, differential protection, zone 1 and zone 2 distance protection, a retrip, and a breaker failure. These are going to be more along behavioral testing rather than just a simple point verification. I am going to go ahead and reset our simulation. Everything should be set back to the normal situation. If I run this recloser test here, what we should see is that the recloser here has passed. What that means is that if you watch, I will run the test again, you will see over here that this breaker will open because a fault was detected and then it will close as the recloser tells it to close back. Keep your eye on this breaker right here.
 Hit open, and then it will close. And now we've gone through and monitored all of the signals in the system and verified that it was received. So to see something a little more dramatic, we could take a look at the breaker failure event. So the breaker failure is basically going to tell this breaker to open because there's a fault, but the breaker refuses to open. It's going to then cause all of the other breakers in the system to open up to prevent electrical flow. And as you'll see here, now this breaker is then opened along with these, and power has been cut off to the transformers below. This would be an example of an IED, IO point test, or a full system test as you swap in the physical devices. So let's take a look at the script. I'm not going to go into too much detail with this script as I don't want to bore you guys, but I can give you a basic overview of how it works. You at the very start define what your tests are, the name of the test and a brief description of what they are. Then you actually can go through and set up the specific test. So this is the recloser test. What you do at the very beginning is set up the initial conditions, and then you will wait for the specific signals you expect to see in the system if everything is proper. At the very end, if you receive all the signals that you're interested in, you then return with a successful test and a brief description of how long it took. That's what we did for example, but if something goes wrong, you can return a value of exactly what did go wrong to give your technicians the ability to troubleshoot more down as to what the problem was. So at this point, we're going to take our first break to answer some questions. So I will hand it back over to Beth. Okay. We do have a couple of questions that have come in. We've been answering several online as well. So let me go over to how can you use multiple computers with DTM? So yeah, that is something I alluded to earlier on. If you look over here in the network tab, what you'll see is I have my DTM network and only mGreen PC. That's just the machine I'm working on right now. But let's say I had a virtual machine or a laptop or something else. I can set that laptop or virtual machine up as what we call a DTM slave, and that machine can hook up directly to my mGreen PC, and only one of those machines need to be licensed. So that way, you can simulate devices not only on your master machine, my mGreen PC, but I could also delegate some of the workload out to, say, mGreen VM. Okay. Another question. If you are testing SCADA, can you simulate multiple sources of data, ICCP, DNP, et cetera, simultaneously? And there's a second part. Can you create a script with different events in the simulated data? Yes, absolutely. So if you are testing your SCADA system, you can mix and match all these protocols beneath it. So right in this example, we have a upstream, say, MDNP device. That's what our SCADA is, and that's going through a data concentrator. But of course, in your substation or multiple substations, you could have a variety of different protocols. As for scripting, yes, you can set up all sorts of events, regardless of protocol, to set that up. But later in the presentation, I'll show you different ways that you can set up the initial conditions, including scripting, but there are also a few other ways you could do that as well. Specifically, I'm thinking like a CSV file, where you go through and you set up the initial conditions for all your different devices at once, and you could step through different states.
 Okay, we've got another one here. As far as the substation testing setup, is there a library of predefined tests available for typical tests like those that are shown, or is there in-depth programming knowledge required? So when it comes to setting up these tests, it really depends on what your specific use case is. One thing that we've learned is that, for every customer we work with, their situation is similar, but not exactly identical to exactly what we've done before. So we generally will help you build up these tests if you need. As I kind of showed off earlier, it's not an extensive programming language that you have to spend months and months learning. It's just JavaScript. All of the support online that you can learn JavaScript with will be built straight into this, and there are a few hooks that we put directly into DTM, like getting points and setting values and things like that. But those are all pretty straightforward as well. So you don't have to have 10 years of programming experience to set up these types of tests. If you purchase DTM, that comes with one year of maintenance, and we're happy to help you get off on the right foot, learn what you need to learn, and make sure that your time with DTM is successful. Okay, and we'll take one more, and then I think get back into the presentation. Can I use DTM to run test automation of IEC 62050 servers and clients? Oh, yeah, absolutely. So the first thing is you'd obviously have to add your devices, which we'll be going over later on in the presentation. But yes, it covers servers and clients for 62050, Goose, sampled values. Everything you'd expect in 62050 is built right into DTM. After you're simulating all of your devices, that's when you can come through and you do your automated testing, whether that's through the test manager or CSV playback or anything along those type of lines. But yes, absolutely. Okay. And what I would say is keep the questions coming. We are continuing to answer them online as Matt continues on with the presentation, and we will stop again in a little bit for more questions and answers. But go ahead, Matt, if you want to continue on. Sure. So I've showed you the final workspace that we have set up for our substation testing. Of course, you can run any of the scenarios that we've already talked about in the PowerPoint in this by just going in and disabling the device under test that we would be testing, replacing that with a physical device, and running your test that way. I've showed off the data concentrated testing and the IED testing and showed how this workspace works. When you get DTM, you'll have this workspace built into your sample workspaces, but you're probably going to want to be able to create something more specific for your use case. So we're going to take a step back now. I'm going to close this workspace out, and then we're going to start from scratch. So if you need to, you could always come here, do a file save workspace. That way you could load it later. But I am just going to close it and start from scratch. So when you install DTM and you run it for the first time, this is what you're going to see. An empty workspace here. You're going to see your getting started page, which essentially will allow you to load our sample workspaces. There's instructional videos, demonstrations, all sorts of things that you can click on here, and then you'll have your DTM network. So let's start with just the basic, how do you create a device? Well, the first thing I would say is you probably want to go over to your workspace and add some folders to keep things organized. So you could right click, manage, add, and then put a folder in. And I'll name this DNP Masters.
 Then, the next thing that I'm going to do is, I'm going to add a outstations folder. The exact same way. I'm going to right click, manage, add, folder, and DNP outstation. So essentially, these are not mandatory. You do not need to do them. But when you have large sections of devices, you're simulating 20, 30, 40 of these things, it can get out of hand pretty quickly. So if I wanted to say, create now an outstation, instead of doing it at the workspace level, I can do it from the outstation. So I can right click here instead, manage, add, DNP, outstation. And this is going to bring up our configuration wizard. Instead of going through and doing each individual configuration at a time, we've put them all in a series of tabs. The very first tab here is the device tab. And that's just really the global information. That is, where is my device going to be simulated? Again, to talk about the previous question that was asked, this is where you would determine whether it's running on your master machine or your slave machine. So my PC or my VM. So if you click this, you'll get a dropdown of all the machines in your DTM network. Currently all I have is in green PC, so that's what I'm going to choose. Then you've got a name of the device. By default, we just put in SDNP. But you can go in here and change this to whatever device you're simulating. The second tab is the channel tab. This is where you can come in and basically outline the behavior of this. By default, since you added a DNP outstation, we've set up slave, and TCP, and server. You could do serial. You could do UDP. You can change all these configurations. But the majority of the time when you create a slave DNP, this is what the user would like to see. We've also set it up so by default, the local address is 0.0.0, and the remote address is star.star.star. Essentially, this outstation is saying, if there's any master out there that knows where I am and wants to talk to me, let him talk to me. But you might want to be more specific than that. So you can come in here and do a dropdown and select any of the IP addresses associated with your computer. You can also come in here if you wanted to and say type in a different IP address that's going to be more realistic to your system, and then click assign IP address. This will allow you to actually assign this IP address to your network adapter. And then after your workspace is closed, it will be taken off of your network adapter. So that way, you don't just have a bunch of floating IP addresses around. But be careful. This is more than just a simulation. If you put an IP address that your coworker next to you has, there will be IP conflicts. So for the sake of this, I'm just going to select the default IP address that I have. And put that as the remote address as well. For DNP3, the port number is 20,000. But because this is a testing tool and you may not have a whole laundry list of IP addresses assigned to your network card, we do allow you to increase, decrease, or set the port number to whatever you would like. The next tab is the sessions tab. And this is going to be where the protocol-specific configuration options are. So the ones that are most likely interacted with, we've put on this tab here. This includes for DNP a link layer addressing, so your source and destination, whether you want to send unsolicited messages and secure authentication. That being said, if there's something that you need to simulate your specific device and it's not in the three previous tabs we've been talking about, there is an advanced tab. And this is where you're going to find everything, all the nuts and bolts of what you need to configure your device. Any sort of frame sizes, network type, link retries, anything like that is going to be buried in here. If there's anything that you need to simulate DNP3, it should be in here. If it isn't, send us a message and we can definitely
 get it in here. Let's take one step back to the database. This is what a lot of your device configuration will be related on. What that means is that this is going to be all of the points that your devices are monitoring. By default we throw in basically 40 of each type. It gives you something to play around with and learn with. That way you can go in and figure out how the binary inputs work, the double bits, the analogs, anything along those lines. But if you're creating a specific device, you can always go in, reset your database, and then add them back individually. Say you need three binary inputs and you need five analog inputs. Now I've gone ahead and created this database with the values that I specifically need. If you're going to create multiple of these similar databases, you can export the database to a CSV file. Then if you need to create this device again later, import it from that CSV file. You can also, if you have a CSV file already pre-set up with what the database is going to look like, you can either immediately import it from your design team or potentially write up a small script or something that will translate it in a way that will be easily imported. That way you don't have to go through this manual process. I'm going to put this back to our default database. At this point I have my device configured, the channel configured, the protocol specific configuration options done, and the database setup. If I hit okay, it would go ahead and create my DNP outstation. The last tab here is the multiple devices tab. Once you go to this, you'll see a radio button. When you click that, it will gray out the previous tabs. Essentially why it does that is it's taking all the configuration options you did for this one specific device and it's throwing them right here in this table. It's saying you want to create multiple devices. You want to create it with the same configuration that you've done in the previous tabs. Now I can increase or decrease these devices as I need to. We do need to keep these devices unique so they won't all run over each other on your configuration options. We've done two things. We've changed the device name, that way you have unique names, and we've increased the port number. You can still come in here and click on any of these configuration options and get a quick update for that field. If you did have, say, three different IP addresses, you could change them to your three unique IP addresses, change the ports all back to 20,000, and you're good to go. I'll hit okay. What you'll see is the devices are now thrown into this workspace, and you'll see that they have these X's over them. That means they're not fully started up, but now the X is gone. They have fully started up. One more thing I wanted to mention is that you'll see that these orange kind of network indicators are next to each of the devices, and what that means is that the device is up and ready to connect to something, but it has not connected to anything yet. Let's go through that same process and create the masters. Right-click, manage, add, DNP, masters. What you'll notice is this is the exact same thing. The S is now an M since we're creating a master. The same thing about choosing what PC you want to run it on. You've still got your IP addresses here that you can set, and create that just so they match. You've got your session information. One thing that will be different is the database because by default we're going to go ahead and have our masters go out and talk to our outstations to figure out what their database is, but you could add them here as well, and advanced is still here for specific master and DNP configurations, and then we've still got the same old place. It's exactly the same.
 click create masters, go up to three, you will see this is all incremented in the same way. If I hit okay, you will see that the masters have now been generated for the workspace and all of the connection indicators have turned green. That means that not only is your outstation out there but it is also communicating directly with the master. This would be exactly the same process if your device under test was your outstations. You could just plug your outstations into a router or switch that eventually gets led to your computer. You could just simulate the masters in DTM. You would see your masters go green because they are connected to your outstations and you would be ready to run your tests. That is how you do DNP3. This process is almost identical for 101, 104, and Modbus. That leaves ICCP and 61850 out. Let's talk about how you can make 61850 devices. I will right-click and add another folder. I will call this 61850 servers. The same process, right-click, manage, add, 61850 servers. What you will see is that there are far less tabs this time. We still have our friendly advanced tab with all of the 61850 configurations. Because 61850 is built up around an SCL file where all of the configuration is already done, we have allowed you to import that file and it will automatically configure the device as needed. You select your SCL file and import the configuration. Then let me go to the file. Then you grab your SCD file. From here, I hit okay. It will fill in the information here. Since this is an SCD file, it is the entire system of your 61850 devices. Underneath IED to load, you will get a drop-down and you can select all of the IEDs inside of that. I will select line one bay controller. Here is where you can select your goose adapter if you are using goose. Sampled values is in the advanced tab because we wanted to make sure if you are doing sampled values and bogging down your system, you did not just install DTM and put it on your corporate network and there is a bunch of sampled values being shot around and delaying your network. We made it a little bit more difficult to get the sampled values but it is in the advanced tab as well to make sure you know what you are doing when you enable it. If I hit okay now, what you will see is line one bay controller has been created inside of a folder. If you wanted to create every single device inside of the SCD file going through and doing this process over and over, it could be cumbersome. We do have file import substation and that will bring up the same type of window we saw before. If you don't have your SCD file selected, you can import it like I just showed you but since I have already created one device out of this file and imported it into the workspace, it already knows about it. I can click here and hit okay. This will bring up our import substation window. What this will allow us to do is quickly import all of the devices inside of the SCD file. This import substation can be run in three different modes. The first mode is SCL file restrict. You have the file from your engineers. They say it is in tip top perfection state and you need to test exactly what is in the file. You are not going to have the ability to change IP addresses and port numbers and subnet masks. The only option you can change in here is the hosts. That is because that is a DTM specific requirement. What machine is it running on? You can click the host and then you can click the host and
 and then do a shift click to highlight everything, go down to the multi-editor, and then select my machine as where it's going to be and apply that. Now, if I went ahead and say click import all, it would take all of the importable devices and throw them into my workspace. So what are the other modes that we can run in? There's the user-defined mode, and essentially what that is is allowing you to define whatever you want. You can change the IP addresses, you can change the port numbers. Everything is up to your configuration, allowing you to not have IP conflicts with other people in the network or things like that. There's also loopback addressing, and when you do loopback addressing, it puts an IP address that you already have on your computer on all of the devices. That way, you can very quickly click this button, click import all, and be done with it. So when you're importing these devices, you can look at the importable column here and see exactly what is able to be imported, and you'll see everything says yes except for this one right here, and that's line 1bc because I've already brought that device over from the previous configuration. So you can come here and say multi-select the ones that you're interested and drag those over, and that'll go ahead and generate those devices, or of course, you can just click import all and get everything. So now I have 61850 and DNP in the same workspace. It allows you to mix and match these protocols to build up a more realistic system, or allows you to test a more complex device such as an HMI or a data concentrator. To see how you can automatically test HMIs or data concentrators, feel free to check out our previous webinar, and the link to that will be provided at a follow-up email. So now we've got our devices simulated. What we want to do now is create a single line diagram. So you right click on the workspace, or anywhere in the workspace, and do insight, which is where our displays are stored, and do add display. I'll name this single line. And what that will do is it'll go right here in this middle section and bring it up in an edit mode. So what are some things that we're going to need? Well, we're probably going to need a bus bar. I'll pull that over. This is our palette where all of our visual elements are. Probably going to want a breaker. Switch both sides of the breaker. Probably want a current meter, and then some load down here at the bottom. So I have now just visually created a very, very simple one line, or a bay, or something like that. So I can come in here and then connect everything up. But what you'll notice is that these icons are not actually connected to anything in the simulation. So we're going to want to do that next. So how we can do that is we can display the databases that I was configuring before. So I'll come over here to this outstation, and I will do a right click, show database. And this will bring up all of the databases that we configured. You'll see it's just the 40 of each. So I will drag this over, put it right next to our single line. Readjust this so you can see it a little better.
 Now, what each of these icons have is if you right click on them, you can do a property. If you do show properties, you will see over where the palette was, we now have a property editor. You can use any of the values inside of your outstation to reflect its state in these properties. The ones that I will be working with today are the state, which is essentially if it is open or closed, and the right tag name, meaning if the state changes, it gets written back to this point. You can just take the point you are interested in and drag it directly over. What you will see now, if I do show points again, is that both the state and the right tag name have changed to reflect back to this T1P0 of that DNP. Let's go ahead and do the same thing with the second switch and the breaker. Now, all three of these points are connected to the single line diagram being displayed. If I come over here to this value, if I click the value off, it will change this one right here that I drug it to. Now it is true, should be able to go through, follow the same process. Now our outstation is monitoring the electrical devices in our single line diagram. Now we have a way to visually see what is going on in our network. Let's now talk about how we can change values. As you just saw, I was going in, I opened the database, I manually changed it just by clicking on the value and changing that. You can also say go down to these analog values, come in here, change those, whatever you want. There are all sorts of ways you can interact with it manually. Sometimes there are just too many points or there are too many devices you are trying to change at once. There are a few other ways that you can show that off. The first is very simple. It is autodata changes. This is essentially what you are going to use if you just want to do traffic. You right click on your device that you are interested in, you do simulate, autodata changes. What that is going to do is look through all the default points that you have inside of your device and that is going to be for DNP, class 1, 2, 3 and all events. There are 202 points inside of this that autodata changes thinks you are likely to want to change. Of course you can open this up, you can remove all the points, you can remove out single points by dragging it here to the trash can. You can grab points specifically from your device and drag it into the autodata change. It is completely configurable. We are just trying to make this as streamlined as possible. You can give it a name, say if it is for a specific test. You can change the interval. I normally do two seconds. You can change the percentage of these points. If you have 200 points being changed and you only want 100 to change per interval, you put that to 50 and you are going to get 50% of those points changing every two seconds. Now when I hit okay, you will see a little hammer has appeared right here on the workspace. What that allows you to do is you can interact with it. If you just want to change the values once, you can right click and change values once. You will see all the values out here have changed that were in those 202 points. If you want to start playback at that periodic rate, you can click start autodata changes. What you will see is every two seconds, the values of everything will change. That is how you just create kind of a default load on your network. Just wanting to toggle points, increase analog values, just kind of generate traffic. You can also be more specific with
 this, if you want to set up certain states, you can use what is called a CSV simulation file. You just right-click, simulate, CSV simulation file, and it will import that. What that is going to do is bring in this simulation file, which is CSV file. If I hit okay now, it's going to bring up the configuration window for this. Essentially, it's going to ask you a few questions. How do you want to describe it in case it's for a specific test? When the simulation has fully gone through every state, do you want it to loop back again at the start? I'll do that. How long do you want to wait? I'll just do two seconds. And then when you are doing periodic playback, how often do you want it to change? Again, I'll put that at two seconds. Of course, if you just wanted to run through a series of steps and then stop, you would not put restart playback when complete, and then you would monitor for the specific responses you expect your system to have. When I hit okay now, we can do a right-click, show data points, and what that's going to do is this is going to show the data points inside of this outstation that this CSV file pertains to. It's going to be right here in these values right here. Now, if I right-click and say apply state, I can select one of the four states that I have. I click state one, click apply, and we'll see that all of these values have now changed. You can go to state two, apply, again, all of these states have changed. Similar to the auto data changes, you could then do start playback, and you'll see that they are going to every two seconds cycle through each of these states, and then if I configure it properly, it should start back over. That's how you do playback for CSV. This is how we envision you setting up initial conditions or scenarios for a test. You want to have these three things happen and make sure that your system responds properly. The last thing that we're going to be looking at is scripting, and I've showed you that a little bit today, but I wanted to give you a little more into how it's created. You right-click, insight, and then add a script. From here, since I don't have the script outside of DTM, though you could write it up outside, I'm just going to name it test. Now, this is a JavaScript fully built into DTM, meaning anything that you can do with JavaScript outside of DTM, you can do in here. This includes conditionals, this includes loops, state machines, anything along those type of lines, you're going to be able to do in here, importing files, any of that type of stuff, but we have added a few other functionalities to it to help support your DTM interaction. For one, you can grab some of your points and drag them specifically over. This will give you a variable that links directly to that, so I can say binary input one, and now I could go through and say binary input one equals on. Then if I run this, this value will go through and press it, or you could do if binary input one, and then come down and do a conditional based on that. It's just a script editor. This is how we do a lot of our behavioral testing. This is how we do some of our test manager scripts like I showed you before. This is all just built straight in. If you need assistance or you're not super strong in scripting, this is absolutely something that we help out with. I'm not going to save those changes off. That is how you create your devices, how you display your values, and how you go about changing those values. I think we're going to take a second break here to answer some more questions.
 And then we will get back to the PowerPoint. So Beth? Okay. We definitely have the questions pouring in. Hopefully you guys are seeing the online answers as well. Let me go ahead and give you some math. First one, can we import the previously existing CID files for many devices here? Yes, absolutely. So today I only showed off the SCD file because I wanted to show off the selector and the import substation. The SCL file, when you import, it does not have to be an SCD file. I know I'm using a lot of three-letter acronyms, but you can do any type of SCL file. That's IIDs, ICDs, CIDs, anything along your 61850 needs, you can import through there. Okay, next question. How many outstations and master stations can be simulated using DTM? So that is definitely a good question. We have customers who regularly simulate hundreds of devices on their workspaces, though some customers only do 5, 10, 20. It really depends on what their case is. For example, my personal laptop that's about seven years old can simulate about 100 devices with about 70,000 points total when the auto data changes are changing about every second. But of course, this is going to be really dependent on what your use case is. We do have some customers who do up to 1,000 devices, and we're always happy to outline what requirements you need, what type of PC you'll need, RAM, CPU, anything along those type of lines, and give you a good estimate on what DTM's going to need to run certain simulations. Okay, next one I have, and you might have already covered this in the demo itself. Can I add multiple IP addresses with the same port number? This needs to configure under multiple devices to manually, or is it automatic? He's asking specifically for your DMP, but I don't know if it works across all protocols. Yeah, so it sounds like you're probably getting closer to deployment with what you're working on, and the answer is yes. That's the whole reason we added the add IP address function to DTM. If you're getting close to deployment, or you want to use your realistic IP addresses, you can go ahead and add the entire subnet of IP addresses that you need directly to your machine, and all use the same port, because you're talking off a different IP address. The only thing you don't want to do is have the same configuration options from top to bottom, IP address, port number, session, any of those type of things. If they're all identical, you're going to have IP conflicts, but if you add a bunch of IP addresses to your machine, or you already have them, you can absolutely put them all on port 2000, or 20,000. We've gotten a couple of questions along these lines, so I'm going to sum it up, I think, a little bit more. Someone's asking, does DTM functionality cover the facilities of protocol test harness? Again, there's been a couple of questions coming in asking about really the differences between DTM and test harness. It sounds like a bunch of you already have test harness. What does DTM do differently? Test harness is normally considered to be used as a tool that will help you configure, say, your single device. You can run your conformance tests through test harness. There's no conformance tests in DTM. DTM is really when you want to simulate multiple devices at the same time, if you want to do behavioral system tests, if you want to do any of the things that we showed in the PowerPoint earlier. It's not normally for a one-to-one test. You could use DTM to simulate, say, your one DNP master, and you have your physical one DNP outstation. It's a little bit overkill, and that's where test harness really shines. They do similar things, but they definitely have different use cases in the field.
 Okay, does DTM have readily available test scripts for 61850 and Modbus? So if you're talking about test scripts being the test manager, we have a few examples built into the sample workspaces that you can go and see how we went and set things up. Specifically we're talking about the gateway one that I showed off very heavily in the previous webinar, and you can absolutely go in there and learn how those test scripts are done and apply the same ideology to DNP or any of the other protocols. And then we have a sample workspace for, well, multiple sample workspaces for all the different protocols where you can go in and see examples of CSV files, of scripting, of controls, of everything that you kind of need to know to get a good start there. And as I've mentioned probably a few times in this webinar, we really love to see how customers want to use our applications and figure out what their use cases are and how we can better design our tool to meet those needs. So we love working with people. So if you need an example of how you could perform X, Y, or Z test, we're absolutely ready to sit down with you and talk to you about it and provide you samples, examples, or potentially the workspace that you might need to get started. So don't let scripting scare you off. Okay. Okay. With that, we've got about five minutes left of our scheduled time. So Matt, I'll hand it back over to you. And then once we wrap up, we can certainly go back to questions and answers. Sure. So I'm just going to jump back over to the PowerPoint. I think we're just going to do kind of the summation of what we've talked about so far. Today I've shown you how DTM can simulate your missing hardware and how it can be used to simulate protocol traffic in your network or even be used to simulate device behavior inside of a network. We've also showed you how your team could be more confident in your test sets. DTM provides you with a safe environment to learn tests. It helps you diagnose problems ahead of time and hopefully reduce time while on-site when you are forced to travel. This webinar will be posted to our GoToWebinar page and also our YouTube channel. This way you can share the webinar with others who may not have been able to attend today or potentially to review information down the line if you decide to pick up DTM or you need a reminder on what we talked about. We also have some documents about DTM, which you can download directly from the GoToWebinar app that we have open right now. And as Beth has mentioned, we're going to stay on air after the webinar to continue answering questions or any specific needs that you may have. But if you need to jump off, there's any unanswered questions, things that haven't really been addressed and you have to go, you have another meeting or something coming up, feel free to send any of your questions to sales at trianglemicroworks.com. Even if it's not sales related, we're just trying to group them all in the same place. If it needs to be forwarded on to me or any of the other engineers here, we absolutely want to make sure everyone gets all of their questions answered before we leave today or at least have some sort of response to your questions if you do have to duck out. And with that, I think we're kind of going to open up just to more questions and see how things kind of continue to trickle in. Okay. Yeah, so just as Matt said, I would encourage you guys to still continue to ask questions online. Again, they are being answered by the engineers that are monitoring right now. I'm going to start to go through, we've got a little bit of a list here, but as long as they keep coming in, we'll keep answering them. And then Matt, just let me know if you want to throw it over to someone else, one of the other panelists that we've got answering them online. So the first one I'll start with here is, is it possible to capture data packets while doing tests?
 Yeah so we have a built-in protocol analyzer and what you can do is you can open this protocol analyzer and actually see what values that DTM is sending and receiving and it's all scope based so if you do it at a folder you're gonna see all the protocol traffic going in and out of the devices in that folder. If you do it at the top level workspace you're gonna see everything that that workspace is generating. So that way you can see all of this stuff you can log that information you can export it and have access to all of the protocol traffic that you're seeing during your tests. Can we directly, oh I'm sorry one second, do you have a template slash model scripts for objects such as pumps and valves? Not yet we are working on some generic behavioral scripts for some of our protocols. It's not officially out in DTM right now but we will be posting these out eventually and allowing people to kind of adapt how we've done our kind of behavior in a substation. But again if there is a specific test or behavior or something contact us we can try to get you something maybe either ahead of time or something specific for your test case. And this one I believe is covered in the demo but just to make sure everyone understands, can we simulate 62850 servers? Yeah absolutely. Hundreds of servers depending on what your use case is. You can have you know only servers, only clients, you can have a mixture of the two. I'd say about half of this, I'm sure this question was asked a little early on, but I'm about half of this webinar in the workspaces that I was working on was 62850. It is definitely one of our popular protocols. Yeah that was a recent one that's why I wanted to make sure we kind of hit that message. Can we check interlocks in between substation elements in IEC 62850 that we are normally giving using DCU? Yeah so we that's why I added the two switches on the side of the breakers. We actually have some interlocking code written into that the very first finalized workspace I showed you. So interlocking can absolutely be monitored and tested there as well. It all just comes down to what kind of behavior and and scripting you have behind your simulation. And I've got a question here that I can go ahead and answer. It's actually come up a couple of times and I've tried to answer online. I've got a bunch of folks asking if there's an evaluation license that you can download for DTM to try it out yourself. And yes there absolutely is. You can go to our website www.trianglemicroworks.com and go under login or go under the DTM product page and you will be able to download a 21-day eval so you can start to play with DTM and try it out yourself. If you are working on a VM please email sales at trianglemicroworks directly and we can get you a key. The download will work on your PC without any you know interaction with Triangle but again I need to give you a key if you're working on a VM. Okay so next question. Does DTM support work with virtual port training and training purposes? Can you repeat that one more time? Does DTM support work with virtual ports and training purposes? Not quite sure. Yeah um let's pass that one over to Dave to see if he has a little more insight on what that means. We definitely have configurable ports and it's definitely used in training scenarios but I'm not quite sure what the questions asking directly.
 Dave, are you on the line with us? He is there and I'm attempting to unmute him. There you go. I'm not sure I understand the question either. I mean, certainly with DTM you can configure any protocol on any address supported by the machine and then any port number, so you can have a hundred servers all on the same address with different port numbers or you can have different addresses in the same port number. It doesn't matter. As Matt pointed out before, as long as those two values aren't the same for any two, you're all set. I don't know what the term virtual port would be. I mean, assuming it's some variation of that, but I've never heard that term before. Yeah, so if we didn't address your question or answer it properly, maybe just shoot us an email with more specifics for it. I would assume if it's just kind of a networking term, it's absolutely something that we cover. It's probably something that we've discussed and like Dave said, use different terminology for. But if that didn't clear up the question, feel free to shoot us an email and we'll look into it and get back to you. Okay, next question. Can you record behavior answers from a real IAD and then save and reproduce them with the DTM simulator? That is definitely something that we are adding into DTM where you can monitor the responses of a physical device in the hopes to replay that behavior later. But currently that's not built straight into DTM. So I think that that should be on the horizon coming up soon. But you could absolutely script up the behavior, monitor the behavior of the device, figure out what it did, and then script that up inside of DTM and perform that. That can be done through scripting or through CSV playback. Is it possible to work with real IADs in the simulator? Oh, yeah. So I may or should have mentioned this earlier on, but when you simulate a device inside of DTM, the external world has no idea that DTM even exists. It thinks there's an actual device sitting right there on your network, meaning if you send out a goose signal and you have another physical device inside your testing network that's subscribed to that goose signal, it's going to see that. Any sort of DNP, Modbus, 101104 protocol communication that gets sent out of your network adapter is going to go out into your network as if it is an absolute real device. There's no smoke and mirrors going on behind the scenes. As far as the world is concerned, these are real devices. So yes, absolutely. The whole idea is you simulate your whole network, and you put real devices in it, and you verify that the devices are working. So yes, absolutely. Is it possible to simulate triggers or pulses with a duration of milliseconds? Sometimes it's important to know if a scanner detects faults from relays. Yeah, so we are running on a Windows machine. So that means that we can only get down to the milliseconds level that Windows allows. But that is absolutely something that you could handle, again, within scripting. You could go in, set a value, and then say, wait 200 milliseconds, and then pull that value back down. And that's how you could generate your pulses. But again, if you start trying to get down into the really, really slivers of these milliseconds, you may have a little bit of difficulty, as Windows only gives us so much power.
 line animation in a single line diagram? I would say yes. I would have to know a little bit more what visuals they are looking at seeing. From the single line diagram I showed off today, I didn't necessarily point it out directly, but red essentially meant hot, energized, and green meant not energized. So when I did the breaker failure, you would see at the very bottom that the transformers were no longer energized. That is one of the types of animations we do. Depending on screen size and how well I was zoomed in, you could see the breakers opening and closing. When I did the DNP single line diagram, the whole background color was red and green just in case you can't see the little lines in there opening and closing. So yes, all sorts of properties are inside of each of those icons, and they can be tied to all sorts of different things, and their behavior can change based on that, and that's all animation. Can we conduct FAT testing using DTM, and how about time synchronization in DTM testing? If you are going to do a time sync, what you need to do, I think, and Dave can correct me if I'm wrong here, is to have whatever machine is simulating your devices sync up to whatever is the controlling time sync out in the network. So if you are sitting in a testing network at, say, a lab somewhere, you just sync up with whatever is in the lab, but if you go out on-site or anything like that, you are going to sync up with whatever master is there as well, and that should automatically cover time sync for you. Can we write TCL commands in DTM like we used to write in Test Harness? We are doing everything in JavaScript in DTM now, so we aren't doing, I think that's TCL, inside of DTM right now, just because it was really hot and popular when we released Test Harness, and now JavaScript seems to be what a lot of people are using, and seems really user-friendly. I mean, really user-friendly. It's something you can learn probably the basics of it in an afternoon. There's so many tutorials online and resources you can use. That being said, I think in the past we've had our JavaScript call TCL functions that we've previously written up, or other programming languages, so I'm not going to say it's impossible, but I would say, again, don't be scared of the scripting, contact us if you need help, if you need examples, and if you can do TCL, you can do this. Is it possible to do relay testing remotely through SCADA with DTM? Yeah, so it depends on, well, yes. It depends on where DTM is installed. You could have DTM installed on a desktop inside of a test lab, and you could absolutely remote desktop into that desktop, and run DTM as if you're sitting there, and be hooked up to the switches, the IEDs, anything in your lab, and run your tests, export them, and get them back to you any way you need to. So yeah, that's definitely something that's really powerful, especially in an environment we are now where some people aren't able to get into work, whether they are quarantined or at home or advised not to leave. Install DTM up on your machines at work, and remote into it, and you're good to go. Okay, I've got a little bit of a two-part question here, specific to 621A50. Does the SCD import require any cleansing before import, for example, removing private fields? And the second part to that is, does the 621A50 simulation
 properly simulate the server? Does it simulate multiple report control boxes and changes? Let me answer the second part because that is the easiest part to do first. If you bring in a 61850 server and it has multiple report control blocks, goose messages, any of this type of stuff, once you have all of the adapters configured and the device simulated, it will behave like you would expect it to if it was in physical hardware. That means sending goose messages out whenever those values change, receiving controls and commands that will respond properly to those. If values change, it will automatically push out the reports, so it will behave just like a 61850 server should. As for cleansing your files, we try our best to pull in any sort of SCD file or SCL file that we can. If it is incomplete and has missing fields or is not up to a certain standard, we may not be able to pull it in. That is when you would use something like navigator, another tool we have. Bring in your file, verify your configuration file, find any errors or major warnings that we have in there, fix that, and then we should be able to import it. We have customers that are 61850 experts and give us the most beautiful files. We have people who are really just starting out with 61850 and they are still trying to learn the ropes. We can help both of them. Is it possible to simulate analog values, meaning can we show power flow on the developed single lines and how the power flow may change during a break or failure event or load shed event? We have a few workspaces like that where we work directly with customers that show not just the breaker positions, but the voltages at each line, where it is getting its voltages from. There are several examples of things we have done with that. For this example, for the webinar, I was trying to keep it clean so you could see exactly what was going on as I built these things up, but do not think it is just binaries. Anything that you have inside of a simulated device, you can put onto a display, onto a single line diagram. You can write up scripts to monitor power flow, where the power is coming from, when it fluctuates, if breakers open, anything along that type of line with voltages and currents and things like that. I have had a couple of questions come in along these lines, so I am going to start with the first one, but it really segues into a little bit about our gateway, Matt. Do you have any products which translate 62050 MMS client to OPC, and does DTM have an OPC server feature? The first for does DTM have an OPC server, the answer to that is no, but that is something that we have been discussing and something that we are definitely highly considering. I think it will be a few releases from now, probably. To answer the first part of the question, I mentioned a few times throughout the webinar, just very briefly, we have an application called SCADA data gateway, that is our protocol mapping, it is a translator, a data concentrator, these are deployed into substations. This is not a test tool, this is an actual deployable application. Granted, you could use it as a test tool if that is what you needed just for something in your lab, but it is robust enough to be put into a substation. It has built-in OPC, UA, DA, A&E, it has client and server capabilities, so it has all of that stuff built into it, and so that is how we would recommend doing OPC.
 inside of gateway and again because we already have all this code backing up and supporting in in gateway we're hoping the transition to put OPC inside DTM will go smoothly does DTM support custom-made complex symbols like a base symbol that can be replicated so we have a ton of symbols and icons that you can put on these displays but we do have a section for import it's like JPEG or icon or import something and you can go out and select a icon or a symbol or something that you need and import that directly into your workspace and then reuse that so we do have this generic import section I think you can also import sounds and videos or anything along those lines as well so yes if there are communication problems between IEDs with the master is it possible to to determine the error yeah so one section I didn't really highlight in this webinar at the very bottom is an output and you can configure what shows up in this output but any sort of communication errors is going to be popped out down there so if you connect and then it's immediately rejected because you lost say someone ripped an Ethernet cable out of your computer or you lost wireless or whatever you're doing you'll see the the failure right there down in the output and in case things are going too fast there's too much going on at once you can't keep up with the output there's also a error log button that you can click and it will bring up all sorts of errors and then another tab is all the warnings so you can go back and actually watch what happened when the error happened and figure out oh the very first thing that happened that before all these error logs is I lost say my Ethernet cable or something and that had a trickling effect of all my devices disconnecting or whatnot so there are two ways that you can monitor that directly is DTM able to synchronize with NTP protocol during FAT NTP I'm assuming that's a time let's let's go back to Dave on this one is that is it's just time sync yeah network time protocol is NTP yeah Windows not DTM Windows supports NTP SNTP PTP basically most of the major time protocols so you can actually configure your window box to sync to those time servers if you have a typically people have a local time server running one of those protocols as their standard and you simply sync the Windows box to that time protocol and yes it's worse network time protocol the simple network time protocol or PTP how easy is it to move licensing from one machine to another timer temporarily or permanently so we didn't touch on licensing too much during the webinar so I can go in ahead and explain that a little bit when you purchase DTM there's a few configurable things for your license first is how many devices are you going to simulate and then the second major one is which protocols you're interested in another configuration option which is going to touch more directly onto your question is what type of license it is so if you've got a seat license that's specifically made for a desktop or a laptop or a VM and it's for that specific machine there's probably a way you can transfer it from one machine to another doing that but if you're going to be doing it frequently I'd recommend one of the other two options the other two options is you can have a network license which just means someone you have a network license and say you have two users on it the first two people who come in and decide they're going to work on DTM that day can start up DTM on their local machine there can
 to the network, they get a license, they finish their test, they close down DTM, the license gets relinquished back up to the network, and then another person can come in and get that as well, and you can have up to two users at once if you have two licenses, or one, or five, whatever you want. And then the other option is you could purchase a license dongle, which is like a little USB stick. I'm sure you have seen them before, and the license is strictly on that. So whatever machine that's plugged into is what's going to have that license. We do caution with dongles, though. There have been times where you lose the dongle, you lose the license. We try to help out with that as much as we can, but it can be scary to have them lost. So make sure you are very careful with your licensing, regardless of the way you decide to pick it up. Okay. How do you check responsiveness and latency? We did that today using the JavaScript. The JavaScript, you can monitor the initial time that your initial conditions were set up, and then after you get through the entire fleet of tests, whatever you are checking, you can then determine an end state and find the difference there. That's how I was getting the 100 milliseconds, 150 milliseconds I was showing with the gateway mapping tool. That's how I was getting the two seconds with the recloser test. So you can definitely do that via scripting super easily. And there's also in the output at the bottom timestamps associated with different actions that the system can perform, so you can obviously look at those timestamps as well, and then, of course, if you're inside your protocol, your values and stuff are going to have timestamps as well, so you should be able to go back and look at those as well as the protocol analyzer. So all over the place. There's multiple ways to do it. Can you use DTM as an actual SCADA program, or does it work with existing SCADA software already used in the network? The answer is yes. It's not something we specifically designed DTM to be used for, but absolutely it could be. You could create an insight display with all the values that you're most interested in, organize them as you need. You could have devices that connect up to different physical devices out in your network, but know that DTM was designed as a testing simulation tool, and that you might be able to do some things from DTM that other HMI or SCADA systems might frown upon. Okay. Can DTM be used to set up a real-world substation auto test? What hardware software in general would be needed? I think this question is asking, can you take DTM into a substation and run automated tests? I would recommend, especially if you're doing 61850, to look more into our Test Suite Pro application because that's more about diagnosing and monitoring what's happening in a real substation. There's a lot of extra features and tools built into it to really help show you what is happening. Again, DTM is more of a simulation tool, and it allows you to do some things that you may not want to do inside of a substation. For example, if you're in a physical substation and you have a 61850 server running, and you accidentally flip a bit and tell it to send a goose message out to open a breaker, like I said, the network doesn't know that's simulated unless you set the test or SIM bit or things like that. You could actually have breakers open or responses into your physical system that you didn't really expect. Again, DTM is kind of a test tool, but it could be used as that. Just be very careful.
 Is it a 70,000 database for all the 200 devices or one device supports 70,000 points? I think you're talking about when I was talking about my specific load test I did on my laptop. Again my laptop is seven years old. It's a good laptop. It's got a 2.4 gigahertz processor, it's got 16 gigs of RAM, it's a great computer. But when I was referring to the 70,000 points, I think that was across all 100 of the devices so I think they had 7,000 points that I was changing data with every second or something along those lines. But hopefully as you saw in the webinar, we've had some gigantic devices with thousands of points, hundreds of thousands of points inside of one device, but we've also had some workspaces that we've simulated where they had only, you know, 100 points in it. So it's all absolutely configurable and you can set that up however you need. And of course if you distribute the load of your workspace across multiple computers, then you're going to face a lot less load restrictions than if you try to simulate massive amount of points and data devices on one computer. Can we log all the events for 200 devices if we run for longer duration like three days? So yes, logging is definitely built into DTM. The problem runs about when you start running into resource issues. So it depends on, you know, all sorts of things. I mean, could you record every single, could you log every single point change every two milliseconds for an hour? Probably not. I mean, that could be a lot if you're changing data super, super fast like that. If you are doing more of a, you know, every 30 minutes we're doing a thing, we need to monitor it and we want DTM to run, well, absolutely you could do that for three days. So it really depends on what your computer can handle and what type of test you're running. So I will say yes tentatively. Okay, and this question has come up a couple of times asking about some different protocols. What about PROFIBUS DP simulation? Is that offered? So the protocols I talked about today are what's built into DTM. We're definitely open to looking into new protocols going forward and figuring out what the industry is using and what would help the majority of our customers. But currently it's 101, 104, DMP3, Modbus TASI 2, and 61850. Okay. Well, I think we've gotten through all the questions. If there are no other questions, I think we can go ahead and wrap it up. I want to thank everyone for joining us and hanging on even a little bit longer. We're happy to get so many questions and be able to answer them as well. There will be a recording sent out in a follow-up so you can review this again. also send out some additional information on DTM. And just if you, again to recap, you can definitely download an evaluation license at DTM from our website. And if you have any other specific questions, please email sales at trianglemicroworks.com. Well thank you again for joining us and we look forward to seeing you at our next webinar. Thanks for coming by guys. Bye. Bye. Bye. 


Video ID: v0fRoanUyog
Welcome everyone, thanks for joining us today for our latest webinar, a holistic approach to industrial security, how to detect cyber attacks in substation networks. My name is Beth Capellas and I'm the marketing manager here at Triangle Microworks. We are ready to get started, but I'd like to go over some housekeeping items while folks are continuing to join. The webinar is scheduled to be one hour long. We typically get a large amount of questions, so we are happy to stay on longer to answer as many questions as possible. With that said, I'd like to encourage questions throughout the webinar. Since the audience is on mute, please submit your questions using the webinar menu under the section titled questions. You can start asking your questions as soon as you have them. Don't feel you have to wait until the end. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly. And again, we will have some at the end to go over questions and answers. So with that, I would like to introduce our speakers. First we have Matt Green. Matt is a senior applications engineer here at Triangle Microworks. He has been with us for about six years and specializes in QA testing and all our simulation tools in Gateway. Our second speaker today is Sharon Rizanal. Sharon is a vulnerability research team lead at Clarity. He is responsible for finding new ICS attack vectors and has over seven years' experience in network security, malware research, and vulnerability research. So with that, I would like to hand it over to Matt to go ahead and get started. Okay, Matt, I'm making you presenter now. You should have the screen. All right, so we're going to start this off a little bit by talking about a little bit of our companies. So as Beth said, my name is Matthew Green. I am a senior applications engineer here at Triangle Microworks. A little background about Triangle Microworks is that we were established in 1994, and we've been working in the protocol communication fields ever since. We have customers all around the world, including equipment manufacturers, system integrators, and electrical utilities. We have a wide array of products that we sell, including source code libraries for DMP3, 61850, 6870-101, 102, 103, and 104, TASI-2, and ICCP, and Modbus. You may have also heard about our SCADA data gateway, which is our gateway application for protocol data mapping and concentrating, and we also sell a collection of testing tools like the 61850 Test Suite Pro, Distributive Test Manager, Test Harness, and IRON. Hi, everyone, this is Sharon from Clarity, so I would like to introduce you to Clarity. Clarity is the leader in operational technology security and improves the availability, safety, and reliability of OT environments for enterprises and critical infrastructure operators. So far, Clarity has raised more than US$100 million, and we have around 200 employees around the world. Our investors include leading companies such as TomSec, TeamAid, but also leading OT vendors such as Rockwell, Schneider, and Simmons. In terms of integrators and integrations and partners, we have many partners and platforms we work with.
 including Checkpoint, CrowdStrike, and more. Next, please. So what you see on the screen is my team. I'm leading a group of researchers that explore the ICS threat landscape and looking for vulnerabilities in both hardware and software solutions. Our daily job mostly consists of building and breaking ICS-related equipment. And we do security assessment research to find bugs and vulnerabilities. Once we find those, we safely disclose the bugs and vulnerabilities to the vendors. And we're actually working hand-in-hand with them in order to remediate all the found issues. As you can see on the right screen of the screen, we have disclosed more than 55 vulnerabilities, which are identified by CV numbers just in 2020. We have much more. And apart from that, my team has participated in multiple hacking competitions, including CTFs in DEF CON, S4 Conference, and Pwn2Own. Thank you. So I'm gonna talk a little bit about our agenda today. The first thing we're gonna do is we're gonna introduce Triangle Microworks' Distributive Test Manager, or DTM. We're gonna talk about different malware, including specifically focusing on the industry's payloads. Then we'll move into the DTM demo for the substation and malware simulation that we're doing. From there, we'll move on to the ICS targeting malware. We'll go much more in detail about what the industry does, its history and analysis of it. And then we'll finish it up with a CTD demo about detecting malicious activity. We will be pausing for questions and answers in the middle that's sprinkled through here, and of course, at the end. So let's talk about very basic substation networks. What we can do is we have Distributive Test Manager, which is our substation network simulation tool. And we'll be presenting that today. It allows for protocol devices, including 6870-101 and 104, ICCP or TASI-2, DNP3, Modbus, and IEC 61850 devices to be simulated. DTM will be used to simulate the substation that Clarity will be monitoring. DTM will not only be simulating the 61850 protocol communications, but also the substation behavior. We will be adding a script to this that will be simulating the attack similar to the industry's malware attack. So this is kind of a normal network layout. In our simulated network, we'll have our substation IEDs that will be monitoring information and sending them up via 61850 reporting to the substation HMI. There will also be goose published among the IEDs themselves. We've also added a semi-periodic control from the substation HMI down to a select few of the IEDs. While it'll be more frequent than you'd expect in a normal system, it will add a little more detail to the simulation for a more well-rounded system for CTD to monitor. So let's talk about malware and indestroyer for a little bit. The type of systems that we just described can be attacked by malware. The indestroyer is a type of malware specifically designed to attack.
 system substations. So our demo today will be focusing on the inDestroyer, but the same idea that we've presented here for inDestroyer could be applied to many other forms of attacks. So let's look exactly at what inDestroyer does when it comes to the protocols themselves. What inDestroyer does is it gets into your network and performs several tasks. The first thing it's going to do is it's going to try to figure out who it can communicate with. It can either do this by two ways. The first is if there's a file that is provided with this, if someone has some insider information, that file can be filled out with the IP addresses of all the devices it can communicate with. But more likely what it's going to do is it's going to analyze the machine that it's infected. It's going to look at each of the network adapters on that machine, find that machine's IP address and subnet mask, at which point it will ping all the IP addresses that it possibly can using those two bits of information and wait for responses. Once it gets responses, it will then generate connections to all the IPs that it was that got responses from on port 102, which is 61850's primary port. After connecting to the IEDs, it'll perform a discovery on all the responding devices, essentially gathering up all the data attributes and models stored within all those IEDs. The malware will then search through those models looking for controllable or writable points and it will then try to perform any sort of switch or breaker position commands, essentially toggling them back and forth. The main intent here is to cause as much havoc as possible. After that, it will record a log of everything that's accomplished and it will step out of the protocol section of its malware. So let's take a look at what that looks like. So we've got our normal network layout again, then the indestroyer can enter the network via many routes. For example, it could infect the substation HMI. Then it'll start looking for IEDs to attack via ping and 61850 discovery. Once the IEDs are discovered, then it'll issue commands to control certain points such as breaker or switch positions. So now we're going to pause here and look at DTM a little bit. I'll give you a quick overview of DTM and show you exactly how we simulated that in DTM. So this is our Distributed Test Manager or DTM tool. Effectively, what we have to match it up with the diagram you're looking at before is we have a folder for the IEDs, we have a folder for the HMI, and we also have a folder labeled attack. So inside the IEDs, you'll see all of the 61850 devices that we are simulating. You'll also see a single line diagram here in the middle and the values are being updated live. So we have a few CTs or current transformers here and you'll see that this information is live. So if I opened up this breaker here, this line no longer has connectivity through and through, so the current will drop to zero. We also have a few things I won't be touching on today, like some of our protection schemes going on in the background, and we also, as you can see here in the output, have a semi-periodic command causing this breaker that is highlighted here to open and close. On the HMI level, we have a singular device connecting up to a few of the devices in our system. We also have this folder here called attack and this is where the malware attack is going to be simulated. This simulation is done via these two scripts here. The first will generate the attack, which will be basically the master JavaScript, and then I also have a separate JavaScript called ping that will perform all the ping functionality in the system.
 broke up between two different scripts is because originally I had this going across computers and I needed to run the script on one computer and this one on a different one. Let me execute this. What we will see down here in the output is we are sweeping all of the IP addresses that the malware thinks it can connect with. For the sake of time I have limited it to the first 25 but you will see the red is no ping response whereas we are getting some connection responses on the black return text. At this point what it is going to do is start building up our 61850 clients and perform discovery on each of those devices. The reason we are doing this is that way the malware will know exactly which controllable and writable points it can interact with. You will see that most of these devices are not connecting. As you can see the connection indicator here is orange but on the ones where the ping did respond we will see a green connection meaning that a 61850 communication has been established. We just have to generate a few more devices here. Of course in the output you are also seeing when there is a successful connection to our servers. Now that all of the devices have been created we are going to sweep through a large array of controllable points. A few of them you will see in the single line diagram like the switch states here and the breaker position as well. Because of the randomness of how these commands are sent you may see an output like this. Essentially that was the malware trying to set a breaker position to close when it is already closed. You will see here that this line that could go to a neighborhood or a whole different area of the power grid is now off. There is no current going through. All of the switches are open and this is obviously not the nominal behavior. That is how we go about simulating a substation in DTM including all of the IDs and how we can set up a simulation of a malware attack. I have one more slide that I will be going back to and then I think we will be stopping for Q&A. Today we focused on 61850 attacks. The indestroyer is actually much more intelligent. It has a similar type of payload for the IEC 6870 101 and 104. It has different types of attacks for systems that contain OPCDA and the Siemens CProtect devices. At this point I will hand it back over to Beth to see if any questions have been piling up or have things to answer and then we will move on. We do have a question that came in. Is it possible to import SSD or SCL 61850 files to configure DTM? Absolutely. Our DTM and TestSuite Pro tools can use a wide variety of SCL files including SSD, CID, ICD, IED, SCD. I am sure I missed a bunch. Essentially we will try to take the widest, we cast the widest net possible in order to simulate as much of the 61850 devices as our customers can have. Okay. Okay, that's it for the questions now. So I'll go ahead and switch it over to Sharon to get started with his presentation. All right, Sharon, you are the presenter.
 Okay, so hi everyone from Clarity. So, up until now we've seen the simulation of such an attack like in destroyer but it can be any other malware, but before we're diving in into the technical details of such an attack. I would like to go over a bit on how OT networks are constructed and why and how attacks occur inside these OT networks. So, first of all, I would like to go over on a couple of news reports from the last couple of years. So in recent years we've seen many cyber security incidents in different industrial sectors, including nuclear plants, including electrical grids, water pipelines, a lot of industrial sectors. So first was TaxNet, which delayed the Iranian nuclear project in a few months by messing around with the Siemens equipment, controlling the centrifuges. We also witnessed Honda shutting down some of its factories because of a targeted ransomware attack, which was a deadly malware named Snake. There are many other hushed stories like this that companies are trying to hide from the public for obvious reasons, they don't want to be embarrassed. But the message here is clear, cyber attacks on OT network is happening and it needs to be taken care of. This is the Holy Grail of nation state cyber warfare and hackers are targeting these networks on a daily basis. So now I would like to discuss how such an attack is being done, why, and finally show a demo of how we at Clarity can detect such an attack. So first of all, we would need to ask ourselves why hackers are targeting OT networks. And I've came up to the conclusion of two main categories of reasons. The first one are political reasons, the cyber warfare. Nations are fighting each other. And another dimension of warfare is the cyber, the cyber world, the Internet. So in 2011, we had Stuxnet, the world's first publicly known digital weapon. The malware attacked Siemens PLCs and caused a massive disruption in a nuclear facility by silently playing with centrifuge speeds. So it changed how the centrifuge speed is spinning and it caused massive disruption to the content, to the uranium content itself. In 2017, we had the Triton. Triton was industrial safety systems. In the Middle East, that was targeted by sophisticated malware named Triton. The malware attacked mainly Schneider's Triconics controllers. And again, it caused huge damage to the factory. So these are all cyber warfare reasons, political reasons, but they're also activist reasons. We sometimes see activist groups trying to harm and infiltrate OT networks in order to demand a political thing or in order to make a statement. Another set of reasons would be financial reasons. So sometimes we do see hackers trying to hack companies and factories in order to extort money out of it. For example, in 2020, we had Honda attacked by the Snake Ransomware. It was deliberately attacked Honda's network.
 for extortion reasons. So now that we understand why attackers are hacking OT networks, we need to discuss what is the end game. So when we're talking about attacking OT networks, the end game will always be destruction. So chaos, disruption, destruction, anything that can cause a real damage. It can be done by multiple ways. First of all, shutting down equipment can cause manufacturing delays and disruptions. So for example, this talk today is focused on Indestroyer, which turned on and off electrical switches, relays in an endless loop. And by that, it caused massive disruption. It caused almost the entire Kiev city in Ukraine to shut off. So it destroys the equipment and causes massive electricity disruption. Another attacking vector would be to modify the parameters. For example, Stuxnet changed and played with the speed direction and the amount of the centrifuges. And another dimension of attack would be to wipe or to encrypt the data on the OT computers. So we have a couple of examples like that. For example, the snake ransomware, which attacked Honda a few months ago, it caused massive disruption just by being a ransomware, just by encrypting the files on the OT network. And the result of each one of these attack vectors would be massive disruption. So this is the result of a successful attack. Almost the entire Kiev city in Ukraine was blacked out completely in 2016 due to a deadly cyber weapon called Indestroyer. The electrical grid simply collapsed due to a targeted attack, which we're about to explain in the next few slides. So first of all, before we're diving in into the technical details of the Indestroyer malware, we need to understand how OT network is built. So let's discuss how attackers are infiltrating the internal and sensitive OT networks. The diagram you see in front of you is the Purdue model, which describes a typical OT network layout and how it fits in a larger IT network. So let's start to cover it from top to bottom. At level five, the outer layer is the internet layer, which the machines sitting in the internet layer are basically machines facing the internet and devices such as external firewalls, VPN servers, and more. Below that level four is the corporate IT network. This is typically the IT network as we know it today, where the primary business functions occur. This is where people are managing the business related activities, the manufacturing operations. And you usually know this by ERP. So this is the primary system that manages the entire operation. It establishes the basic plant production, schedule, material use, shipping, and inventory levels. Below that we have level 3.5. So this is the demilitarized zone, DMZ. A recent addition over the last decade, this is actually a level that includes security systems such as firewalls and proxies used to separate or air gap the IT and the OT networks. So this layer is used to keep the OT network safer. In fact, this is where the IT and OT world converge.
 increasing the attack surface for the OT systems. Below that, we have level three. This is the manufacturing operation systems. This is where managing production workflow to produce the desired products. And this includes batch management, manufacturing execution management systems. It'll also commonly known as MES systems. This is where maintenance and plant performance management systems are sitting and data historians and related middlewares are sitting as well. Below that is one of the most important levels in OT network. So this is the process network. This is where supervising, monitoring and controlling the physical processes involving real-time controls and software. And in this layer, we have DCS systems, HMIs, human machine interface, SCADA systems and other monitoring and controlling machines. Below that, we have the level one, the control network. And this is where the hackers would really like to get. This layer contains the process brains, PLCs and RTUs. The main logic of how to manipulate and control the field devices is found within those devices. Attackers that will be able to access and interact with these devices will be able to influence the control processes. So for example, shutting down the electrical grid will be conducted by communicating with one of the devices in this layer. Below that, we have level zero, the field devices, the physical processes. So it defines the actual physical processes, or in other words, sensing and manipulating the physical processes. This includes sensors, actuators and other related instruments that are acting in our physical world. That's why we call them field devices. So just to conclude here, attackers would like to get to level one. To level one because they would like to communicate with the brain of the processes. They would like to affect how the PLCs or RTUs are communicating with the field devices and by that influencing, try to influence the process control. So now we would like to go over how hackers are penetrating OT networks. So usually hackers would like to get to level one in order to communicate with the PLCs. And to do that, they would need to go from the IT network to level 3.5, which is the DMZ between the IT and the OT network. And from there, they would need to go to the OT network. So first of all, they would need to infiltrate the IT network somehow. Then they would need to understand how to get into the OT network by exploiting or by lateral movement through the DMZ. So this is one possibility of how hackers are infiltrating the OT network. Another way of hackers to infiltrate the OT network would be through remote access solutions. So as we've seen in the Purdue model here, sometimes we have VPN or remote access solutions that would give operators and engineers a direct access into level one. And that's because they need to control and maintain remote sites. So they themselves need somehow to get into the PLC networks without being in the networks, especially in the COVID era where everybody is working from home. So another possibility for hackers to hack into the OT network would simply be to steal credentials of an engineer or technician or operator that are managing remote devices. Another way of hackers to infiltrate the OT network would be by
 misconfigurations. So sometimes there are machines with multiple legs, one interface in the OT network and one interface in the IT network. So sometimes if such misconfigurations occur, hackers can exploit this. And we also have something I call jumping laptops. So we have an engineer laptop moving around between networks. So if this engineer gets attacked by malware, the malware can infiltrate the OT network once the laptop is brought into the OT network. And finally, we have a physical access, you need to remember that OT network is a spread across a very large area. So we have multiple sites, sometimes we have huge factory, sometimes we have multiple factories across countries or continents. So once hacker gets a physical access to one device, one PLC, from there, it can do lateral movement, it can pivoting inside the network and can reach other network devices inside the OT network. So this is how hackers are getting into the OT network. But we would like to discuss Industroyer. And Industroyer is a malware is the final goal, the objective of hackers once they're inside the network. But before that, there are a couple of stages of development. So I call we call this the ICS cyber kill chain. So this is how hackers start with the idea of they want to hack somewhere, how they develop their tools, how they deploy the tools and finally get the actions done get their objectives. It all starts with the reconnaissance phase. In the reconnaissance phase, hackers will collect data on the target can be by harvesting email addresses, conference information, it can be by understanding what kind of operating systems or other systems are being used in the in the IT or OT network of the company they want to hack. And by collecting all this information, they can move on to the second phase, which is the weaponization phase in this, in this weaponization phase, they will, the hackers will start to build and develop their own tools based on the information they collected. So if they know a specific Linux version is being used, they will develop a specific malware targeting this specific Linux version. So this is the weaponization phase. In the weaponization phase, hackers will also start to develop exploits exploits to exploit the specific equipment they were able to understand being used in the network in the reconnaissance phase. Next, they will deliver it in third stage, they will deliver it to the company they want to hack. And by delivery, it can be by phishing by email, they can build a special website, malicious website that once a victim goes onto it, it will attack the victim, it can be by exploiting remote VPN or firewall that we've seen in the couple last couple of months, it can be via any method that the hacker think it's suitable based on the information they collected. Next would be the exploitation phase and the exploitation phase, the phishing email or this malicious website will attack the victim. And it will move on to the fifth stage of installation, where some kind of a stage one, or something we call a backdoor will be installed on the device on the machine on the first entry point of the attackers to the network. Next, the backdoor will call home just like eating, it will call home and ask for directions, what should I do? And usually it will call the command and control server, the command and control server is the server that will direct the backdoor what to do next. And usually these commands would
 be to install new malicious software or to exploit more devices in the network, or it can be to execute the final objective with step number seven to execute the final payload. And in the industry case, the final payload was to attack IEDs. IEDs are the machines that monitor and regulate the power and electrical grids, which we'll cover in a bit. So now, after we understood how hackers are attacking OT networks, why, and what is the ICS cyber kill chain, we can move on to understand how the industry malware that we're trying to simulate and detect with this demo is working. So now we'll move on to the industry and we'll discuss a little bit about it. So first of all, history. Industry is a special cyber weapon with the objective of disrupting electricity system, electrical system. It was used in the cyber attack on Ukraine's power grid on December 17, 2016. And it caused mass disruption. The malware attacked and was able to cut off fifth of Kiev, the Ukraine capital Kiev, fifth of Kiev power. So most of the people in Kiev in 2016 December had no electricity because of a cyber attack. This is huge. The attack itself, the incident, was the second cyber attack in Ukraine's power grid in two years. And the first attack occurred on December 23, 2015. So it wasn't the first time that this network was attacked and the hackers knew what their objective was. They knew what to do. They knew how to hack into the network and how to deploy their sophisticated malware. And what's important to say here that the industry is the first ever known malware specifically designed to attack electrical grids. And the target of this malware was these devices. These are IEDs. So in the electric power industry, an intelligent electronic device is an integrated microprocessor based controller of power system equipment, such as circuit breakers, transformers, and capacitor banks. In other words, this is a device with a simple goal of monitoring and do power regulations. And once this device was attacked by the industry, there was no power regulation. There was no monitoring. And basically, what the industry did is to open and close circuit breakers all the time in order to cause massive disruption in the electrical grid. And it did succeed. And for a few hours in 2016, there was no power for Kiev's population. So it really makes sense that the industry and malware tried to attack specifically these IEDs in order to influence the process control and to damage the electrical grid. Now, let's move on to understand how the malware operated within the network. And by that, I want to start with a simple diagram of how the malware was architected. So first of all, we have the command and control server. This is where all the commands to the malware got from. So the main, the first, let's call it the first component of the malware is actually the main backdoor. And the main backdoor was used to control all the other components of the malware. It connects to the remote command and control server in order to receive commands from the attackers, from the system.
 It did a couple of things. First of all, it had a launcher. The launcher component is a separate executable responsible for launching the payload components and the data wiper component. The launcher component contained a specific activation time and date to start the attack. We've seen a couple of payloads for the launcher. We've seen 101 payloads. This is a protocol stack 101. With the only goal of attacking IEDs and actually open and close circuit breakers. The attackers implemented multiple payloads, multiple protocol stacks related to ICS, related to electrical grids protocols, in order to open and close circuit breakers. This is the launcher. The launcher activated the payloads. The main backdoor also installed additional backdoor for a backup and it used additional tools that we'll discuss in a bit. Let's dive into the backdoors themselves. Once the attackers obtained administrator privileges, they were able to upgrade the installed backdoor to a more privileged version than is executed. It was actually a Windows service program that did all the magic. With just replacing the image path in the registry, they were able to elevate permissions and execute themselves as a service on Windows. The main commands were to execute another processes or to copy and upload files to the CNC. As I said before, this backdoor had additional backdoor. It was actually a Trojanized version of the Windows Notepad application. Anytime the operator opened the normal Windows Notepad, it actually activated the secondary and backup backdoor. The launcher is a very simple binary. You can see here the command line for the launcher. All it needed was a working directory, the payload to load and the configuration file. Each payload had a different configuration file. You can see here the industry had a couple of payloads. It had 101 payload, it had 104 payload, 61, 850. This is what DTM simulated in the demo today. It had OPCDA payload as well. The attackers implemented a complete protocol stack of all of these protocols in order to cause massive disruption via communicating and legitimate protocols. The final component of the destroyer was the additional tools. It's a couple of additional components. We categorized them as additional tools. It had a port scanner. As you can see here, this is how we ran the port scanner. You can see it's a very basic port scanner. It had a data wiper, which just deleted files on all the computers the industrial ran on to delete any traces, to delete any forensic evidences. It had a Siemens CProtect denial of service exploit, which is marked as a CV 2015-53-47. This exploit actually communicated with a Siemens IED. As you can see here, this is the CProtect IED. It sent a malicious payload to the exploit, and the machine just crashed. This is a very known exploit, and the machine just completely crashed and got into a denial of service condition. The entire...
 goal of the industry was to destroy the machinery, the IEDs inside the network, again, in order to cause mass disruption of electricity. Now, as I said, all the protocols being used are legitimate protocols. So the question is, how can we detect? How can we detect illegitimate traffic when it is being abused? And the answer is an holistic approach, which combines multiple detection engines of different fields. So Clarity's flagship platform is called CTD, Continuous Threat Detection. And CTD consists of a couple of detection engines, which together provide full monitoring coverage of IT security and integrity events. So we have a couple of engines which I would like to cover. And then I will show you a demo of how it works, everything together. So our engines include anomaly detection, which identifies changes in the communication patterns, which we call baseline. So we're trying to see if something deviates from the normal baseline of the network. We have some security behaviors, which identifies adversary techniques used in attacks against IT or OT networks. We have known threats, which includes IOCs, Snort, Yara, and other engines. We have operational behavior engines, which identifies OT operations, such as a firmware upgrade, download or upload of configuration for devices and controllers. We have custom rules, which identifies user-defined events. And we even have cloud connectivity to get the wisdom of the crowd. So before moving on to the demo, I would like to ask if you have any questions. Yep, we did have a couple of questions that came in. Let me start with the first one. Your assumption is that the HMI has been infected. Does that mean that the attack will come from the same IP as the HMI has? Most of our satellites only accept connections from fixed and configured IP. Yeah, sorry for interrupting, but the answer is yes. The HMI was compromised and all of the malicious communication that was received was from the HMI. So it's a kind of hard and difficult to understand whether this kind of traffic goes out from the HMI malicious or not. And using the holistic solution I offered, it kind of solved this problem. Okay, next question. It was mentioned that hackers want to access level one. Do you have any indication of target changes due to the digitalization of the field devices? It will be much easier to attack these devices and attack them through the level one devices. So I'm not sure if the question points to field devices being digitalized and adding some features like cloud connectivity or not. So I'll try to give some abstract answer. In the recent years, we've seen some sensors and actuators and other field devices being added some cloud connectivity. And Clarity are researching this field as well. So we're trying to monitor this connectivity as well. Okay, for cloud solution, VPN is most likely not part of an OT network. Where would the vulnerabilities be? So usually in
 OT networks, we have remote access solutions, so sometimes they're being referred as a VPN and sometimes they're being referred as a remote access solutions. But we do see OT networks with remote access solutions and the vulnerabilities would be. So actually, Clarity had a nice research on this topic a few months ago, so you can see it in our blog, but usually it will be a in a couple of ways. One, it can be in the VPN client or the remote access solution client. So we were able to attack the clients themselves and then steal their credentials and log in on behalf of them. But it can also be that the vulnerability can also be in the edge server, which receives the external connections. And we were able to prove again that some servers, some remote access solutions are exploitable and we're able to remotely from the internet, exploit this server and infiltrate into the network. Okay. Okay. We do have a couple of other questions, but I'm going to hold off on them for now because I want to make sure we have about 15 minutes left of the scheduled demo, but we will stay on to answer these questions. And I do want to encourage people to continue to submit the questions while Sharon continues on. Just a reminder, we do have a couple of folks behind the scenes. You're hearing from several industry experts and we have several industry experts ready to answer you real time. So keep the questions coming and we will get to them towards the end of the presentation. So Sharon, if you want to continue on. Yeah, thank you. So I'll just conclude a bit. So up until now, we covered quite a bit of content. We discussed the history and the technical details of the industry malware. And we also discussed some theoretical ways to detect legitimate protocols when they're being abused to cause disruption. Now we'd like to continue to our demo. And usually CTD, our product is fed with live network traffic from a SPAN port from the switch. But in this case, for the demo purposes and due to the COVID situation, we decided to separate the demo to DTM and CTD separately. So previously you've seen DTM demo and now I'm going to show you CTD demo separately. So as you've seen with Matt's demo, DTM is able to mimic industrial behavior by simulating its network traffic and how the malware utilizes Goose, MMS and other ICS related protocols. While running the simulation, Matt has recorded the network traffic and captured it and stored it into a pickup file. So he sent them over to me, sent the network captures files to me and I upload them to CTD as if it was a real live network. So now what you're about to see is our CTD system after all the traffic was digested and analyzed by the traffic received from Matt's simulation. So let's get started. So this is CTD. This is the main dashboard of CTD. So what you can see here is an aggregation of all the network events, all the stuff that is going on in the network. So you can see here how many assets has been discovered. Currently I'm filtered on the left here, but you can filter how many new assets were discovered today or yesterday. You can see alert status. So how many open events you have in the network. And we can see here that we have one story which is critical, which is classified as critical. And we're going to discuss in a bit why we have one story.
 while we have 29 alerts. We also have a couple of insights. So insights is our way of telling our customers what's important, what should be prioritized, and what important and special events happened in the network. So next, I would like to go to the Assets page. And in the Assets page, I can see all the different assets in the network. So everything that Matt simulated, we can see here as an asset. So we can see here many assets in the network. And we can see the vendor, in this case, Siemens. And we can see lots of devices. We can also take a look at all of these devices as a layered topology view. So it's very easy to understand what's going on in the network once we have the topology view. And we can see, indeed, just like I discussed previously, we're categorized all the assets based on their location in the Purdue model. So all of these are probably IEDs. And this is probably an HMI device. So we can see we have this HMI, as Matt showed you before, communicating with all these devices, probably monitor regulating. Or in this case, all of these IEDs were attacked by this HMI. So if we're diving into the HMI, we can go to see more details. So first of all, our system automatically detected that this is an HMI and specifically detected what kind of operating system it's running on this asset. And it's really good for asset management. We also have some graphs and statistics, which I don't want to go over right now. I do want to go over to device information. And here we can see what kind of protocols this device communicated with. We can see where it sits in the Purdue level. And we can see even what kind of installed software it has. We also 3CDD managing the risks and vulnerabilities of the devices in the network. So for example, if we're detecting an old operating system and we know that there are a couple of vulnerabilities for it, we will aggregate all the information into this screen. And this way, we're telling our customers what to prioritize first. So anything that has higher risk should be prioritized and patched first. We also have some insights like what kind of operations this asset was involved with, what are the CVEs or vulnerabilities that this asset has. And if it's running a web client, for example, a browser, we can also present what kind of files were downloaded. We also have threat detection. And this screen actually tells our customers what happened in terms of security events to this asset. So we can see here we have lots of events, lots of alerts, 29. But they're all aggregated into one story. So we decided that showing lots of alerts is not beneficial for our customers because it's not telling a story. And so we were able to understand the scope of all the alerts going on in the network and combine them into one story, one meaningful story. So if we'll go to the story, we can really, as it sounds, really tell a story through all the alerts.
 So here again, we're presented with the HMI and all the IDs. And here we can really understand by going through the alerts, what happened. We're telling a story. So for example, here we can see that the suspicious file transfer was found. And we can see here that a file was downloaded. So this guy downloaded a file from this guy. So this is probably the CNC and this is the HMI with the industry because we're seeing here that our Yara engine determined that a file that is part of IndustWire was downloaded. So we can see here a couple of files that are marked as IndustWire were downloaded through this guy. So now we're starting to understand that this HMI actually downloaded a malicious file from the CNC. And next, it started to run, it started to run the payloads against all of these devices. And we can see this because suddenly some new commands were starting to run in the network. And this HMI started to attack all of these devices. And we can see here by all the IPs. We can even see more details if we'll go to event details. But instead of doing that, I would like to go to our process control. So not only that we're tracking all of the baselines, so here we can see all the baseline summary, what kind of protocols were used, what kind of ports, what kind of requests, we're also seeing what kind of process values or tags were transferred in the network. So for example, if the malware, the IndustWire malware wanted to send a request to open or close a circuit breaker, it would actually send a tag request or a write request, which we're capturing and monitoring. So here we can see all the different tag write requests. And by that, we can even filter by protocol. And by that, we can do a very good forensics of what really happened in the network. What kind of tags were written? What kind of protocol requests were sent in the network? And we can start to construct the complete image, the complete picture of the entire attack. So this was our demo of CDD. Of course, we have lots of more engines. We have lots of more features to show, but for this demo, we just wanted to focus on the relevant features for IndustWire. So I think now it goes to Matt. Yes, thank you. I was answering a few questions. Sorry for the delay. So that, like you said, that concludes our CDD demo. We're gonna kind of move into some final thoughts here and talk about a little bit of what we've seen today. So the next thing we're gonna talk about is you've seen the damage that malware can cause to your system, and you might be interested in ways to kind of minimize this risk. The easiest first step into cyber security is to add some sort of form of device authentication. We can use Garibaldi's key distribution center or a protocol level secure authentication can be added to your system to do this, and they both work in a very similar manner. They work by verifying the identities of each of the devices on your network via private keys. This guarantees that your network is not under any risk of malware. The second step is to add a secure authentication of your network, and we'll talk a little bit about that in a second. But the first step is to add a secure authentication of your network.
 that you're not receiving controls or commands from rogue devices or malware on your system. Next. And so we want to give a couple of tips, security tips, to secure your network and minimize the risk inside your network. So obviously monitoring and understanding the threats in your network is key. So monitoring all the assets, understanding what kind of assets you have in your OT network, because we all know that OT networks contain hundreds if not thousands of network devices. Also creating a testing incident response plan would be good for a dark day. So you need a plan to understand what's going on in your network once a cyber event is happening. Also assessing potential risk in your network by identifying old versions of operating systems or old software versions or old devices would be always good to prioritize what's going on in terms of what should you patch first. And finally, some kind of a generic tip that is always good to have is using a multi-factor authentication specifically for the remote access solution. We have seen before companies not using MFA for remote access solutions and once the credentials of one user were stolen, the entire network was compromised. So we've talked about DTM today and how it can simulate partial or entire systems that includes not only the protocol devices and their communication but also behaviors. This allows you to increase your confidence in your cybersecurity software by simulating attacks in a safe simulated environment. During these simulated attacks, this is when you can verify that your system is properly responding to the attacks and potentially eliminating any sort of risks or damages to your systems. Yeah and with the CDD, we showed you how it can help you monitor in your network including doing asset management, doing segmentation in your network, doing threat detection. It can help you with understanding what kind of malware is and what kind of vulnerable devices you have and by doing vulnerability management, you can prioritize what should you patch first, what should you take care of it first. So thank you and I think if you have questions and now is the time. Yes, we certainly did get a lot of good questions coming in while you were walking through that demo so I'm going to start with that. First, let me just acknowledge that we have hit the hour mark. Like I mentioned earlier, we are going to stay on until all the questions are answered so hopefully you can continue to stay on and with that we'll just jump right into the question and answer section. So Sharon, I believe this first one is for you. Did all the IEDs have to be replaced following the Keeve attack or was the operator able to reset the equipment and reuse? So I'm not sure exactly what happened in this case. Usually what they would do is they would reset all the configurations on all the devices and they will start to clean up the network. Sometimes it includes formatting and restarting the devices but sometimes it would mean to replace the devices completely. So in case of a sophisticated backdoor that it's impossible to remove, sometimes they would replace the entire device. Okay and I'll just make a note to the other organizers that are on the call. I do have everyone off of mute if anyone wants to jump in with an answer to any.
 question. Next question, how can we clean or stop the malware activated in the victim? So it really depends on the situation. It's hard for me to say anything too much broad and generic, but it really depends on the situation. So in case of ransomware, sometimes just formatting the computer and uploading backups would be enough, but in other cases some more countermeasures needs to be taken care of. Okay, the next question, in case an IED is infected, which possibilities does an attacker have to attack the SCADA computers? Are you able to give examples? So I'm not sure about the question, the IED was attacked and then what? In case an IED is infected, which possibilities does an attacker have to attack the SCADA computers? So usually it will go the other way. It will go from the SCADA systems, like in this case the HMI, to the IED, but in this case what you're referring to is the bottom-up, which is not a likely case with IED, but it is a possibility. So in this case it will be, first of all, very suspicious that the IED is starting to communicate and starting to send requests from the IED to the SCADA machine, and for instance our CDD will detect it immediately because this is obviously a baseline deviation of the network. Okay, Matt, I believe this next question is for you. IED and ICD files, can you give details about all the connected devices known? So I think this might be one that I already responded to. Can you repeat that one more time? Sure, and you did, yes. IED and ICD files, can you give details about all the connected devices known? Yeah, I thought this was a really good question, so I'm glad you kind of brought that up. So some SCL files, like SCD files, do have some of that information stored inside of it, but the big fear is that the malware, especially the industry, and I'm sure other forms out currently, don't really need that information. It has the ability to look at the device that it infected's network adapter, specifically its IP address and its subnet mask, and sweep the entire system that it may have range to attack to. So it can actually locate all of the devices out in your system without necessarily needing an SCD file or insider information. Okay, and I do believe this next one is for you, Matt, as well. Which protocols are used to discover the devices? So that question, I assume they were talking about 61850 Discovery. And so what 61850 Discovery is, is it's the ability for you to instance a 61850 client that may not know what its model is, and as soon as it talks to the server, it essentially says, hey, I need to know your model so I can reflect that on my side. It's a very powerful tool for when you are trying to connect up to servers and you don't necessarily have the files, which makes it perfect for malware because people who create these malware or these viruses, the worms, whatnot that infect systems, they don't have
 have access to the configuration, the engineering files, that were developed for these IEDs. And so it allows them to gather information that they wouldn't originally have access to. Sharon, I'm going to switch back over to you. What kind of ability do you have to detect new attacks exploiting previously unknown vulnerabilities? So once again, we have a couple of detection engines. They're all working together. We're trying to be as holistic as we can. So in case of a new exploit, it will generate the usage of this exploit attempt will generate new type of traffic in the network. And since we're always looking for deviations in the protocol request, we will detect this attempt as a deviation. So first of all, when we're deploying our systems, we're baselining the network. We're trying to understand what is the normal state of the network, what our protocols are being used, what ports are being communicated. And if we'll detect an exploit attempt that was not part of the baseline, we will declare it as a deviation. OK. Next question is for you, Sharon. What kind of probes agents is needed for CTD to get this information? So all is needed is a SPAN port. So the basic usage of our solution is completely passive. We're just receiving network traffic from a SPAN port. But we do have some other more advanced engines like Active Engine that can scan the network by itself. But the basic usage is simply giving a network traffic feed into our system. So that's it. OK. Is it possible to access a COM port by two different processes? He's got a follow-up comment here. The COM port access to send payloads is exclusive to the legitimate protocol software. So specifically in Industriall, what it did is it first killed all the processes that use the COM ports. And then it initiated the attack. So specifically regarding the question, it is possible by injecting packets into the process that was occupying the COM port. But in the Industriall case, the solution was much simpler. It just killed all the processes that communicating through the COM ports. OK. Is the CTD solution deployed corporate-wide or at a site level? How is it licensed? So about licensing, I'm not sure. You'll have to ask our sales department. But in case of deployment, we can deploy CTD on each site. And then we can control all the sites together. We can have another oversight solution called EMC, which controls all the CTD in all sites together. So we can control from one central location all these instances in all the sites. OK. How will CDT monitor COMs when they are encrypted? So it really depends how we're seeing the traffic and where. So sometimes we're deploying our CTD after or before the encryption.
 so it really depends the situation but all in all if we don't have the keys for the encrypted protocol we cannot decrypt it. What is the method you are using to detect attacks and how are you preventing them? So what is the method you're using to detect attacks within CTD or CDT and how are you preventing them? So yeah as I said we're not preventing we're a passive solution we're just monitoring but how we detect attacks is again we're using the holistic approach of a couple of detection engines where we have a behavior the detection engine we have our baselining and trying to understand deviations in the network we have our URI and SNORT engines so we have a couple of different engines that are all working together in order to detect attacks in an OD network. Can you install CDT in any substation? Yes. And this might touch on a little bit of what you just answered how do you implement such a system is it a passive IDS with simple monitoring ports? So our solution in its basis is an IDS intrusion detection system but we have multiple engines working together so it's not just a single IDS like SNORT we have sophisticated algorithms that are trying to understand the deviations in the network so we're not trying to understand just one thing we're trying to understand the whole picture. Okay what are the other protocols supported other than 6870, 101, 104, OPC and 61850 by CDT? Well we support dozens if not hundreds of protocols you'll have to refer to our website for that. Since you show us some protocols that are in use in OT how deep do you inspect 61850? So we have a very thorough detectors that are able to understand all the protocols in a very high level of detail so our DPI engine is very very good as I've showed we're really understanding not just the request but also we're extracting all the values and we're able to show your customers exactly what kind of tag was written or read from and how many times so we have a very sophisticated DPI capabilities. Okay can the device IED address be recorded or encrypted instead? So I'm not sure what it means be recorded. Okay that person is still on if you want to send a clarification comment. I'll skip over to our next question we can come back to that. Matt I believe this one would be for you is it possible to simulate the analog value continuously by varying measurements in an incremental way with DTM? Yeah that was a good question so in DTM there are several different ways that you can edit your values whether it be a Boolean or analog value we've got scripting auto data changes we've got CSV file playback.
 But, if we're talking to the level of sampled values, which is going to be in the milliseconds or 10 milliseconds, we do have a little bit of a time constraint at that level, because we do run on the Windows operating system, but up at the hundreds of milliseconds level, we keep up with pretty well. It really depends on your definition of continuously, so I would say tentatively yes, but if you're talking about sampled values doing a full sinusoidal wave or a power curve or something like that, in the ones of milliseconds level, that's not something we generally can do right off the bat. Okay, another one for you, Matt. If you don't need to see the SEL file information, how are functional problems detected in 6.2050? Yeah, I wanted to be a little clear with this question, because I wanted to make sure there wasn't much confusion. So, when we are simulating the devices in DTM, Distributed Test Manager, we do use all forms of SEL files, all the way up from SCD files down to IID, ICDs, CIDs, anything along those lines. That's how DTM knows exactly what type of device to simulate and how it's configured. When you don't, when I was talking about you don't need the SEL files, I was talking about specifically for the attack done by Indestroyer, while if they had that information, they might be able to have a more efficient attack, they don't necessarily need the SEL files because of its ability to scope out the network adapter and figure out any of the devices on the system that it can connect to and attack with. And at that point, once it's able to perform discovery, get the model of the devices attacking, then it knows what it can attack, specifically for this demonstration, it was switches and breakers, because once you open these breakers and switches, there's no more power flowing on those lines, which means, you know, as shown in some of the pictures, cities are losing power, lights are going off, you know, the really damaging stuff that can happen to the world. I hope that accurately answers that question. But I wanted to make sure I answered it from both sides to prevent any confusion. Great. I'm going to switch back over to you, Sharon. Do you have any vulnerability database implemented in your solution? Or do you do a vulnerability scan? We're not, we're not active, we're passive and we do have a vulnerability database with all CVs. Another question for you. If an attack is already detected, then what should be the first line of action to prevent further damage to the network system? So first of all, I would recommend to cut off the power for all the network devices so that attackers could not communicate to their CNC. And then a thorough investigation must must occur. So the organization must understand how the attacker is infiltrated inside in order to close this vulnerability and this hole. And they need to understand how they spread and pivoted inside the network and what kind of devices were infected and then start mitigating all the holes being found and format the computers restore from backups, et cetera. Okay. This question has kind of come up in a couple of different ways. So I'm going to ask it of you, Sharon. One several folks have asked if they're able to get a evaluation license to try it out. And is it a subscription based licensed product? And I know you mentioned that might be more of a sales question, but if there's anything you can shed some light on there.
 Honestly, I'm not sure they'll have to email the sales department. So I think it's sales at Clarity.com, but you can see it in our website. You can request a demo through our website. And I will be sending out follow-up information that we'll have a recording of the webinar. We'll have contact information for DTM. You can download an evaluation license directly from our website, trianglemicroworks.com, and try it out. But I will include the Clarity information in there as well. I've just seen that question come up a couple of different ways a couple of different times. So I did want to make sure I got to that in general. Okay, another question for you, Sharon. How can the software analyze the traffic in case of a VPN tunnel from upstream all the way to the relay level where the traffic is wrapped with TLS? So again, if it's wrapped with TLS and we don't have the keys, we cannot decrypt it. But it really depends where our solution is deployed. If it's deployed in the, let's say, level one, the Purdue model, then we're seeing the traffic after it was decrypted, which means after the tunnel. So it really depends where CTD is deployed. Okay. Is a separate computer required to run CTD in substations? Is it possible to access data of CDT and control center from all the substations? So in order to run CTD, you just need a box. It can be virtualized or not. So you can access this box from anywhere. Okay. In an OT environment, how many numbers of ports slash devices can be monitored? We have networks with tens of thousands of devices. So I'm not sure what's the limitation, but it's very high. Is CDT able to monitor firmware upgrades to PLC and how is it achieved? Yes, we're able to do that because we have a very thorough deep inspection protocol mechanism. So we're able to understand most protocols in OT environments, if not all of them. And we're able to understand all types of requests. So in case of a firmware upgrade or a logic upload or download, we'll be able to understand it and notify our customers. Okay, great. Matt, I do have a question for you. Is it possible to build test case and execute for testing functionalities of IEDs by DTM? Yeah, absolutely. So that's one of the things that I didn't really go over too much today when I was doing my DTM demo. I showed off one kind of simple use case for DTM, but it actually is designed to sweep several different use cases. And one of the things that we didn't talk about was our test manager. And what the test manager allows you to do is set up multiple tests that you can run, say as regression, or you can run each time your system updates in any way, allowing you to automate these tests, run these tests, go get a cup of coffee or run them overnight or something along those lines, and come back, and you'll actually have a full printout of everything that was run while you were gone. And it also allows you to export these test results, giving you a paper trail of how things are going as your project is moving along. But yes, absolutely. And that is totally possible. 
 customizable, cross-protocol, anything that DTM can do, you can pretty much fit into the test manager one way or another. Great. Okay, Sharon, back over to you, and I think this might be the last question. I think we're getting to the end of them. Is it possible to SDN switches minimize cyberattacks by strictly controlling all the flows? Can you repeat, please? Sure. Is it possible to SDN switches minimize cyberattacks by strictly controlling all the flows? So generally, yes, any restriction will help. It's just like firewalls. Okay, did you just have one more pop in? Do analysis reports of the attack and previous history of the attack, are those generated? So we are able to generate the report in our system with all the open alerts and all the stories and all the aggregated data, so yes. Again, I think you've already touched on this answer before, but it did just come in, so I want to make sure we cover it. Is it possible to access data of CDT and control centers from all the substations? Mm-hmm, yep. Okay, what format do you use for your SYSlog messages? I'm sure I'm not saying that right, but Syslog messages? Syslog, what kind of format? The most common one, I don't remember by heart, but the most common one for the Syslog. CEF, he's asking? I think so, yeah. Okay, do we have any backup systems for CDT, like main and standby? I'm not sure about this one. Okay, and we can follow up with the unanswered, any unanswered questions in the email. We do usually attach that with the follow-up that will get sent out, so if anyone had a question that did not get answered either online or here, we will send that in the follow-up email, but that does bring us to the end of the questions. I mean, we had a lot of good questions come in. I appreciate everyone that has stayed on. We had a very large crowd stay on for those questions, so that's great to see. Again, just want to remind everyone, we will be sending out a follow-up email with a link to the recorded version of this webinar. It will also have information on DTM and on Clarity and where you can download evals and get more information on certain things like the licensing and subscription that was also asked here. So again, thank you everyone for your time today and staying on even longer, and continue to be on the lookout for our next webinar that we will be hosting in January. Okay, thanks everyone.
 you you


Video ID: PwPOmh-P698
Welcome, everyone. Thanks for joining us today for our latest webinar, Debugging Goose, How to Streamline Troubleshooting and System Validation. My name is Beth Kapalis, and I'm the Marketing Manager here at Triangle Microworks. Before we get started, I have a couple of housekeeping items that I'd like to go over while folks are still joining. The webinar is scheduled to be one hour long. We do typically get a large amount of questions, so we're happy to stay on as long as possible to answer as many questions as we can. With that said, I'd like to encourage questions throughout the webinar. Since the audience is on mute, please submit your questions using the webinar menu under the section titled Questions. You can start asking questions as soon as you have them. Don't feel that you have to wait until the end. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly. We will also stop a couple of times throughout the webinar to read and answer questions. And, of course, as I mentioned, there will be time dedicated at the end just for questions and answers. Next, I'd like to introduce our speakers, starting with Christoph Brunner. Christoph is a leading utility professional with over 30 years of industry experience with knowledge in both the utility and automation industry. He has been involved in the development of the 62850 standard since the beginning. He is president of IT4Power in Switzerland, a consulting company to the power industry. He is the organizer of the working groups responsible for the core parts of 62850. He is active in several working groups of the IEEE PSRC, which is the Power Engineering Society Relay Committee, and a member of the PSRC Main Committee and the Subcommittee H. He is a guest professor at Shandong University of Technology and an international advisor to the board of the UCA International User Group. We also have with us Jim Coats, president of Triangle Microworks. In 1994, he started Triangle Microworks to provide communication protocol software to the utility industry. He continues as president with a primary role of product research and design. Jim is currently chief financial officer of the DMP3 User Group and participates in the IEC Technical Committee 57 Working Group 10. Before we jump into the webinar topic, I would like to give a little background about Triangle Microworks and what we do. As I mentioned, we were established in 1994, and we have over 25 years' experience working with SCADA communication protocols. We have customers all around the world spanning over 70 countries working with equipment manufacturers, system integrators, and electric utilities. Many of our employees sit on the Standards Committee, such as UCA and DMP User and Technical Committees. Being active with these committees help us to stay up-to-date on the protocols as well as influence the specs based on our customer needs and what we are seeing in the industry. We have several categories of products. We started with source code libraries for DMP3, 61850, 6870-5. That includes 101, 102, 103, and 104. We also have libraries for 6870-6, which is also known as TASI-2 or ICCP, and for Modbus. We provide technical support not only during initial implementation of our libraries, but also support for any communication issues that may arise with your devices in the field. We have a collection of testing tools that support the standard SCADA protocols.
 like 62050 TestSuite Pro, Distributed Test Manager, Test Harness, and Iron. Today, we'll be mostly focused on TestSuite Pro. You may have also heard about our SCADA Data Gateway, which is a Windows application for protocol translation and concentration. Our gateway supports OPC and the SCADA protocols mentioned earlier. We are planning to release a Linux version of the gateway in the next several weeks, so stay tuned for more information on that. And if you haven't already, please go to our website at trianglemicroworks.com and sign up for our newsletter for upcoming product releases and webinars. With that, I'd like to hand it over to Christophe to go ahead and get started. Okay, Christophe, you should now be a presenter. Well, I just shared the wrong screen. Sorry for that. Okay, so what I wanted to introduce here is a little bit the background of our demonstration today. For those of you that have been in previous webinars, they probably already know our little substation that we use as a demo application. So we have a substation with two lines, two transformer bays, various IDs. And for the purpose of this demo, the substation will be simulated in DTM. So what we see here is the single line diagram of that substation with the two line bays and the two transformer bays that we will use as a playground for our testing. This slide shows the overview of the substation with the different IDs that we have in the base. So we see that we have in each of the bays basically four IDs, a bay control unit, a protection unit, either the line protection or the transformer protection unit. We also assume that we have a fully digital substation with switch control units and with merging units in each of the bays. On station level, we have an HMI. We have the busbar protection unit, a merging unit for the busbar voltages. And we also have designed in the system a client that we can use as a test client. In general, a good practice when you do a 650 system design, make a spare client available where you can connect your test tool so that you can also get the reports and you don't need to rely on having reports from another HMI available that is not connected. As mentioned, this whole part here will, for the purpose of this demo, be simulated in ETM. But the testing would be exactly the same if you would have the real substation devices behind. The webinar today is mainly focused on GOOS, so I provide a quick summary again about GOOS messages. We have here an example of a GOOS message where we use between a bay control unit and the switch control unit. As I mentioned, we assume a fully digital substation. So we have a separate device which is interfacing and controlling the switch. That's where, from a 650 perspective, we implement the logic and all the XCBR. On the other side, we have our way control unit, which implements the CSWI. And then we have to open and close signals that are going from the VCU to the SCU in order to perform the controlled operation.
 In a similar way, the protection device sends the trip or a re-trip from a breaker file also as GOOS over to the SCU. The whole system has been designed with HALINX STS, which is a system configuration tool and allows you to graphically design your signal flow that you are requiring to implement GOOS messages. The GOOS message itself, so what we have in XML then, is completely generated automatically by the tool. So you don't need to worry about configuring datasets, configuring extras, configuring GOOS control blocks. The tool takes care of that for yourself. And as an important element, when we talk about GOOS and GOOS messages, is the logical node LGOS that has been introduced in addition to the standard. The LGOS basically tells you from an IED subscriber perspective if everything is OK, if it receives the message it is supposed to receive. So this is a good testing feature as well when you have these in the IEDs, because sometimes when something doesn't work, you are never sure, is it not sent correctly, does the device not receive it, is it something internal in the device. With the LGOS, you get a clear indication if at least the interaction works, so if the device receives the GOOS message. So that's something that you will see later on in the demonstration, how this can be useful to identify and classify possible problems that you have in your system configuration and implementation. With that, I will quickly give an overview of the different tests we want to demonstrate. In the first step, we will demonstrate how we can do a unit testing of a device. And then in the second step, we will demonstrate how we can do system testing. The unit testing of a device is basically, I mean, we can have multiple aspects of unit testing. The first one would, of course, be to do a full functional unit testing. So you have your device configured as you want it to have in your design, but before you build the complete system together, you want to be sure that each and every device behaves and is configured as you expect it, so that you can already eliminate these kind of configuration errors. When we do a full functional testing of the unit, we will need to simulate all the other devices that interact with the unit. We could do that as an example with DTM. But for today, where we focus on communication testing, we will only do a unit testing from the perspective of configuration and communication aspects. So basically, we will verify if the device is configured the way how it is also specified in the STL file, in the system configuration description file. We will verify that the device is publishing all the GOOS messages, and we will verify that the GOOS messages the device is subscribed is supposed to receive are also received by the device, so that everything is configured correctly from this perspective. And this you can do using just TSP and the device under test. Jim will now go and demonstrate to you how this is done. So with that, I hand over to Jim. Thank you, Cristal. Okay, Jim, you should now be a presenter. All right. Can you see my screen? Yes. Great.
 So this is TestSuite Pro. You have tools over here on the left, and then this divider bar separates out the displays. The display buttons are down here, and you can see tool tips to get you familiar with what the different tools do and then displays do. So this is the system status group over here, and it shows you the error conditions that TestSuite Pro has detected automatically. And these should be resolved or acknowledged before using the other tools and displays. So this is kind of the overview. It shows the IDs and the connection status, the IP address, the goose adapter, and so forth. You can see that since this is a unit test, most of the devices are not connected, but the device under test is connected and looks good. So we can go ahead and acknowledge these warnings, and we've dealt with the system status indication. The next thing in the system status group is the compare model, and you can see that it goes through and does a discovery and compares that with what's in the SCL file. Since all of that is green and it looks good, the device is configured as expected, and we can continue our testing. The next one is GooseTracker, and GooseTracker shows all the rows here are streams and the columns or properties of those streams. So you can see here are the devices that are not connected, and here's our device under test, the goose stream from our device under test. And for example, this column shows if the stream is present, and we have received a message from it in the time allowed to live. So for the ones that are not there, it's false. For the one that's under test, it's true, and that one looks good. This column shows whether the goose stream that was found in the SCL file matches what's on the wire. And so for the ones that aren't there, it's missing on the wire, it matches. So again, we're looking good for our device under test. We can acknowledge the other devices. Next, we look at LGOS. As Christophe explained, LGOS tells you what the IED thinks it's subscribing to, right? So you read over 6250 MMS that the device is receiving by goose. Again, the rows here are goose streams, and the columns are IDs. This symbol indicates which IED is the publisher. And so for our device under test, it's this column one right here, and it is publishing. And this goose stream right here, so that our goose tracker sees that it's valid, you can see on the tool tip. And then it is subscribed to goose streams from this goose stream and this goose stream, and are located in line one LPU1 and line one SCU1. The subscriber status is color coded in this table. You can see that purple means no connection to the IED.
 ID, which is kind of what we expect here. This is a test. So all these align. We just have our device. And the red means that the SIM status is false on that LOSS. We're not receiving from those strings, which is correct, because they're offline, right? These are the transmitters. They're not online. We can also see that information when we hover on these cells. You see the first line in the tooltip shows the status. You really don't need to look back at the legend if you don't want to. So the red square on this goose stream means that our device under test is not receiving that goose stream. We can also see here that the goose tracker is not receiving that stream either. And then you also see on that tooltip what data set members that stream has. And if you go over here on the cell, you can see which of those data set members that ID needs for its internal processing. So same with this guy. So like, for example, the line 1 BCU needs the recloser information and the sync check information. And the SCU needs the breaker failure and the PTRC information. So if that information is needed for the tests that we're doing, we can go simulate that information in the goose publisher. We can right click on this stream, go to the goose publisher, and it'll highlight that stream in the goose publisher. So again, here in the goose publisher, rows are streams. We can expand out here and show the data items for that row. So this is the data set that is associated with that goose stream, and these are the data attributes. If I want to change one of these data attributes, I can simply click on it and change the value that will be published by Test Suite Pro for that goose stream. These columns are properties of the goose stream that we will publish, like that sequence number, time allowed to live, application ID. These are the simulate bits that will go out, and we want those to be off since we want the stream to look like the real device. So the organization of these is they're grouped by bay and then by IED. So right here, we see the bay and the IED, and then these are the streams within the IED. So go ahead and publish this, and then go back to LGOS. We notice that we are not automatically reading this data from the LGOS. We have an automatic update of the LGOS values that I've turned off for now so that I can demonstrate this. So the value that the chart shows now is that SD should be true because the LGOS is being, it's in the chart that the last red value of the LGOS was that it was false. But back over here, our goose tracker said that the stream is there. So we show an error condition there that the LGOS should be true. If we go then read it, that will turn green to show that the device is correct and that the LGOS is there. So if we want to go through that one more time with this other one. So on this one, SD.
 sim was false, I mean, I'm sorry, sim was true, and the GooseTracker was true, so we have a green box there. This one, the LVAL status is false, and it matches GooseTracker, so it's red. But we can go here to GoosePublisher, turn that guy on, go back to LGOS, update the data, and we are all ready for our unit testing. So now we'll go back to Christoph to demonstrate the system testing applications. Okay, Christoph, you should be the presenter now. And I would just like to encourage everyone to go ahead and submit any questions that you may have. I want to remind everyone, we do have several 620.50 experts behind the scenes ready to answer your questions, so it's just a great time to get 620.50 questions answered, as well as stuff that you're seeing on the screen. Okay, so welcome back. I will now go with you through the steps that we typically would do when we do a system testing. We will start to check the system status. That means, is the whole system configured like it is specified from the SCD file? So we compare the configuration of the system against our STL file that is loaded in the TestSuite Pro tool. We will verify the presence of all the goose messages, the subscription status from all the devices. So that's the initial steps. And once we have done that, then we are sure that at least from a configuration perspective, everything works as we want. And then we can start functional testing. And we've just explained to give two examples of functional testing. In the first step, we will show how we can do a control from the client and can test that. And the second part, we will look into the testing of a protection scheme, a protection and control scheme. In this case, the heat closing that we have as part of our design. Okay, so with that, let's get started. So when we start up, like Jim has explained before, we get this overview. So TestSuite Pro, when it starts up and connects to the system based on the STL file, it's making a couple of automatic checks for you and summarizes you the result. So what we see now, in this case, we have the whole substation present. We have all devices that are simulating. That are operating. And we see we have a possibility to connect to all these devices. So that's already good. All the devices communicate and we can connect ourselves to these devices. However, in the next step, we see that there is apparently a problem with the model. The model compare tells us there is a problem with the model. So we have to the model compare tells us there is an error. So let's have a closer look on that. We can select here, we see all the IDs and when we scroll down the list, TestSuite Pro has marked us the ID with the problem. So apparently the busbar protection unit has a configuration problem compared to what we expect from the STL file. So we now go on that. We can go through the tree and for that, I will make this a little bit larger. So we can dig into it and we see, okay, there is a problem with the
 GOOS control block, the GOOS control block, GSTB underscore F1, and apparently there is a problem with the GOOS ID. It's an element from the GOOS control block, and when we see over here, the discovered value that we get from the workspace, or from the system, the value should be GCP underscore F1. But according to our SL file that we have, the value should be GCP underscore F2. So with that, we can easily find all kind of configuration errors that we may have in our system, and then, yes, of course, go and resolve them. For the moment, we know that we still have an issue with the busbar protection unit, but we will not use that for the rest of the demonstration. So I don't need to fix it now, but of course, when you want to do the system testing, you need to be sure to fix these things before you use them in your test. The next step we see, we have also a problem with GOOS tracker. So let's have a look. What is the problem here? Apparently, we have three errors. And if you go down here, OK, we see we have two stream errors, and we have one configuration error. Two of these messages are from our busbar protection unit that we have just already identified. We have a configuration error. And indeed, when we look what we see here, test 3 Pro tells us it is missing a GOOS message with this GOOS ID, which is the GCP underscore. No, it's missing that in the file, so it's detecting a GOOS message on the wire with GCP underscore F1, but it doesn't know this message. So that's why it declares this message as missing on the wire. On the other side, it is expecting a message GCP underscore F2, and this one is missing on the wire. And obviously, as we just have seen before, because there is a misconfiguration, that's more or less what results in these two errors. So these two errors here, they are already cleared. That's because of the misconfiguration. So once we fix the configuration of the busbar protection unit, this should go away. But there is another error, and apparently, the bay control unit of the transformer bay 2 is not sending, or at least we don't see the message of the bay control unit of the transformer bay 2. So there is apparently a problem, and we need to go and see why this message is not published. We then also have an error in LGOS. So we go now to the LGOS and see what is the problem there. And for that, I make it a little bit bigger here. Then we see the whole matrix. OK, so we see two red squares here. One is the message from VCO1 that we have just detected. It's not on the wire. It's not on the wire, but SU1 should receive it, should subscribe to it. So apparently, SU1 cannot report that it gets this message because the message is not there. So that's an expected result of the fact that this message is missing. So that's OK. But then we see also that another message, the one that is sent from the transformer bay 2 switch control unit, test switch pro sees the message on the wire. But the VCO1 from the transformer bay 2 that is supposed to receive it doesn't declare it in its LGOS that it received.
 this message. And by coincidence, this is the same device that is also not sending. So I would suspect here is a problem with the configuration of the goose spot on the IED. So that gives you an idea what could be wrong. And then, of course, you need to go into the IED configuration and analyze what is wrong there. But from a system perspective, we can track that down and we see where we have apparently a problem. OK. So with that, we now know we have a problem with the busbar protection unit. We have a problem with the bay control unit of a transformer bay tool. We can fix those, or we can still start the test with the bays that are working. And that's what we want to do now. So now that we have identified the system is at least for the line 1 and the transformer bay 1, OK. So let's start to do the testing there. And in the first step, what I would like to do, as I mentioned, is to do a functional testing. So I want to verify that a control that is coming in from a client is received by the server. The server will operate the switch and we get the right information back. For that, we can use what we call the custom display in TestV Pro. What we see here is an overview of the substation with our bay line 1, the bay line 2, the transformer 1 bay, and the transformer 2 bay. We then also see the breaker. We can go to the breaker and we get more detailed information of where it comes from, of the status. We have the busbar switch and we have the line disconnector. So the busbar is connected, the line is connected. We get these informations. We have an option in this custom display to provide you more information. If you go here, you get the detailed information where it's coming from, the prefix of that logical node. And you also see with this little symbol here how TestV Pro is receiving that information. It is apparently receiving that as a goose message. But now to test the line 1, we will go to a more detailed diagram that I created where we now can also do operations. So what we added here is information from the logical node CSWI, which is in the bay controller, which is this part. We see that we receive the information through reports. And I also added information like the operation counter and the open, operate open, and operate close. As I have quickly shown in the introduction, when we are doing control from a bay control unit to a switch control unit and a communication in between, then we have a goose message where we have the open signal and the close signal that is sent down to the SU. And that are those signals that I have also put in this display. Now let's start and try to do a control. For that, we can click here on the Open. Test3Pro asks me if I really want to operate the switch. I say yes, because I'm in testing the system. So there is no risk behind. The substation is not yet connected, so I click Yes. And we see the breaker opens. And the CSWI information as well indicates that it's open.
 and allows us to close. What we also see is that we had here events for the OP open and we had an event for the OP close. If I click again on it, now we do the close and the switch is closing. OK, so with that, I could, in principle, test each switch manually. But in order to make that easier, we also have the possibility to prepare test sequences. And for that, we have here the test sequencer. So here, we can program a series of operations that we want to do from the test 3 Pro versus the system and the test. And this little test where we test the control for line 1, we basically see that we first do an operation of the breaker, then an operation of the busbar switch, of the line disconnector. Then we verify that all the switches have opened. Before we start the test, we verify that all the switches are closed. And then we close them back again and we verify that all the switches have closed. So instead of doing that step-by-step manually, we can do that here as an automatic test sequence. And I will show you how that looks like. So I will start the test here. And in order that you see what is going on, I will show you that in the custom display. I start the test here. We go to our custom display. And you see the breaker opening. You see the busbar switch opening. You see the line switch opening. You see the switch is closing again. So the test is obviously at the end. And we can go back to our test sequence. And yeah, we see everything has passed. So everything is good. So everything is good. We also see that up here where we have graphically all these steps visible as well, how they are executed in sequence. So the test apparently has passed with success. OK, so far, so good. Now let's do the same for the transformer bay. I also have prepared a script here. And now we run that same test on the transformer bay to see if everything works OK. So again, I can start the test. And this time, you observe it from the test sequencer. I start the test. The first steps are passed, initialization, breaker closes. Busbar disconnector opens. The line disconnector open operation has passed. But now the verify apparently fails. Now we have a red square. That means the test has failed. And if you look through the, and then we abort the rest, because we don't know if we still can do the rest. So now if you go down here, apparently there is a problem with the position from the line disconnecting switch. It would be expected to be on. But it's still, no, it would be expected to be off as we switched it off, but it's still on. So apparently we have a problem with the configuration of the line disconnecting switch of this transformer bay 1. Now we want to analyze that a little bit in more detail.
 could be the problem. So first, we reset the system so that everything is closed. And now, we have various means to look into more details. Let's first go back to our custom display and to the transformer bay. So as I mentioned, there should be goose messages with this open and up close signal going down from the bay control unit to the SCU. And in order to visualize this goose message, we can use a tool that we call logic analyzer. And here, I have the possibility to select signals I receive through goose to visualize them. And in order to do that, I use the data miner. I go to my transformer 1 VCU. And I basically need to go to data mine. Sorry. We are on our transformer 1 bay. That's the bay we have tested. We are going to the VCU control. I also show you how an operation looks that was successful. So we will see the operation of the CS of the line. Of the busbar disconnector, we want to see the position of the busbar disconnector. We take the position here. And now, we also want to see from the line disconnector that one that was failing. We want to see the up open. And from the SCU, we want to get the position of that same switch and direct that over also. OK, so that are the signals that we want to observe. And now, we can start the capture here. I go to our custom display. I first open the one that I know it works. It takes a little time, because switches typically have some time to open. And now, I try the one that did not work. So we see here, the up open is true. So that's good so far. But apparently, we don't get the feedback. So it abandons. And if you look at this in the logic analyzer, and we can stop capturing. And we can stop capturing. So we see here, the first operation of the busbar disconnector, we had the up open. After a while, the position changed from on to off. So far, so good. Then, we tried the other one. And here, we never got the position change. And that's why at a certain moment, we had a timeout. I mean, every control operation has a timeout. So when we got the timeout, it aborted. So apparently, the signals that we sent out from our VCU seem to be correct. But there seems to be something in the SCU. As we have seen in ALGOS, the SCU indicates that it receives the goose message. That's still there. So it still indicates the SCU, the TX1 SCU that is here. It still indicates it receives the goose message from the VCU1, which is this one. So it still receives that. So there seems to be another problem. If you still want to verify, we can also verify.
 With GooseViewer, we can verify messages. So we can capture messages here. And I can go back to my custom display. I can try and operate again. It sends the op open. It's waiting, apparently. It's still waiting until this cover will disappear. Now it has disappeared. And if I go to the GooseViewer, I see we have indeed captured a Goose message. We have captured two Goose messages. The first one was when the op open went to true. And the second one was when the op open went back to false. So the behavior is correct. The Goose message is correct. The only possibility that remains is a design error in the device, in the SU internally, that we need to look in the SU itself. At least now, we could at least prove that from a system configuration perspective, everything is working OK. And now we will go to the last test. Now we will do a functional test of protection scheme. And for that, we go back to our overview diagram first. So what we will do now is we will simulate a fault on line 1, a differential fault. And then what we should see, and then we will simulate the non-permanent fault. That means a fault that is cleared by the reclosing. So what we should see is the breaker to open, and after a while, to close again and remain closed. So if you observe this breaker while I start the test, you should see it quickly turn green and then turn red again. I click now, it turns green, and it goes back. It was apparently a successful reclose. I can now simulate a permanent fault that would have an unsuccessful reclose. And again, you can look at the breaker. And I click to apply the fault. It gets green. It recloses and reopens again because the fault was still present. So at least from a first view, that seems to work. But of course, you want to know it for sure. So you want to know a little bit more details. So for that, we want to analyze what is going on with our GUSIs and what is going on with our reports. We clear what we have captured before. We start a new capturing. We go to the report view. We start the capturing. And now I generate again a fault that opens and is a successful reclose. We can stop, continue capturing more signals. And we can go to the GUS view, can stop continuing capturing more signals. And now let's see what our breaker positions did. So we can look at the GUS message from SU. If you click on that, we basically see it sent the first GUS message when the breaker position was going off. And it has sent a second GUS message when the breaker position was going on. And if you look at the line protection, you can also look what happened here. The line protection, when we started.
 OK, we also have the release from the synchronous check there, which is always true in our simulation. We then had, in the first message, the trip that was true. The second message was when the trip went back. Then we had the up-close from the reclose that was becoming true. And that went back again. So that's the expected interactions of the signals. We also can look at our reports. We can quickly have a look at the report viewer. We have a lot of reports, because we have the measurements that are cyclic. We can filter that out. We only are interested in the event-based reports. So we can set the filters. Now we really see the reports that have been created as part of the event. And we can look at the line protection unit where we have the recloser. And when we look at here, the recloser has a state. As you know, a recloser is typically a state machine. IC61850 standardizes these states. And we see those standardized states that we walked through. So we had a trip from protection. We had the fault that disappeared. We had the wait to complete, CV closed, and a successful reclose. So that's what we can analyze based on the capturing we did. So that's more or less the features that you have in test suite in TSP Lite. But in the full version, we have some additional means to analyze what exactly is going on with this report, with this reclosing. And I want to introduce that with the single flow. So we have here another diagram that we call the single flow diagram. And this is showing the interactions between the different logical nodes and IEDs. So everything, so these light green boxes, those are IEDs. The dark blue is the logical devices. Those are the logical nodes. So what we see from our design, the trip from our PTRC is routed as a goose signal. You see that on this yellow square here, that this is a goose message. It's going as a goose signal to the breaker logical node. The breaker logical node has the position signal in a goose message that goes to the breaker failure, that goes to the recloser. And then, from the recloser, we have a up close in our design here. We have decided we do the reclose through the switch controller in order to make a reclose with SynchroCheck. That's where we get the SynchroCheck. And we have the up close in our design here. And we have the up close in our design here. And we have the up close in our design here. We reclose with SynchroCheck. That's where we get the SynchroCheck information in. And then here, what we already discussed before, we have the up open and up close, which is going into our logical node XCBR. So now we can, this is the static configuration. We also show some internal signals. So those are internal signals. They are not from goose messages. They are not necessarily send us goose message to this breaker failure. We still can capture them from the goose message to visualize. And then you, of course, have also the internal signal that operates from the different protection elements that go into the trip.
 Now, let's try and do a reclose. I can start the recording here. And I can, again, do a reclosing action. I mean, a fault that will result in a reclosing. And that all went fast, so it's not easy to follow. That's why we can stop now the recording. And this tool allows us to go step by step through what we have recorded. So we can start. We see the first thing was the op general that became true into the PTRC. The next step, when I click here, was that output, the trip general, that became true. And the flashing logical nodes, they received it. At the next step, and those steps now were more or less at the same time, apparently the breaker has opened. The opening goes back in the position. And as a result, all the fault is going away. And this is all happening more or less at the same time. The position is now indicating the switch is off. The breaker is off. The trip has been taken away. The next thing now is the breaker off. And before it has been initialized, the recloser now will, after the reclosing time, will send a reclose signal to the CSWI. The CSWI itself will send the close signal to the breaker. The breaker will change its position, send that back to the recloser. And as a consequence, those stop closes inactivated. And we are at the end of what was going on. So with that, we can analyze in detail what was happening during our test, how the different events run together. That's one way to look at it. We have already seen before the logic analyzer. Of course, we can also look at that in the logic analyzer. And I already have prepared a set of signals. And we will do a new chart set. But with those signals, we start recalling. And I will do again an operation. So you should now see the same thing that we have seen before in the logic. In the signal flow diagram, you now see that as a nice logic analyzer, this display like when you would have real physical wires and you would connect your logic analyzer to these physical wires. Now, Test3Pro is a virtual logic analyzer that takes the virtual signals that we get from our GOOS messages. So what we see, what is happening, we have the PDF that operates, the trip that operates, the trip that operates, which sends the recloser into the state trip from protection. After a while, we have the breaker that opens, which sets the recloser in the state fault disappeared. As a result of that, also the operates and the trip signal go away. We wait for the reclose time. After that, the recloser activates the OP-CLOSE. The recloser goes to the state wait to complete. So he waits until he gets the close position back from the breaker. He takes the OP-CLOSE away. Now he is in the state circuit breaker closed. He waits for the reclaim.
 time to make sure that there is not a re-trip again and after the reclaim time he went to the state successful, he waits for the ready time and after the ready time we are back to the state ready so the recloser is armed again. So there we can nicely analyze that everything behaves as we have designed it. And with that I hand over to Jim for the finder wrap-up. Jim before I switch it over to for you to be a presenter there is just a couple of questions it would be good to go over as it's something that Christoph just showed. Sure. Christoph can you please explain LGOS and its tracking feature again? Okay so LGOS and let me start a little bit before when we had edition one when we had introduced GOOS we realized that sometimes when something was not configured correctly things didn't behave as expected but it was hard to say is the communication misconfigured does the device really receive the GOOS message or is something in the device logic in the device connection misconfigured so it was hard to identify these issues. That's why in edition two we decided we need to have a mechanism that we know at least the configuration of the communication is correct. So that's when we introduced LGOS and an instance of a logical node LGOS is present in a device for each of the GOOS messages it is supposed to subscribe so if a device receives six different GOOS messages like we have typically here for our switch control units and I can show that again in the matrix as an example here the switch control unit from line two receives six different GOOS messages from different senders and so we have six LGOS in that switch control unit and whenever the communication stack receives the GOOS message that he is configured to he will indicate in this data object ST which is the status of the subscription that he receives it then we know at least the communication is designed correctly so if we still have problems then it must be a wiring internal in the ID. The LGOS is also useful when we talk about simulation virtual testing virtual isolation which we'll do in one of our future webinars but so basically when we have simulated streams on the wires compared to the to the real ones simulated streams from test equipment for those of you that are a little bit familiar with that we also would see in the LGOS that the device is now receiving a simulated stream so it's ready for the test in there and it's prepared and isolated so I hope that answers the question. As a follow-up question is LGOS mandated in addition to? No it's not mandated so it's still left to the designers of IDs but if you are a utility I would strongly recommend that you request from your suppliers to include it to have it available. From what you explained earlier does that mean the IAD should support the logical node LGOS? As I said they are not mandated but I would strongly recommend that as a design as an ID manufacturer yes I would strongly recommend you to support that because it helps your customers it also helps you when you have to do some testing of your devices.
 Because of time, I'm going to go ahead and switch it over to Jim. I do see a lot more questions coming in, a couple of them also on DTM, which we will get to. But, Jim, in the meantime, I'm going to make you presenter, if you want to go ahead and take it over, and then we will wrap up shortly from there with addressing more of the questions. All right, so we have created a tool kit for 61850 that contains all the products that support the protocol. Hold on, let me... Can you see my screen okay? Is everything working? We can. Just put it on presentation mode. Yeah. I think I didn't quite click it. There it goes. Okay. So, what we did was divided this functionality into three executables so we could simplify these applications. You know, if you take TestSuite Pro into a substation, you want that to be as simple and straightforward and obvious as possible, so we split the other functionality out into separate applications so you could have the simplest thing in TestSuite Pro. And as we saw today, you can validate, troubleshoot, and do simple automated testing with TestSuite Pro, and you can visualize substation data during the test. For DTM, that was what was driving the simulations behind TestSuite Pro during the demonstration today. And to create that simulation, DTM can parse an SCD file and automatically create a majority of behavior to simulate logical nodes and substation, like the recloser that Christoph showed you. DTM can also do complex automated testing to validate things like gateway mapping, state tables, or test sequences from a spreadsheet. It can also test reaction of a system to a simulation of extreme conditions that are difficult to create in real devices. For Navigator, like the name says, it navigates between related items in an SCL file, such as report control blocks, data sets, and data objects. It also automatically detects errors in an SCL file, and it can fix errors in those SCL files so they don't slow up a project waiting on revisions from another department or vendor. These changes can be tracked by Navigator by comparing the original SCL file to the revised file, and they can later be fixed long-term at the source, such as a vendor or another department managing the system configuration tool. So this shows the progress of a typical project going from concept and design to factory automation testing to commissioning and maintenance. And we have TestSuite Pro going through all three of those phases, and DTM simulating everything in the concept and design phase, which is then monitored by TestSuite Pro. Once we get to factory acceptance testing, the IEDs that we have available, we put them in the system, we turn those off in DTM, and we use the same workspace in TestSuite Pro that we developed during the concept and design, and you really shouldn't notice a difference in the operation of the system there. But that tests the IEDs that you have available at the time. Once you get to the substation, you take those same workspaces with TestSuite Pro and without DTM, and you're very familiar with the way they're set up and what behavior you should see because you've been working with them for a while. So the way this relates back to the 61850 engineering process, you have the ICD files from the vendors.
 that come into Navigator, you can compare different versions of those files and see what's changed in them. Maybe some errors got fixed or something got broken, but you can compare and see what the differences are. You can verify and see if there are any potential problems in there, and then you can also fix those, like we were talking about before, if you need to keep the project moving and then get back to the vendor and have them fix the original file. So you put those into the system configuration tool and create an SCD file, which can then be passed on to the IED configuration tools, loaded in the IEDs, passed on to DTM, loaded for the simulation, and passed on to Test Suite Pro to monitor and troubleshoot. So during operation and maintenance, many of our customers are continuing to use the DTM workspace created during development to feed their SCADA master that's used for training. They can also test new devices on this network and monitor or drive the testing with Test Suite Pro. Technicians can also be trained and evaluated on this network to diagnose and repair common issues that are seen in the field. Now we'll go back to Beth to finish up for questions. Okay. Let me just go ahead and take the screen back. Give me one second. Okay. So we are getting to the end of our hour, so I do want to just wrap up real quick. Just in case you came in late or a lot of people have asked these questions throughout the whole webinar, you will be able to review what you saw here today. We do post all our webinars onto our website, onto GoToWebinar, as well as a YouTube channel. I also wanted to point out that what you've seen here as part of the 621850 engineering process has referenced Helinks as a system configuration tool. We do act as a reseller for Helinks, so please feel free to contact me if you'd like to learn more about that or to set up to see a demo. And then as I mentioned, this does conclude the presentation part of the webinar, but we still have quite a few questions that haven't been answered, so we're going to go ahead and start to go through the questions and answers. Please feel free to continue to submit them. We will stay on as long as we have questions. So now is a great time to definitely ask the expert. So with that, I'm going to kind of go back to where we had left off with LGOS. There was a couple of other questions around that related to DTM as well. Can we supervise LSVS as well as similar to LGOS using DTM? I'll throw that out to Christoph or to Jim or anyone else. I just responded to that one in the chat. Oh, did you? LGOS is supervised. I mean, the visualization of the value of LGOS is not made by DTM. It's made by TESTV Pro. So it's reading the logical node from the server. So it's made by TESTV Pro. And yes, we can do the same with LSVS with the sampled values in principle. I don't think we don't yet have the matrix as such designed because we don't yet see so many LSVS, but in principle, it can be done as well. Okay. Is RGUS in R sample values supported as well? If not, is this on your roadmap?
 Throw that out to Christophe or to Jim. Sorry, I was muted. Yes, we do support RDs. Okay. Here's a little bit more of a high-level question. Can you explain the differences between DTM and TestSuite Pro? Well, DTM is for simulating large and complex systems and then also doing more complex testing scenarios where maybe you need to simulate a slave, go through a data concentrator, and then simulate the master above that to verify you got the right data through the data concentrator, whereas TestSuite Pro is more for monitoring and diagnosing things either on a simulation or in the substation or a combination thereof. And it does some very simple simulation of a boost stream or just of one or two IEDs but not a whole complicated system. And it can also do a simple test sequencer test where, you know, you have a sequence of commands you're sending out and validating values and so forth but not the kind of going into the simulation and changing values and then seeing how that affects the operation of the device under test. Okay. When a subscriber receives, which attributes in LDOS has to be checked for confirming the receiving? I probably can take that one. As I mentioned, I think I mentioned it in the LDOS, we have a couple of data objects. I'm not sure if you can go back to the slides if you want me to make you present again. Sure, I can do that. Then I could show you the slide that I have from LDOS because I think that's the easiest. Okay, you should be presenter. Yeah, and you should see my screen. So here is again the logic in LDOS as it is defined in the standard. And it's this data object ST, which is a single point status. So it has a data attribute ST1. If this becomes true, that means the IED is receiving that goose message. And if this is false, that means the IED is not receiving the goose message. And in order to know to which goose message this applies, we have here this configuration setting, the goose control block reference. So that's where I refer to the goose control block, which message that we indicate based on that LDOS. So as I mentioned before, if my SCU, my switch control unit is receiving six different goose messages from six different devices, it will have six of these logical nodes, each of them configured to one of the goose control blocks of the sending device to the one that it has to subscribe. And in the ST here, it will indicate that it receives it, that the subscription is working, that it gets the data. The simulate status that I mentioned before, that's where you would indicate that he gets a simulated message as well. I hope that answered the question. We have additional information like which was the last state number that we received from goose, what is the expected configuration and addition number. That can also be helpful in some problems where we don't really subscribe correctly. That can be a misconfiguration or whatever. We can get a lot of valuable information out here. Okay. Do we know what IED manufacturer?
 manufacturers support LN nowadays? I'm not up to date with all the latest, but I would, based on what I have recently seen, I think most of the IED manufacturers, the larger ones, are now supporting it, but I cannot guarantee. I would need to check. I'm not sure, Jörg, you happen to deal a lot with customer projects. Are you typically starting to see LGOs there for the different manufacturers? Maybe Jörg can answer that. But Jörg, you would have to answer that. Yeah, I need to always find the unmute button. Yeah, as you said, the big vendors, they meanwhile all support the LGOs and also LSVS if they support sampled values. The way the LGOs is configured is a little bit different from between the manufacturers, but the support is there. I know some smaller implementations, smaller vendor suppliers, I think they do not have it all, but for the bigger companies and let's say the newer relays, so the latest generation of relays, there you can find the LGOs nearly everywhere. Okay. Thank you, Jörg. Do we need real IEDs for this software to work? Can we just use the simulation software with relays, ICD files? Yes. You can just load the ICD file and simulate from that or you can load an SCD file and simulate the substation and the interaction that's been defined in a system configuration tool like Helinks to define the behavior of the substation. Okay. Can we analyze IEC 61869-9 SV profile using DTM and how many sample value streams can be analyzed together? Is Sri on the line? Could he take that one? Yeah. First of all, I think it's not DTM. It should be TESTV Pro. I mean, we analyze the messages with TESTV Pro, not with DTM. DTM would generate the simulation of them, whereas TESTV Pro would analyze the evaluation. So, right now in TESTV Pro, we have a sample values tracker, like we have the boost tracker, and it shows the streams that are out there and you can track the streams that are on the wire. And, you know, I'm not aware of any limit on that, but we could certainly test that and see what the limit for the number of streams is. You know, it's just like a goose tracker in that you see the sample value streams as rows and then as columns, you would see the different properties that each stream has, either defined in the SCD file or on the wire. Okay. Is the tool able to test sample values in digital status through ProcessBus? Can you repeat the question? Yes. Can you repeat the question?
 Sure. Is the tool able to test sample values in digital status through ProcessBus? Well, I think digital values would probably be goose values coming over, right? Would you agree, Christoph? Yeah, I would assume so, and yeah, basically, when the tool is connected to the ProcessBus, yeah, then that's what we basically just showed. Yes, then you can analyze the goose messages on the wire, and you can analyze sample value messages on the wire. So yes, it can. Okay. So we don't really care whether you have a separate ProcessBus and a station bus. We can connect to multiple adapters. You just tell it where the source of your signals are on which adapter, and it will just kind of combine that all into one display and show it together. Okay. The next question I have here is going back, Christoph, to when you were showing test sequencer. Is it possible to parametrize these sequences? Sometimes the bays are very similar, only IED names are different, but the projects are copied from a template, for example, so all the logic is the same. At the current state, not yet, but it's definitely planned as one of the next features. Yeah, if you'd like to get back with us on that, we're looking at different ways to do that, different approaches. If we can look at how your data is organized and how you templatize that stuff, maybe we could optimize our implementation of that to fit your application. Okay. Can you explain what is LSVS and how it differs from LGOS? Yeah, in principle, it's almost the same, except that it has – the sampled value configuration has a few different parameters than the GOOS configuration. You have seen before the LGOS data that I have shown. Some of these things, like needs commissioning or so, last date number, of course, the last date number doesn't exist in LSVS, but the key elements, like the reference to the control block, the ST to indicate that the subscription is okay, or the SIM ST to indicate that I receive a simulated status, they are the same, and it is the same behavior. It's just as the GOOS message has some different characteristics than a sampled value message, some of the data objects in the LSVS are different than the ones in the LGOS. Okay. Here's one asking about both DTM and TestSuite Pro. How many IEDs can be simulated in DTM, and how many IEDs can be monitored, specifically GOOS, with the help of TestSuite Pro? Typically the biggest systems we're seeing these days are about 200 IEDs. It's going to depend really on the complexity of the SCD file and the complexity of the IEDs, the number of data attributes and logical nodes that are in each one. We haven't run into any performance limitations. The fact that DTM is distributed across multiple computers, we can always expand to more computers to host more devices, and you can do the same thing for TestSuite Pro, where you can limit the scope of the TestSuite Pro.
 workspace that you're using to say a bay or something that you're focused on so the size of the substation is not really a big concern. Okay well I think that wraps up most of our questions. I see the questions start to slow down. It looks like we've answered majority of them online as well as in person here. We will be sending out a link to the recording of this webinar as I mentioned earlier as well as a copy of all the questions and answers that were asked on the webinar. So I appreciate everyone taking the time to spend with us and learn a little bit more about our tools and hanging on a little bit longer to get through everyone's questions and we look forward to hearing from you at our next webinar. Thanks everyone for joining today. Thank you.


Video ID: z6LEBEPv8eE
Good morning, everyone, and good evening. My name is Beth Capellas, and I'm the sales and support manager at Triangle Microworks. I'd like to welcome everyone to our latest webinar, Virtual Isolation and Testing with IEC 61850. Before we get started, I'd like to go over some housekeeping items while everyone is still joining. The webinar is scheduled to be one hour long. We will be taking a couple of breaks to answer questions. The audience is on mute, so please submit your questions online using the webinar toolbar under the section titled Questions. You can submit questions as soon as you have them. Don't feel that you have to wait until the end. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly. We will read and answer some questions at the breaks, and we will have some time in the end to go over questions and answers. Before we get started, I'd like to introduce the folks behind the scenes answering questions. Today, we have with us Jim Coates, president of Triangle Microworks, and Matt Green, senior application engineer and QA tester. Next, I'd like to introduce our speakers. First, we have Christoph Brunner, president of IT4Power. Christoph is a well-known utility industry professional and 61850 expert with over 30 years of industry experience. Christoph has been involved in the development of the standard since the beginning. He is convener of Working Group 10 and member of Working Group 17, 18, and 19 of IEC TC57. He is an IEEE fellow and a member of several IEEE working groups, as well as a member of the PSRC main committee and subcommittee H. Next, we have Jackson Moore, application engineer at Triangle Microworks. Jackson received degrees in both electrical engineering and computer engineering from North Carolina State University and has a background in power system engineering. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer where he designed and developed load management control systems for multisource microgrids ranging from 1 megawatt to 30 megawatts. In his current role of application engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges our clients face. So now that you know everyone, I'd like to give you a little background on Triangle Microworks before we get started. We were established in 1994 and have over 28 years of communication protocol experience. We are headquartered in Raleigh, North Carolina, and have customers in over 70 countries. We work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs in development time. Licensing the source code library also includes support for your implementation and any issues that may arise while you're in the field. We also provide testing tools supporting these libraries and protocols. These tools are used for communicating, testing in the lab or the field. They're designed to troubleshoot, simulate, and automate testing for both a single device or a complete substation.
 Lastly, we also provide a SCADA data gateway, supported on both Linux and Windows, for protocol translation and data concentration with standard SCADA protocols as well as OPC. It is available as software only or as a pre-installed on hardware. We are heavily involved in the standard committees that support SCADA protocols like UCA Group for 61A50 and DNP User and Technical Committees. Through our participation, we help define the protocols, stay up to date on the changes taking place, and ultimately provide feedback based on our customers' needs as the protocol continues to evolve. So now that you know a little bit more about Triangle, I'd like to go ahead and hand it over to Christoph to kick off the webinar. So Christoph, I'll go ahead and make you presenter. Thank you, Beth, and welcome, everybody, as well, from my side. Okay, I hope you see my screen now. Yeah, we can see it. So before we do a demo as a second part of the training of this webinar, I will do a little introduction into the aspects of testing in a live substation using virtual isolation. So when we talk about virtual isolation, that's typically what we have to apply when we encounter a live substation, when we are testing in a live system. Now what are the issues? When we look in a conventional system where we had wires between relays, between our IEDs to implement the protection schemes, we had wires going down into the switchgear, connecting the trip signals to the breakers, connecting the analog signals from the CTVTs back, and we wanted to do a test in a live system. The isolation of the IED under test was relatively straightforward because we typically had built-in test switches that we could open, and once the test switch was open, we could connect the test equipment, inject our test signals, and perform the test on a single IED while the rest of the system was still in operation. Now that we have IEC 61850 with process bus, the situation looks a little bit different. Instead of the wires between the relays, we have communication services like goose messages or sampled value messages coming from merging units that are exchanged over the network between multiple devices. So when we now want to test a device without impact having an impact on the rest of the system, we need to have different mechanisms because it's not that easy anymore to physically isolate it. That's why we introduced basically virtual isolation that we will explain today how it works in 61850, and our test equipment basically will just be connected to the communication network itself, can observe what is going on, can inject test messages, but of course we need to make sure that these test messages don't interfere with other messages and that messages from the device under test will not initiate actions in other devices. So how is this done? That's what I try to explain to you now in the next few minutes as an introduction to this webinar. So basically what we need to do is on one side we have to separate the signal flow that we have between the different functions. The signal flow that is produced for test purpose, we need to be able to separate that from a normal signal flow. Then we need to be able to have dedicated messages from test equipment that stimulate the test.
 like that, initiate certain conditions so that we can perform the test on the equipment under test. We may also make sure that we block physical outputs so that our breakers don't operate if we don't intend it. And last but not least, even with blocked physical outputs, we should still be able to support the observation of the test results. Before we go into the detail, I would like to make a little bit of clarification about what is a single flow and what are the messages that we have between the equipment. There is some differences and there is also differences then how we handle them in the standard. So what we see here is a couple of IEDs, the yellow boxes are IEDs. They run applications like protection algorithms, control algorithms, they may have interface functionality into the switchgear. And these functions are typically represented in 61850 with logical nodes that have data objects as outputs. And they may also have inputs that are not standardized inputs, but typically they also receive signals in to perform their activities. In order to implement our applications, we have to exchange information between these applications represented by the logical nodes, which are the function elements of the application. So in our practical world, we have signals going on between different logical nodes, different functions realized by these logical nodes, and they are received by other ones. So that's completely independent of IC61850. That's how applications work. In a conventional world, these signals that have to be exchanged were routed on physical IOs, on physical outputs here, physical inputs here, and then internally they went again to the application. So that's the single flow. Now with IC61850, we replace the wires, the physical wires that we have here with communication. So our signals are not going to physical outputs anymore, they are now going to virtual outputs. And then we have our communication part in our IED, which is the 61850 communication, and that takes these virtual outputs, puts them in a message, a goose message or a sampled value message, communicates them over the wire. Over here, we receive the message, we extract the virtual inputs from the message, not necessarily everything in the message is required by every device, so we extract what we need and then we have again the signals available. So there's a clear differentiation between the signal flow, which is between applications, and the message flow, which is the communication itself, which is typically done from the communication stack. Now let's go into details on how IC61850 supports our isolation and test. So there's basically three topics that we will address, separating test and normal signal flow, then separating simulated and normal messages, so that's the two layers we introduced before, and then also block outputs and support observation. And what is summarized here, how it works, we will show in the slides. So let's start with separating test and normal signal flow. The first element that we use for that is what we call a mode that we can apply to a logical node or a logic device. IC61850 supports five different
 modes and we see them here on this little demo application where we have the first logical node off, we have a logical node that is on, we have blocked, test and test blocked. So that are the five modes that the standard supports. And what we see when the logical node is off, the signal that is sent, the goose message will still be published but there is no valid value behind the signal that's why the quality is set to invalid. The little application here is basically assuming a busbar protection and we have different PCUs that control different breakers and just for the illustration they are all in different modes here. So that are the five different modes. Now what is the impact of these modes? So the first before we go to that maybe another point so we have as I mentioned before we can apply the mode either to an individual logical node or we can apply it to a logical device. The important thing is then the behavior of a logical node which is basically a combination. So in this little example we see here this first circuit breaker has the mode on but the logical device has the mode set to test so the behavior of that circuit breaker will be test because it inherits the stronger restriction from the logical device on the hierarchical level above. The second logical node which is in mode blocked also inherits the mode test and as a result its behavior will be test blocked and for that we have this data object behavior that is mandatory in every logical node which indicates us what is the real behavior of this logical node derived from its own mode setting if available and from the mode setting of the logical device above. So now how does the mode impact our system? So the first when quality of the sending device so when the logical node that is publishing is in mode on quality will be set to good and that means all of our logical nodes independent of the mode they are they will receive. So even if something is in mode test it will still receive signals from a logical node that is in mode on. It gets more interesting when our publishing logical node is in mode test. So the first thing that we now see is that the quality of the signal besides being valid it also has an additional information now a flag that tells this quality was sent from a device that is in mode test. As a result logical nodes that are also in test mode will continue to receive that signal because it may be part of a test that they participate. However logical nodes that are not in test they are either on or just blocked they will not receive anymore that signal which is visualized here with this little cross at the entry and the reason is indicated here it's blocked because of the behavior the behavior is on but the quality is test so that's why the signal is not received and with that we basically can make sure that if the device is under test sends out a goose message it will not as an example when we test the protection it will not initiate somewhere a breaker failure function that is not in test because that breaker failure function will not react on that signal because the quality
 is set to test. OK, that was the first part. Now, how do we deal with simulated and normal messages? The standard supports the possibility to add, besides the regular goose message, a message that is exactly identical with the exception of a simulation flag in the message to be used basically from test equipment to control the test. So in order to differentiate them, they have a dedicated flag in the message header, which indicates the message that is coming from this test device as a simulated message, whereas the regular message is a normal one. Now, in order to make ID switch over to receive simulated messages, we have a logical node. In the logical node LPHT, we have the data object SIM that we can activate. And when that is set to true, the ID will switch over for any of the streams that it receives to the simulated one if there is one present on the wire. So we see here now we have the same ID like before. We illustrate a second ID that is now indicated. Its LPHT SIM is set to true. For the moment, there is no simulated message present, so it receives the normal message. But as soon as a simulated message becomes available, it will switch over to the simulated message, and it will not use the other inputs anymore. And this is typically handled at the stack level because that's where we are on the message level. So the stack will now put into the virtual inputs the signals from the simulated message instead of the signals from the regular message. So with that, we can differentiate our normal GOOS streams and the simulated GOOS messages that are sent from a test equipment in order to send the test, the signals that we require to do the test. An important element that we have as well to check that everything has switched over is the logic node LGOS. So what we see here, we have a table. We have an overview. This is a message that is being sent by a device here. The message exists as well present from a test equipment. We have this first device that is not set to receive simulated message, so it continues to receive the normal message. The second device is set to receive simulated messages, and then it's switched over to that, and we can verify that in the data object SIM status of the logic node LGOS. Now this is also true besides the status that indicates that the subscription is working and it's receiving it. Here now it indicates, and by the way, I'm now receiving the simulated message and not the normal one. So when you start the test, it's always a good idea to verify that everything is configured as you wanted before you start the test. And LGOS is one of these means to do these verifications. Last element is how we can block outputs and how we can still observe the results. So basically, first of all, it has to be understood that the mode blocked and test blocked only applies to physical output. So if you have an ID that is an example, an interface to a breaker, and it receives information to operate the breaker, either through a control service or through a GOOS message, it will process that request, and usually it will then activate maybe a.
 open coil or a closed coil, but when it's in mode blocked, it will not activate that. But still, we may want to know that it would have activated as in order to verify that the test has succeeded. And for that, we introduced a couple of data attributes into the standard. The first one indicating that a request has been received. Then the request is processed, and at the moment when the IED activates the output, it also activates a binary Boolean signal that the operation is OK, and it provides the timestamp of when it would have activated the output, or when it would not be in mode blocked, or when it has activated the output when it's not in mode blocked. So we get always these signals back, independent if the output will really go out due to a blocking. And the timestamp and this Boolean can be used to verify, and with the timestamp also measure the performance on how long it took to operate. So that's basically the data attributes that we have added. And with that, I hand back to Beth for the first question and answer session. Yes, we did have several questions come in. So what I'm going to do is Jackson, I'll go ahead and make you presenter so you can get ready on your end. And then Christoph, I do have a couple of questions for you. First question. When I do a site acceptance test of my system, do I need to apply virtual isolation? No, we basically only need to apply virtual isolation when we are testing in a live system. So a site acceptance test where I test the normal behavior of my system is done without virtual isolation because the system is not live yet at that time. However, it is important that you also, during the site acceptance test, test your virtual isolation. So as part of the functional testing, you may also want to do some of the virtual isolation tests, like when you would be in a live system. So to verify that everything works. So when you have to do the tests later in the live system, that you don't have surprises. OK, I have an additional question. What if a goose message does not include the quality of the DO? How can I ensure that I do not get misoperations when the device that is under test sends out a goose message? Well, when the goose message does not include the quality, you don't really have any possibility to do virtual isolation because you will not be able to differentiate. That's why we always recommend that in a goose message, you always have to include the quality. It's not only for virtual isolation, but the quality as such also gives you information if the signal is valid. So just sending a status value over in goose without the quality itself is not really a good idea. OK. OK, and then I'll do one last question and then we'll move over to Jackson. If an IED is in test mode and does send goose messages, does it need to send the goose message with the simulation flag set to true? Or is it sufficient that the quality attribute test will be set to true for each DO that is in the goose message? An IED that is in test mode should never set the simulation flag to true. The simulation flag is really used for messages coming from test equipment. So an IED is not supposed to set the simulation flag true. That's why we have the quality attribute test. With that, we can indicate which signals are coming from a source.
 is in test and that's sufficient. The simulation flag is really reserved to messages from test equipment, messages that are created instead of the regular ones as proposed of our test. Okay, and actually I am going to ask one last question before we switch over. We had a one just come in. How does Edition 1 and Edition 2 differ with regards to SIM, test, and LGAS, etc.? Sorry, can you repeat that? Sure. How does Edition 1 and Edition 2 differ with regards to SIM, test, and LGAS? Okay, first of all, Edition 1 didn't have LGAS at all. That was added later. SIM existed in a somehow similar way in the goose message as a test bit flag, but the usage was not very clearly specified. It didn't exist in the sample value message, and the test quality as such existed, but again, the mechanisms how to use this were not very clearly described, so it was more or less left to the vendor to deal with it how he wanted to deal. So most of these features have really become usable only with Edition 2. Okay, thanks Christoph. With that, I'm going to go ahead and hand it over to Jackson to take us through the demo. All right, just make sure you can still see my screen, correct? Yes. All right. Thank you, Beth, and thank you, Christoph, for your presentation. As Beth mentioned, my name is Jackson Moore, and I'm an application engineer here at Triangle Microworks, and today I'm going to be using two of our software tools to demonstrate some of the virtual isolation concepts that Christoph just introduced. So the first tool I'm going to be using is DTM, or Distributed Test Manager, which you can see on my screen now. So you'll see me referring back to DTM several times throughout the demonstration, so I just want to go ahead and give it a brief introduction. DTM is our system simulation tool, and today it's being used to simulate the protection, automation, and control functionality of a 61850 substation. The simulation that you see here was generated from an SED file, which for those of you who are less familiar with 61850 is a system configuration description, and it outlines the communications and functionality of the substation. So over here in the IED folder on the left, you'll see all of the 61850 devices present in our simulation, and in the window to the right, you can see the physical layout of our substation in the form of a single line diagram. Now we can interact with this simulation in a couple of different ways. We go directly over to our devices and see their data models, or we can interact with this one line by double-clicking on a breaker to open and close it, or clicking one of these red buttons over here. That will trigger a process behavior such as a fault or breaker failure. So a simulation like this can be used of a substation early on in the engineering process, even as early as the system specification stage when you would have an SSD, and continuing on, you can have all of your IEDs simulated once you have your SCD, and then additionally, once physical IEDs are available later on in the engineering process, they can be connected via an Ethernet network, and the corresponding simulated IED over here in the tree can be disabled. The real IED will communicate with the rest of the simulated ones, and this allows us to verify the behavior of the real IED, confirming that it's going to respond as expected during normal operation, fault scenarios, and even virtual isolation, which is what we're discussing today. So while the simulation will be running in DTM, the bulk of our time today will actually be spent in another tool, TestSuite Pro or TSP, which is where we're going to be monitoring and testing our substation. TestSuite Pro, which I'm bringing up now,
 is a collection of troubleshooting and testing tools specifically tailored for 61850. Not only is TestSuite Pro vendor agnostic, meaning it'll work with devices from any vendor which support the 61850 standard, it's also the most complete and feature-rich tool of its kind on the market today. Now before we dive into our demonstration of virtual isolation, I do want to give you a quick tour of the application and point out a couple of its highlights. So on our left-hand side, we see our selection of tools. The tools that we see here are what we're going to be using to interact with our 61850 devices. When I select one of these tools, it fills the pane on the left. Now on our right-hand side, we see our collection of displays. These provide different ways to visualize and monitor data collected from connected devices. The list of available displays is seen here at the bottom and I'll just click through a couple of these. We have logic analyzer and signal flow that we'll be looking at a little bit closer here in a minute. But first, I want to walk you through a couple of the tools and displays that we are going to be using today. So let's start with system status seen here on the left. This is a home page of sorts where we're able to manage connections to the IEDs that we're going to be working with. You can see that I'm connected to all devices present in the SED. Importing an SED was how I started this testing tool today and we can ensure that all those devices are online and accessible on the network. It's worth noting that although we are using simulated devices, this is really transparent to TESTV Pro. By that I mean that TESTV Pro is actually communicating with these devices via the 61850 protocol and in that sense it doesn't really know or care that these devices are physical or simulated. So let's move on down to the compare model tool. This tool compares the configuration file loaded into TESTV Pro, in this case as I mentioned in SED, with the configurations discovered from the connected IEDs, which you can see from the drop down list up here at the top. I can select which one I would like to compare. It serves as a great sanity check and it helps you make sure that you're comparing apples to apples before moving forward into more complex tests. Mismatches will be highlighted in red as I expand out the compare model, but in this case we see all green which indicates that there's no discrepancies between our SED and our configured IEDs, which means we're ready to continue. I do want to point out this green check that you might see over here on the left in our system tool selection area. If there were any errors in any of our devices that we'd want to compare a model on, instead of having a green check, we would have a little yellow circle with a number of errors indicated. But as we said, we're all good here so we're ready to move on to our next tool, the goose tracker. So this is a network sniffer of sorts, which monitors the network for goose traffic. Since we've already loaded an STD file into TestSuite Pro, which defines what goose streams should be published, this tool is able to confirm that all expected goose streams and only those expected goose streams are available on the wire. Duplicate, missing, or misconfigured goose streams can be quickly identified and caught using this tool. But as we see here, all of our goose streams are valid and they match what is defined in our SEL file, and we have our green check over here on the left. So that means we can go ahead and jump on with confidence to Elgoss. So Elgoss, in kind of a similar vein, provides an overview of goose subscriptions in the form of the matrix that we see here. The colored flags which make up the matrix are determined by a combination of two things. The data gathered from the Elgoss logical nodes on the goose subscribers and the goose messages themselves that TestSuite Pro has detected on the wire up in goose tracker. So the result is that you can quickly determine the subscription status and any possible errors at a quick glance. In this case, we see we do have four warnings. If I hover over that device, I can see what they are.
 Our test subscriber, which subscribers are listed up here at the top, test subscriber being on the far right, does have four warnings. And that's because it does not have an LGOS logical node, so therefore we're unable to read the subscription status to know if this device is actually receiving these streams. That's expected behavior today, so I'm going to go ahead and acknowledge those warnings. So now I'm going to take a look at the two tools and the two displays that we're going to be working with most closely today. The first of which is the mode, behavior, and sim tool. So this serves as a centralized control interface for monitoring and changing the mode, behavior, and simulation configurations of a device. As Christophe mentioned in his slides, these settings are what allow us to configure what types of data our device will respond to and what that response is going to look like. We'll be seeing this in action in a few moments, but as a quick reminder, this LPHD sim status up here determines if the device will react or respond to received boost and sampled value messages, which have the simulation bit set, whereas the mode down here determines the quality of data attributes sent from the logical node, as well as what external references received by the logical node will be used, and that's based again on their quality. So the behavior right next to the mode, in a general sense, as Christophe mentioned, this is a feedback that you're going to receive from a device to let you know that the mode command has in fact been received. And finally, the last tool that we're going to be working with today is Goose Publisher, which as the name suggests, is used to publish Goose messages. So I can see each one of these little play buttons is next to a different Goose stream or Goose control block. If I expand one out, I can see what's available, the data attributes in that data set, and if I want to, I can change these and publish them. Other values that you might find in a Goose message header are found up here at the top, and those can be changed as well. Now let's shift gears and take a look at our displays seen over here on the right, starting with the test sequencer. So the test sequencer is used to build and run automated test sequences. Now during a typical test in the test sequencer, we're going to be setting some initial conditions to perform some action and then verifying the expected results. So in the test that I'm going to run right now, we're actually going to be that we have control operation over the breakers in our simulated substation that we saw over in DTM. So as a first step, it's asking us to ensure in the custom display for line one that all switches are closed. I'll come up here to my custom display, switch to my line one view, and I see that in fact both switches and my breaker are closed, so I can go ahead and hit yes, and as I come down back to the test sequencer, I see that it's going to open those, which we can see if we come back to the custom display and then close them back, verifying that we do in fact have control over our breakers. And we'll see this applied a little bit later to a test done in virtual isolation. So now we come to signal flow diagram. So the signal flow diagram is where we're going to be spending a good bit of our time today. This provides kind of a flow chart-like view of our 61850 communications, allowing you to clearly see how signals are flowing between IEDs. To make this display digestible, sometimes you're going to make multiple of them instead of one for your whole substation, usually broken up based on functionality or role. So let's take a look at how signal flow works before moving on. I'm going to go ahead and hit record down here.
 here, and now switch back over to TestSuite Pro where I'm going to trigger a differential protection on line one by clicking this little red button up here. Now when I do that, it will cause the breaker to trip. This breaker here on the fall left, it'll pop open very quickly and then reclose as the reclose functionality kicks in. All right. If you blink, then you miss it, but this breaker did pop open very quickly and then reclose. But it's okay if you did miss it because we captured it over here in the signal flow diagram. So I'm going to begin to step through the sequence of events that occurred. As I hit this arrow here on the bottom right, we can see the first thing that happened is our differential protection fault came to true. That was a result of me clicking that momentary button. Stepping on through, that triggers a trip signal to be sent out from our PTRC, which was received by our breaker interface unit in the form of the XCVR, as well as our breaker failure function and even our reclosing function down here. As I just click the button for a moment, we see the differential protection drop out, or I'm sorry, the differential fault drop out, the PTRC drop out its trip signal, and then finally, we do see the breaker open in the form of this pause ST valve being opened. And if I hover over the line, you can see its initial condition was on, meaning the breaker was closed, and it's now switched off. And that signal has been published to our breaker failure, our recloser, and our switch controller. And now what we wanted to see, our recloser has issued that reclose request, which has been passed to the switch controller, and in turn, back up to our breaker interface. And finally, our breaker has, in fact, reclosed. So you can save these sequences that you see here as a recording to play back later, or you can clear them out if you want to start with a blank slate for the next time you want to start or run a test. I'm going to go ahead and just clear the data out. So now as we move into the demonstration portion of today, I want to do what I mentioned earlier and switch to a little bit of a smaller signal flow for the sake of simplicity. And this is going to make things a little bit easier to follow. So let's do that, and then we'll go ahead and get started. So in this first example, or this first test that I'm going to be performing, our goal is to test the control functionality of the breaker QA1 and line 1, the same breaker that we just saw open and close a moment ago in our signal flow diagram. The control functionality for this breaker is implemented in a device called a Bay Control Unit, or BCU. So our device under test is going to be the BCU, represented here in our diagram right here in the middle. In order to not impact the rest of our substation, we want to virtually isolate this BCU from the rest of the system. By doing so, we'll make sure that our breaker control functionality works without actually opening the breaker. Next we're going to configure the device to receive simulated inputs, or more specifically simulated goose messages. And finally, we will be publishing those simulated goose messages and manipulating them in order to exercise the BCU and the breaker control functionality. So step one, we need to virtually isolate the device under test. To do this, I'm going to go back over to our mode behavior and sim tool that we spoke a little bit about earlier. So here I need to do two things. I'm going to be changing the LPHD sim and the mode. But first for the mode, I want to change the mode from on, which is the typical mode, and change it to test. And when I do this, this is just a control interface.
 interface, but it's actually sending out a request to our IED, which is going to receive it and decide, yes, I will switch into mode test and then report it back to us in the form of behavior to let us know that it did, in fact, receive that request. And in a moment, we should see that reflected over here on the right in our line 1 BCU changing to mode test. Let me just go ahead and clear out data. Yeah. All right. All right. So the next thing I need to do is change our LPHDSIMP status to set that to true. So what that's going to do is it's going to set our device up to receive simulated inputs. Now, as Christophe mentioned, there is a little bit of a latch behavior here, meaning that our IED will remain subscribed to the normal stream until a simulated goose stream becomes available. At that point, the device will latch into the simulated stream and remain subscribed to the simulated goose streams and only the simulated streams until LPHDSIMP is set back to false. So we're going to see this kind of in action right now because even though our device is going to switch to have LPHDSIMP set to true, we don't yet have our simulated messages out on the wire. So that means we're going to continue to process the real goose messages. That's also indicated by the little red arrows that you see coming into the control switch right here. In a moment, those will switch over to red Xs, kind of like what we see over here on our XCBR in our system control unit. Because this device is still in on or normal mode and is not receiving or processing messages with quality test, we have these little Xs over here. And if you hover over them, it'll tell you why it's there. As I mentioned, the behavior is set to on in our SCU on the right, but the quality of the messages here at the entry point are set to test. So therefore, they're being ignored. So now we've set up our line one BCU outputs to carry quality test and have LPHDSIMP set to true, meaning that we'll receive simulated input messages. It could be said that our BCU right here in the middle is virtually isolated, meaning that it'll no longer impact the rest of the substation. However, in order to actually exercise some control functionality on this device, we need to begin manipulating its inputs or subscribe to goose streams in this case, because that I briefly showed earlier. So in the Goose Publisher, as I mentioned, we see all the goose streams defined in the SCD. Expanding one, I can see the data attributes found within it. Now, in order to test our BCU, I can see that I'm going to need to simulate three data attributes, because I have three inputs coming in, the first of which is the breaker position coming up over here from the XCBR on the far right. So that is found in the SCU. So let me navigate down to the SCU, expand that. Yep, I see the breaker position right there, and I'll go ahead and begin to publish that device. Additionally, I'm going to need a reclose or close signal coming from the LPU and a synchro check release. And those, as I mentioned, coming from the LPU, so let me expand that out. Yep, I see my signals coming in right there and right there, so I'm going to go ahead and begin publishing that device. All right. When I did that over on the signal flow diagram to the right, you will have seen these new yellow boxes come up. So now our simulated messages are being published out on the wire. And because our device is set to LPHT SIM equal true, it's latched onto these simulated messages and is now ignoring the real goose messages that are coming from our actual device. And this is interesting.
 indicated, again, let me just zoom in a little bit so you can see, by these red Xs right here on the entry point, whereas we have the red arrows coming from our simulated devices. Same thing for our breaker status, which is right over here, and I just want to rearrange this to make it a little bit easier to read. All right, so to recap, we've both virtually isolated the BCU and begun publishing the necessary simulated inputs. We can now begin testing the BCU by manipulating those inputs without fear that they're affecting the rest of our substation. One more way to give us a little bit of confidence would be to come back up here to our LGOS, where we do see these little green boxes from earlier have, in fact, switched to yellow. And if we look down at our legend down here at the bottom, we can see that that's indicated when SIMST is equal to true and ST is equal to true, which, as Christophe mentioned, this indicates that our device has switched over to the simulated stream. So we know we're good to go. So I want to verify now that our BCU is able to operate. What I'm going to do is come back over into DTM and actually... Now we see this reflected on our signal flow diagram with the off signal up here. That's the breaker position, off indicating open. And in order for our breaker to close, it does, in fact, need to be open. However, our BCU, as we see with this little red X, is not actually subscribed to the breaker position coming from our real XCVR. It's actually subscribed to the breaker position coming from our simulated goose stream. So let me go down to Goose Publisher and change that position to off or open, in this case, as well. All right. And we see that reflected here. The last step we need, last initial condition we need to set up so that our BCU or breaker control function will operate. We do have a requirement that our device needs to be synchronized or both sides of the breaker need to be synchronized in order to close. So I'm going to expand out my SCU, Goose Publisher, and I'm going to change the release sync and set that to true as well. All right. So now we're ready to perform our test. But before I do that, I'm going to go ahead and hit record. So what I'm about to do is I'm going to pulse the reclose or close signal coming here on the left. And we're going to make sure that our breaker control function does, in fact, pass that on. So let me go ahead and do that. Set it to true, publish, and then set it back to false because this is typically a pulsed signal. All right. Let me stop the recording and step through the results and see if it's what we expected. So first we see the reclose request coming in from our simulated Goose Publisher. It was set to true. That passed into our breaker control function. And as expected, it did send out our op close. Now because our breaker interface unit in our SCU over here on the right was not set to mode test and that message was, in fact, ignored. So everything worked as expected. We were able to test our breaker interface unit without affecting the rest of the substation. In fact, we can switch over to DTM and we'll see that our breaker is, in fact, still open, letting us know that this test was a success.
 So now let's repeat this test, but with a little bit of a twist. This time around, I'm going to expand it to include the breaker interface unit, or breaker interface function, seen over here on the right, implemented in a switchgear control unit, or SCU. Virtual isolation tests like this, which span multiple devices, are not uncommon, and they have the added benefit of allowing an entire protection or control chain to be tested simultaneously. Now in this case, we do not want to operate the physical breaker still, physical breaker being the breaker that's simulated in DTM. So to accomplish this, we need to not just put the device in mode test, but mode test blocked. And this is going to block the outputs of the SCU. As Christophe mentioned earlier, in order for us to determine if the test is a success, and to tell if the breaker would have, in fact, closed had it not been in block, we're going to be monitoring the pause op OK. Let me just go ahead and expand this device down here a little bit, which is the data attribute that you see down here at the bottom. Op OK is a data attribute used in 61850 for this exact purpose. Its whole purpose is to just let you know if an output would have operated had it not been blocked. So in this case, if we do see pause op OK go to true, we'll know that our breaker would have operated, in this case closed, had the IED not been in a blocked state. So our first step is, just like we virtually isolated our BCU before, we now need to virtually isolate our SCU over here on the right. To do that, I'm going to come back over to mode behavior sim, find my SCU, and change it to mode test blocked. And as I do that, in just a moment, you'll see these little x's disappear as our device changes from being in mode. I'm sorry, I forgot to clear out the results. So we're still seeing the recorded results. One moment. OK, so those red x's have disappeared. The reason that's happened is these incoming goose messages, which have quality of test, are now being processed by this device because it's also in mode test. However, as we said, the outputs will, in fact, still be blocked because it's in mode blocked. Because all the inputs are just coming from this device, which is in test, we don't need to publish any additional messages like we did for the BCU via the Goose Publisher. So we're ready to go ahead and switch back over to the Goose Publisher and begin executing our test. Now, I am going to just toggle the value of the breaker just to let our breaker control function know that it did, in fact, close. And that's just going to kind of reset it so we don't think it's in some sort of breaker failure state. Our synchro check is set to true. So we're ready to go ahead and issue our close signal from our recloser again. Before I do that, I want to go ahead and hit record. And we'll go ahead and set that to true. Publish. And as I mentioned, I'm just going to be pulsing that signal. So I'll set it back down to false. All right, we can stop our recording and step through our sequence of events. Just like last time, we see our recloser close request come true. And just like last time, we see it passed on from our BCU over to the SCU. But here's what's different. Because our SCU is now in mode test block, that close request was, in fact, received and processed. And our op close was set to true, indicating that had our breaker not been in mode blocked, it would have actually closed our breaker. Again, we can come over to DTM and see that the breaker is still closed.
 open. So, had we been in a real substation, a live substation running this test, we are confident that we would not have actually affected anything else in the substation aside from our test. And with that, I'm going to go ahead and turn it back over to Beth, who's going to do another quick question and answer session, at which point she's going to hand it back over to me and we're going to take a look at test sequencer again, which in this case be used to validate configurations when we're turning back to normal after finishing some virtual isolation tests like the ones that we just performed. Thanks, Beth. I'll go ahead and turn it back over to you. Yep. Okay. Thanks, Jackson. So, I am going to do a couple of questions, but because we're getting a little bit tight on time, I just want to reiterate that we will stay on past the time to continue to answer questions. So, continue to submit them as long as questions are coming in, even after we're finished with the demo itself. We will stay on as long as needed. So, with that, this question has come in a couple of times, so I think it's worth clarifying for the audience. Is TestSuite Pro connected to physical devices or is simulation driven by DTM, and how can you tell the difference between the physical and simulated devices? Sure. So, in this case, we are connected to simulated devices in DTM, but the point I want to drive home is that's really transparent to TestSuite Pro. TestSuite Pro is communicating with DTM via the standard protocol, 61.850. So, and if I go back up here to system status, you can see DTM has a different IP address for each one of these simulated devices. If this simulation, instead of all running on my PC, was running on a different PC over the network, and you were to plug in Wireshark in between, you would see all of the 61.850 flying between the devices. Okay. Next question. Does each signal view need to be built from scratch? I'm curious if anything auto-populates when you select a particular device or DO. Yes. So, there is some manual configuration when creating these signal flow diagrams, but we do have a tool, the Data Miner, over here on the left, and with that, it's kind of an intelligent way to sort and browse data that is found across all of your devices found in the SCD file. And you can see over here in the different columns if a device, I'm sorry, if a data attribute found in the Data Miner is used in a GOOS report block or has subscribing, other devices have subscribed to this data attribute via GOOS. So, when you drag one of these devices over to the custom display when you're building it, it does help you by pulling in any other logical nodes that are subscribed to it. Okay. And because we're a little running short, a little bit on time, I'm going to go ahead and hand it back over to you to finish up the demo. Yep. Thanks, Beth. So, now that we've looked at how to configure these devices manually using the Mode, Behavior, and Sim tool, let's take a look at how you can return it to normal operation automatically. And to do this, we're going to be using the test sequencer. In this case, like I said, we will be returning from virtual isolation testing back to normal mode, but you could just as easily do this the other way around. Instead of manually changing your devices to be in Mode, Behavior, Sim and setting, I'm sorry, manually setting the simulation bit and the mode, you could use a test sequencer like the one that I'm about to execute to get you into this mode. This is particularly useful because you're able to run it against the simulation ahead of time or against your real substation if you have it available. And then if you ever are in a mission-critical situation,
 where you're working a live substation during an emergency and you need to perform some testing, you've already got your routine built to get you into this mode. Instead of having to click, you know, all the buttons that I just did, you just have this test sequence pre-configured to get you into that mode so you can do your testing and then get back out. So, as I kind of alluded to, this test is going to connect to the two devices, the BCU and the SU, who we made changes to. By changes, I mean we changed the LPHT sim and the mode. After connecting to those, it's first going to disable the LPHT sim bit on the BCU. So, we'll see this up here change from yellow and go back to normal once I update. Next, we're going to verify that by reading that value back from the device just to confirm that it did in fact change. Next, we're going to be reading the LGOS logical node to make sure that our device did in fact switch over from our simulated streams to our regular streams. So, coming back up here to LGOS, these devices will switch back to green or I'm sorry, these flags will switch back to green to let us know that we did switch to our normal GU streams. And we're just going to be verifying that here in this test. Finally, going back to mode behavior sim, we'll be changing the mode from test back to normal on the BCU and then on our SU the same thing. We'll be changing it from test blocked back to normal and then we'll read back the behavior from both of those devices to make sure that that command was in fact received and processed. So, let me come back up here, make sure I got my test selected and then hit run. And we'll see the test running through in two different places up here at the top in our kind of flow chart like view. You can see stepping through the various steps and then down here at the bottom where we configured the test, you can see the test as they each pass as well. So, I'm going to go ahead and update my mode behavior sim view to make sure, oh yeah, I forgot I did this one manually earlier. So, let me go ahead and change him, change mode. Sorry, that was just a misclick on my part. And then if we come back up here to GooseTracker, we can see, I'm sorry, LGOS, we can see that we do in fact have green again because our devices have switched back over to the real streams and are no longer subscribed to the real streams. So, having sequences like this in your test environment are very helpful because as I mentioned, once you get to the real deal and you need to run this in an actual substation, it's as simple as a click of a button to know that you're ready to go. And with that, I want to thank you for your time and turn it back over to Beth for any final questions. Okay, perfect timing. We just have a couple minutes left, so I would like to just wrap up some final points. I'm going to go ahead and take the screen back from you, Jackson. Jackson? Okay, so just to wrap up, we will be publishing this recording on our website in the next couple of days. So, if you just go to trianglemicroworks.com on the homepage, you'll scroll to the bottom and see the recorded version of this. They can view at any time. When you hang up today, you will be presented with a survey. It's very quick. We'll take under a minute to fill out. We really appreciate your feedback on what you thought of the webinar, some future topics that you would like to see. So, please take a minute to fill that out. You can also download an evaluation license to both TestSuite Pro and DTM today from our website. So, the functionality that you saw here today, and I saw that someone had asked that question, you know, what is the release date for what you're seeing here today? It's currently in the tools. This functionality is there. Go ahead and download an evaluation license and try it.
 out. If you're working with a VM, please just shoot us a quick email and we can give you a temporary license key to try it out firsthand. And then, of course, if you have any questions or additional comments, we welcome them, so please email sales at trianglemicroworks.com. So with that, we just made it in time. It's 12 noon exactly. As I mentioned, anyone that would like to stay on longer, we'd love to continue to answer your questions online. You can continue to submit them. And with that, I will start to open up the questions to the floor. So I'm going to kind of go back just a little bit because we had some good questions come in. So this could be for Jackson or Christophe. Instead of using sample values, is there any other way to read analog values from an IED? I'll ask Christophe or Jackson or anyone. I can answer that probably. Can you hear me? Yep, we can hear you. And then I think as I have answered in the chat as well. So I know the sample values are really used when we need to have the waveform from a signal, like when we have a CT or a current transformer or voltage transformer measurement where we have with a digital substation emerging unit that has the analog input and then is sampling the signal and really sending with sampled values the individual samples over so that we have in the receiving device, like a protectionary device, we can receive the complete waveform. That's where we really use sampled values. If you just have calculated analog values like a measurement, like a frequency measurement or a calculated current and voltage RMS measurements, they can be communicated using the MMS services with reporting as an example. We can read them with MMS so we can also put them in the GOOS messages. I hope that clarified. Okay. Can TestSuite Pro publish sample values virtual secondary injection testing? No, we are not able to publish sampled values. We can monitor sampled values on the wire via the sample value tracker, but we cannot publish sample values with TestSuite Pro. However, our tool DTM does have some ability to publish sampled values. Okay. We did have a question come in about hands-on training sessions that are available and I would like to mention that with any purchase of our tools, we do offer a one-hour free training session to familiarize you with all the functionality within the tool and that could be for an individual or for a team. And then, of course, we've got training videos on the website as well. Okay. Which we can, can we, hold on, let me find the other question here. Okay. So this one's going back just a little bit. You showed the compare tool. Does this only compare the model or does it as well compare values? And there's a second part to this. Can I use the tool to ensure that as an example, my protection settings in the relay are still as they should be? Yes. So in an SCL file, you can have settings parameters that can be pre-configured. And in that case, yes, compare model will compare the values discovered on the IED with those defined in the SCL file. On the other hand, I guess I would point out that values such as like a breaker status or something like that, that is not pre-configured like a protection setting would be, those would not be compared. But yes, if you want to make sure that someone has an
 gone in and changed protection setting parameters on your IED, this is a perfect tool to do that. Okay, thanks, Jackson. Next question, is it possible to generate test reports from the tool with test conditions? Absolutely, yes. Once a test has been run, you can export those results as a CSV or Excel file, which does include conditions as well as results. Okay, next question. So if I understood correctly, in LGOS, I see that the device is switched to receive the simulated message. What do I do if my device does not have an LN-LGOS? So that's a great question. If your device does not support LGOS, there's really not a way to verify that it has, in fact, switched over to the simulated messages. That doesn't mean that you can't perform these tests, but it does mean that there's a certain element of faith required in that situation. Because yeah, LGOS is, that is its purpose, is to let you know that it has switched over. So yeah, it is one of the benefits of having LGOS. Okay. Yeah. Next question. I can see how these test specific data flow, but what if you miss a subscriber in the scheme you're trying to test? Are there any flags to alert you that there's another guy you're leaving out? That's a good question. There are a number of ways that you could catch that. So one of those would be in the data miner. You could check and see who your subscribers are, or in, I'm trying to think of where else you could catch that. But yes, in general, we can double check that. I do want to point out that when you have the mode set to quality test, whatever device, if you don't catch a device that's subscribed to that message, it's going to just ignore it. Assuming that other subscriber has a quality set to on. Okay. Next question. The station computer that I use allows me to display the values of LGOS from the various IEDs. Why do I need the LGOS tool from your product that you demonstrated? Sure. So yeah, we definitely get this a lot. You're correct that there is some overlap between just standard LGOS reports and what we show in our matrix view. But where our LGOS matrix really differentiates itself is the way that TestSuite Pro is sniffing the wire. And so it's comparing with what it finds on the wire with what your LGOS is reporting. So as an example, let's say that a publisher is publishing its goo stream, but a subscriber is reporting that it's not seeing it. Well, you can decide some level of truth using TestSuite Pro and say, yes, I do in fact see that stream on the wire, but it's not getting to my device. So now you have an extra layer of diagnostics to know where to troubleshoot. Maybe it's a networking issue or maybe it's a misconfiguration on the subscriber's end. Great. Thanks, Jackson. Next question. Are there any IED standards, for example, between relay manufacturers that require outputs to block when in test mode or does this functionality need to be custom programmed by a setting engineer? Maybe I can take that. Yes, please. Yeah. So basically, I mean, it depends on the test what you really need to block if you need to block an output or not. So typically, when you do a test in a live system, you will block the output as an example from the breaker that it doesn't trip the breaker and you do that.
 on the logical node XCBR that interfaces to the to the breaker. So when that's in blocked it will not generate the output. So typically I would assume logical nodes that are going to outputs in the device will support the mode blocked and and then we can set them in the mode blocked if we want to do so from the test. So it's at the end of the test engineer to decide what needs to be blocked and what does not need to be blocked depending on the test he is executing. Okay thanks Christoph. Next question. Are there any options to track control services? And in parentheses he has LTRK. Yes the logical node LTRK has the capability to track all the control requests all the control service requests that's one of the purpose for that logical node. Okay next question. From SCADA can we trigger goose messages? Okay maybe as well for me. So in principle okay I'm not sure if you're talking about the remote SCADA which may be network control center or a local SCADA in the substation. Goose messages as we have shown them here are working on the layer two so they remain within segment. If my SCADA my local SCADA is connected to that in theory it can also send goose messages. I don't really see huge applications for that but it's definitely from the standard it is possible any device can send out goose messages if it has the stack built in for. When we talk about the remote SCADA then we are getting out of our network segment so the normal goose does not work but with an extension of the standard mainly used for synchro phaser and system integrity protection schemes we introduced what we call a routable goose that works as well on an IP network. So in theory then we can also send gooses from a remote SCADA across the network and as I said that's mainly used for system integrity protection schemes or also to transmit synchro phaser information. In that context then we also need security in many cases so there's a complete additional layer of information that comes on and I think we had in the past a webinar where we also introduced the security and a little bit these areas of the routable goose. Okay next question. There are quite some steps required and in parentheses he has writing mode, writing lphd.sim, checking bay and LGOS. Does your tool support automating these steps so that an engineer that has to go to a substation to troubleshoot does not make a mistake? Absolutely so yeah that's really where the test sequencer shines. It can be several steps that you really need to pre-plan in order to configure your system for virtual isolation testing and you do have to kind of determine those steps but once you determine them you don't need to repeatedly do them manually. You can both make those configurations and verify that they're correct using the test sequencer and that's a little bit of what I showed here at the end. It was not quite the same as the question we were actually exiting virtual isolation testing and getting back to normal mode but you could totally flip it the other way around and have a test sequence that prepares you for virtual isolation.
 Okay. Well, I think with that, we are concluding our webinar. Looks like the questions have slowed down. I think we've covered most of them. So again, I want to thank everyone for joining us today and staying on a little late to get through most of the questions. Again, please check out our website to download these tools today, as well as see the recorded webinar, which should be posted in the next couple of days. And we look forward to seeing you at our next webinar. Thanks everyone. Thanks very much.


Video ID: LSD1fK314NE
Hello, everyone, and welcome to today's webinar, Automate Testing of Your SCADA Gateway Data Concentrator or Protocol Converter. My name is Beth Capellas, and I'm the Marketing Manager here at Triangle Microworks. Before we get started, I'd like to go over some housekeeping items and some introductions. The webinar is scheduled to be one hour long. We will be stopping for questions a couple of times throughout the webinar, and we'll stay on afterwards to answer as many questions as possible. The audience is on mute, so please submit your questions online using the webinar tool under the section titled Questions. We will be answering questions online during the webinar, as well as reading and answering questions during the breaks and at the end. Next, I'd like to introduce the folks you'll be hearing from today. I'll start with our panel of experts who will be answering your questions online. We have with us Dave Gautkenhauer. Dave is our Senior Developer for Distributed Test Manager, also known as DTM. This will be one of the main tools you'll be seeing here today. We also have with us Greg Galewski, our Engineering Manager here at Triangle. And last but certainly not least, we have with us Eric Febro, one of our lead QA testers who works mainly on our SCADA data gateway. Next, I'd like to introduce our speakers. So, our first speaker is Jackson Moore. Jackson recently joined Triangle Microworks as an Application Engineer. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer where he designed and developed load management control systems for multisource microgrids ranging from 1 megawatt to 30 megawatts. We also have with us Matt Green. Matt is a Senior Application Engineer here at Triangle Microworks. He's been with us for over eight years and specializes in QA testing with all our simulation tools in gateway. Matt is also one of our lead product trainers and presenters for webinars and customer demos. So, now that you know everyone, I'd like to give you a little bit of background on Triangle Microworks before we get started. We were established in 1994 and are headquartered in Raleigh, North Carolina. We have customers in over 70 countries and work mostly with equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs in development time. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing the lab or the field. They are designed to troubleshoot, to simulate, and to automate testing. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. Now that you know a little bit more about Triangle, I would like to go ahead and hand it over to Jackson, who's going to get started with the topic that we're here to learn more about. So Jackson, I'll go ahead and make you organizer, and you now have the screen. Thank you, Beth. Thank you, Beth.
 All right, thank you, Beth. As mentioned, my name is Jackson Moore, and I'm an application engineer here at Triangle Microworks. In a short time, my fellow application engineer, Matt, will be leading us through a demonstration of our new automated gateway test manager. But before we get into that, I want to provide us with a little background on what the gateway test manager is and why we are so excited to be showing it to you today. Let's start by taking a moment to establish some common terminology. Specifically, I want to share what we here at Triangle mean when we use the term gateway. In a broad sense, when we refer to a gateway, we mean a device which collects data streams from various other devices, speaking various communication protocols, consolidates those data streams, and passes on a new combined data stream. Data concentrators perform much of the same functionality, but usually without the emphasis on support for multiple protocols. In much the same way, protocol converters also overlap with our definition of a gateway, but generally without the emphasis on support for multiple devices and data remapping. But in the end, much of the distinction boils down to semantics. And although we will be using the term gateway today, I think it's best for us to think of it as an umbrella term, as our discussions are really applicable to devices of all three types. So now that we've established some common terminology, let's take a look at a scenario where a gateway might be used. In the image to the left, we have a network diagram depicting a simplified version of a common network topology. Starting from the bottom of the diagram, we see three generic end devices, which in this case, communicate using three different protocols. Data streams from each of these devices pass into the gateway, where they are remapped and converted, and then pass on to the SCADA system and historian at the top of the diagram. In much the same way, data might flow in the opposite direction, with control signals originating from our SCADA system at the top being passed through the gateway before being dispersed to our end devices at the bottom of the diagram. So now that we've established what it is that a gateway does, let's discuss how a gateway is configured. Following the principles of any engineering design process worth its salt, before you jump in and begin configuring a gateway, you must first decide what its function will be. In the case of a gateway, this means defining what data will flow in and what data will flow out. This information is captured in a document, commonly referred to as a data map, and typically looks similar to the figure on the left. Commonly in a spreadsheet style format, gateway data maps specify a device name, protocol, and data address necessary for each incoming and outgoing data point connection. In some cases, scaling factors and other metadata may be present as well. In large SCADA systems, it is not uncommon for there to be hundreds of devices, each with thousands of data points. Before gateway configuration can begin, these connections must be defined in a gateway data map. After intended gateway functionality is defined, it is ultimately the role of the SCADA engineer using the data map as a blueprint to configure the gateway. And believe me, I speak from experience when I say this is a tedious task, error prone, and not for the faint of heart. So given what we know about this error prone manual task of gateway configuration, how do we verify that the configured gateway behaves as described in the gateway data map?
 The answer is you test it. Traditionally, gateway validation testing has been performed manually and requires all hardware to be present. This means the end devices and SCADA systems that you see in the diagram to the left must be configured, powered up, and present on the same network as the gateway before testing can begin. Because all physical hardware is rarely present and connected in the same physical location prior to final commissioning, traditional gateway validation testing may only be completed at the end of the engineering process. The common approach to validation testing is for an engineer to connect to each end device individually, manipulate a specific data point, check to see if the manipulated data makes it through the gateway to the SCADA system, and then rinse and repeat. Given the size and complexity of modern SCADA systems, thorough gateway validation testing can be incredibly time-consuming. Additionally, manual processes such as this are notoriously prone to error, and any late changes to the gateway might mean that validation testing needs to be repeated. All of this has led to Triangle Microworks seeing the need for a better solution. And today, we're excited to be introducing that solution. The Gateway Test Manager, a module within our flagship test and simulation tool, Distributed Test Manager, or DTM. As those of you who have used the tool will be aware, DTM allows for accurate communication simulation of each device within your industrial network. This includes both clients and servers in a range of common protocols, including DNP3, IEC 61850, TAS E2, Modbus, and more. However, new to this release is your Gateway Test Manager, which enables the quick and easy specification of thousands of point-to-point tests between those simulated devices, all with your gateway placed right in the middle. Perform simple point A to point B mapping checks. Perform complex tests of quality, flags, class, and other protocol-specific metadata. Check how your data arrived, be it pulled or unsolicited, and make sure it arrives within the time frame that you specify. You can even test SCADA controls being passed down to end devices. And best of all, this can all be accomplished with no scripting required, thanks to our newly introduced Actions testing concept. But before we dig into Actions, let's take a look at a simple test plan and see how it works. Here we have two figures, which serve to illustrate a series of basic tests. Starting on the right, we have a simplified test plan, where each row represents a single test. Or more specifically, each row represents a single test of an action. The particular action specified here, named Monitor Data Point, checks to see if the gateway has a mapped connection between a source and destination data point. On the left side of the slide, we see our end devices, which are being simulated in DTM. In the middle, our gateway, the physical device whose configuration and performance is being tested. And on the right, our SCADA system, which is also simulated in DTM. As we begin execution of our test plan, the gateway test manager will check each connection individually. As stipulated by our chosen action, a known value is placed in the source register of the simulated end devices. In order to pass the test, the gateway must map this value through, providing it to the designated destination.
 register in the simulated SCADA system, which is being monitored by the Gateway Test Manager. This process is completed for each row or test within the test plan. Now before we move on and take a closer look at actions, I do want to take a moment to highlight something. You may have noticed that our test plan, the table on the right, looks similar to the Gateway data map from earlier. You will recall that this was the definition document used to configure the Gateway. This similarity is not accidental. In fact, the Gateway Test Manager was designed from the ground up with your existing data maps in mind. And while there are several methods for creating a test plan, all of which Matt will be demonstrating shortly, I hope you will pay special attention to our data map import feature, as it is one that we are particularly proud of. But before we get to that, let's take a closer look at exactly what an action is and how you will be using them. The first step to building a test plan is deciding what you want to test. In the Gateway Test Manager, this means choosing which actions you will be using. The example test plan we just looked at used several instances of the monitor data point action. This is a high-level protocol abstracted action with relatively few inputs. However, we also provide protocol-specific actions, which allow for more granular testing and, in turn, more specific results. So let's take a look at one of those. ReadAnalogInputClass is an action specific to the DNP3 protocol. Like the generic monitor data point, this action will perform a source-to-destination value mapping test, specifically on a data point of the type AnalogInput. However, unlike the more broad action that we used before, this action allows for testing of features and metadata specific to this protocol and data type. For example, class, quality, flags, and timestamps can all be tested with this action. To achieve these more detailed test results, we will require a few extra inputs. For example, if we want to check a data point has a specific expected quality when it arrives at its destination, we will need to provide that expected quality as an input. These inputs are generally optional and details of action behavior is covered in our companion documentation. While we have only looked at a few example actions included within the Gateway Test Manager, are a host of similar protocol-specific actions for use in detailed testing applications, with more and more being added in the future. And finally, we do offer the ability for users to customize existing actions and even create their own. Written in JavaScript and with access to the entire DTM function library, users have the option to not only tweak and customize our included actions, but also to build entirely new and powerful actions unique to their application. And by no means do we see applications limited to gateway testing. And now before we get to our hands-on demonstration, let's take a brief pause to answer any questions that you might have. Okay, we did get a couple of questions in. The first question, what protocols are supported in this test manager? So on day one, we will support 61850 and DNP3. And in the near future, we'll be adding support for every protocol within our DTM library. Which just to follow on, would include 6870-5, Modbus, and ICCP. Our next question...
 can an R-TAC be tested in this way? Absolutely, an R-TAC is an example of a data concentrator and it could absolutely be tested in this way. Another question we've got here, who writes the actions for the action-based tests? So provided out of the box are a host of actions specific to protocols as well as more general ones in which you can choose depending on how detailed your tests need to be. And then we do provide the ability for you to create your own custom actions afterwards should you need to. And we are available for support if you need help with that. Okay, I think what I'm gonna do is switch it over to Matt now to start the hands-on demo portion of it. We do still have questions coming in, so please feel free to continue to submit those. You'll start to see answers from the online panel. So we will get to those questions. With that, Matt, I'm gonna go ahead and make you presenter. And you should have the screen. Thanks, Beth. Thanks, Jackson. So today we're gonna be looking at DTM and our new action-based automated testing for gateways. But the first thing we need to do is get familiar with DTM and its environment. So let's take a look at that to start with. On the left-hand side here, we have what we call the workspace pane. This is effectively where you can put all of the things that build up your simulated environment. You can add folders to organize things. You can add in devices, such as the 61850 bay controller device or this DNP3 outstation. You can add in our testing nodes. So I've pre-made one of these today that I've just named automated testing. You can also add in things like auto data changes, JavaScripts, different sort of GUI displays, basically anything that needs to build up your environment and your testing scenarios can be put here in this workspace. I also wanna take a look at this testing node here. This is effectively gonna be how you're gonna be interacting with this new testing way for testing gateways. In the middle here, you have all of the tests that you'll be running, a description, an optional description of what that test is and what it's gonna be testing. And then if the test is run, we'll have a log down here of what happened during the testing. We have a few options over here, including managing the tests and managing the actions that we'll be using during these testing. And of course a way to export the results that you have. At the bottom left here, I have our outstations and our 61850 servers model. So currently I have here the 61850 bay controller model that we have. And that's just based on the ICD file used to create this device. And I know specifically we'll be looking at the breaker position that this device is monitoring. So I'll go into the XCBR, into the pause, and then keep an eye on that ST bow. Likewise, we have our DNP3 outstation. Its database is shown right here. So effectively I'm gonna have all of our downstream devices on this left side. And then on the right side here, I know that our SCADA data gateway is presenting its database to us as a DNP3 outstation or a database. So I went ahead and created our SCADA master as an MDNP, and I'm showing its database here. Speaking of which, we won't be looking at it too much today, but this is our SCADA data gateway application. As Beth mentioned, this runs.
 on Linux and on our Windows, and it will be working as our data concentrator and data translator today, and it will be our device under test. Now that you see how the layout of DTM is, let's take a look at how you would create something like this. Effectively what we are going to do is we are going to try to create this scenario. We have our SCADA data gateway here in the middle, and that is our device under test. We need to simulate all of the inputs and all of the outputs into our gateway. Down in DTM in the downstream devices we have X number of downstream devices, in this situation we just have two, but that can obviously be scaled up, and we also have our SCADA master that we need to simulate. How would we go about actually creating this environment for our SCADA data gateway? The first thing we can do is add these downstream devices. To do that you just go to any node, like a folder or a workspace node, and do a right click, manage, add, and we will look at DNP3 first, DNP3 outstation. That will pop up a configuration window, and it will walk you through it similar to a wizard. The first tab here is going to ask you where you want this device to be simulated on, and what hardware is it going to use. So DTM stands for distributed test manager, the distributed in that means that you can actually distribute the simulation across multiple machines. For the sake of this demo I only have the one ThinkPad, so that is obviously where I am going to host this device, and we can give it a name. The channel tab is where we are going to be putting in TCPIP or serial information, that includes your local address, your remote address, ports, things along those lines. By default, what we have done is we have put in the most generic information, the most open information we can, so 0.0.0.0 means basically use any of the IP address that my network adapter allows, and the remote address, star.star.star.star, means accept a connection from pretty much anyone. It's not specifying the IP address that my SCADA gateway is sitting at, it's basically saying if there is a master DNP3 out there that wants to connect to me, go ahead and let it. Then of course, for DNP3, the default port is port 20000. Being as this is a simulator tool, all of this is configurable though, so you can go to whatever port number you would like, you can choose any IP address that you would like, you can set the remote IP address, you can even come in here and type in whatever IP address that your gateway is expecting you to be communicating on, and actually assign that IP address to your adapter. Now note that your network adapter can have as many IP addresses as you want to add to it, but we are actually inside of DTM going in and reconfiguring your network adapter, so if you are trying to simulate a piece of hardware sitting on your desk right next to you and you're using the exact same IP address as it, make sure that you're on a testing network or maybe you switch off the other device, or else you might get IP conflicts and it can cause all sorts of errors. The session tab here is filled with things specific to this protocol, so in this case we're talking about the link layer addressing, which we put four and three in, whether unsolicited messages will be supported and secure authentication. There is also the database here. By default, we've just kind of put 40 of everything in there. This allows you to kind of play around with it when you're first using DTM, but when you're wanting to simulate a specific device, we recommend that you reset the database and say, I want 41 binary inputs, and maybe I want some analog inputs, I want 12 of them. You can build up your database however you would like, and then you can also go in and configure these points even further for things like where should this data show up in, so is it a class one, class two?
 class 3, what sort of variations do you have. It is absolutely configurable. The last tab here is the advanced tab. This is where you will go and find the 10-15% of the stuff that you are not going to be using super frequently. If you are going to be simulating a device and you need to do it down to its frame sizes and buffer sizes and timeouts, you can dig into the advanced and find any of these parameters in here. Just because it is here and we have gone through the rest of them, let's take a look at multiple devices. Basically, when I click create multiple outstations, it will take the configuration of all of the previous tabs and put it into this table. Then I can increase and decrease as much as I would like to create more or less of these devices. Of course, we cannot have all of these devices sitting on the same IP and port. We have incremented the port here, but this is all editable. You can come in here and change the IP addresses to something else or whatever you would like to do. That is a way you can quickly bring up these devices. I will go ahead and cancel this because I already have my device out here. The second thing we are going to look at is 61850. I will right-click, manage, add, 61850, server. You will notice that a lot of the tabs here are not needed for 61850. That is because 61850 supports a configuration file called an SCL file. It is ICD, SCD, anything along those lines. You can go and pick that file. You can go to your import configuration. That will bring up a Windows Explorer that you can move through and find your file. If you have already imported into DTM, you can select it and hit okay. That will go and configure this device to match exactly what is inside that ICD file. Of course, because it is a simulator tool and we want you to be able to have flexibility when it comes to your simulation, you can go back and make changes to it. By default, we have already filled all of this out for you. Just like before, we do have an advanced tab for the settings that you may need to go in and make small tweaks to if you are trying to replicate exact frame sizes or things like that. That is how we will simulate all of our downstream devices. Let's take a look at our SCADA master. Our SCADA master here is simply just an MDMP. I would go through the same process as before. I can right-click, manage, add, DMP3 master. You will notice this is exactly like the outstation configuration. It is also going to be similar to our 101, 104, Modbus. All of those things are going to have the exact same layout. You have where you want the device, what is the connection information. We have our link layer addressing. There are a few things that are slightly changed because this is a master and not an outstation. We have some extra parameters here that you can interact with. All in all, it is the same process. Once you learn to do it one way, you should be able to do it with most of our other protocols. The next thing we are going to want to do is find a way to get the data from the SCADA data gateway into DTM. We can obviously change the data in the downstream devices, but we need to be able to verify that with the upstream devices. I am going to drill down in here and show that I have unsolicited messages and an integrity data poll here. Those are just a few of our options. To add those, you would do a right-click, commands, and then you have all of the different commands and controls that this device can send. For example, I have the integrity data poll here, but you can also do unsolicited messages as well. I just brought up the unsolicited messages. What we can see here is we can check the event data classes that you are interested in, and you can obviously enable or disable it depending on what this control is going to do. Another thing that might be interesting when doing this is the scheduler down here. For things like enabling unsolicited messages, if you click on connect, that means that you are going to be able to do that.
 the first time that this master connects up to our SCADA data gateway or any DMP device for that matter, it will automatically enable unsolicited messages, so you are already going to start getting this event data into your device. Whereas periodically would not make too much sense for unsolicited messages, it would make a lot more sense for things like the integrity data poll. You can set this up to do an integrity data poll every 24 hours, every few hours, every minute, down to the seconds, depending on what you need. We have automated it so you do not have to go in and manually interact with these controls and commands. You can set them up on a schedule and have them happen automatically. Now we have talked about how to add controls, how to add the SCADA master and all of the downstream devices. Let's take a look at how to add a testing node. I have already created this automated testing node, but I figure we should start from scratch and I can show you exactly how we can build something like this up. You right-click, manage, add, test manager, gateway test actions. I will give it a name. Now we have a blank one of these. What this will allow us to do is build up our test from scratch. Let's assume that we are just going to do everything within DTM and we are going to build this up right away. The first thing you are going to do is go to manage test because we have no tests here. It's going to give us a screen that looks like this. What we are going to do is add a new item. I'm going to go ahead and monitor a data point. That is the test action I'm going to be performing. Then I'm going to get a little bit more real estate here so we can see this without having to scroll around a lot. The test name is going to be breaker on. I'm going to go ahead and interact with this breaker position here. I could go through and click through and type in each of these things, but since we already have the source of where the data is coming from and the destination of where it should end up, we can grab the value and drag it in. It will format it for you for what is necessary. For 61850 you only need path 1 and path 2. We will look at the destination now. I know, or maybe I have a table next to me or a file open somewhere on my computer, that this breaker position maps to my binary input 0. I can take that value and drag that up to my destination and it will go ahead and format that as well. We are turning the breaker on, so into the on position. The set value I will put on in here. A weight, I will come in here and say 20 seconds, 20,000 milliseconds. The initial value will allow you to set up the initial conditions of your test. Effectively what you can do is if you are setting the value to on, but it is already on, that may not generate a report. It may not send that data across. We will set up the initial value of off, and the expected value, we are turning it on, so we would expect the binary input 0 to also be seen as on. We also have some sections here for quality. I will not be touching on quality too much today, so I will be leaving these blank. What happens when you leave a parameter blank in the test manager, it goes ahead and tries to assume the nominal state for you. When it comes to quality, if we leave it blank, it will assume the data is valid and good, and its expected quality on the receiving end also should be valid and good. Likewise, we can do that with a few other fields. Let me show you how that would be done. I will add another test here. I will do breaker off. Again, this is going to be the same 61 at 50 points, so I will drag that over. Because it is the same point, it is getting mapped to the same place, so I will drag that over.
 The value, I want to set to off, and the weight, I will do 20,000 again. But for this situation, I will leave the initial value blank. I will say I don't care what my initial conditions are for this, I can leave the expected value blank as well, because DTM can assume, if you are setting the value to off, I will assume it should be the equivalent value in whatever protocol you are mapping to, and I will leave these blank as well. And then, just to show I can do it, I will make a third one here, and I am going to do DNP, I know that my binary input zero on my DNP device is going to get mapped to the binary input one on my end device, so I will put those there, I need to give it a name, so I will say this is DNP3 on P0, so I will take P0 in my DNP device and set it to on, and again, I will set this to on, but the rest of the stuff, I can pretty much leave blank as it will make an assumption. Now I have configured these tests, so if I just hit okay here, we will see that those three tests then do show up. So I can come into here, and just double click on this test, it will go ahead, and let's look at the log here, it went ahead and set the value to on, and sure enough, my downstream device did set to on, it was waiting for a change in my Skata master, that value came across as on, it took a little over a second, and so the test passed. So I can come in here and see any sort of output messages and informational data as necessary. Again, I can come in and run the second one, we can get that value to go off, we can see the value come across as off, etc., etc. So that is how you can go ahead and manually create these tests, and that will allow you to jump into DTM and get your test up and running. Now the question then comes in about how you can expand these, right? So let's go back into manage tests here, and I have got these three tests still configured here. What you can do is export this to a CSV file, and I will export it to this custom gateway test. I will go ahead and replace it, yep. And then I am just going to go to that and open it up. So this should look very familiar as to what we were looking at before, but now it is kind of in the Excel format, right? Which gives us all the power that Excel already has to increase our test coverage. So I can just highlight this one row here, and then drag it down, and get a whole new test, right? I have got another monitor data point, this one is on P1, the same value, if we are going to move it to P1, I need to change this to actually be P1 instead of just P0 again. The destination path is probably going to change, because I know the mapping is different there, and Excel does what Excel does, it did try to increment the wait time as well. Would one extra millisecond really make a difference? Not really. So now I have added an additional test, but you can use all the powers of Excel to do this however you want. So you can use equations, you can use any sort of formatting you are comfortable with. Basically as confident you are with Excel, you can use any of that stuff to try to make your own tests. For example, I know my gateway follows the same pattern, where each of P1 maps to P2 in our gateway application, and then so forth, P2 maps to P3, etc. So I can actually just highlight this as a pattern, and then just drag it as far down as I really want to, and now I have created a bunch of new tests.
 So, if I come in here, save this off, and then I'm going to import that CSV file and import the same one that we were just working on. Well, now I have 30 some odd tests, right? So, if I hit okay now, it'll go through and it will populate my testing in here. And then I can just come in here and, you know, double click on my tests, and it will run through and actually start running some of these tests. So I can just run them one by one, I can, you know, multi-select them, and then say run selected tests, or I could come in here and do like a run of all the tests. This will allow me to go to get a cup of coffee or take a walk down the hall, talk to a coworker. Heck, you could even set this up to run as like a nightly thing where you just come in and all these tests are already running for you. It's also worth noting that after you make all these changes and after you run these tests, you can then export the results out so you can have kind of a log of what's been done yesterday, what's being done today, what's hoping to be fixed in the future, what things have been improved, and what things have regressed. So you can export all this results as XML, CSV, HTML, and just as a simple text file as well. So we have that there so you can, you know, every time you hit reset test results, you don't have to feel like you're losing your progress. You have all that stuff saved off as well. So this is where we're going to come and take another small break. We're going to do another Q&A section and see if there's any other questions coming up about DTM or potentially even the slides. So I will pass this back over to Beth for a second. Okay, thanks, Matt. Yep, we do have a bunch of questions coming in. So let me start from the top here. Normally, do you have to test again on the field after installation of devices? The practice says that you probably should test. So you can obviously test in a testing environment when you're doing things like this with your gateway, and that will make sure a few things. If you're doing it in a simulated environment, you can, number one, make sure that the device is able to do all the functionality that you need. So if you need specific class reads or if you need specific timings, you can do all this in a simulated environment. Number two, what you'd want to do is verify that the actual mapping has been done. I think Jackson mentioned earlier, it's a fairly manual process to go in and map all these points across, and it can be really easy to accidentally put the wrong point in or the wrong destination point or something like that. So you might get values going to the wrong place. So in the simulated environment, you can verify all that's done too. Once you've done that all in the simulated environment, you've confirmed that the behavior, it works, the functionality is there, and that your configuration is proper. When you go and put it into a physical environment, you can already weed all of those out as potential issues if you do run into issues in a real environment. So it could be a wire crisscross, or it could be something not turned on, or maybe a device isn't generating the reports you expected. You can start to look at outside of devices because you can be very confident that your gateway is performing properly. Okay, great. Next question. Is it possible to have simulated devices, for example, a Modbus slave with a physical Modbus master? Oh, yeah, 100%. So anything in our simulator tool here in DTM, and pretty much all of our applications, are acting as if they were real-world devices. So when we plug a SCADA data gateway or any sort of gateway, data concentrator, things like that, into our simulated environment, as far as it's concerned, all of the upstream and the downstream devices are physical devices. Likewise, if you just have a Modbus master or DMP master or whatever, if it hooks up to a simulated outstation slave server, whatever it is inside of DTM, as far as it's concerned, it is an actual physical.
 device and you can test that just as easily. Next question, how many IEDs are supported and recommended with the gateway? Inside of our gateway? Our gateway is highly configurable. You can connect up to as many different devices as you need to in order to use it as a data translator or data concentrator, however you need to use it. In DTM, we support as many devices as you really need to run your simulation. It's all configurable when you pick up a license or something like that. If you only need tens of devices, then we can configure it to be like that. If you need hundreds, we can run it as well. Some of our really big powerhouse users can even simulate thousands of devices, but that's also where the distributed nature comes in. If you start to need more devices than a single computer can handle, you can actually use the resources, the RAM, the CPU of additional computers with no extra cost, no extra license requirements or anything like that. You can have as many computers inside of that environment as you need to simulate your devices. Just with keeping an eye on time, let's go ahead and go back to the demo. Certainly keep asking your questions online. We'll continue to answer them, and we'll have some time in the end as well. I'll shift it back to you, Matt. Perfect. We really haven't touched on the action types today, and I definitely want to do that. So far, we've really only been using the generic point-to-point tests, and that's been good enough to show off the base functionality of this feature. We've also created and used a CSV file that was built off of a seed exported by DTM, but the question is, is what if you already have a gateway engineering CSV file and you would like to use yours? This is where we can start to take a look under the hood, and we can go over here to manage actions. This is what we've been running off, which is just the default for DTM right now, and I'm going to go ahead and reset this and show you how you can build this up manually. The first thing we're going to look at is we're going to look at, again, the actions we're already familiar with, so I'm going to go and add just a gateway action here. This will allow you to bring in the whole library of gateway actions or potentially just make a custom one based off of one. I'll go in and say, okay, this is the monitor data point test that we've been already familiar with, so I'll bring that in, and you could give it a custom name or whatever you want to do. If I hit okay, that'll bring in the monitor data point test as an action into our testing environment here. If I click on this, you'll see all of the things that I have been looking at in the other windows as parameters here. We've got our test name, our source, our destination paths, the initial values, anything along those lines is brought in there. The generic monitor and control data point actions can be used to determine if a mapping of a gateway is correct. It doesn't really consider the method in which the data is moved through the gateway, like polling or reporting. It's also really convenient because you can use these two actions and work across multiple protocols as I showed off with the 62850 to DNP3. The simple functions we're looking at right now are likely to be used by the integrators who are trying to validate mapping of their configuration, and that's the main point of their testing. There's also a bunch of other libraries in here we can take a look at. Let's look at DNP3. I'm going to go ahead and just bring in the whole library of these just so you can see what it would entail. All of these were part of the DNP specific library. These are more likely to be used when not only mapping and configuration needs to be verified, but they'll also allow you to have tests for more specific requirements. They can be used to determine if the behavior of the gateway is correct as well. They support a more detailed...
 version of testing of quality and timestamps and allow you to specify acceptable ranges and acceptable qualities coming in from your data. They also allow you greater control on how the protocol requests and commands are formed. These functions are likely to be used by OEM vendors to verify equipment behavior or to perform regression tests. They can also be used by integrators in cases where more specific tests are required and more detailed investigation needs to be done. So for example, I have this read AI class, so read analog input class, and you're gonna see a lot of the parameters here kind of similar. We've got our source path, we've got our destination paths, we got our set values, our read delay is basically how long are we going to wait, but what you will notice here is we've got some other things like class. So if you're reading a analog input class and you want to make sure that that data that you're looking for is part of the correct class, you would need to specify which class you actually want to read. And you can actually see a brief description of what each of these parameters here is on the right. You also might have your flags for quality. And then another thing that you might look at is, especially because we're looking at an analog value here, is this delta value or this deviation value. So let's say your downstream device is dealing with an analog and it's recording like a 10.5 or something, right? But you know that your end device, your SCADA master, it only deals in integers. Well, you're never going to get a 10.5 in an integer. So this deval here, this delta, allows you to configure a number of, if it's within a certain range, if you sent 10.5 and I got 10, that's acceptable. If you sent 10.5 and I get 11, is that acceptable? So you can put, you know, one in there to allow a full range of one or 0.5 if you're trying to just do some sort of rounding. So you can watch for that. In a likewise way, we can do that with a timestamp. So very rarely do values get, you know, translated and converted and then moved through a gateway and have a hundred percent precision on your timestamp. You might be a few milliseconds off, you might be, it might be in a different format, there might be some changes there. So what's the differential you're allowing to be different from the time the event occurred on the downstream device and the time that is received by the gateway, yeah, by the gateway and then mapped up to your SCADA master. So you can fill that information out as well. I'm going to go back to our monitor data point action here and I'm going to show you how you can bring in your CSV file used to configure your gateway into DTM and start automating this test right away. So the first thing you're going to need is you're going to need a file like this. This is our SCADA data gateway mapping. So let me go ahead and open this guy up. We've got obviously the company that made the SCADA, it's us, the time that these tests were created or this file was created. We've got a purpose of what each of these rows are and we have some of our other information in here. So this is a file that we can use to help us verify and to set up the mapping inside of our SCADA data gateway. So you can look and figure out which points should be mapped to which points. So let me go ahead and close this. I'm going to go ahead and import and I'm going to go to our SCADA data gateway mapping, double-click on that and sure enough we'll start to see some of our column headers coming in here. So what this will allow us to do is start to map these things over. So test name is not required but I have purpose so that'll be good enough. The source path, that's my device name that's going to be connected with my external IDs. The destination, that's my SCADA device, SCADA type, SCADA point. Initial value is IVAL. Set value is
 value. It is worth noting if your CSV file may not have all of these points, you can come in here and right-click and add. If I did not have value in here, I can right-click and add and type in value and drag that over. We can use all of the abilities we have used previously to build up our tests before. We can use manual input or the CSV file method or whatever. Expected value is operation results. Quality time, there is a weight. I will drag the time over here. I will hit okay. What that will do is change what we have access to when we go into manage tests. Now when I import, I can import that same data gateway mapping file. It will automatically propagate all of the values into the defined places that I have dragged and dropped them. The only thing I did not have inside of my data gateway was this concept of test actions because it was not necessarily set up for that. Luckily, I can just click at the top, scroll to the bottom, and then do a mass select, and I will just do the monitor data point. Apply those, and then hit okay. What that is going to do is it is going to go and it is going to redefine all of our tests. Now instead of having two, three, 10, I think we have 100 tests now set up to be able to run. I can come in here and run these values and be able to see, yes, the position of the breaker again was changed on. There was an event. It took 1.1 seconds to get across. Today, we have looked at a bunch of different passing tests. Let's take a look at what would happen and how this tool can help you find things that fail. I am going to go ahead and run this test here. What this is is in this test, I am going to go ahead and let's say I made a mistake inside of my gateway. What I have done is I forgot a not, or I changed a value came in as true, and I accidentally put a not in there, or I should have converted or whatever. The value just came through, and it is not what I expected. We obviously failed the test. The values did not match. It expected a false. It received a true. So that lets me know I need to look at my gateway and see if someone put a wrong equation in there or some sort of mapping. This next test I am going to run is going to be something that in the gateway I just completely forgot to map. I mapped it maybe to the wrong point, or I just did not map this point at all. Effectively, what it is going to do is it is going to change this value downstream, and then it is going to wait for it in my Skata master to show up, and it is never going to show up. If you see something that says timeout waiting for point to change, that means that the point never changed. It did not get the wrong value. It just did not get any value at all. You want to look to make sure your mapping is valid there. The third thing we are looking at today is at an analog value. On my downstream devices, it is monitoring voltage or something, and it is reading 22K, so 22 kilovolts. What is going to happen is I am going to expect my gateway to have scaled that up, so that way with my Skata side, I can look at it and just see the 22,000. Sure enough, I forgot to do my scaling. The values obviously did not match again. It expected 22,000, but I only got 22,000, so I can quickly see I forgot to put my scaling factor in there, and I can go and fix that. At this point, I am just going to go ahead and run all of our tests and just run through this process here and let you see how this looks as it goes. To summarize, today we have defined what a gateway is, how it is configured, and most importantly, how it is tested. We have shown that testing gateways, data concentrators, and the like can be largely automated. We discussed how complicated these data mapping files can be and how prone to user error testing may be when performed manually. Using DTM, we have demonstrated the ability to simulate both upstream and downstream devices, as well as our
 new gateway test manager. With this new automated action-based testing tool, you can quickly generate and execute detailed plans tailored specifically for your gateway and your existing data maps. We hope this tool will help you move beyond the traditional error-prone and time-consuming manual gateway testing and allow you for quicker and more accurate test results. And with that, I'm going to pass it back to you, Beth, for final thoughts and Q&A. Okay, I'm going to take the screen back, and we'll do a quick wrap-up, and we've got a lot of good questions coming in, so I am going to flip over to those questions in just a minute, so hang on for that. Okay, so just in case you did come in late or you wanted to review what you saw today, a recording of this webinar will be posted on our website. You can still also access it through GoToWebinar as well as our YouTube channel. The functionality that you saw here today will be available to try as a beta in the next six to eight weeks. If you are interested in receiving a temporary DTM license to try what we've seen here today, please let us know by responding to the survey that you're going to receive directly after this webinar ends. There is a question specific to receiving a beta license. If you select yes, I will reach out to you once the beta is available and provide you a temporary license with installation instructions. And of course, if you have any other questions or would like additional information, please feel free to email sales at trianglemicroworks.com at any time. So that does conclude the presentation part of the webinar. As I've mentioned several times, we will stay online as long as we need to, to continue to answer questions. I do see quite a few coming in. So with that, I'm going to just flip back over to reading off some questions and open that up to the speakers and possibly the panel. So first question, does DTM have an interface to enable running tests from another system? For example, continuous integration system. So we work really hard at allowing access to our manual process in DTM via scripting. So we have a JavaScripting engine that we run inside of our DTM tool. And what that will allow you to do is basically set up and run different tests and test plans and things like that through JavaScript. And then we've interfaced that JavaScript to Python, to Jenkins, to all sorts of other scripting engines to command line. So we do have an interface to allow you to kind of do that. I know that we absolutely can do that with our test manager tool. When it comes to our gateway test actions, we should be able to leverage some of that and get that quickly. I don't know if that'll be in the beta that's coming up soon, but that's something we'll definitely support and we have in the past. Okay, next question. How would I use actions to test higher level functionality such as interlocking? Let me pass this one off to Dave, who is our DTM engineer. I know he's done things specifically like this in the past, so I think he'll have a better answer. Yes, this is Dave. This is one of my pet topics. Basically, the actions that we've described today are typically point-to-point. I call them closed loop, where you've got a source and a destination and you're changing a value in the source and then monitoring the destination. That works fine for monitor data points in a gateway or data concentrator. It also works well for control data points. We can issue control tests where you send an operator control on your SCADA system and it makes its way through the gateway and down into the remote device, which would be simulated.
 intercept the control in the simulated device and make sure that the information is correct and things like that. We believe that's a very significant use case for this type of thing as this point-to-point testing or closed loop. However, we can also build these actions to do higher level functions. For example, interlocking is a common one that is very time consuming to test. You have to basically go through every breaker in a system and make sure that the conditions are set up and then the breaker will not operate if the interlocking conditions aren't met and it will operate if they are not or vice versa. I get that backwards. Anyway, the point being is all that done. So what you can easily envision is developing an action that takes multiple breakers in on the input side and sets up the required conditions. And then if at any point, if you run this action, it will look at all of those source breakers and then watch the target breaker, the one that you're actually testing at the particular time to make sure that it does the appropriate thing, either operates or not, depending on whether interlocking is satisfied. What we see a lot is that utilities and integrators will come up with a scheme for interlocking that could be, you know, that's common throughout their systems or certainly a system, but usually throughout multiple systems. And what you could do is develop an action that is then applied to all of your substations. And, you know, the same action could actually be applied to all of the interlocking throughout a system, or you might have two or three different types of interlocking, things like that. I can see this actions being developed for breaker failure and protection schemes as well. These are not things that we'll have in our first release. It's interesting because frequently they're very device or very system specific. So it might be something that, you know, are developed custom based on a particular vendor, a particular integrator and how they do things. We do plan to investigate, is there a way for us to provide a generic version, if nothing else is a starting point for people to refine theirs. But those are really the kinds of advanced actions that, you know, you could actually test substation behavior, you know, using the same technique. The idea, of course, that one person or a small group of people or whatever could develop these actions, and then they could be used by the, you know, entire organization to test systems with not everybody having to know actually how to develop this action. Just, you know, like really just one or two people would actually have to develop the action, multiple people use it. Okay, great. Thanks, Dave. Next question, and I'll open this to the speakers and also Dave, this might also be you. How are data points attributes specified for actions? So we might need a little more information on that question, but I think the ways I showed off today was you could obviously go down to the database or the model or whatever and drag that into the test taking place. And I'll go ahead and put that value in there for you. Or if you have that predefined inside of a CSV file, when you import that CSV file, it'll populate that column, that row for you. I'll jump in and give a little more detail, and maybe depending on how the question was, was where did it? We struggled, we, you know, looked at it for a while to try to figure out what was the most generic way to specify a data point throughout DTM, again, because the idea is it's multi-protocol. So, you know, and they're working to support multiple protocols and we needed a generic way to support. So we've come up with a scheme that all of our actions, all the actions that Triangle develops, we'll typically use three parameters to specify a data point. And these three parameters are,
 will be a little different depending on which protocol and how you're using it. But the first parameter would typically be a device name. So again, if your substation has unique device names, that works great. The first parameter could also be a DTM path, which admittedly no one's going to have in their predefined spreadsheets, but it's fairly easy to generate if you need to. That would allow you to work with scenarios where you don't have unique device names in your substation. But the first parameter, for example, might be a device path. So we'll find the appropriate device. The second parameter would be protocol dependent for 61850 or CASI-2 ICCP. It will be a path, the MMS path or the 61850 path to identify the attribute in that case. For DMP or the other protocols, the second parameter might be an object ID. So the object index or I guess for 6870, it's the request ID that you use to identify that. Finally, for the point-based protocols like DMP and 6870 Modbus, the third parameter would be a point index. So we've come up with a scheme to identify points as flexibly as possible with the hope that people can map this to what they're used to using and what they already have available in their spreadsheets. I will also point out that if the way we identify points doesn't work with the data you have in your spreadsheets, it's relatively easy to go in and tweak the algorithms that we use to parse these parameters so that it could parse what's available in your spreadsheets. But that would require going in and doing a little bit of JavaScript to tweak the way those points are identified. Okay, thanks. Next question. Is it possible to monitor goose traffic? I would assume so, Dave. There's no reason that goose traffic wouldn't be sent from the gateway up to our master as long as we get the value. We don't really care how we got it, right? For the most part, there's really two aspects to that question. Typically in our system, the way we can monitor goose is by simply monitoring for point changes or attribute changes. Suppose the goose that you want to monitor is between the outstation and the gateway, so it's on the bottom half of the diagram. One way we would look at that is we would simply see changes in the model of the device in the gateway, and we could monitor these point changes, which are a result of the goose stream, but we don't actually look at the goose message in that case. We're simply looking at the results of receiving a goose message. We also have tools for monitoring the goose stream and pulling information directly from the goose stream. It is anticipated that our 61850 actions will allow you to drill down to that level at some point. They don't currently look at the goose stream to, again, extract information from the stream, but as Matt pointed out, we have the capability within DTM to actually look at the goose stream, but our actions do not currently drill down and look at that. They will simply monitor for value changes, but we do anticipate doing that in the future. Sample values, I'm not so sure about. That's going to be a different challenge. Okay. Do you offer a virtual version of the gateway for testing purposes? So our SCADA data gateway is a software application, and so you can basically put that on a Windows machine, a Linux machine. It's supported for both of those. As for testing purposes, I think we have a trial that you can use with the gateway, and obviously you could pick up a full license of it if the trial does everything that you really need. Yes, we do have trial versions available on our website. We also can...
 provide license keys if you do need to work on a VM for our gateway and for DTM and really any of our tools. Okay, next question. Is it possible to send several changes of a lot of data points at the same time? Oh yeah, absolutely. So there's two ways you could do that. If you're trying to focus more on like a load test type thing, I could recommend looking into maybe auto data changes, which will allow you to change pretty much every point that you could possibly want at the same time and see where your, you know, performance numbers are with that. But also these test actions can be built up and made in any way you want. So if you wanted to use the gateway testing tool that we were showing off today, you could obviously have a custom action for that. And honestly, you could probably do a mixture of the two. If you really want to see how your gateway performs under stress, you could set up some auto data changes and set that up to run periodically, kind of like the commands I was showing you earlier. So maybe every, you know, two or three seconds, 10,000 points are changing, and then you could specifically monitor one point using the gateway testing tool and verify that that value did get through. Though, do be a little cautious there because you don't want the value that you're monitoring to be part of the auto data changes changing every second. You want one of them to be controlled that you can verify the value of, and the other one's auto data changing, which would just be like on, off, on, off, or, you know, 10,000, 10,001, 10,002, 10,003. So you could use pretty much any of those three methods to do it. Okay, next question. Can you use external source devices, such as those with protocols not supported by DTM with tests, such as testing an RTAC gateway and connecting physical SEL relays since DTM cannot simulate those? Dave, do you want to take this one too? Sure. First of all, the point that DTM doesn't simulate SEL devices, we actually do regular. Now, we don't simulate exact behavior, but in this particular case for gateway tests, you don't need behavior. We load the model of the SEL device, and we simulate it from a protocol perspective, and we can, you know, send signals, every signal in the model to the gateway. So typically, yes, we absolutely test with SEL and ABV and all kinds of other devices on the, both the downstream and the upstream side. We don't need to simulate the actual behavior of the device for these particular tests because all we're doing is testing the protocol. This being said, we, you know, can replace either outstations or SCADA with hardware. The problem becomes that most of these tests, and one of the big advantages of these tests, is that they're closed loop, and that DTM drives both ends, so they can change the source value and then monitor the resulting value, which means it can be run totally unattended. You don't have to sit and watch them, and, you know, you can fire these things up overnight because with tens of thousands of points, it might take two or three hours to run through all of them. So you set things up, you fire it up, and you go home and come back in the morning and see what happens. So if we're not doing closed loop where we have both sides, you know, unless there's some way to communicate with that side, you know, the SCADA, then we can't really tell that this has changed. Now, one of the things that we're going to ask, I think is a follow-up, but I'll mention it here, is on the SCADA side, we've certainly considered adding OPC support to DTM so that we can actually communicate with SCADA systems. So if you wanted to test with a gateway and the actual SCADA device and then simulate at outstations, we could still do closed loop testing by monitoring the SCADA using, you know, most SCADA systems will support OPC or something like that, and we could actually use that to monitor what's going on. On the outstation side, you know, we've
 We've looked at ways to trigger real, so if you wanted to do what I call mixed mode testing where it's partly simulated, partly hardware, we've certainly looked at ways to easily drive and control actual hardware, actual outstations. There's nothing currently supported to do that, but we could certainly do that. But again, if you want to simulate an SEL device in a gateway and all you're looking for is protocol traffic, I want to change this attribute in the device and see it go through. We simulate SEL models all the time. Again, the clear point is when we say simulate here, we're simulating the model and the protocol, but not necessarily the behavior. So if you wanted to plug an SEL device into your substation simulation and have it behave the way an SEL device would in the field, you would have to program the appropriate or the desired behavior into DTM using either JavaScript or in future versions we're going to use 61131 to program behavior of devices. But that's kind of a different piece than what we're talking about now. That's if you want to simulate a substation's behavior as opposed to just doing point-to-point testing of the gateway or data concentrator. Okay, thanks Dave. Next question. Can I configure multiple IP, in parentheses, different IP series for a single gateway? Yeah, definitely. So basically, in DTM, you can add as many IP addresses as you need to run your test and connect up to your gateway. And our SCADA data gateway application will allow you to use any IP address that is already on the computer that it is running on. So you can use as many or as few IP addresses you need for both applications. Great, thanks. Next question. If DTM acts as a master for IEC 104, in that scenario, can we see, whether we can see buffered data from a physical RTU? So you have a, so when you run as a master for 104, you'll obviously get any of the data in through the database, as you can see there. And then if you, I can't show you right now, but if you right click on that master and do show protocol analyzer, you can see all of the protocol traffic coming through. You can see pretty much everything that you would need to see, the requests and what's being sent out, what's being received, any sort of the additional information that might be stored in there as well. So you can basically see anything coming across from the slave or the controlled station to the master, you'll be able to see either in the database or in that protocol analyzer. Thanks Matt. And I will take this last question as a wrap up. If someone asks, will we be sharing the questions and answers in the recording of this webinar? And the answer is yes to both of that. You will automatically get a recording, a follow up email tomorrow that will have a link to this recording, as well as I mentioned earlier, it will be posted on our website in a couple of days in our YouTube channel. In the survey that you receive, once you sign off from here, you can request to receive additional information. If you do want additional information, you will also receive a copy of all the questions that were asked here today, including ones I did not get to read along with the answers. So as long as you select yes, please send me more information, you will get the questions and answers as well. So with that, that does conclude the question and answer session for today. I really appreciate everyone joining us today and hanging on a little bit longer for some additional Q&A session. And we hope to see you at our next webinar. Thank you, everyone. 


Video ID: pPi_KOahC_I
Good morning, and welcome to today's webinar, The Easy Button for IEC 62050 Security, Authentication and Encryption for Routable Goose and Sampled Values. My name is Beth Kapalis, and I'm the Marketing Manager here at Triangle Microworks. Before we get started, I'd like to go over some housekeeping items and some introductions. The webinar is scheduled to be one hour long. We will be stopping for questions a couple of times throughout the webinar, and we'll stay on afterwards to answer as many questions as possible. The audience is on mute, so please submit your questions online using the webinar tool under the section titled Questions. We have several folks answering questions online, specifically Dave Gaukenauer, a Senior Developer here at Triangle, working mostly on Distributed Test Manager, one of the tools that you'll be seeing here later today. We also have Matt Green, a Senior Application Engineer here at Triangle. Matt has also presented at many of our past webinars, does product trainings and demos, as well as being one of our Senior QA Testers. In addition to answering questions online, I will read and answer questions during breaks and at the end. Next, I'd like to introduce our speakers. We have three presenters today. First, we have Mark Adamiak, Principal Consultant from Adamiak Consulting. Mark started his career in the utility business with American Electric Power, and in mid-career joined General Electric, where his activities have ranged from advanced development, product planning, application engineering, and system integration in the protection and control industry. Mark is an original member of the IEC 62050 Working Group, a Life Fellow of IEEE, a registered professional engineer in the state of Ohio, and a GE Edison Award winner. Mark was the Principal Investigator for the EPRI Intelligrid Project to develop a reference architecture for the smart grid. Next, we have Herb Falk, Managing Director of Outside the Box Consulting Services. Herb has over 40 years of experience working in the automation, information exchange, standardization activities, and system integration. Herb's previous employment was Westinghouse Pneumologic and Cisco, where he was responsible for the 62050 and SIM integration projects and products. Herb has been involved with 62050 technology since 1982 and cybersecurity since 1993. Herb is an editor of the IEC 62050 8-1, editor of several cybersecurity standards, and is the Vice President of Testing for the UCA International User Group. His work for the UCA has involved the coordination of IEC 62050 interoperability testing in 2011, 2013, 2015, and 2017. Lastly, but certainly not least, we have Jackson Moore, Application Engineer at Triangle Microworks. Jackson received his Bachelor's of Science degree in Electrical Engineering and Computer Engineering from North Carolina State University and has a background in power system engineering. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer, where he designed and developed load management control systems for multisource microgrids ranging from 1 megawatt to 30 megawatts. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve unique and complex challenges.
 that our clients face. So now that you know everyone, I'd like to just give you a little bit of background on Triangle Microworks before we get started. Triangle was established in 1994, and we are headquartered in Raleigh, North Carolina. We have customers in over 70 countries and work mostly with equipment manufacturers, electric utilities, and system integrators. The products that we offer all support the standard SCADA protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and greatly reduces their costs and development time. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in both the lab or the field. They are designed to troubleshoot, simulate, and automate testing. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols, as well as OPC. We are heavily involved in the standard committees that support SCADA protocols, like UCA Group for 620.50 and DMP User and Technical Committees. Through our participation, we help define the protocols, we stay up to date on the changes taking place, and ultimately provide feedback based on our customers' needs as the protocols continue to evolve. So now that you know a little bit more about Triangle, I'd like to hand it over to Mark to kick off the webinar. Thank you, Beth, and good morning, everybody online here today. We're going to start off by looking at use cases. You may have heard of these terms of routable goose and routable sample values. We're going to look at what are called use cases for the development of these protocols, or these profiles, as they are referred to in IEC 61.850. So these use cases, most of which are actually in service, and one or two could become in service. First one is the concept of transfer tripping. In many utilities today, this architecture of having, say, a 69 kV bus, in this case here, the breaker, stepping down to multiple 69 kV to 13.8 kV transformers. In this architecture, for a fault between the circuit breaker and maybe an internal fault inside the transformer, protection at 69 kV cannot see that fault. What is done today is the application of what is known as a ground switch. When local protection does detect a fault in this region, it operates a ground switch. The ground switch literally places a line-to-ground fault on the 69 kV system. That line-to-ground fault can now be seen by the 69 kV protection, and subsequently goes and trips the 69 kV breakers as well, and the other breakers on the feeder also trip subsequently. It is not a good idea to put a ground fault on a 69 kV system. What is happening, first of all, as the system goes,
 grows in strength, the ground fault Secondly, the ground switch itself is reaching the limits of the technology. So an alternate solution has been sought. Alternate solution here is to add digital devices that can connect with the protection at each remote substation. And when the relay that is covering that range of fault between the breaker and the transformer detects a fault, it sends a message to the local communication relay. That local communication relay can then send a message from one device to all the other devices. In fact, I just want to clarify a term here. This is what's known as multicast. In multicast, one device sends a message and is received by multiple other devices. Obviously, the multicast message trips all breakers on that system. A cost-benefit analysis can be created to look at the benefits of a ground switch versus a transfer trip system. The purchase and installation cost of a ground switch is about $50,000 per switch. And as much as you may have as many as seven of these ground switches per line, that's a total cost of $350,000. The protection system cost is the same for both solutions. Now, if we look at a multicast solution, we're looking at seven devices at about $25,000 per device installed. We're looking at $175,000. So clearly, and the assumption here, by the way, is that the communications exist, which may or may not be the case. But in this particular customer, it was in existence. So as a result, we have a difference between $350,000 and $135,000 savings in migrating to the communication-based solution versus the ground switch-based solution. I'm going to say a growing implementation these days on the utility infrastructure is what is known as remedial action schemes, or typically abbreviated as RAS. RAS can best be thought of... Now, this is beyond traditional protection. This is power system protection. This can best be visualized in terms of today's cars, where you have an automatic braking system. If your car detects you're going to hit the car in front of you, it automatically applies the brakes. This is exactly what happens with a remedial action system. The remedial action system has three components. The first one is the When the remedial... The remedial action system has three components. It has monitoring devices that measure the state of the power system through either status and or power flow measurements. There is logic processing. The logic processing collects information from all these substations and puts it into a logic engine that analyzes the results.
 Today, based on pre-existing or pre-identified braking conditions, we'll call it. And then finally, if it's determined that the brakes need to be put on the power system, the operation or the control messages are sent out to mitigation devices that actually execute commands to put the brakes on, typically to either remove generation or to remove load. So this is the architecture for remedial action. What is needed here is the ability for multicast, meaning that, again, the monitoring devices can send the message to multiple logic processors, and the logic processors can send the message, one message, to multiple mitigation devices. So, if you look at the cost-benefit of putting in a centralized, so there are many distributed RAS systems today, and they'll be put in for one particular scheme. The North American Electric Reliability Corporation, NERC, requires that all RAS schemes be tested once a year. So this particular cost-benefit analysis only focuses on the testing aspect of the RAS. This does not go into the architecture savings of the centralized RAS architecture. So for testing of a central logic system, two engineers at $2,000 a day can test all the RAS schemes from one central location. If we assume that there are, say, 20 RAS schemes in the utility, then you're looking at each scheme, an engineer having to travel to the site, two engineers traveling to the site at $2,000 a day for 20 schemes, you're looking at $80,000. So the, if nothing else, at least $76,000 per year savings in the centralized remedial action scheme. As we've all heard, there are significant issues with broken wires falling on the ground and causing ignition of flammable materials on the ground in remote locations. This is typically happening out in the West Coast, California in particular, but Washington and Oregon would have similar type of situations. An application, so the goal here is there is a challenge to try and eliminate broken wires from falling on, energized broken wires from falling on the ground and igniting material on the ground. There's an interesting solution that has arisen using something called a phaser measurement unit or a PMU. The PMU measures the instantaneous angles at both ends of the power line or at multiple ends, as many PMUs as there are, these measurements can be made. When a power line breaks, as shown here, the phase angles at the two different ends of the power line change dramatically and almost instantaneously. So by sharing the information, having PMUs at each end of the line send information not only to the other end, but to the other ends that may be, you may have a Y connection at a point where one, where a Y splits and goes in two or three different directions. This again identifies the need for what is
 is called, we'll call it routable sample values. That means it's a multicast of synchro phasor values from one location to multiple other locations. The calculation today, this is in place actually at online on several lines in the utility. And today, all of the data goes back to one centralized location for analysis. However, this architecture with routable sample values can now send a message from PMU to other local PMUs where such a calculation can be made. Cost savings from this, you ask anybody in California, it's immeasurable. People would love to have many of their houses that back that have burned down. An evolving technology on the utility front is frequency-based variance. The power system being made up of mostly electromechanical components, although that is changing, responds as an electromechanical system. Specifically, if you're looking at the picture on the screen here, when you drop a stone into water, it sends out ripples from the center point of where the stone was dropped. This exact same thing happens in the power system. When you drop a power plant on a system, on the grid, that power plant sends out frequency ripples throughout the entire interconnected grid. This is shown in these pictures here that are part of what is called the frequency network or FNET, and the pictures run from left to right. On the left picture, you see some coloration there that shows the dark red here is nominal 60 hertz frequency. The lighter blue and the yellow represent dips in frequency at this given location. As we go to the next one, we see deeper blues as the frequency continues to go down, and we see the other frequencies start to ripple out across the power grid. Many, if not all, independent system operators would be interested in seeing this kind of a plot. What this plot shows is exactly where the power plant went down. That's the point where the pebble got dropped into the water. So such data distribution, again, requires multicast communications, where the frequency measurements of all these points are sent to multiple other operating centers all around a particular grid area. The cost benefit of that, by the way, again, isn't measurable because knowing where a power system, a power plant went down, can allow the operators to more quickly react to that scenario. Lastly, we're gonna look at the concept of surgical load shed and demand-side management. Many years ago, there was a test at American Electric Power, which I was involved, and a demand-side management system was put in place. Demand-side management says that the goal is to manage the load on the grid through price signals. In this particular application, pricing's varied from low, medium, to high, and to low.
 to critical peak pricing, whereas the low price might have been three or four cents a kilowatt hour, the critical peak pricing was 30 to 40 cents a kilowatt hour for electricity. In the homes in these locations, there were low responsive devices. So for example, the hot water heater upon critical peak pricing could be turned off. You don't need hot water at 30 cents a kilowatt hour or 40 with inflationary times today, it could be 50 or 60. The HVAC system, instead of turning it off, what it would do is to change the set point. So if you were cooling your house at 71 degrees, you could automatically change that set point to say 73 or 74 degrees without having any major discomfort in your house. An electric dryer has a heat cycle, which is a 5kW load that can be turned off based upon demand side pricing. Same for a dishwasher, dishwasher has a heater in it, a heater can be turned off for a period of time and not affect anybody's lifestyle. This requires getting a message from one central controller to hundreds of thousands or even millions of homes. Multicast can provide this solution. That testing has shown that like with 900 megahertz radio, one radio can reach a radius of some 30 miles with this communication technology. There is a side part of this, which is the surgical load shed. On the previous slides, when we talked about a remedial action, one of the remedial actions is to completely turn off a feeder. When that feeder is turned off, well, you lose everybody there. So in this case here, a multicast message can be used to distribute the controls and the pricing. The cost benefit in this application is very interesting because in the cost benefit, if you had to meet the load, which right now is not being met, for example, in California, the ISO there has already announced that on hot days, there's not enough generation to supply all the anticipated load. Building new loads is going to cost about $500 per KW. This would be for a gas turbine or about $2,500. Demand side management allows for the implementation of negawatts. That is to take watts away from the load. Such a system is estimated about $1,500, which is a cost difference of about $1,000 per home. For a million homes, you're looking at $1 billion cost savings and no carbon emissions. So just real quick, the solutions for this are based on a IEC 6100. There are now four profiles for this multicast messaging. The original one known as the GOOSE, the Generic Object Oriented Substation Event, which is a multicast model. This has been enhanced in the most recent release of IEC 61850, which adds in the concept of routable GOOSE and routable sample values. These routable protocols use a routable IP multicast address which allows the one message to be sent via networking in many, many different locations. Okay, that's what I have on this section here. Beth, I turn it back over to you.
 Okay, well, thanks, Mark. Next, we're going to hear from Herb. So, Herb, I'm going to go ahead and make you presenter. And you should have the screen. What are you guys seeing? Full screen? Yep, we're all set. We can see your presentation. Okay. So, Mark was talking about GOOSE and routable GOOSE. And routable GOOSE comes with security. And it was designed for resiliency. So, I'm going to go a little bit into the technologies behind the security and the routable GOOSE itself. So, when we talk about security priorities, there is a large difference between IT priorities and OT priorities. And you can basically classify them as CIA for IT, which is confidentiality, integrity, and availability. Or AIC, which is availability, integrity, and confidentiality for the OT world. If you think about the threats in the utility industry, we really need to have tamper detection, a way to authenticate controls, protect from spoof and replay, and for confidentiality, it's information leakage. The counters to most of these threats are what are called message authentication codes or hashed message authentication codes. And the use of public key infrastructure and common keys. Now, Mark introduced the concept of multicast. And it turns out that multicast presents an issue where you have to share a common key amongst the participants. And those have become symmetric keys. And we'll go into that a little bit as we progress here. One of the things that is coming, and I am not a NERC auditor, is this statement in the NERC SIP version 7, part 5, that all external routable connectivity for routable connectivity must have an electronic access point. And what's significant here is the word of connectivity and not protocol. And this has a significant impact on a lot of the applications that are using Layer 2 tunneling through L2TP over VPNs. I am not a NERC auditor, but that statement leads me to believe that if you tunnel Layer 2 over a VPN using IP, that is routable connectivity. And therefore, you have to have an EAP for that connection. Additionally, there used to be an exclusion for Layer 2 goose, and I cannot find that exclusion.
 any longer in SIP 7. There is also a lot of work going on in what's called Defense in Depth, based upon IEC 62443. And this would basically require mutual authentication of the ends. And it really means a land-to-land VPN is not enough to accomplish this task. There's also a lot of discussion in the industry about what's called Zero Trust. And Zero Trust, I've highlighted the major concepts here, and it really requires end-to-end authorization, regardless of if you're on the same network or not. So this is from NIST 800-207, and NERC is actively discussing this concept for potentially future versions of NERC SIP. And again, this kind of emphasizes that land-to-land VPNs are not enough. If you go to Zero Trust, you need mutual authentications end-to-end between the peers. It's unknown the implications from a NERC SIP perspective on internal substation communications, but if you look at this highlighted here, it would seem to apply there as well. So routable GOOS, routable sample values, and secure Layer 2 GOOS, and Layer 2 sample values are really structured and setting up to support the concept of Zero Trust. So the way GOOS and sample values and routable GOOS and routable sample values works is, basically, you have a control block and a set of subscribers in a system configuration description file, which is how things get configured in 61850. Also in those files are all the communication addressing. One of the things this does not typically address is bringing in test sets into the substation, and we all want to bring in test sets or monitoring equipment, and you really don't want to have to decommission security to bring in this equipment. So the security plane, which is the management of the policies and the keys in 62351-9, is set up to allow a transient cyber asset to come in and be authenticated without having to change your security configurations. So the transient test set can come in. It's got credentials that can be authenticated by what's called the key distribution center here, and then it can participate as normal for testing. So a group's really defined as a triplet of information.
 which is the service type. Are you using Layer 2 goose or sample values or routable goose or routable sample values? The destination address, which could be a Layer 2 MAC address or a multicast IP address and the dataset reference, which represents the information to be, that's being published. The theory is, is if any of this triplet varies, there can be different keys and different policies associated. So it's very strong if you take that approach, because even if you break a key for one group, you have not disrupted the security for the other groups. 62351-9 for the use of our goose and goose and sample value security has two mechanisms. One's called a pull and one's called a push exchange. Everybody has to support pull and push allows the KDC to push new keys and policies out to the group members, which are the publishers and the subscribers. From a resiliency and a resynchronization perspective, when a group member powers up, it has to do a pull to reestablish synchronization in case it missed a push. If it receives a push from the KDC that is of an item that it's interested in, but it can't decrypt or designed the packet properly, it has to do a pull to re-sync. And we're gonna go a little bit under the covers of how this works next. So in a group pull, there are two phases. One is phase one and one is phase two. So we're looking at a simplified view of the information that's exchanged. A group member, when it sends the initial message on a pull, specifies a Diffie-Hellman group, an array of proposed information, which consists of a Diffie-Hellman group, the key type it wants, the size of the key, the lifetime of what's called the security association, and some other parameters. When the KDC responds, it receives a pull and responds with one selection. The group member then generates a nonce and provides it to the KDC, and a nonce is a cybered random number, basically. The KDC sends its nonce back, and all of this information is used to derive ephemeral keys that can be used for encrypting the next set of messages, which provides the identity certificate of the group member and which contains the public key of the group member. There's other information in this packet, but related to this, there is a signature, which is signed with the private key associated with this certificate. So if the contents are tampered, the signature won't match. If the identity...
 certificate being provided does not match with the private key, the signature won't match, and then the KDC provides its certificate and signature back to the group member. This provides mutual authentication between the KDC and the group members, and as part of PKI infrastructure, the KDC or the group member can validate that these certificates are signed by the appropriate certificate authority, and through either online certificate status protocol or certificate revocation list can verify that the certificate is still valid. The RID indicates that these exchanges are basically encrypted, so in phase two, as long as the security association exists, a group member can request keys and policies for a given set of, for a given group. It gets an accept or a deny with certain policy parameters. The group member acts it, and then the KDC delivers keys and policies. It delivers two traffic encryption keys that overlap, and a key encryption key that's used for the push if it is to be used in the future. The reason for overlapping is for resiliency purposes, and these keys expire typically every 24 hours, so by delivering two keys simultaneously, you're delivering 48 hours worth of operational keys that can be used within the substation, and again, this is to create the availability and the resiliency of the system. So, what are the policies that are delivered? It's the rotation time of the keys, the activation delay, which is what creates the overlap between the current key and the next key, the confidentiality algorithm to be used, and please note, none is allowed. So, by using none, the payload is not encrypted, but it is still signed, as you will see, in the operational plane, and this can allow edge inspection as is required by NERC SIP. The message authentication algorithm, which is what's used to sign the published packets for goose and sample values, and here are the options that are currently codified within 62351-9. There's also, as Mark mentioned, a key delivery assurance policy, and if that is turned on, the group members have to acknowledge that they have received the keys, and for a particular group. Once a given level of assurance is reached, the KDC can inform the publisher that enough group members have
 receive the keys and have acknowledged them, and therefore it's okay to rotate the keys when the keys need to be rotated. There is a subtlety here. Most people would go to 100% of a group needing to receive keys, but it is really a design choice between resiliency versus the need to continue security key rotation. This only works in reality when keys are being pushed. In the real-time plane, what you're looking at here is the security information was added to the end of the goose packet without impacting the actual encoding of the payload of goose or sample values. The benefit in doing this is you can have a mixed mode substation where some messages are published with security and others are published without security, and a subscriber can be non-secure but receive a secure record message here, unless it's encrypted of course, and still process the goose payload as normal. And that's just the quirk of Ethernet because this stuff down here looks like Ethernet padding. The routable uses UDP IP and then wraps security around the normal goose or sample value payloads. Encryption is optional here, but authentication is mandatory. So because of all of this, and the ability to not encrypt, the utilization of our goose in 62351-9 allows edge inspection, and as I mentioned before, it can be used in a zero-trust environment to help establish mutual authentication. So one of the big problems with security or concerns for users is how hard is it to configure security and maintain it? And basically, I've been consulting with a company called PCI Tech for a product called Garibaldi, which is a key distribution center, and you can literally configure and commission security in less than five minutes. So what we're going to demonstrate here is logging into Garibaldi, configuring the database and the database credentials, importing the security authority certificate so that group members in the KDC can be authenticated with each other, importing the identity certificate for the KDC, setting the global policies, and importing the SCL file. And that's all that's required to actually get started with security. Now I'm not going to do this live, but in order to achieve this in five minutes, it requires that you actually plan this all out. So this is a video that I'm going to run that goes through the steps.
 steps. So the first step is we're going to start a timer, and we have to wait for this, and we're going to log in, and we're going to log in with a two-factor based digital attributes certificate. We're going to select SQ Lite as the database, and when we save the schema, and the database is actually being created here, and you can see that was successful. We're going to go and just configure a name for the KDC at this point in time. It's just for display purposes. Then we're going to go and add the certificate authority certificate that's used for the identity certificate, and this is a public certificate that's been added, and now we're going to go and install the identity certificate for the local KDC, and this is a certificate that includes the public and private key, and the private key is protected by a passphrase, so that has now been added. Now we're going to go and evaluate, do we want to do key confidentiality? What hash algorithm do we want to use for the message authentication code? And we're just going to keep it with CBC. We're going to enable push. We're going to keep the interval for key rotation at 12 hours, and we're going to turn on KDA, and now we're going to go import the SCD file, selecting which publication control blocks we want to manage, and this is a small SCD file, so if it's a larger file, it's going to take longer to import, obviously, and there you go. This shows you all of the publication control blocks. If they're marked local, they're actually being managed by this KDC. None means it's not being managed at all, okay, and you can see we finished this in really two and a half minutes, and two minutes later from here, from this point, the KD Garibaldi will actually start serving keys, so with that said, oops, I didn't want to do that. Excuse me for the FUBAR. If you have two relays, and prior to key distribution, if you look into them, you can see that all of the keys are expired. After the KDC starts up, you can see that all of them have the two keys, and if you look, here's the overlap, so when this becomes active, this key has not expired, so the group member, if it receives a key ID for either one of these, it can use those keys.
 We can also show if the keys have been delivered. So you can see in a lab that the keys were delivered here and that the publisher received the keys. And there are additional functions in Garibaldi that we won't cover at this point in time, but you can use radius or username password login. There's logging, an extensive logging infrastructure, which includes structured list, syslog and the ability to change policies per data stream. And with that, I am going to turn it back over to Beth to go to Jackson. Thanks, Herb. We are running a little bit short on time, so I'm not going to spend too much time on questions right now, although we do have some stuff coming in. One question I would like you to answer, Herb, before I switch to Jackson, is we had someone ask, can you clarify what KDC is? Key Distribution Center. It's the application or appliance that actually the group members talk to to receive their keys and policies. OK. Thanks, Herb. With that, Jackson, I went ahead and made you a presenter. You want to share your screen and we can see your screen. Jackson, you're on mute. My apologies. Thanks, Beth. And thanks, Herb. Now that we've discussed how emerging use cases for 61850 extend beyond the substation, as well as the growing need for security and how we can implement it in these wide area applications, I'm going to walk through a demonstration where we can see routable, encrypted and authenticated 61850 communications in action, as well as demonstrate some of the reasons why they provide benefit. For this demonstration, I'll be using an upcoming release of our SCADA communication testing and simulation tool, Distributed Test Manager, or DTM, seen here and how it fits into the 61850 process. At its core, DTM is used to simulate clients and servers from a variety of industrial communication protocols. Today we'll be using 61850. However, what sets DTM apart is the variety of powerful ways a user can interact with and control the device that's being simulated. If you have attended previous webinars or used the tool yourself, you may be familiar with these features, such as our automated image image GUI, support for several scripting languages, and most recently, our test action samples. Today I'll be using DTM to simulate a system configuration description file, or SED. An SED describes a complete 61850 substation in detail using the substation configuration language, or SEL. It contains a description of the physical substation layout, communication definitions, as well as IED and data templates found within the substation. Several of the IEDs described within the SED I will be simulating today have been configured to communicate via encrypted and authenticated routable GOOS messages. I want to point out that this simulation is powered by the underlying Triangle Microworks 61850 source code library, which includes support for routable GOOS sample values, as well as encryption and authentication. This means our 61850 monitor and troubleshooting tool, shown on the last slide, TestSuite Pro, will also soon include support for routable and secure communication. Let's take a look at what the imported SED looks like in DTM.
 When the SED was imported into DTM, several things occurred. First, this single line diagram was generated from the substation section of the SED. Devices such as breakers and switches, as well as protection and control functionalities present in the single line, are automatically linked to data attributes from the data model of various IEDs contained within the SED. For a detailed look at how this process works, I suggest revisiting some of our past webinars, for example, 61850 Tools and Techniques. Next, I will browse over to the IEDs folder, where we see each IED found in the SED has been created. In this case, these IEDs are 61850 servers. From a 61850 communications perspective, these IEDs are indistinguishable from the physical devices they are simulating. As previously mentioned, several of these servers have been configured for secure and routable GOOS communication. Let's take a look at the settings unique to those features. First, because we are using routable 61850 today, I want to highlight that we have a routable IP address selected. This is the outgoing IP address which routable GOOS and sampled value messages will include as a source address. If an IED has multiple network interfaces and IP addresses, this allows the user to specify which IP address will be used for the routable communications. Next, I'm going to come over to the Advanced tab, and I want to highlight settings that are specific to security. We have indicated the IP address of our key distribution server, or KDC, which Herb talked about during his talk. We are using Garibaldi as our KDC server. When our IED performs a pull sequence to retrieve keys from the KDC, this is the address our IED will use to reach out. We'll take a closer look at that sequence in a moment. Below that, we see our passwords field. This allows the IED to access or open the signed identity certificate and extract the private key. Certificates are seen below. These include the certificate authority, which is shared with the KDC, and the signed identity certificate unique to this IED. Note that these certificates are not contained within the SED and must be configured in each IED, which utilizes authentication and encryption. To understand how these certificates are used, let's take a look at a network capture showing the resynchronization process, which occurs between the IEDs and the KDC server when the IED first boots up. The group of messages we see here are known as a group pull sequence. This occurs when our IED first comes online and reaches out to the KDC. The first two messages establish the parameters for secure association, or SA, and this information is used to derive the ephemeral symmetric key that's specific to the secure association. The derived ephemeral key is used to encrypt messages three through five, or I should say message exchanges three through five. The third message exchange coming from the IED contains an encrypted payload that allows for the protected exchange of the X.509 public identity certificate. The KDC validates the certificate and checks if the IED is indeed allowed to request keys. If yes, the KDC responds to this third message with an encrypted payload so that the IED can mutually authenticate the KDC. The fourth message
 which is used by the IED to request a key for a specific data stream. In our case, a goose stream. The KDC responds with policy information, and the IED verifies its reception with a fourth message. And finally, in the fifth message coming from the KDC, keys are delivered, which the IED will use to encrypt the data stream. So now that we've seen the vertical resynchronization process, which takes place between an IED and the KDC, let's look at the horizontal communications, which occur between two IEDs. Moving back to DTM, I'm going to trigger a secure goose message to be sent between the two devices. In the right pane, we see the subscriber, and on the left, the publisher. This is just a view of the data model. As I set this to true here, you'll see the message over here comes to true. When I go back to false, the subscriber sees the message come to false. And this is all happening via 62850's secure goose communications. Now this is another IED which looks very similar, but it's not been configured for security. And on this one, I'm going to do the same thing, set it to true, and we'll see that on the subscribing IED, it's set to true, then I'm going to toggle it back to false. And now we're going to switch back over to Wireshark and take a look at what this exchange would look like in a capture that I've previously taken. We'll start out by looking at the unsecured message, or traditional layer 2, which does not utilize security. Having a packet within this capture will look familiar to those of you who have analyzed 62850 network captures in the past. The entire packet is visible, but what I really want to highlight is the data portion here at the bottom. What we see is plain text, the payload of the message. In this case, we can see the RREC ST.CLOSE was set to false in this message. If we sorted through this, we could find the exact message that I just triggered. Now I want to switch over to the same type of message, but coming from a device which is configured to use security. The first thing you'll notice is the header section. Within this section, we see several fields which we did not see on the unencrypted stream. Let's walk through a few of those. The timestamp of the current key startup usage. This indicates when the key became valid. Below that, we see countdown time until key rotation. As Herb mentioned earlier, this is a setting which is configurable in the KDC. Finally, we see current key ID. There's two different keys that were delivered as part of the resynchronization phase, and this indicates to the subscriber which of the two keys should be used by the publisher during encryption. And in turn, which key should be used by the subscriber during decryption. But this brings me to the payload itself, which is really the most interesting. What I want to highlight here is that it just says unknown. This is because it is encrypted and the data is unintelligible. This means that a bad actor sniffing the network would be unable to discern the contents of the captured message. To further illustrate how powerful this point is, let's discuss a few ways that security events can impact the system. Specifically, I want to talk about a type of network attack known as spoofing. A spoof attack is a form of network attack in which a fraudulent packet or group of packets is injected into a network. Due to their fraudulent nature, payload and header information of the spoofed packet are chosen by the attacker, making detection of a sufficiently sophisticated network.
 attack, difficult, if not impossible, to detect without authentication. To demonstrate this, I'll be injecting two spoofed messages into our network using a tool called Playcap. Specifically, these spoofed messages are going to change the same two values which we played with earlier. The opex.general on RBRF1 of IED line 1 LPU and the line 2 LPU. Again, line 1 has been configured for security, and in this case, authentication, whereas the line 2 LPU has not. So when I hit Play, we're going to watch as the subscribing device receives these messages. This top one is the secure, and this bottom one is the one which has not been configured for security. So we see the bottom one toggling from true to false. This is due to the fact that the subscriber is receiving both legitimate and spoofed messages, with the legitimate messages sending a false, where the spoofed messages are sending a true. And this illustrates 61850's known vulnerability to spoof attacks. And I'll go ahead and do it again. In contrast, we see the messages that are In contrast, we see the message configured for security did not change. This is because the payload of the message was altered, and therefore, the message could not be authenticated by the subscriber and therefore, was discarded. I hope this serves to demonstrate the power of security and what it means for 61850 and its applications, especially as they extend beyond the local substation. Now let's shift gears, and I want to bring up Garibaldi, which Herb showed us in the video earlier. Garibaldi is the key distribution center server, and I want to demonstrate where you can see the settings and alter the settings that are specific to this publisher subscriber group that we just looked at. When the SED file was imported into Garibaldi, as Herb shown, it shared configuration information about each IED shown here, publication groups shown here, and subscribing IEDs present in the substation. For example, which IEDs are subscribed to which streams and which streams should be encrypted and authenticated or routable or not. Also seen here is configuration which is not contained within the SED file, such as which encryption algorithm will be used, the intervals at which keys will be rotated, and other settings. These may be set as a global policy applying to the entire SED or set individually. And again, I want to highlight that all of these settings can be set individually and altered, even the publication groups, without the need to re-import an SED file. This is particularly useful for testing and troubleshooting your secure environment. And it also serves to illustrate my final point, which is that streams can exist within the same network or even IED and have different security settings or even no security settings at all. Additionally, routable and non-routable goose and sample values can coexist on the same network or even the same device. This means that your migration to a secure environment does not need to be wholesale. If encryption is deemed necessary for only a portion of the IEDs in your network communications or even a portion of the communications coming from a single IED, that's perfectly fine. You can mix and match layer 2 routable, authenticated, and authenticated encryption.
 And with that, I hope you have enjoyed getting to see substation security in action. And I'll go ahead and turn it back over to Beth for any questions. Thanks, Jackson. Before we get to questions, I know we've run over time just a little bit here, so I appreciate everyone staying on. And like I mentioned earlier, we're happy to stay on as long as questions are coming in. Before I shift to that, I just want to mention a couple of closing thoughts. The webinar will be posted to our webpage in a couple of days, so just feel free to visit trianglemicroworks.com, and you should see it on the homepage if you scroll down. You also will receive a survey after you leave this webinar today. If you answer the questions on that survey, you will automatically get an email that will have a link to the recording as well, and we certainly do appreciate your feedback on the webinar and the information that you saw here today. You can download an evaluation license to DTM. This is a tool that you just saw. If you'd like an evaluation license to Garibaldi, you can email us directly at sales at trianglemicroworks.com. And certainly, if you have any other questions that we can help with, please feel free to just use an email. We would always appreciate hearing from you. So with that, we do have some questions still coming in that I'd like to just share with the entire audience. We do have quite a large number of folks still on, so I appreciate you guys sticking with us. So I'm going to open this up to the panel. I'll just start to read from what's come in most recently. Regarding the encryption, what about network bandwidth? Wouldn't this be an issue for old substations? Old com devices need to be changed. Did the cost savings shown earlier consider this upgrade? I'll open that up to Mark or to Herb. I'll take it, Beth. So it really depends what your infrastructure is. Depending upon what you select the algorithm for, for encryption and signing, it can be as minimal as approximately 32 bytes in addition to the Argoose header. In another of the questions, somebody asked what type of bandwidth is required to use Argoose outside of a substation. Looking at a major North American utility, they're doing 40 Argoose messages coming out of the substation and with a T1, both over land way and technology and over a microwave. So it's not that much of a requirement, but you can calculate it all based upon your performance requirements and the number of devices and that type of stuff. Okay. Thanks, Herb. I've got another question that I think will probably be for you, Herb. I see that you answered online. Even with the encryption process, can the transmission time requirements for Argoose messaging still be accomplished? And I think the answer to that is yes. There is a misunderstanding of what the three milliseconds really represents. With modern hardware, with crypto acceleration, I've not seen anything beyond a one millisecond addition, but it does depend on your protection requirements.
 Okay, thanks, Erb. Next question, can you please show the sections in the SCD file where the security configurations are set? Also, does the SCD contain information about KDC? Okay, so there's a communication section in the SCD, which is where, you know, destination GOOS and RGOOS are defined, as well as the addresses of the IEDs themselves. And for Garibaldi, it is supplied with an ICD that indicates that it is a KDC, and let's see if I can find that, and maybe I can show that. I don't know. I can change the presenter. Hang on, I'm searching. Okay. While Herb is searching, I can confirm that the security aspects only add the one millisecond on an end-to-end basis, which means that maybe about 500 microseconds on the encoding side and 500 microseconds on the decoding side, and that's been tested. It's Mark. Herb, any luck? Yep. I've got luck. So, if you'll make me presenter. Yep, there you go. Can you see this? Yes. Okay. This is the ICD that typically comes with Garibaldi. I don't know how familiar the person who asked that question is with SCL, but an ICD, the... You might be cutting out, Herb. ...template is reserved to indicate that it's coming from an ICD. And you can see that here at this access point here, there's this attribute that says KDC equals true. So, that indicates that for this IED, which gets renamed and readdressed in the communication section as part of its integration into the SCD, is where you can reach a KDC. So, hopefully, that answers that question. Yes. Thanks, Herb. I think very thoroughly. Another question, I think, for you, questions about certificates. How are they generated? Well, the ones that get distributed with Garibaldi use an open-source tool called XCA. It's freely downloadable. It's a lot easier to use than OpenSSL for certificate management. In a real utility infrastructure, you would use something like that or a PKI certificate infrastructure. Probably.
 with an intermediate or a regional security certificate authority set up. It really depends on the utility deployment scheme. Thanks, Herb. Next question, what is the impact on delivery time on a goose message when additional security features are being used? I think we've mentioned that. It's no more than one millisecond. OK. Next question, which phase, one or two, are more critical to assure our goose flow in the network? Has to be both of them. I think we might be coming to the end. I have one last question here. What equipment is needed to implement our goose between two substations approximately 20 kilometers apart? Probably routers. And if it's a BES system, a medium impact BES system, probably a firewall with ACL capability. I would augment that to say, make sure it supports IGMP version 3, and also supports protocol-independent multicast in the routers. Yep. Yep. Thanks, Mark. So Beth, there are two other open questions here. Do you want me to just read them? Yep, go for it. How many IED vendors are supporting our goose and our sample values currently? So GE and Toshiba are IED vendors that currently support it. There are at least three other vendors I'm aware of that are probably going to support it within the next year. But I can't speak to them. One vendor, I'm going to leave it to Mark, but I think routable sample values, the GEUR support, correct? Yes. Certain versions support routable sample values, and they're actually in service right now. Security, go ahead. Yeah, so mid-July, there's going to be an interop of 61850, and our goose testing is being done there between Triangle, Toshiba, and maybe GE. I don't know if they're actually bringing it at this point in time. So the last question here is, the IP assignment is made with IPv6 or via IPv4? The standard allows for both. Right now, Garibaldi only supports IPv4. As do the relays. As do the relays, yeah. OK. Well, that comes to the end of our question and answer session. I appreciate everyone staying on a little bit longer to stick with us, and thank you for your time. As I mentioned just a couple of minutes ago, when you do sign off, you will receive a survey. We'd really appreciate it if you filled that out, and certainly want to get any feedback that you might have. And we'll see you next time.
 please visit our website for a recording of this webinar later this week. Okay. Thanks, everyone.


Video ID: 4XSLwdK5bvw
Hello, everyone. My name is Beth Kapalis, and I'm the sales and support manager here at Triangle Microworks. I'd like to welcome everyone to our latest webinar, Charting a Course to a Digital Substation, How to Integrate IEC 62050 in Your Existing Solutions. This is a three-part series, and part one was hosted on April 13th. If you missed it, you can find the recording at our website at trianglemicroworks.com. You do not have to have seen part one to understand part two. They are related, but can certainly stand on their own. Today's webinar, part two, is focused on IEC 62050 SEL and the engineering process. We'll talk a little bit about part three, which is focused on strategies for integration of DMP3 and IEC 62050 a little bit later in the webinar. However, you can always find out more details on our website, again, trianglemicroworks.com, under the training tab. So before we get started, I'd like to go over some housekeeping items while everyone is still joining. The webinar is scheduled to be one hour long. We have a lot of material to cover today, but I will try to take at least one to two breaks to answer questions. The audience is on mute, so please submit your questions online using the webinar toolbar under the section titled questions. You can submit your questions as soon as you have them. Please don't wait until the end. We will begin answering questions online as we have several engineers monitoring the questions and responding to them directly. We are also happy to stay on after the webinar is over and answer as many questions as we can. But your best way to get an answer is to just submit the question as soon as you have it. So with that, I'd like to introduce the folks behind the scenes answering the questions. Today we have with us Dave Galkinauer, Lead Developer for Distributed Test Manager, DTM, one of the tools that you'll be seeing here today. We have Phil Young, Lead Developer for Triangle's IEC 62050 source code library. We have Eric Fabre, our Senior QA Tester here at Triangle, and Andy Reuter from Helix. Next I'd like to introduce our speakers. First we have with us Christoph Brunner, President of IT4Power. Christoph is a well-known utility industry professional and a 62050 expert with over 30 years of industry experience. Christoph has been involved in the development of the standards since the beginning. He is Convenor of Working Group 10, a member of Working Group 17, 18, and 19 of IEC TC57. He is an IEEE Fellow and a member of several IEEE Working Groups, as well as a member of the PSRC Main Committee and the Subcommittee H. Next we have Jackson Moore, Application Engineer at Triangle Microworks. Jackson received Bachelor's of Science degrees in both electrical engineering and computer engineering from North Carolina State University and has a background in power system engineering. Prior to joining Triangle Microworks, Jackson spent five years as a microgrid system engineer where he designed and developed load management control systems for multisource microgrids ranging from 1 megawatt to 30 megawatts. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges that our clients face today. So now that you know everyone, I'd like to give you a little bit of background on Triangle Microworks. We were established in 1994 and have over 28 years of communication protocol experience. We are headquartered in Raleigh, North Carolina, and have customers in over 70 countries. We work mostly with
 equipment manufacturers, electric utilities, and system integrators. The products we offer all support the standard scale of protocols and fall into three categories. First category is source code libraries. We license source code directly to manufacturers for use in their devices and systems. This allows for a much faster time to market and generally reduces their costs and development time. Having a source code library also includes support for your implementation and any issues that may arise in the field. We also provide testing tools supporting these libraries and protocols. These tools are used for communication testing in the lab or the field. They are designed to troubleshoot, to simulate, and automate testing for both a single device or complete substation. Lastly, we also provide a SCADA data gateway supported on both Linux and Windows for protocol translation and data concentration with standard SCADA protocols as well as OPC. It's available as software only or pre-installed on hardware. We are heavily involved in the standard committees that support SCADA protocols like UCA group for 61A50 and DMP user and technical committees. Through our participation, we help define the protocols, stay up to date on changes taking place, and ultimately provide feedback based on our customers' needs as the protocols continue to evolve. So now that you know a little bit more about Triangle, I would like to go ahead and hand it over to Christoph to kick off our webinar. Christoph, I'll go ahead and make you presenter. Thank you, Beth. Welcome, everybody, from my side. OK. So in the first part of this webinar series, we compared IC61850 with DMP3, and we explained some of the concepts of IC61850 related to communication and to modeling of application data. As part of that, we also have compared the DMP3 XML file with the ID-related files of the IC61850 system configuration language, or shortly called SCL. The files are used as part of the engineering process described in IC61850-6. They can describe a template of an IED, the ICD file, or an instance of an IED. In the case of an instance, the file contains already some specific configuration information for the system in which it will be deployed, like ID name or communication parameters. The engineering process with IC61850 will be the focus of today's webinar. We will start to introduce the process as it is supported today. At the end of the webinar, we will provide a little preview on some future developments regarding this engineering process. The process starts with the specification, which supports the machine processing description of the substation topology, basically the one-line diagram, and the specification of the data model to support the required functionality. As part of the design, then when we start with the implementation, IEDs need to be selected, functions need to be allocated to them, then we need to do communication engineering for the IEDs, and the signal flow required for the application needs to be translated in GOOS and sampled value messaging, as well as in reporting. Finally, the functions need to be implemented in the IEDs, which may include some logic configuration and signal binding. During the process, in addition, as published today, we basically have file support in SCL. When we finish the specification, we can export what we call a system specification description.
 which can then be used by the system configuration tool together with the ID capability description that we get from the vendor tools to start the design. The whole design is then done using the system configuration tool when we do a top-down process. At the end, we can export what we call the system configuration description file, which contains all IC61850 aspects of the system design. This is then used by the IED tools to finally configure the devices and create final configuration of the devices. We will now go step-by-step through these different stages and discuss them in a little bit more detail. And we'll start with the specification. As already mentioned today, when we talk about specification in the SCL or system configuration language, this mainly includes the one-line or single-line diagram, where we have a hierarchy behind where the substation is hierarchically divided into the voltage levels. Under the voltage level, we have the base. And within the base, we have the conducting equipment, like circuit breakers, switches, or instrument transformers, as well as the description how those are physically connected to each other. The second part that we can put in a specification is adding the functional requirements in terms of having the logical nodes representing the functional elements that we want, as well as to add the signals that we require from the logical nodes. When we compare that to a conventional approach without the IC61850, when we do a specification, the functional requirements is mainly done in a textual document. What we can do is to have a single list, as well, that can be specified, the single list of the signals typically that we want to send to a gateway, to the scalar system. And the single-line diagram is provided as a drawing. In the case of 61850 using SCL now, as mentioned, we can do some of these parts in a machine-processable way. And the result is basically of this first step that we introduce in an SCL file of the type SSD file that we add there, what we call the substation section, which is more or less containing the requirements and the substation topology. So that's the first step. Now let's have a look how this looks in practice. So before we show that on an example, I would like to introduce you an example that we will use in today's webinar along the engineering process to show you how the different steps are working. So the little application that we have chosen is a substation with four feeders. We have two line feeders getting into the substation, and then we have two transformer bays where we go to the lower voltage side, and we also have a measurement bay. We assume that we have a solution with a process bus. So we have per bay, we assume we have four devices. Besides the bay controller and the protection device, we also assume we have a merging unit that is getting us the samples of current and voltages, and we have a switch control unit which provides the interface to the switch gear. For the functional requirements in our little application for the line feeders, we assume we have distance protection with two zones, a distance protection scheme, line differential protection, break a failure, reclosing and synchro check. And for the transformer bays, we have differential protection.
 As well, we assume we have a differential busbar protection additionally. As a little reminder on the IC61850 data model that we discussed last time, that's an extract of the data model that we use for our application. So here we see the 61850 model for some of the different functions and signals. Here we would see how this would look when you would model it in DMP, the different kind of data that we would have as DMP point. So as an example, our break-in interface where we get the position of the breaker from. The break-in interface in 61850 as a function element has the logic node XCBR. The data object of the position where we find the position information will be XCBR.POS.STVAL. And in DMP, this would basically be a WBIT input with a certain index number. Similar way, our protection functions, protection elements, measurement, just as a few examples. But we see them now as well when we look at the example. So that's the little task that we want to do today. We want to design, specify, and then design that little substation. Now how do we do the specification? Specification can be done with a system specification tool. And here I show a few screenshots how this would look later on. Jackson will demo the tool as well in reality. So we are using the Helix STS, which is both a 61850 system specification as well as a system configuration tool. The first step when we start to create the project is typically to enter the single line diagram. So we design more or less our application, the single line diagram in the model of 61850. So behind, the complete 61850 model is built up to describe this single line diagram. In the next step, then we can enter into the different base and provide the specification of the functions and signals that we are requiring. So as an example here, we have functions like interfaces to the switches, interfaces to the breakers, the current and voltage transformer measurements. We have our protection functions over here, the different elements of the protection functions. Protection related things like breaker failure, reclosing, synchro check with the corresponding 61850 logical nodes, as well as the control of the breakers and switches. What we also can do at that stage already is to allocate that to virtual IEDs. So to express the requirements, how we think we would like to have these functions later on allocated to devices. So as an example, current voltage transformers here associated to emerging unit. We also can define signals that we require. So we took an example here of the XCVR. So we can provide in detail the signals that we require for our application from XCVR. And we express that from the data model of IC61850. Behind, when we do that, the tool will build up the SL model of this substation structure. And that could basically look like this. I show here the hierarchical view. Behind that, we will have an XML file according to the schema in 61850.6. But here we see the hierarchical view, substation, voltage level, base, and in the base, the conducting equipment. And at different levels, we can associate the logical nodes that express our functional requirements. So that was the specification stage. Next stage is now IED.
 selection and the import of the IED into the system tool. So basically, we can also call that IED pre-engineering. So in this stage, we get from the device manufacturer the IED capability description, which contains communication and engineering capabilities of the IED, as well as the data model according to the functionality. This corresponds, as already mentioned, basically to the DMP XML file that we have when we will do a DMP3 project. The ICD file can be available for a product version depending on the functionality, but it can also be customized using the IED tool by selecting the required functions if you don't really need the complete functionality that is supported by the device. The model that we have behind, when we look at an IED, we basically have an IED that includes servers. An IED has an access point where we have the communication access, and in our SCL model, that's modeled as a hierarchy where we have the IED with the access point. We also have a section that defines the services, which defines the capabilities. We will see that later in an example. Under the access point, we have our server that then provides the complete data model of the IED. So basically, when we look at our SCL files, the system configuration language files, what we have in this stage is what we call an IED section, which is called template at this stage because the IED is not yet an instance. But from an SCL file, we have one element in that we call the IED section. And with that, I would like to hand over to Jackson to give a little demo and a practical example of an ICD file. OK, Jackson, you should have the screen. All right, thank you, Christoph, and thank you, Beth. So as Christoph mentioned, one of the important stages of the engineering process is compiling your different IED capability descriptions, or ICDs, which is what you see up on my screen now. And I'm viewing this with our tool SCL Navigator, which is an application for creating, viewing, editing, and verifying 61850 system configuration language files, or SCL files, and also as a built-in intelligent editor. So just to familiarize yourself with a couple of the different parts of an ICD, up here at the top, we have just some version information. And this is going to be dependent upon the vendor who comes up with this capability description, and then provides it to you for integration into your project. Moving down, we see the header section, where it's just an editing history. We can see the different changes that were made by the vendor when they created this IED capability description. Below that, we see the communication section. And Christoph is going to talk a little bit about this in the next step of the engineering process. But at this stage, because this ICD has not yet been instantiated or associated with a specific device, the communication section is going to be pretty empty. It might just have some basic default values. The next step below is the IED section, which is where we start to get into the meat. And contained within that are two parts, the services section, as well as the access points. So the service section is where we really get to see the capabilities of this IED. So 61850 is a large standard. And there's many different parts or features that could be supported in an individual IED. So when a vendor or manufacturer makes an actual IED, and they want to document it, they want to provide a document to the IED. So they can document it. They can provide a document to the vendor, and they can provide a document to the customer.
 what its capabilities are, they can do that here by listing what services are supported. So for example, we could take a look at line 37, where we would see the goose attribute. So this would indicate that this device is, in fact, capable of goose, and that it can support 10 concurrent goose streams. That'll be useful when we import this ICD into our system configuration tool. To take a look at another one, we can look at line 41, the configuration for signal references. And here we see the number of, for example, goose streams that our device can subscribe to. So in summary, the combination of all of these things are the documentation of what my device is capable of. And it's in a machine-readable format, which we can import into our system configuration tool. Below that, we have our access point. And within that, we have the different servers that are on our device. In this case, as is common, we have a single server, but that's not necessarily always the case on our single IED. And then within that, we have different logical devices. In this case, we have a protection logical device, and then actually a backup or secondary protection logical device. But you could break up control and protection or other different functionalities that your device performs into different logical devices, even though it's all on the same single box. And again, this would be done by the manufacturer. It's just a bucket to place your functions or applications in. Within that, we see different logical nodes. And this is kind of what 61850 is famous for, the hierarchical data model that's browsable, as well as logical nodes. Logical nodes, as Christophe mentioned, being different buckets to hold data associated with a given functionality that our device is performing. For example, we see this RBRF down here. So that's a logical node class, and it is associated with break or failure. So any data attributes that are associated with the break or failure functionality of my device might be held within this logical node. Another one would be the PDIF. So that's going to be a protection indicated by the P, leading, and then DIF for differential. But you'll notice that I cannot expand these logical nodes further to see what data is inside of them. And that's because it's defined out in the data template section. If there was multiple implementations of a single logical node, like I see up here with my LGOS, then I would only have to define the template for it once. So if I right-click on, let's say, my RBRF, in the tool, I can use the navigate to feature and go down to the template. Down here in the template, I can expand out and see the different data objects, which make up this logical node. And then if I want to take one of those, for example, the OPX would be an operate external DO, I can right-click on that. DOs themselves can be made up of more DOs or data attributes. In this case, I can right-click navigate to and find that individual DO and see the different data attributes which make it up. So in this case, I would have a general. So as we'll see here in a little bit, once we navigate to some of the other tools, this whole path that I went down through the navigate to features, well, that's going to make up a whole path to a data attribute, starting with our IED that we saw up at the top, moving down into our logical device, and then our logical node, for example, the RBRF, the data object and data attribute. And we'll get to see a little bit more of that in action here in a couple of moments. Christoph, back over to you. Thank you, Jackson. Okay, we should have the screen. Yep. Okay, so now we have seen how we create the specification.
 We have seen what we receive from it related to the IEDs, and now we can start the process that we do with the system configuration tool, which means import the specification, import the ICD files, and start to do the whole design. When we do a top-down engineering approach in the first step, we typically can start with the communication engineering. That's similar when we do a conventional approach, when we have our communication drawing, where we have all the devices. And in addition to that, we may have a table with all the addressing information per devices. So it's a similar step that we are doing now, which basically means, based on the ICD files that we have imported, we instantiate IEDs, give them unique names. We then link the IEDs to the process part. So what we have specified before, and we design the communication network. So this looks initially like something like this. So we create a communication network, we create subnets, we instantiate IEDs, we provide unique names to these IEDs, and we also give them addressing information, the IP address, but as well as some additional addressing information that we require in the 61850 communication world. In SCL, the communication section that Jackson has briefly mentioned before, that's basically a collection of subnetworks. And under the subnetworks, we have access points, access points that associate basically the IED with the communication subnet. In the access point, it's also where we find the different kinds of addressing information. We also can configure target addresses for the goose and sampled value messages. That's also a part that is expressed as part of the communication section in SCL. I also mentioned that we have to assign the logical nodes of the IEDs in relation to the logical nodes that we initially created during the specification stage, where we have associated logical nodes to the different conducting equipment or to the bay level. Now we can link them to the IED, and as such, we know now which instance of an IED, which logical node is in that IED, is exactly doing, as an example, the interface to this disconnecting switch, which one is doing the control. So we can, with that, directly associate what is implemented in a device with where this functionality is located in our one line diagram. So as part of these steps, what we are basically doing is we import the specification file, the SSD file, which creates us in our SCL file, the substation section. We import the ICD files, create instances out of them. So as an example, we have four bays with four bay controllers. They're all identical from the models. So we import the ICD file for the bay controller, create the first instance and the second instance for each of the bay, and give them unique names. And as mentioned at the end, we also configure the communication section, which basically then adds this communication section to our SCL file. So again, how does this look in our project? And I have, again, a little screenshot here.
 So if we go back to our project now, as an end of this step, we have our system drawing or comms drawing where we see the different IEDs connected to networks. We have here one switch per bay. In a bay, we have the four IEDs starting with the merging unit, the switch control interface, the protection device, and the bay controller. And we have that in all bays. We also have some station-level devices like busbar protection, like the merging unit for the measurement bay, the gateway. But then we also added test tools because it's always a good idea to add the test tools in your design so that they can also get the information that you may need to get for the testing. So that was the communication engineering. And now the next step, that's where we really build our applications where we create the engineering for the signal flow to implement our protection and control schemes. So what we are doing here, we basically, when we look at an application, an application is typically realized by different functional elements that exchange signals between them. That's a generic concept of an application, independent of 61850. In IC61850, the functional elements are the logical nodes. And the information exchange between the logical nodes can either be internal in a device or it can go across two devices. And if it goes across two devices, we typically would use the GOOS service or the sample value service, which is basically replacing the values that you would have in an approach without GOOS messaging. So in a conventional approach, the similar step would mainly be to draw the wire diagram, which shows the wires going between the devices, which spaces to implement the signal flow between the devices. So in 61850, an application could look like what is shown here, which is the breaker failure application. We have a couple of protection logical nodes shown, which would go into the protection trip logic. This would generate a trip that goes through the breaking interface, will trip the breaker. We may also initiate the breaker failure function. The breaker failure function may be, but it's not shown here, a re-trip of the same breaker. If that still fails, it has to trip all the neighbor breakers that can then help to clear the fault. And it does that with the signal RBRF.OPEX. So this is just the signal flow between the different functional elements. And now, depending on how we allocate the devices, and in our case, we have chosen devices like this, now you see some of this signal flow will remain internal to the device. So that's currently out of scope of IEC 61850. Some of the signal flows are now between the devices, and that's where GOOS comes into the game. So basically now, instead of having a physical output here with a wire going to a physical input, we have now a virtual output with the signal from the 61850 model going in a GOOS message. And we have here a virtual input that is going then as an initiate breaker failure into the logical node breaker failure. In a similar way, the external signal will go to all the IEDs that interface to the neighbor breakers. That's basically how the application is looking in IEC 61850.
 In a conventional world, as already mentioned, you would just have wire diagrams showing these wires. In IC6150, we can then, in our data model that we have with the IEDs data model, we can now also provide inputs. So as an example here, we can provide inputs to an XCBR, which would be this different opt external. And with that, we will now demonstrate that in the HALINX SDS tool, Jackson will show you how this configuration is done in the tool. All right. Thank you, Christoph. So as Christoph mentioned, the 61850 engineering process The 61850 engineering process largely revolves around a class of tool known as a system configuration tool, or SCT. One such SCT, which Christoph has already introduced, is HALINX STS, which can be seen on my screen now. So one of the purposes of a good SCT is to provide a layer of abstraction in the engineering process, allowing users to build and interact with their substation applications at a graphical level, and then letting the tool generate the necessary system configuration language, or SEL files, for us in the background. So with that in mind, the screen that you see here is where I've defined the high level applications that I want my substation to perform. Let's take breaker failure as an example. Let's give us some context for how we want our breaker failure application to perform in this specific substation that we're building. Let's take a peek briefly back over at the single line diagram that Christoph showed us earlier. So looking at this single line, we see that we have four breakers in this substation, or one in each of the four bays. So I think it's safe to say we could assume that if any one of these breakers were to, say, fail to open, we would want to clear the bus here at the bottom by opening the remaining three breakers. So now I know what I wanted to do. Now how can I configure that? Well, I'm going to head back over to my application schemes page. I'm going to right click on my breaker failure and open the application editor. So here I'm editing the application that I've named breaker failure. And we can see four boxes on the left and another group of four boxes on the right. The group on the left represents the breaker failure functions for each of the four breakers that we just pointed out over on the single line, whereas the group on the right represents the interface units to those same four breakers. Another way to think of this would be to say that the boxes on the left represent outputs and that the boxes on the right represent input terminals, where I can draw a line between the two and I'm sort of making a connection or routing a signal between them. So let's take a look at the box in the top left as an example. Here we see an output name opx.general, which in this case is an output being used to indicate that a breaker failure has occurred. By drawing a line or several lines across the page, I've connected that signal to the trip inputs on the other three breakers present on the single line. As Christoph pointed out in his slides, in a sense, what we see here is really not that different from how this might be done in a traditional substation. Although in a traditional substation, we might think of this as a wiring diagram of sorts with each line representing a distinct copper wire. However, here, each line that I draw coming from the left will tell my system configuration tool, Helix, to configure the devices on the left to publish the associated signal in the form of a goose message. Similarly, every place I connect a line
 on the right will tell Helinks to configure the device on the right to subscribe to that goose message. And if you're new to 61850, this can all seem like a lot to take in. But what we're looking at here is really the crux of the signal engineering portion of the engineering process which is one of my favorite parts of the engineering process and I feel like it really encapsulates a lot of what makes 61850 unique. So I do wanna take a moment to appreciate what just drawing this line across the screen has accomplished. By taking a signal that traditionally would go over copper and changing it over to a goose message, we've effectively decreased the number of trenches that need to be dug in our substation, the amount of conduit that needs to be buried, the meters of copper wire that need to be run and the number of terminations that ultimately need to be made out in the field. It's pretty neat when you think about it. But moving on, enough on that, we can now switch over to our communications editor. So here we see what our tool has done for us in the background. By making that connection over in the application editor, the tool has added that data attribute over to a dataset in my datasets tab of the application editor. And here we see the OPX general signal that we looked at before. It has assigned that dataset to a goose control block which I can see by coming over here to goose control. And again, that same OPX.general signal that we routed across. And then up here in that same goose control block, we can see that it's documented the three different IEDs that are subscribing to that goose message, all the way down to the logical node within that IED which is receiving it. So this information that we see here is still in kind of a graphical form within the SEP, but once we generate our SED file, which is our end goal for using this tool, this information will all be encoded down in SEL. Now, finally, before I move it back over to Christoph, I want to show the goose matrix, which is just another opportunity to look and see in a graphical way. Okay, here's my goose control block. And then the device that it's being published on, a little bit of a description. And then as I scroll over, these different columns are the different IEDs which are subscribed to it. The percent indicates that this is the publishing device. And then anywhere where I see a yes in that column indicates that other IEDs have subscribed to that given goose message. So with all of this that I've been able to see here in the communications tab, I can now feel comfortable knowing that my system configuration tool has accepted my settings and has encoded them correctly down in the SED. And I'm now ready to move on to the next step in the engineering process, which is going to be the functional implementation. And for that, I'll turn it back over to Christoph. Okay, thank you, Jackson, for that little demonstration. So now we are more or less getting to the end of our engineering process. We just, we have to implement what we have defined in the system tool in the different IEDs. So for that, the IED-specific tool imports the SED file, but then we have to do some specific configuration, which is IED-specific, which is not yet necessarily standardized. That's why it has to be done on the IED tool. So one of that is the process interface. So we need to configure how the signals that we get from our physical process, like the contacts, the open and close contacts from a breaker, how these are wired in on which binary input we find them so that we can then blend them into the data model of the position. If the IED has a local HMI.
 Today, that's also not yet standardized. So how to configure that local HMI is a specific of the IID tool. We may need to configure logic functions. Also, that's something that is now as well available as part of the standard. And we will talk about that briefly a little later. And we may have to configure parameters that are not expressed in the SIG2850 data model. It's also an area we are currently working on. And then we are more or less ready to deploy the substation. So to summarize again what we have done with the SCL and the engineering process. So the SCL allows us to do a formal specification, which is then expressed in the substation section where we have the topology, the electrical topology. We can associate to that function requirements, signals grouped by the function elements. At the end, when we added our IDs with the data model, the communication, at the end, we have basically a machine process of the documentation of the SIG2850 related configuration that is not only used during the original commissioning and design process, but it can also be used for testing. We can use it for simulation. And it can also be used later during the lifecycle. And how we can use that for simulation will now be demonstrated by Jackson, where he has a demo now of this project that we have created. So Jackson, it's over to you. Thanks, Christoph. So just to reiterate, one of the big benefits of the SIG2850 engineering process that Christoph's been talking about is that it results in machine-readable documentation of the substation. And that is, of course, in the form of the SCD file or substation or system configuration description. As you have seen, if we follow the design principles and best practices being discussed today, it's going to result in an SCD, which essentially contains the 61850 substations equivalent to a traditional single line diagram in the form of the substation section, a communications drawing in the form of the communications section of the SCD, your SCADA data maps in the form of MMS reports and the associated data sets, and the equivalent of your interconnect wiring. And that's going to be in the form of your goose and sample value configurations. So the benefits of all of this are pretty numerous. But specifically from a testing and simulation standpoint, having all of that information available in a machine-readable format creates some pretty exciting possibilities, which is really what I want to highlight for you next. So let's start with the first thing I mentioned, the substation section. As mentioned, a well-formed substation section can contain all of the information needed to recreate your single line diagram, which is what we've been able to do here. When DTM, our flagship simulation tool, imports the SCD that we configured earlier over in Helix, you're given the option to generate the corresponding one line automatically, which is what you've seen here. There's not a need to redraw it or transfer it in some other way between the two tools. It's all encoded within that SCD. Next, I want to come back to the communications drawing equivalent, which is the communication section file of the SCD. For that, I'll be switching over to TestSuite Pro, our 61850 monitoring and testing tool. By loading an SCD file into TestSuite Pro, we can determine the relevant IP information to establish an MMS connection to the different IEDs in our SCD. And that's without any manual configuration or know-how. There's no need to pull up a comms diagram and zoom in and find out.
 which IED I want to connect to and what its IP address is, I simply load that file in and contain within it are all of the connection information and IPs needed to connect to those devices. In kind of a similar vein, we have our MMS reports documented in the SCD. So MMS reports are the equivalent of the SCADA data maps that were really the hallmark of legacy SCADA protocols. But instead of needing to exchange and manipulate and import proprietary spreadsheets, now I can just browse to them using the tool. So in this case, I'm using our advanced client. I've chosen one of my IEDs that's within that project, and then I can browse down into LLN0 and then into my report control block section. I can find the data set that's associated with it and just locate it. Now I can see all the data that's going to be available up to both SCADA or in this case, my testing tool. And finally, that brings me to goose messages. So these are also documented here in the advanced client section. Let's take a look at this goose control block here. And if we expand that out and we look at its data set, then I can see that op general or opx.general that we configured earlier over in DTM. And this, of course, is the breaker failure that's being sent out to other breakers to let them know to open. So goose, just as a refresher, is the high-speed replacement for traditional copper wiring. And by including it in our signal flow in a machine readable format, our tools like TestSuite Pro can not just show it here in the advanced client, but monitor and visualize that data flow in a variety of different ways. That's really what a lot of what this tool, TestSuite Pro, is about. And it's really inundated the tool. You can view and use those goose messages in a number of different ways. But I think probably the most impressive of those is actually the signal flow diagram, which is what I want to highlight next. So for those of you that might not be familiar with our signal flow diagram, either from using it personally or past webinars, I'll give it a little bit of an introduction. What we see here is an application that I've drawn together showing different IEDs, logical nodes, and the goose messages between them. So in this case, it's our breaker failure application. We see the different IEDs that are a part of this, indicated by the light blue boxes. And you can see the different names up here at the top. Within that are some different logical devices that are on those IEDs. And that's indicated by the darker blue. And then finally, the logical nodes, which are the smallest boxes inside. So these lines that we see going between two devices is a goose message that's configured in the SED. Again, by making that machine readable and importing into my tool, my tool now knows that there is a trip signal, let's say, going from the PTRC on my D1 line 1 LPU1 device and being passed over into the XCBR logical node on my SEU for that same line. But that's all nice and cool. It's interesting to visualize it. But what's really neat is the fact that we can then capture that information and play it back. So to show you what I mean, I'm going to go ahead and begin a capture. And then using DTM, the tool that we looked at earlier, I'm going to be triggering a breaker failure. Now, in this case, it is all on simulated devices, but the tool TestSuite Pro doesn't really know or care that it's happening on a simulation and not as part of a real system. In this case, the tool just simply captures the 61 and 50 communications that are happening on the wire. So again, I'm going to disable a breaker so that it will result in a failure.
 I'm going to trigger a fault that breaker will not open because it's failed But it will send the message out to the other three and open them Now that all happened very quickly, but to show you a little bit of what happened I'm gonna stop it and then step through the different events so the first thing we see happened was a trip signal which was sent out from the PTRC and as a result of the fault that Was detected and that was passed to two different places first and probably most importantly to the XCPR Which should send an actual copper wire down to the breaker and energize it tripping a coil Which will actually cause your breaker to open When that happens, we should see the position value update to the RBRF who also received that trip signal and he should say Okay, everything's good. But in this case because their breaker has failed. It's not going to see that position value change Now at the same time the XCPR received that trip signal the RBRF also received it and he began an internal countdown Waiting to see if the position value would change now, of course because it doesn't we can see that the RBRF sends an op-end General which in this case we're using as a secondary trip, which is being passed up to the XCPR So just in case the first one didn't work Hopefully this one will and he's going to go ahead and begin a second timer to wait and see if the position value updates When it does not after that timer expires that RBRF does his job and he sends out his op X dot general which is the signal that we configured earlier over in Helix And he's going to send that out to the other three XCPRs or breaker interface Logical nodes for the other three breakers in our substation and as they are not disabled and are not experiencing a failure They're going to in turn open their three associated breakers, which we see reflected here in this pause dot op Okay value changing to true Another place that we could look at this would be over in our custom display in our tool where we do see red which indicates That our failed breaker is still in fact closed. Where's the other three breakers in our substation have swapped to open So what we saw here is effectively a test that you might perform as part of commissioning of a real substation and Everything went smoothly but let's say something was to not work correctly and not actually operate as it is expected in a Traditional substation that might require a day of walking around the substation with a multimeter Flipping through wiring diagrams for all different cabinets opening up panels and checking Interconnect wiring in order to troubleshoot and trace down the problem but by moving these control signals over to communications in this case goose messages and then Documenting those communications in a machine readable way We could easily perform if necessary the troubleshooting steps to see where this broke down Just by following these lines and seeing where they stopped or where they fail to move forward And with that I hope you have a better understanding of not just what the 6250 engineering process entails as Described to us by Christophe, but also what some of the benefits are that it provides You know how those benefits translate into testing and simulation specifically and with that I'm going to turn it back over to you Beth We are running a little bit tight on time. So I'm going to save the questions and answers for the end I do see that the questions are coming in and folks are getting answers online So that's good. Keep sending in those questions And what we'll do is I want to make sure we get through all the material in the end We're happy to stay on we can typically stay on for a good 10-15 minutes as you submit questions and we read them live And answer them live So with that, let's just go ahead and hand it. I'll hand it back over to Christophe for his next portion of his presentation Okay, thank you Beth
 So as we mentioned at the beginning, we now will briefly give a preview of some of the improvements that we are currently working on in the standardization community to improve the specification process. This is mainly driven by utilities. Utilities that have some very distinct perspective for improved specification is basically the overall goal to be able in the future to completely submit specifications for projects they want to realize in a machine-processable way. So here we see a list of some of the ideas or desires that utilities want. And I will focus on the two highlighted ones, what we are currently doing with regards to those in the standard. So maximize testing possibilities at specification stage and reusability. There are two aspects we are currently working on. And here we see a view of the top-down engineering process as it is a vision of a utility. We already discussed a little bit before. We have seen we start with specification, define the ICD files from the IED tools to our system configuration. And then we configure the process. And at those stages, we can basically verify with simulation the behavior. But of course, in this stage, when we have the real devices, we can also do the system testing. As part of the maximizing testing at earlier stage, we are currently working on enhancements that allow us to do as well a simulation already of the specification. And as part of the improvement of the reusability, we are working on defining as well the templating. So before you really start the project, most utilities, they have standard designs that they have to apply. So from a utility perspective with SCL, in the future, we should also be able to support these standard designs. And one of that is the basic application profile where we can as well express that in a machine processable way using SCL and then simulate as well with the testing. So basically, for that, we talked before that once we have configured the devices and configured our GOOS messages, we have what we call an external reference to document the single flow between the devices. What we are adding currently to 6150 is that already at the specification stage, when we define our logical nodes that we require for our functionality, that already at that stage, we can implement or define what we call our source ref, which basically explains how the different logical nodes or functional elements should interact. This allows them to test the schema already at specification phase before we have configured any configuration, communication, and before we have allocated any IDs. A second aspect, and that goes into the reusability, is that we extend the process with some new variants of SCL files, which are mainly in the template area. So like a function specification description, basic application profile. So this is new template files that we can then import before we start the specification. So a specification tool will be able to import them. And as part of that, we also add the feature to describe functional behavior optionally using a standardized language like 61.1.31, which is function block structure text. So this allows us to describe and specify certain functional behaviors.
 certain behavior as well in a standardized way. And that's what now will be shown as the last demo by Jackson, how we can apply AXIS 1131 for behavior description and simulation then with the appropriate tools. Thank you, Christoph. So as Christoph mentioned, this section of the webinar is really all about highlighting some of the upcoming changes and additions to the IEC 6050 standard. As such, some of these new features are really still in the process of being integrated into our tools. But once those additions to the standard are finalized and tool support is finished, which we're targeting for maybe end of the summer, rest assured that we will have a follow-on webinar series where we really do a deep dive into what is made possible by these enhancements to the specification stage of the engineering process. But for now, I just want to give you a preview of what support for 61131 structured text and function block diagrams will look like. And with that, I'd like to introduce the 61131 module within DTM, which is what you see up on my screen now. So for those of you that might not be familiar, 61131, specifically dash 3, is a family of programming languages typically used by programmable controllers. It's commonly associated with PLCs. So if you're familiar with ladder logic, structured text, or function block diagrams, those all are defined as part of the 61131 family. And DTM 2.0 will include support for importing, editing, exporting, and most importantly, running or executing both structured text and function block diagrams. So aside from just providing users of DTM with another method to script behaviors and tests in addition to the JavaScript support that's already included, this will allow for functional behaviors, a part of the enhanced specification stage, to be imported and simulated directly here in the tool. So what does that really mean? Well, currently, when you want to build a simulation here in DTM, we handle the one portion of it for you completely, and that's the communication side of things. When it comes down to behavior, if you want it to be done out of the box, a couple, let's say, assumptions have to be made. That is no longer the case. If you can now define the functional behavior of your device in 61131, and we can now simulate that exact behavior, this allows for more accurate simulations, and that's with a lot less manual configuration. So with that, let's walk through what a function block diagram looks like. In this case, you'll see that we've associated function blocks with specific logical nodes. And if these don't look like the logical nodes that you're used to, this demo application that we put together is for a DER application, specifically battery systems. But there's no specific rules defined as to how you need to use these function blocks. Do they need to be associated with a logical node? Or can you just write your logic however you like, with ands and ors and counters, and the other building blocks that are part of the 61131 standard? But in this case, that is what we've done. So in some ways, it looks a little bit similar to what we saw over in TestSuite Pro with our signal flow diagram, with signals flowing from one logical node to another. But at this specification stage, we've not yet associated these logical nodes with IEDs. So these may end up being a goose message should these two logical nodes one day exist on different devices, but it's also possible that this will all be logic internal to a single device, which brings me to source refs. So source refs also allow you to do things like this, specify connections between two logical devices before you really get to the communication side of things. You don't need to know or care whether or not these logical nodes will exist.
 on different devices to specify a flow of data between them. Now, this is just a device where we can, I'm sorry, a function block where you can see inputs coming in and coming out. I do want to point out that we've implemented the behavior of this function block using structured text. And to do that, I'll come over here to my DPMC, and I'll just edit the structured text. And you can just get an idea of what structured text looks like. Maybe a little bit C-like, you can see I've defined my inputs and outputs up here at the top. And then I have some logic down here defining what will happen to my outputs based on my given inputs. And then I can instantiate this function block multiple times if necessary, or add it to a library and use it in other projects over here in my function block diagram. So with that, in summary, I just want to say that the upcoming additions to the 61850 standard are really going to enhance the specification stage. And specifically, the 61131 support combined with source refs are going to allow for more complete simulations at an even earlier stage in the engineering process, specifically the specification stage, than ever before. And with that, I'm going to turn it back over to you, Beth, where we will be doing question and answers. Thank you so much for sticking around. OK, thanks, Jackson. Yep, so we are just at time. So what I'd like to do real quick is just wrap up and go over some details. And then I will switch over to questions and answers. We've had a lot of good multi-questions from one person come in, so a lot of detailed answers going back out. Before we get back to that, I do want to just mention that the webinar will be posted to our website probably within the next couple of days. And again, right now, if you go to our home page, you will see part one there right front and center. So you can go look at that at any time. When you do log off of here, you will be prompted for a survey. It's only five questions, takes less than two minutes. Really appreciate if you fill it out and give us any feedback that you have on the webinar itself, any future ideas of what you'd like to see in webinars. So we really appreciate any feedback that you can provide. Also want to point out, you can sign up at any time for part three. The focus of that webinar is strategies for integration of DMP3 in IEC 621850. That will be hosted on May 24th at 10 AM. So again, you go to the website now and sign up for that and get it on your calendar. And of course, if you have any questions at any time, please feel free to email sales at trianglemicroworks.com. So with that, I would like to go over to the questions and answers for anyone that would like to stay on longer. And I think I'll just kind of take it from the bottom. We've had, like I said, quite a few lengthy questions come in. So the first one, and I believe this one will be for you, Krista. In our substations, we don't have complete SEL files. Is it possible that any tools in Triangle to get SEL files effortlessly? And then we make all the ICDs that we already collect into SEL files with Triangle software. Okay. Thanks, Beth, for the question. I think it really depends. When you say you don't have complete SEL file, it depends a little bit on what is missing. I mean, we certainly can import an SEL file, as an example, in the HALINX STS tool that we have demonstrated. And we probably would need to do some reverse engineering. As an example, if the substation section is completely missing, so the complete one-line diagram is missing, that would need to be reverse engineered. So there is some work involved in that. So it's not necessarily possible to do.
 that on a push button. But yeah, so it really depends a little bit on what does not complete STL file mean. Next question, I believe this will be for you as well. Christoph and certainly anyone, any of the engineers that have been answering the questions behind the scene, you may feel free to speak up as well. So I'll just read it. I have a question regarding the creation of the CID file. How would it be constructed? What elements of the IEC 62050 do you need to be included? OK, so the CID file, and there was a similar question related to that, that I probably also just answered. The question was, can we import the CID file in the process as well, other than just an IID and an ICD file? So the difference between an IID file and the CID file is an IID file just describes the instance of the device with all the data model, all the configured values of parameters, and the configured communication. The CID file is intended for the IID that we configure to have everything it needs. So besides the information that we have already in the IID file, which is everything related to the IID and the consideration, we need to add information like communication addressing. So there will be elements of the communication section in that file, because the device needs to know its IP addresses. But if the device is publishing or subscribing GOOS, so if it's publishing GOOS, it will also find in the communication section the communication parameters for the GOOS message it has to be publishing. And if it's subscribing GOOS, in addition to having the communication parameters of the GOOS subscription, it also needs to know the structure of the GOOS message. And for that, the CID file needs to have additional extracts, at least from the IIDs that are publishing the GOOS messages that it has to receive, because that's where it finds the data set and as such how the GOOS message is structured. I hope that clarified. There is a follow-up question to that, but I think we might need some additional information. So maybe you could just, I'll read the question, Christoph, and ask if you do need additional information to answer it. Is there any way to merge two CIDs if there is two collateral substations that need to speak with each other? So that was for me as well? Yes, it's a follow-up to the first question. He has two in a row. Yeah, I'm not really clear what is meant there. The CID file describes one IID. As I just discussed, it describes the IID and what it has to subscribe to. There is a process of getting information from two substations where the devices have to communicate with each other. There's a process defined in SCL through what we call an SCD, or system exchange description file. But that's basically exporting from one project the information related to an IID that I have to subscribe on the other side. And then the project for the other side would add this device into its project and be able then to subscribe as well to messages from there. So in some way, yes, that's merged, whether it's done on a CID basis or rather on an extract from an SCD file. That depends a little bit on the implementation. But yeah, in principle, it's possible through the tools to, I wouldn't really call it merging.
 but to include in the other part the source device which is from the other substation. Okay, thanks Christoph. I believe another one for you. How in Goose message communication is the failsafe maintained for communication failure with the subscriber IEDs? That's as well for me I guess. Yes, that's for you Christoph. Okay, so the Goose mechanism has, or the Goose messaging in 6150 has some mechanisms built in that help to deal with that. As you probably know, Goose is sent out as a multicast, so it's more or less sent out over the network. We don't get an acknowledgement of reception from the subscribers because that would anyway not be helpful from a timing perspective. But the Goose mechanism has on one side a repetition built in, so when there is an event happening and the new Goose message is sent out, that message will be repeated. So if you have a communication disturbance at a certain moment, the repeated message should get to the subscriber. On the other side, it's the responsibility of the subscriber to be sure that he gets the latest information. So with this repetition mechanism, we also have a built-in supervision mechanism, so the subscriber knows if the publisher is not publishing anymore, and as such he then needs to behave accordingly in a similar way, like he would not get a physical input, he needs to behave in a failsafe behavior. But he definitely can know when the source is not there anymore and does not publish, and then it's the responsibility, and it depends, of course, on the signal. When you expect a trip there, you just don't do anything. When you expect sampled values, you may need to block your protection function. So it's the subscriber, depending on the signal, that needs to behave in a failsafe way, but he definitely will know if the publisher is dead. Great. Thanks, Christophe. Jackson, I believe this one is for you. Can we simulate the IEC 62150 server and client using T&W software, and which software will be used for which application? Absolutely, yeah. So you can actually accomplish that with both DTM and TestSuite Pro, depending on what your goals are. If you want to simulate behavior of your device that's firmly a DTM application, so for that you would need some sort of SEL file, maybe an SCD or an ICD, and you could import that into the tool and use it to spin up a server and a client as well that would connect to it. And then using scripting, I mentioned in brief that JavaScript is supported. There's a variety of other ways to interact and control your simulated devices. If you simply want to bring up a simulated server and interact with it, let's say manually, by clicking and changing values in the advanced client, or I guess the equivalent, which would be the server simulation data model, then you can do that in TestSuite Pro as well. And if you want to reach out afterwards to our sales and support team, we can help you pick out which tool is going to be best for your specific need. Thanks, Jackson. Okay, next question. I believe this will be for you, Christoph. Different ICD template files can have different data type definitions. How do I make sure all the data type definitions from different ICD template files are unique in the SCD file? Well, in principle, that could be a question for Andy as well. That's the system tool that needs to handle that. So when the system tool imports...
 and a new ID can be an ICD file or an IID. It needs to be sure that the names of that is unique. If not, it needs to modify it. It can modify the names. But at the end, it's the tool that is combining them that has to make them unique. A good practice, however, would be that when does in the data type templates fill in as well the ID type, because that's an optional field, and that would make it easier to keep it unique. But it's the tool that combines different IDs that is responsible to keep them unique. And a major add to that, so Helix SDS will distinguish those data type definitions for you as you import them, and ensures that they're always consistent and unique by appending a unique identifier to it. And then when you export an ICD, you can be sure that there's no weird type collisions going on. And I don't know how other tools do it. It's just SDS doesn't like that. Great, thanks, Andy. Thanks, Christoph. I have another question. I'll just throw it out to whoever the best person is to answer this. So if I understand correctly, some LNs may have behaviors dependent on the manufacturers. Yes, so I think one way to think about it would be an LN is a collection of data attributes using 6150 term, variables, points, whatever you want to think of them as associated with a specific function. So we talked about that breaker failure function earlier. So how a breaker failure function works from manufacturer to manufacturer probably isn't that different, but it could be. But the assumption is that they want to use a similar set of data attributes between them. And they can customize that a little bit in the data template section. There's some required and some not. But as you get into, let's say, protection algorithms specifically, that really could vary from manufacturer to manufacturer. They might have a different algorithm for a differential protection if you go to SEL versus if you go to ABB. Great, thanks, Jackson. So I think that wraps up our Q&A session. There's a couple other questions that have come in that we will review. And in the follow-up, I will send out the full question and answer document. So you will get access to that as well. I appreciate everyone's time and staying on with us a little bit past the hour as well. And we hope to see you at our next webinar on May 24 for part three. Thanks, everyone. Thanks. Bye-bye.



**********Triangle Microworks Website:*************

URL: https://www.trianglemicroworks.com
                                                                                                     0 1 2 3 4 5 6 7                     OUR PRODUCTS SUPPORT THE FOLLOWING INDUSTRY STANDARD COMMUNICATION PROTOCOLS:     DNP3 | IEC 61850 | Modbus | GOOSE | IEC 60870-5 (101, 102, 103, 104) | 9-2 (Sampled Values) | IEC 60870-6 (ICCP/TASE.2) | 90-5 (Synchrophasor)                     Software Libraries  Add support for industry standard Communication Protocols directly in your device. Available as:  • ANSI Standard C Source Code Libraries  • C++ (IEC 61850 and IEC 60870-6 only)  • .NET Communications Protocol Components  •  Learn More>          Testing & Simulation Tools  We provide a variety of test tools that can be used to test and validate devices and networks. We also provide tools to help configure your devices.  •  Learn More>                  SCADA Data Gateway  The SCADA Data Gateway can be used as a Data Concentrator or Data Translator or both at the same time. Check out the features of this advanced, yet affordable gateway.  •  Learn More>              Training Videos and Workshops   Check out our extensive training video library for communication protocol training, product overviews and demos. Keep up with the latest training classes being offered by industry experts  •  Learn More>                 SIGN UP to receive updates about   SCADA communication protocols,  trainings and the latest product releases                      Missed our last webinar? Watch NOW!  Securing Grid Communications:  Unleashing IEC 61850 Routable GOOSE & Sampled Values               UPCOMING EVENTS      PacWorld Europe   Athens | Dates: 17 – 21 Jun, 2024        IEC 61850 Interoperability Testing Event (IOP)   Birmingham | Dates: 15 – 20 Sep, 2024                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/Login
                                                       Triangle MicroWorks Customer Portal  Please register by entering your email address below or log in to do the following:    Access your products  View licenses and maintenance status  Download software evaluations  If your working on a virtual machine, please email support@trianglemicroworks.com for an evaluation key         Login | Register        Email Address  Please enter a valid email address  Please enter your Username.       If you are registering, we recommend using a work email address so that your order history and maintenance information can be linked to your account.  Your contact information will not be sold or used outside of our organization as explained in our privacy policy .          Next                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                              The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products
                                               Protocol Libraries  We provide ANSI 'C' source code libraries for IEC 61850, DNP3, IEC 60870, and Modbus that can easily be ported to your device.            Testing and Configuration Tools  We provide a variety of test tools that can be used to test and validate devices and networks. We also provide tools to help configure your devices.            Gateways  The SCADA Data Gateway can be used as a data concentrator or a data translator or both at the same time. Check out the features of this advanced, yet affordable Gateway.                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries
                                             Source Code Libraries  Our industry standard  IEC 61850,  IEC 60870-6 (TASE.2/ICCP),  DNP3, Modbus, & IEC 60870-5 Source Code Library stacks are used by vendors for cost-effective implementation directly in the target hardware.    IEC 61850     DNP3     Modbus     IEC 60870-5     IEC 60870-6   (TASE.2/ICCP)                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools
                                             Tools  Triangle MicroWorks provides a full range of software based testing tools to assist in the development of devices which incorporate the protocols supported by our source code libraries.    61850 System Toolkit    The 3 tools below may be purchased individually or as a complete toolkit that supports the entire IEC 61850 Engineering Process lifecycle including Design, FAT, Commissioning, & Maintenance phases.     Communication Protocol Test Harness   The Test Harness is a powerful application for testing devices which use DNP3, IEC 60870-5, or Modbus.     SCL Navigator   Visualize, verify, repair, or create IEC 61850 System Configuration Language files     Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool   This tool allows for the creation of clients and servers for TASE.2/ICCP.      61850 Test Suite Pro   Diagnostic tool used to monitor, troubleshoot, and confirm the behavior of a substation.     Online SCL Verify   A free online tool to check your SCL files, including the data model, control blocks, and dataset definitions.     Distributed Test Manager   Simulate communications for all or a portion of the IEDs in a substation and create automated tests to confirm individual device behavior.      DNP3 Forge   The DNP3 Forge is a tool that creates and modifies DNP3 XML Device Profiles.     Garibaldi   Configure and Manage IEC 61850 Security.     Helinks   IEC 61850 Configuration Tool                                                           Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway                               Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)                   The SCADA Data Gateway (SDG) is used by System Integrators and Utilities to collect data and translate it to other protocols. For example, the SDG can collect data via OPC (UA & Classic), IEC 60870-6 (TASE.2/ICCP), IEC 61850, IEC 60870-5, DNP3, or Modbus Server/Slave devices and then can supply this data to other control systems supporting OPC (UA & Classic), IEC 60870-6 (TASE.2/ICCP) Client, IEC 60870-5, DNP3, and/or Modbus Client/Master communication protocols.  Triangle MicroWorks' SCADA Data Gateway has been used globally in more than 70 countries for over 20 years. The latest release v5.1 is built on our extremely reliable platform which includes support for both Windows and Linux with many new features to increase ease of configuration and flexibility. This version of the SCADA Data Gateway has been redesigned to incorporate a web-based user interface and now provides for user management and user roles. The user interface has a familiar look and feel from previous versions so the learning curve is minimal, but the browser based interface allows for remote configuration and monitoring of gateway operations.  Check out the new features available in v5+. You can also view the System Requirements to see which Windows and Linux platforms are supported.            Overview   Performs the work of an OPC Server (UA & Classic) or Protocol Translator.  Translate between any number of available protocols.  Allows translation between data types and control methods from different protocols. Supports mapping of points between Master and Slave, two Master, and/or two Slave protocol components.  Configure up to 200,000 server points for large scale projects.  Supports Secure Authentication for DNP3, IEC 60870-5, and IEC 61850  Supports Report by Exception (RBE), which transmit only data changes, saving communication channel bandwidth.  Supports selective logging of event data into a time-stamped Sequence of Events (SOE) log file.  Built in equation editor supports the creation of new data points based on raw data points and /or other equation points.  Option to use a 90 day license of Distributed Test Manager to verify mapping using Test Action Manager. This allows the real gateway to be tested within a simulated test system, which can automatically run through all data points and document the results of the protocol mapping.              Key Features     Web-based Configuration Interface  User Access Management with Role Based Access Control (RBAC)  Audit Logging to track users' actions  Workspace Support for Different Sets of Configuration Files  Protocol/System Logging with Filters  Drag and drop to map multiple data points in one action  Special Views to see performance metrics and system health  Search and Filter Point List from Web Interface         Web-based Configuration  (Click image for larger view)       New Customer?  Watch a 5 minute Quick Start Video and Download an Evaluation Now.       Existing Customer?  Watch a 5 minute video on how to migrate to SCADA Data Gateway v5.1+.        Ready to get started?      Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/downloads
                                               Downloads Thank you for you interest in our products. Triangle MicroWorks provides product downloads for a full 21-day evaluation or upgrade to the latest version of the following products.   Note:  Virtual Machines require a trial product key which can be requested from our support page. For physical PCs, a trial license is automatically created during installation.         61850 Test Suite Pro       61850 Test Suite Pro is a Windows application with multiple tools for testing IEDs in the lab, performing troubleshooting, and assisting with the commissioning process. Tools include a test client, IED simulator, SCL tester, GOOSE sniffer, GOOSE subscriber, Report viewer, data miner, and data visualizer.                61850 Test Suite Evaluation Download      61850 Test Suite Overview        Distributed Test Manager (DTM)               Distributed Test Manager Evaluation Download      Distributed Test Manager Overview       The Distributed Test Manager (DTM) is a Windows™ application which helps to test distributed systems by simulating the communications and data models of each device in the system. DTM complements other Triangle MicroWorks testing tools which are designed for testing the communications of a single device. DTM enables system level testing by providing a distributed environment which organizes and manages many Clients/Servers and Publishers/Subscribers through a single interface.           SCL Navigator    SCL Navigator is a Windows application for editing, viewing, and checking SCL files and creating ICD template files for IEDs.                SCL Navigator Evaluation Download      SCL Navigator Overview           Communication Protocol Test Harness                  Test Harness Evaluation Download      Test Harness Overview       The Communication Protocol Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.  The Test Harness can s imulate Master or Outstation Devices, monitor communications with a built-in protocol analyzer, view device data, and perform conformance tests.        SCADA Data Gateway       The SCADA Data Gateway (SDG) is an application used by System Integrators and Utilities to act as a data gateway and protocol translator.  The SCADA Data Gateway can collect data from OPC, OPC UA, IEC 60870-6 (TASE.2/ICCP), IEC 61850, IEC 60870-5, DNP3, or Modbus Server/Slave devices and then supply this data to other control systems supporting OPC, OPC UA, IEC 60870-6 (TASE.2/ICCP) Client, IEC 60870-5, DNP3, and/or Modbus Client/Master communication protocols.                SCADA Data Gateway - Windows Support Evaluation Download      SCADA Data Gateway Overview              SCADA Data Gateway - Linux Support Evaluation Download      SCADA Data Gateway v5 Overview        DNP3 Forge                  DNP3 Forge Evaluation Download      DNP3 Forge Overview       DNP3 Forge supports the creation and validation of DNP3 XML Device Profile documents. Including script based and built in validation, IEC 61850 Mapping, Import/Export of CSV (Comma Separated Value) files.      Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool       Iron is a Windows™ applications that supports testing of  IEC 60870-6 (TASE.2/ICCP) Clients and Servers.   Iron can create compliant Clients and Servers for testing TASE.2/ICCP devices,  including bi-directional Peer-to-Peer applications.  InSight functionality is built-in to create automated script-driven tests, custom graphical displays, and simulation design/debug via a flowchart GUI.                Iron TASE.2/ICCP Test Tool Evaluation Download      Iron - TASE.2/ICCP Test Tool        IEC 61850 Library Evaluation Kit   ANSI-C Library for IEC 61850 Server and GOOSE                IEC 61850 ANSI-C Library Evaluation Kit           Evaluation Kit for the IEC 61850 Server and GOOSE ANSI-C source code library.  The Eval Kit includes:   - Microsoft Visual Studio Project - DLL for IEC 61850 Server and GOOSE source code library - Example demo application - Documentation for installing and using the example          IEC 61850 .NET Library Evaluation   .NET Components for IEC 61850 Client, Server, GOOSE, SV     .NET Components provide a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, and Sampled Values.  This evaluation package includes:   - .NET Components for IEC 61850 Source Code Library (including GOOSE and SV) - Documentation including API and user manual - Supports all .NET Languages (C#, J#, Managed C++, VB .NET, etc.) and tools - Supports .NET 4.x framework, .Netcore3.1 and .Net5                 IEC 61850 Client, Server, GOOSE, and Sampled Values      Evaluation Version for .NET Components           .NET Library Evaluation   DNP3 Modbus IEC 60870-5 (-101, -102, -103, -104)                 DNP3, Modbus, and IEC 60870-5 (-101,-102,-103,-104)      Evaluation Version for .NET Components       .NET Components provide a quick and cost effective way to implement a DNP3, Modbus, or IEC 60870-5 Master or Outstation device.  This evaluation package includes:   - .NET Components for DNP3 Source Code Library - .NET Components for Modbus Source Code Library - .NET Components for IEC 60870-5 (101, 102, 103, 104) Source Code Library - Documentation including API and user manual - Supports all .NET Languages (C#, J#, Managed C++, VB .NET, etc.) and tools - Compatible with .NET 4.0 and later Frameworks       IEC 60870-6 (TASE.2/ICCP) .NET Library Evaluation   .NET Components for IEC 60870-6 (TASE.2/ICCP) Client & Server        .NET Components provide a quick and cost effective way to implement an IEC 60870-6 (TASE.2/ICCP) Client & Server.  This evaluation package includes:   - .NET Components for IEC 60870-6 (TASE.2/ICCP) Source Code Library - Documentation including API and user manual - Supports all .NET Languages (C#, J#, Managed C++, VB .NET, etc.) and tools - Compatible with .NET 4.0 and later Frameworks                 IEC 60870-6 (TASE.2/ICCP) Client & Server      Evaluation Version for .NET Components                                                                  Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/video
                                             Video Playlists  We have organized our web based video training as playlists of related topics. Click on one of the boxes below to see the playlists for that topic.      Product Overview   These videos demonstrate features available in Triangle MicroWorks products.     Communication Protocol Training   This set of videos provides general training on protocols supported by Triangle MicroWorks, including DNP3 and IEC 61850.                                                             Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/video/product-features
                                                   Product Overview Playlists  Product Training  Product Demos  Protocol Training       Product Overview Playlists    Product Training    Product Demos    Protocol Training        Product Overview Playlists  This page contains video playlists to introduce our products. Playlists for specific products typically begin with general information and an overview of features, followed by product training videos, and finishing up with video demos and use cases. Playlists on this page are configured to start with the general information section; however you may navigate to any video by clicking the "Hamburger" icon in the upper right of the video and selecting the desired video.  Once the video is playing, you can select the playback quality by clicking the gear icon in the lower right.  On desktop browsers, you can view the videos in full screen mode by clicking the icon in the lower right. Press escape to exit full screen mode.  Note that YouTube may revert back to original settings for full screen and playback quality for each video in the playlist. If that happens, just re-select your preferences.     Triangle MicroWorks Product Overview    Key videos:   Introduction and Overview  Add Industry Standard Communication Protocol Support to Your Device  Integrate Industry Standard Communication Protocols into a Project  Test, Troubleshoot, and Configure Devices  Learn About Industry Standard Communication Protocols  Conclusion               Triangle MicroWorks Products in the IEC 61850 Engineering Process    Topics include:   Introduction and Overview  Concept & Design phases  Factory Acceptance Test (FAT) phase  Post-deployment phase  Conclusion                  61850 Test Suite Pro  Diagnostic tool used to monitor, troubleshoot, and confirm the behavior of a substation.        Key videos:   Signal Flow Diagram & Logic Analyzer  Introduction to Test Suite Pro  Use in the 61850 Engineering Process  Sample Servers  Data Miner, Logic Analyzer & Custom Displays  Workspaces              Distributed Test Manager (DTM)  Simulate communications for all or a portion of the IEDs in a substation and create automated tests to confirm individual device behavior.     Key videos:    Substation simulation, point mapping, and Test Manager  Use in the 61850 Engineering Process  IOP 2017 Demo  Automated testing of protocol mapping  Data changes  Creating and using Workspaces  Introduction to DNP3 in DTM  DistribuTech 2015 Demo Intro  Demo: ICCP control operations  Demo: Breaker failure simulation  DNP3 SAv5 & User Management              SCL Navigator  Visualize, verify, repair, or create IEC 61850 System Configuration Language files     Topics Include:     SCL Navigator's role in the IEC 61850 Engineering Process              Source Code Libraries    Product Features   Reduce Development Cost  Free internal resources to work on proprietary aspects of your company's products  Proven compatibility reduces software development risks              Communication Protocol Test Harness (TH)    Product Features:   Simulate Master and/or Outstation Devices  Monitor Communications  Perform Custom Functional Tests  Perform Protocol Conformance Tests              SCADA Data Gateway (SDG)    Product Features:   OPC Driver  Protocol Translator  Data Concentrator              DNP3 Forge    Product Features:   Create DNP3 XML Device Profiles  Validate DN3 XML Device Profiles  Configure/Manage IEC 61850 to DNP3 Mapping                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/video/product-training
                                                   Free Webinar  Previous Webinars  |---Introducing SDG v5.1  |---Easy Button for IEC 61850 Security  |---IEC 61850 Tools and Techniques  |---Strategies for Upgrading to IEC 61850 Edition 2.1  |---DER Solutions  |---UCA International User Group (UCAIug) Supporting Users  |---What is a Data Gateway in the SCADA world?  |---A Holistic Approach to Industrial Cybersecurity  |---Debugging GOOSE Streamline Troubleshooting & System Validation  |---Simplifying Secure Routable GOOSE & Sampled Values  |---A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  |---A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  |---Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  |---Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  |---Charting A Course to a Digital Substation  |---Automated Testing  |---IEC 61850 Substation Design & Testing  |---SCADA Data Gateway Tips & Techniques  |---IEC 61850 Common Vision  |---Virtual Isolation & Testing with IEC 61850  |---Virtual Isolation  |---Troubleshooting & Conformance Testing with DNP3  |---Securing the Grid  Product Training  Protocol Training       Free Webinar    Previous Webinars    Product Training    Protocol Training        Product Training ("How To") Video Playlists  This page contains video playlists to provide training on our products. Playlists for specific products typically begin with general information and an overview of features, followed by product training videos, and finishing up with video demos and use cases. Playlists on this page are configured to start with the training section; however you may navigate to any video by clicking the "Hamburger" icon in the upper right of the video and selecting the desired video.  Once the video is playing, you can select the playback quality by clicking the gear icon in the lower right.  On desktop browsers, you can view the videos in full screen mode by clicking the icon in the lower right. Press escape to exit full screen mode.  Note that YouTube may revert back to original settings for full screen and playback quality for each video in the playlist. If that happens, just re-select your preferences.     Application Licensing "How To" Videos    Topics include:   Introduction  Online Licensing  Offline Licensing  Transfer License  License Upgrade Options  Simple Upgrade               61850 Test Suite Pro "How To" Videos  Diagnostic tool used to monitor, troubleshoot, and confirm the behavior of a substation.   Key videos:   Getting started with example servers  Data Miner, Logic Analyzer, and Custom Displays  Using workspaces               Distributed Test Manager "How To" Videos  Simulate communications for all or a portion of the IEDs in a substation and create automated tests to confirm individual device behavior.   Key videos:   Install, Configure, and License  Workspaces  Device Creation  Data Changes  Displaying Data               SCADA Data Gateway "How To" Videos    Key videos:   Configuring Master Components  Configuring OPC DA Client  Configuring Outstation Components  Mappping Data  Protocol Analyzer  Create IEC 60870-5-101/104 SDOs with Time Stamps  Constructing Single-line Diagrams using Gateway's InSight  Linking OPC in Gateway's InSight  Testing Single-line Diagrams  Opening Display with Gateway's InSight Viewer               Communication Protocol Test Harness "How To" Videos    Key videos:   Custom Functional Tests  Monitor Communications  Conformance Testing  Data Window Usage  Command Window Configuration  Command Window Sequencing  Front Panel Simulators               IEC 60870-6 Iron "How To" Videos    Topics include:   IEC 60870-6/TASE.2/ICCP Control Operations                         SCL Navigator "How To" Videos  Visualize, verify, repair, or create IEC 61850 System Configuration Language files   Topics include:   SCL Navigator's role in the IEC 61850 Engineering Process               Source Code Library "How To" Videos    Key videos:   Source Code Library and .NET Protocol Component Features  Demonstrations utilizing Source Code Library and/or .NET Protocol Components               Helinks "How To" Videos    Key videos:   Substation Structure  Configuration Videos  Validation  Exporting Configuration  Documentation               Legacy Products The following products are no longer sold. These videos are included to support customers who purchased these products and have decided not to upgrade to newer versions.      Anvil  NOTE: Anvil has been replaced by Test Suite Pro . These videos are provided to support customers who purchased Anvil and have not upgraded to Test Suite Pro.  Key videos include:   Manual data changes  Automatic data changes  Table-driven data changes  Flat View  RCB Options override  Force Read/Write Error  Force Control Response                     Hammer  NOTE: Hammer has been replaced by Test Suite Pro . These videos are provided to support customers who purchased Hammer and have not upgraded to Test Suite Pro.  Key videos include:   Flat data view  Hierarchical data view  Data monitor view  Reports viewer  GOOSE Control Blocks  Log Control Blocks  Managing Data Sets                                                                          Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/video/product-demonstration-playlists
                                                   Product Overview Playlists  Product Training  Product Demos  Protocol Training       Product Overview Playlists    Product Training    Product Demos    Protocol Training        Product Demonstration Videos This page contains video playlists to introduce our products. Playlists for specific products typically begin with general information and an overview of features, followed by product training videos, and finishing up with video demos and use cases. Playlists on this page are configured to start with the general information section; however you may navigate to any video by clicking the "Hamburger" icon in the upper right of the video and selecting the desired video.  Once the video is playing, you can select the playback quality by clicking the gear icon in the lower right.  On desktop browsers, you can view the videos in full screen mode by clicking the icon in the lower right. Press escape to exit full screen mode.  Note that YouTube may revert back to original settings for full screen and playback quality for each video in the playlist. If that happens, just re-select your preferences.     Automated Testing of Protocol Mapping for SCADA Gateways and RTUs  This video demonstrates how to test the mapping of a protocol gateway or RTU in an automated test environment.  The mapping of the protocol gateway is tested by simulating the IEC 61850 IEDs and a DNP3 SCADA Master that connect on either side of the gateway.  The simulation is performed by the Distributed Test Manager (DTM) which tests and documents the mapping of the gateway (in this case the SCADA Data Gateway from Triangle MicroWorks).  The demo illustrates how the Distributed Test Manager (DTM) can be used to test partial systems without needing all components in place and configured.  It also shows that this type of testing can be performed without expensive test equipment to generate signal changes.  This same type of test can be performed for standard SCADA protocols like ICCP/TASE.2, IEC 60870-5 (IEC 104), DNP3, IEC 61850, and Modbus.   Click here to download a copy of the handout.    Topics include:   How to determine and document mapping of a protocol translator/concentrator  Testing partial systems  Reducing need for expensive test equipment   Shown at:   DistribuTECH, February 2016               Breaker Failure and Secure Data Translation  This demonstration is based on the demonstration shown by the UCAIug at the CIGRE 2014 conference. In that demo, vendor equipment was configured by an SCD file to simulate a breaker failure scheme. The Triangle MicroWorks Distributed Test Manager simulates six of these devices.  For the breaker failure portion of the demo, a breaker trip is initiated remotely via a GOOSE message. The breaker attempts to trip and signals a breaker failure. It then broadcasts a GOOSE breaker failure message. Upon receipt of this message, each of the other devices will trip their breaker.  That demo has been enhanced to also show secure substation monitoring and control. The SCADA Data Gateway has been added to translate the IEC 61850 data from the devices to an IEC 60870-6 (ICCP/TASE.2) Client (simulated by Iron) and a DNP3 Master (Simulated by the Communication Protocol Test Harness). All communications in this demo implement security as defined in the corresponding protocol specification.  (Note that in this demo, one of the IEC 61850 devices is simulated by Anvil. Anvil has since been replaced by the IEC 61850 Test Suite Pro.)  Click here to download a copy of the handout.    Topics include:   Demo Overview  Demo Components  Control Commands via ICCP (updated April 16, 2015)  Breaker Failure Simulation  Point Mapping Verification  LGOS  Data Simulation (updated August 11, 2015)  DNP3 User and Key Management  Managing DNP3 XML Device Profiles   Shown at:   DistribuTECH, February 2015        Click here to download a copy of the handout.         DNP3 Secure Authentication and Key Management  This video demonstrates User and User Role Management using DNP3 SAv5 and a DNP3 Authority. It uses the Triangle MicroWorks Distributed Test Manager (DTM) to simulate the DNP3 Master and Outstation, and a DNP3 Authority from Triangle MicroWorks.    Topics include:   Distributed Test Manager (DTM) simulating DNP3 SAv5 devices  Example DNP3 Authority  Demonstration of User Roles  Changing User Roles  Removing Users   Shown at:   EPRI, November 2014               Breaker Failure Simulation Using 61850 Source Code Library on ARM Processor  This demo uses a small ARM processor (the Texas Instruments BeagleBone) running the Triangle MicroWorks 61850 Source Code Library to simulate a breaker failure and the subsequent processing of that failure.  In this demo:  A breaker failure is simulated  The failure is indicated via GOOSE messages  The GOOSE messages are received and processed by a second breaker  The upstream breaker responds to the GOOSE messages and opens     Topics include:   Simulated Breaker Failure  GOOSE Messages  Response to Breaker Failure  Triangle MicroWorks 61850 Source Code Library on ARM Processor (TI BeagleBone)   Shown at:   PACWorld Americas 2014  IEC 61850 Conference 2014                 Simulating IEC 61850 Breakers and Interfacing with DNP3 SAv5  This demo demonstrates how you can use Triangle MicroWorks products to simulate IEC 61850 Clients and Servers and how to generate and use simulation files. While the simulation files can be created manually, the IEC 61850 Client simulator makes it extremely easy to capture states from real-world devices to create these simulation files.  We also show some products that use the Triangle MicroWorks Source Code Libraries and .NET Protocol Components.  In addition, we also demonstrate mapping IEC 61850 to other protocols such as DNP3 using the SCADA Data Gateway, and DNP3 Secure Authentication (DNP3 SA) using the Communication Protocol Test Harness.  Finally, we use InSight™ to create a graphical display.  Note that this demo refers to Hammer and Anvil; these products have been replaced and enhanced by the Advanced Client and IED Simulator components of 61850 Test Suite Pro.   Topics include:   Overview  Using Hammer to Capture States  Using Anvil to Simulate Servers  SCADA Data Gateway, Test Harness, and Conclusion   Shown at:   DistribuTECH 2013               SCADA Data Gateway and Communication Protocol Test Harness  This page provides a demonstration of the SCADA Data Gateway and the Communication Protocol Test Harness.  The SCADA Data Gateway is configured as both an OPC Driver and a Data Concentrator.  The Communication Protocol Test Harness is configured to simulate multiple Outstation devices, which report data to the SCADA Data Gateway. These data are mapped within the SCADA Data Gateway to be reported via OPC and by a DNP3 Outstation component.  The Test Harness is also configured as a DNP3 Master to read data from the SCADA Data Gateway's outstation component.   Topics include:   Introduction and Overview  Communication Protocol Test Harness  SCADA Data Gateway  Conclusion   Shown at:   DistribuTECH 2007  DistribuTECH 2008  IEEE PES T&D Conference 2008               Communication Protocol Test Harness  This demo provides an overview of some of the features of the Communication Protocol Test Harness.  In this demo, the Communication Protocol Test Harness is used to simulate both a DNP3 Master and a DNP3 Outstation. The DNP3 Master reads data via Report By Exception (RBE) data polls, and sends commands via DNP3 Control Relay Output Block (CROB) commands. The Outstation sends the requested data and responds to the requested commands.  This demo also shows how to easily configure custom Functional Tests using the Communication Protocol Test Harness.  In addition, it shows how to easily conduct the DNP3 Conformance Tests using an option module of the Communication Protocol Test Harness.   Topics include:   Introduction  RBE Polling  CROB  Functional Tests  DNP3 Conformance Tests  Conclusion   > Shown at:   DistribuTECH 2007  DistribuTECH 2008  IEEE PES T&D Conference 2008                                                                     Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/video/protocol-training
                                                   Free Webinar  Previous Webinars  |---Introducing SDG v5.1  |---Easy Button for IEC 61850 Security  |---IEC 61850 Tools and Techniques  |---Strategies for Upgrading to IEC 61850 Edition 2.1  |---DER Solutions  |---UCA International User Group (UCAIug) Supporting Users  |---What is a Data Gateway in the SCADA world?  |---A Holistic Approach to Industrial Cybersecurity  |---Debugging GOOSE Streamline Troubleshooting & System Validation  |---Simplifying Secure Routable GOOSE & Sampled Values  |---A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  |---A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  |---Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  |---Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  |---Charting A Course to a Digital Substation  |---Automated Testing  |---IEC 61850 Substation Design & Testing  |---SCADA Data Gateway Tips & Techniques  |---IEC 61850 Common Vision  |---Virtual Isolation & Testing with IEC 61850  |---Virtual Isolation  |---Troubleshooting & Conformance Testing with DNP3  |---Securing the Grid  Product Training  Protocol Training       Free Webinar    Previous Webinars    Product Training    Protocol Training        Communication Protocol Training Videos  This page contains video playlists containing protocol training videos. Playlists on this page are configured to start with the first training video; however you may navigate to any video by clicking the "Hamburger" icon in the upper right of the video and selecting the desired video.  Once the video is playing, you can select the playback quality by clicking the gear icon in the lower right.  On desktop browsers, you can view the videos in full screen mode by clicking the icon in the lower right. Press escape to exit full screen mode.  Note that YouTube may revert back to original settings for full screen and playback quality for each video in the playlist. If that happens, just re-select your preferences.     Introduction to IEC 61850    Key Concepts:   Architecture  Data and Controls  Advanced Models  Reporting and Logging  GOOSE and Sampled Values  Data Model  SCL File               IEC 61850  Data Modeling    Key Concepts:   Part 1: Introduction  Part 2: Elements of the 61850 Data Model               IEC 60870-5 Training    Key Videos:   Introduction  ASDU Structure  Application Layer Functions  Link, Transport, and Physical Layers  Security               Introduction to DNP3    Key Videos:   Introduction and Overview  DNP3 Features  DNP3 Protocol Layers Overview  Data Link Layer  Transport Function  Application Layer  Conclusion               DNP3 Training Videos    Key Videos:   Introduction  Purpose of a SCADA Communications Protocol  DNP3 Overview  DNP3 Messages  Application Layer  Transport Function  Data Link Layer  Benefits of Industry Standard Protocols               DNP3 XML Device Profile    Key Videos:   Introduction and Overview  What is a DNP3 Device Profile  DNP3 XML Device Profile Use Cases>/li>  Mapping to IEC 61850 Object Models  Summary               DNP3 Secure Authentication Training    Key Videos:   Introduction  Introductory Level  Intermediate Level  Expert Level  Credits:  Lead technical content creator:  Grant Gilchrist, Enernex  DNP3 Technical Committee Reviewers:  Eric Thibodeau, Gentec Steve McCoy, Triangle MicroWorks Joel Greene, Triangle MicroWorks                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training
                                               Troubleshooting & Conformance Testing with DNP3  Beginner and Advanced Topics   Free Webinar - Watch Now!  Previously held April 30th, 2024    Since its introduction in 1993, Distributed Network Protocol (DNP3) has become a powerful standardized communication protocol that provides significant advantages for SCADA operations to communicate with remote devices over a large distance. DNP3 has achieved widespread acceptance and has been implemented across many industry applications, particularly electrical and water utilities. As a result, there is an ongoing need to monitor and resolve DNP3 communications issues in the field effectively and quickly.  Problem:  For device vendors, SCADA communication testing is difficult and time consuming.  Interoperability problems in the field cause serious support issues. For Utilities, connection problems and configuration issues slow down device integration times and create downtime.  Solution:  Having the right tools can simplify your testing & troubleshooting while improving efficiency & productivity by allowing you to prevent many issues that would typically occur in the field. Communication Protocol Test Harness is a powerful tool for monitoring and troubleshooting all your DNP3 testing and activities. This webinar is designed to review the common use cases when working with DNP3 ranging from troubleshooting connection vs protocol issues to preparing for conformance testing.    What you’ll learn:   How to troubleshoot & resolve common connection problems  Identify & resolve configuration issues using Test Harness and Wireshark    How to troubleshoot & resolve common protocol issues  Learn to monitor communications with Test Harness or Wireshark  Learn how to connect and configure a compliant Master or Outstation    How to generate messaging traffic for Event & Unsolicited configurations  How to prepare for conformance testing with the official Conformance Test Procedures published by the DNP Technical Committee  Watch how Tcl/Tog scripting and C# plugins can be used to develop full custom tests               Instructors:   Matt Green  Senior Applications Engineer Triangle MicroWorks, Inc.    Matt is a senior applications engineer here at Traingle MicroWorks. He’s been with Triangle for about 6 years and specializes QA testing with all our simulation tools and gateway. Matt is also one of our lead product trainers and presenter for webinars and customer demos.  Who should attend this course:    Protection & Control Engineers/ Managers  System Integrators  Software Engineers/Managers        Check out and view recordings of our previous webinars:      Free Webinar  Previous Webinars  |---Introducing SDG v5.1  |---Easy Button for IEC 61850 Security  |---IEC 61850 Tools and Techniques  |---Strategies for Upgrading to IEC 61850 Edition 2.1  |---DER Solutions  |---UCA International User Group (UCAIug) Supporting Users  |---What is a Data Gateway in the SCADA world?  |---A Holistic Approach to Industrial Cybersecurity  |---Debugging GOOSE Streamline Troubleshooting & System Validation  |---Simplifying Secure Routable GOOSE & Sampled Values  |---A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  |---A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  |---Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  |---Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  |---Charting A Course to a Digital Substation  |---Automated Testing  |---IEC 61850 Substation Design & Testing  |---SCADA Data Gateway Tips & Techniques  |---IEC 61850 Common Vision  |---Virtual Isolation & Testing with IEC 61850  |---Virtual Isolation  |---Troubleshooting & Conformance Testing with DNP3  |---Securing the Grid  Product Training  Protocol Training       Free Webinar    Previous Webinars    Product Training    Protocol Training                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars
                                  Previous Webinars  Our previous webinars are listed below. Click on the webinar for more detailed information and to view a replay.      Securing Grid Communications: Unleashing IEC 61850 Routable GOOSE & Sampled Values       Establishing a vendor independent specification process to support top-down engineering of IEC 61850 Systems                  Tips & Techniques: Gateway Configuration Made Easy!       IEC 61850 Substation Design Workshop Part 1        IEC 61850 Substation Design Workshop - Test Your Design - Part 2        Virtual Isolation & Testing with IEC 61850         Introducing SCADA Data Gateway v5.1       Charting A Course to a Digital Substation Part 1        Charting A Course to a Digital Substation Part 2        Charting A Course to a Digital Substation Part 3         Easy Button for IEC 61850 Security       Automate Testing of your SCADA Gateway, Data Concentrator or Protocol Converter       61850 Tools & Techniques        Strategies for Upgrading to IEC 61850 Edition 2.1        DER Solutions        Migration Strategies for using IEC 61850 Edition 2 Amendment 1 (Ed 2.1)       Troubleshooting & Conformance Testing with DNP3       What is a Data Gateway in the SCADA world?        Virtual Isolation & Testing with IEC 61850       A Holistic Approach to Industrial Cybersecurity       Debugging GOOSE          Simplifying Secure Routable GOOSE & Sampled Values         A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series—Part 2        A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series—Part 1              Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning                  Automate SCADA Testing of your Gateway, Data Concentrator or RTAC                                                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support
                                                         To obtain support, please select one of the following options:   Source Code Library  .NET Protocol Components  SCADA Data Gateway  Communication Protocol Test Harness  IEC 61850 Test Suite Pro  SCL Navigator  Distributed Test Manager   To obtain Sales information:   Request Product Information or Price Quotation                                                            Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/source-code-libraries
                                                        Product Features If you need help with specific features of the Source Code Library, please click here to see our Source Code Library Product Training Videos .             Support Request: Source Code Library To receive support for the Source Code Library, please complete and submit the form below. One of our representatives will contact you shortly.   If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.      Notice: Any information sent to TRIANGLE MICROWORKS ,  INC . will be used for internal use only!      First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Product Name:   Click to add    *Required          (?)                Product Version:   Click to add    (e.g., v3.00.46)          (?)                ProtocolsListBoxField    IEC 60870-5-101 Master  IEC 60870-5-101 Slave  IEC 60870-5-102 Master  IEC 60870-5-102 Slave  IEC 60870-5-103 Master  IEC 60870-5-103 Slave  IEC 60870-5-104 Master  IEC 60870-5-104 Slave  DNP3 Master  DNP3 Slave  Modbus Master  Modbus Slave  IEC 61850 Server  IEC 61850 Client  GOOSE Publisher/Subscriber  Sampled Values   Please select a value for ProtocolsListBoxField.      Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach any additional information that will help us understand your problem:     label label   (log files, drawings, etc.)                                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                              The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/dotnet-protocol-components
                                                      Licensing  If you have questions or issues with product licensing, please click here to see our product licensing videos .     Product Features If you need help with specific features of the .NET Protocol Components, please click here to see our Source Code Library and .NET Protocol Components Product Training Videos .            Support Request: .NET Protocol Components  To receive support for the .NET Protocol Components, please complete and submit the form below. One of our representatives will contact you shortly.  If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Notice: Any information sent to TRIANGLE MICROWORKS, INC. will be used for internal use only!    First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Product Name:   Click to add            (?)                Product Version:   Click to add    (e.g., v3.00.46)          (?)                Protocol(s):    IEC 60870-5-101 Master  IEC 60870-5-101 Slave  IEC 60870-5-102 Master  IEC 60870-5-102 Slave  IEC 60870-5-103 Master  IEC 60870-5-103 Slave  IEC 60870-5-104 Master  IEC 60870-5-104 Slave  DNP3 Master  DNP3 Slave  Modbus Master  Modbus Slave   (select all that apply)  Please select a value for Protocol(s).      Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach any additional information that will help us understand your problem:     label label   (log files, drawings, etc.)                                                                          Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                              The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/test-harness
                                                        Licensing Click here for help with common installation issues - Troubleshooting Help.  If you have questions or issues with product licensing, please click here to see our product licensing videos .     Product Features   If you need help with specific product features or help understanding how to use the Communication Protocol Test Harness, please click here to see our Communication Protocol Test Harness Product Training Videos .            Support Request: Protocol Test Harness  To receive support for the Communication Protocol Test Harness, please complete and submit the form below. One of our representatives will contact you shortly.  If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Notice: Any information sent to TRIANGLE MICROWORKS, INC. will be used for internal use only!    First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Activation Key:   Click to add    (N/A for evaluation versions:          (?)                Product Version:   Click to add    (e.g., v4.00.03)          (?)                Protocol(s):    IEC 60870-5-101  IEC 60870-5-103  IEC 60870-5-104  DNP3  Modbus  Modbus Conformance Test Module  DNP3 Conformance Test Module   (select all that apply)  Please select a value for Protocol(s).      Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach your workspace file:     label label      Please attach a Protocol Analyzer log file showing the problem (XML Format preferred):     label label      Please attach any additional information that will help us understand your problem.     label label   (drawings, etc.)                                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                              The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/scada-data-gateway
                                                        Licensing  Click here for help with common installation issues - Troubleshooting Help.  If you have questions or issues with product licensing, please click here to see our product licensing videos .      Product Features If you need help with specific features or help understanding how to use the SCADA Data Gateway, please click here to see our SCADA Data Gateway Product Training Videos .             Support Request: SCADA Data Gateway To receive support for the SCADA Data Gateway, please complete and submit the form below. One of our representatives will contact you shortly.   If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.      Notice: Any information sent to TRIANGLE MICROWORKS, INC. will be used for internal use only!     First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Activation Key:   Click to add    (N/A for evaluation versions)          (?)                Product Version:   Click to add    (e.g., v3.01.0663)          (?)                Protocol(s):    IEC 60870-5-101 Master  IEC 60870-5-101 Slave  IEC 60870-5-102 Master  IEC 60870-5-102 Slave  IEC 60870-5-103 Master  IEC 60870-5-103 Slave  IEC 60870-5-104 Master  IEC 60870-5-104 Slave  DNP3 Master  DNP3 Slave  Modbus Master  Modbus Slave  IEC 61850 Server  IEC 61850 Client  GOOSE Publisher/Subscriber  Sampled Values   (Select all that apply)  Please select a value for Protocol(s).      Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach your Configuration (INI) file:     label label      Please attach your Point Map (CSV) file:     label label      Please attach a Protocol Analyzer log file showing the problem:     label label      Please attach any additional information that will help us understand your problem.     label label   (drawings, etc.)                                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                              The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/dtm
                                                    Licensing  If you have questions or issues with product licensing, please click here to see our product licensing videos .     Product Features If you need help with specific product features or help understanding how to use the Distributed Test Manager, please click here to see our Distributed Test Manager Product Training Videos .             Support Request: Distributed Test Manager (DTM)  To receive support for the Distributed Test Manager, please complete and submit the form below. One of our representatives will contact you shortly.  If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Notice: Any information sent to TRIANGLE MICROWORKS, INC. will be used for internal use only!    First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Product Key:   Click to add    (N/A for evaluation versions)          (?)                Product Version:   Click to add    (e.g., v4.00.03)          (?)                Protocol(s):    DNP3  IEC 61850  IEC 60870-5-101  IEC 60870-6 (TASE.2/ICCP)  Modbus  IEC 60870-5-102  IEC 60870-5-103  IEC 60870-5-104   (select all that apply)  Please select a value for Protocol(s).      Use Cases  Load Testing HMI Testing RTU/Gateway Testing Production Testing Substation Configuration Cyber Security Education/Research Other   Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach your workspace file:     label label      Please attach any additional information that will help us understand your problem.     label label   (system diagram, etc.)                                                                            Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                              The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/test-suite
                                                     Licensing Click here for help with common installation issues - Troubleshooting Help.  If you have questions or issues with product licensing, please click here to see our product licensing videos .     Product Features If you need help with specific features or help understanding how to use 61850 Test Suite Pro, please click here to see our 61850 Test Suite Pro Product Training Videos .     Product Help  Click here to access the 61850 Test Suite Pro Online Help            Support Request: 61850 Test Suite Pro  To receive support for the 61850 Test Suite Pro, please complete and submit the form below. One of our representatives will contact you shortly.  If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Notice: Any information sent to TRIANGLE MICROWORKS, INC. will be used for internal use only!    First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Activation Key:   Click to add    (N/A for evaluation versions)          (?)                Product Version:   Click to add    (e.g., v1.00.01)          (?)               Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach your IEC 61850 SCL file:     label label                                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/iron-support
                                                     Licensing Click here for help with common installation issues - Troubleshooting Help.  If you have questions or issues with product licensing, please click here to see our product licensing videos .      Product Features If you need help with specific features or help understanding how to use Iron, please click here to see our Iron Product Training Videos .             Support Request: Iron To receive support for Iron, please complete and submit the form below. One of our representatives will contact you shortly.   If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.      Notice: Any information sent to TRIANGLE MICROWORKS, INC. will be used for internal use only!     First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Activation Key:   Click to add    (N/A for evaluation versions)          (?)                Product Version:   Click to add    (e.g., v4.4.4)          (?)               Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach any additional information that will help us understand your problem.     label label   (drawings, etc.)                                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/scl-navigator
                                                 Licensing  If you have questions or issues with product licensing, please click here to see our product licensing videos .     Product Features If you need help with specific product features or help understanding how to use the SCL Navigator, please click here to see our SCL Navigator Product Training Videos .             Support Request: SCL Navigator  To receive support for the SCL Navigator, please complete and submit the form below. One of our representatives will contact you shortly.  If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Notice: Any information sent to TRIANGLE MICROWORKS, INC. will be used for internal use only!    First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Product Version:   Click to add    (e.g., v1.00.01)          (?)                Activation Key:   Click to add    (N/A for evaluation versions)          (?)               Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach your IEC 61850 SCL file:     label label                                                                             Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/scl-c-c-check
                                                       Support Request: SCL Consistency and Completeness Check  To receive support for the SCL Consistency and Completeness Check service that we offer on our website, please complete and submit the form below. One of our representatives will contact you shortly.  If you prefer, you may email us directly at support@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Notice: Any information sent to TRIANGLE MICROWORKS, INC. will be used for internal use only!    First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Phone:   Click to add            (?)                Please describe your question or issue:   Click to add    *Required          (?)               Please describe your question or issue:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.            *Required   Please attach your SCL file:     label label                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/new-licensing
                                                       Licensing in Software Tools           24 hour access to an online licensing service to manage licenses. This includes license activation and transfer without needing to contact support.  Greatly reduce the need for USB-based licenses that have presented several issues for our customers including theft, loss, and reliability problems.  Virtual Machines can be licensed directly without needing a USB license.  A USB license can be converted to a regular PC based license for greater reliability.  A new network license option is available to share licenses across multiple users and computers. The network license avoids the need to physically share a USB license, allows a license can be checked out for remote use (without Internet access), and is very simple to set up. Learn more about the network license.        Learn about Licensing         Watch our videos about the new licensing           There are three licensing options with our system.  If you are unsure which option you have purchased, please refer to your invoice or contact sales .   1) PC Seat License    Supports PCs and Virtual Machines without needing a USB-based license  License can be transferred from one PC to another with online system  License can be activated online or through another computer with internet access  Installation process automatically converts to the new license system by connecting to our licensing server through an Internet connection  Note:  With this option, USB licenses from previous versions will be converted to a PC Seat License during the installation process    2 ) Network-Based License    License can be shared across multiple PCs on a network without needing to physically share a hardware-based license  Allows a license to be checked out for remote use (without Internet access)  Supports PCs and Virtual Machines  Simple to set up during installation of the tool   Extra fees apply for this licensing option because it can be shared across multiple users  Learn more about the network license  Contact sales if you are interested in the Network-Based License.  Note: Network-Based license is not available for the SCADA Data Gateway    3) Hardware-Based Dongle License    For cases where a hardware-based license is needed for sharing across PCs that are not on the same network  Hard-coded USB Dongle that is programmed with your product key and mailed to you  Extra fees apply for this licensing option because it can be shared across multiple users   Customers with active Maintenance and Enhancement Plans are eligible for a one time upgrade to the Hardware-Based Dongle License (Test Harness and 61850 Test Suite)  Please note:  if the dongle is lost or stolen, we require that the full license be re-purchased before we will replace the dongle. In most cases, the Software-Based or Network-Based License will meet all requirements.  Contact sales if you are interested in the dongle license.      Note:  Computers without Internet Access  For computers without Internet access, the licensing process requires exchanging files with another computer that does have Internet access in order to activate the license.  Simply follow the instructions provided by the installation software.        Download Latest Version       Download the latest versions of our software tools from our Customer Web Portal .      Contact us if you have questions about our new licensing, or you can find more information in our TMW Licensing Guide.                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/network-based-license
                                           Network-Based Licensing Triangle MicroWorks is in the process of updating the licensing for our Protocol Testing Tools and SCADA Data Gateway.  As part of the new licensing, a Network-Based License will be offered that allows licenses to be shared across a network.  Benefits The Network-Based License has several important benefits:    Licenses can be shared across multiple users and computers.  Licenses can also be checked out for remote use without the needing to be connected to the network or physically share a Hardware-Based license.  The need for Hardware-Based licenses is greatly reduced. Our previous USB-Based licenses have presented several issues for our customers including theft, loss, and reliability problems.  Virtual Machines can be licensed directly. There is no need for a USB-Based license which greatly improves license reliability.  Setting up a license server is very easy. See below for more detail on setting up a Network-Based license.     How Network-Based Licensing Works           Additional Details  License Server: any computer running a version of the Windows TM operating system can serve the license including laptops, servers, or virtual machines  One User per Concurrency: a concurrency is one license seat that is shared across the network.  Only one user can use a concurrency at one time.  Multiple License Servers: more than one computer can act as a license server on the network.  However, a product key can only be activated on one computer.  Multiple Product Keys or Applications: one computer can serve multiple licenses that are either for the same application or different applications.  Detached License: a detached license can be set to expire at a certain time or can be re-attached to the license server.  If for some reason the detached license is not checked back in, the license will expire and can be re-used.   Download our Licensing Guide to learn more about the Network-Based License                                                          Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/support/installation-help
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Common Installation Issues - Troubleshooting  This page provides help for several possible installation issues.  Topics:   After installation, a HOO41 Feature has Expired error appears  After installation, the licensing service did not start  Runtime Library is too old  After installation, a H0007 Sentinel Key Not found                       Source Code Libraries  .NET Protocol Components  Test Harness  SCADA Data Gateway  DTM  61850 Test Suite Pro  Iron  SCL Navigator  SCL Verify  Licensing for Tools  Network-Based License  Installation Help       Source Code Libraries    .NET Protocol Components    Test Harness    SCADA Data Gateway    DTM    61850 Test Suite Pro    Iron    SCL Navigator    SCL Verify    Licensing for Tools    Network-Based License    Installation Help        Related Products           Test Suite Lite  SCL Navigator  Distributed Test Manager (DTM)          After installation, a HOO41 Feature has Expired error appears  The H0041 message occurs because a trial license was used on the PC and it expired more than 90 days ago. After 90 days, the licensing system will deactivate all components of a trial license which can make activating a product key troublesome.    To fix the problem, we have included a small utility call SNReset.exe in the executable directory of all TMW applications. This can typically be found in C:\Program Files\Triangle Microworks\ application  name . By running this application, the components of the licensing system required to activate a product key will be reactivated.       After installation, the licensing service did not start  The licensing system is controlled by a service that is set to start up automatically. On some machines, this service does not start up successfully after the installation is completed. In this case the service needs to be started manually after which is will run and restart without any additional problems. The error dialog box for this this condition is:    A message indicating that a license could not be found for the application may also be displayed. Another indication that the service is not running is that the ACC cannot be displayed when trying to access localhost:1947 in a browser.  In order to start the service, click the Windows Start button and type “Services”. Start the Services utility and search for Sentinel LDK License Manager. If the service is not running, right click on the service name and select Start from the menu as shown below:         Runtime Library is Too Old  After installing an application and after applying a new license, an error can occur that says the runtime library is too old. The error message typically looks like this one:    This means that the license TMW provided requires a newer version of the Sentinel runtime environment. This can also happen when a network license is being used, but the PC holding the network license has an older version of the Sentinel runtime than the PC that is trying to acquire the license. To see the version of the runtime that is installed on a PC, open the ACC on a browser by going to localhost:1947. The click on the Diagnostics link on the left side of the page. The runtime package version is listed near the bottom of the page as shown here:         After installation, a H0007 Sentinel Key Not Found error appears  The H0007 message occurs because the licensing service could not be found or accessed. The error is similar to the one described in After installation, the licensing service did not start , above.    Check that the Sentinel LDK License Manager service is running. If this service is running, the problem could be that port 1947 is blocked by a firewall policy. The Sentinel licensing system requires that the application has access to port 1947.       Additional Help  If the above did not resolve your problem, please contact Triangle MicroWorks Support .                                                                              Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/reference
                                             Reference  Utility Communication Protocol User Groups and Maillists:     DNP Users Group - A forum for supporters of the Distributed Network Protocol.    UCA International Users Group - User communities for IEC 61850, IEC 60870-6 (ICCP/TASE.2), CIM, and OpenAMI.    IEC 60870-5 Maillist - This is a non-commercial maillist that welcomes anyone with an interest in IEC 60870-5 implementation issues. The objective of this maillist is to create a place implementers and users of IEC 60870-5 can discuss different interpretations of the specifications in an effort to establish a consensus opinion on each topic. This maillist is sponsored by Triangle MicroWorks, but discussion on this maillist does not center around Triangle MicroWorks' Products.    OPC Foundation . OPC is a set of standards that specify communication of real-time data between control devices from different manufacturers.    SCADA Mailing List - A mail list to support and benefit the broader SCADA Community.    IEC TC 57 WG 3 - Technical Committee 57 Working Group 03 (Telecontrol Protocols)    Tutorials and Presentations:   Maximizing IEC 61850 Value with Software Tools (IEC 61850 Europe Conference, 2017)  Communication Testing in Simulated IEC 61850 Systems (UCA Users Group, 2016)  Deploying Secure DNP3 (IEEE 1815) What You Need to Know (DistribuTECH, 2016)  Communication Security Measures for SCADA Systems (PAC World Americas Conference, 2014)  Consistency and Completeness Checking of 61850 SCL Files for Compliance and Interoperability (PAC World Americas Conference, 2014)   Testing Intelligent Device Communications in a Distributed System (PAC World Americas Conference, 2014)  Comparison of DNP and IEC 60870-5     Paper/presentation on Communication Standards in Power Control     DNP3 Overview     Using DNP3 & IEC 60870-5 Communication Protocols in the Oil & Gas Industry (White Paper)   DNP User Group Report by Newton-Evans Research on Summary of Substation A&I Study     Comparison between Modbus and DNP3     Using XML for DNP3 Self-Description and Mapping to IEC 61850 Models                                                             Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us
                                             About Triangle MicroWorks    For 30 years, Triangle MicroWorks (TMW) has been providing solutions for standards-based protocols. Our primary goal at Triangle MicroWorks is to provide innovative and reliable solutions with timely support for our customers. We enjoy building personal relationships with our customers and understanding their needs.  Today, Triangle MicroWorks serves hundreds of software and hardware vendors around the world in more than 50 countries.    Triangle MicroWorks has grown over the years to provide a wide array of Communication Protocol solutions including Communication Protocol Software Libraries, PC-based Test and Configuration Tools, OPC drivers, and Protocol Gateways.  TMW’s Software Libraries and Tools support industry-standard communication protocols including DNP3, IEC 61850, IEC 60870-5, IEC 60870-6 (TASE.2/ICCP), OPC, and Modbus.    Some highlights in our history include:         1994  - Triangle MicroWorks begins work on a DNP3 pilot project with Siemens. At that time, Westronics/GE turns over the DNP3 development/direction to the DNP3 Users Group Technical Committee and it becomes an "open protocol". For TMW, the DNP3 work for Siemens leads to projects using other protocols, including IEC 60870-5 and Modbus.  1997 - IEEE 1379 selects DNP3 & IEC 60870-5-101 as the “IEEE Recommended Practice for Data Communications Between Remote Terminal Units and Intelligent Electronic Devices in a Substation”.  2000 - Triangle MicroWorks releases the initial version of the SCADA Data Gateway .  Many new protocols will be added to the Gateway that Triangle MicroWorks offers in their Source Code Libraries.  2001 - IEC 60870-5 is added to the SCADA Data Gateway  2003 - Modbus is added to the SCADA Data Gateway  2005 - Support for DNP3 Water Industry Telemetry Standards (WITS) is added to the DNP3 Source Code Library   2008 - Triangle MicroWorks acquires Tamarack Consulting, Inc., based in Ann Arbor, Michigan. This acquisition extends TMW’s product portfolio to support the IEC 61850 protocol standard.  2008 - Triangle MicroWorks releases the 61850 Test Suite to support the development and testing of IEC 61850 devices and applications  2012 - Secure Authentication Version 5 (SAv5) adds to the DNP3 Source Code Library  2012 - IEC 61850 is added to the SCADA Data Gateway  2013 - Bi-directional TASE.2 / ICCP is added to the SCADA Data Gateway  2013  - Triangle MicroWorks adds support for IEC 61850 Edition 2 to their IEC 61850 Source Code Library  2014 - The Iron Tase.2 / ICCP Test Tool is released as a separate product from the 61850 Test Suite with support for IEC 60870-6 based testing  2015 - The Distributed Test Manager , a system level testing tool, is officially released  2016 - Released Secure Authentication for IEC 60870-5-101 and -104  2017 - Released Test Suite Pro  2018 - Released 61850 System Tool Kit  2019 - Started Free Training Webinars  2020 - Began reselling Garibaldi , a key distribution and system management tool for IEC 61850 and Helinks STS , a IEC 61850 system configuration tool  2021 - Released SCADA Data Gateway for Linux  2023 - Released new KDC (Key Distribution Center) Client Source Code Library to secure routable GOOSE and Sampled Values.    We work hard to advance Communication Protocol Standards by serving on the technical committees and governing bodies of the protocols we support, such as the DNP Users Group , the IEC Technical Committee 57 Working Group 03 and Working Group 10, UCA International Users Group , Modbus-IDA, and the OPC Foundation . This allows us to incorporate the needs of our customers into the protocol standards and provide this new functionality in the next release of our products. We also participate in interoperability testing to help improve the usage and adoption of industry standard protocols that we support.  Jim Coats, President  Triangle MicroWorks, Inc.                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us/careers
                                             Career Opportunities  Triangle MicroWorks is an established company located in Raleigh, North Carolina specializing in communication protocols for the utility industry worldwide. Our Source Code Libraries and Windows PC based development tools are used by equipment vendors as a cost effective way to support new industry standard communication protocols.   At Triangle MicroWorks, our benefits and work environment are structured to retain an elite team of highly motivated individuals. Some of these benefits presently include:  •  Quarterly bonus plan •  Simple Retirement Plan with 100% match •  Health Insurance •  Life insurance •  Disability Insurance •  Full week for Christmas/New Year Holiday  We are always accepting resumes from qualified applicants, especially those with experience in communication protocols, technical sales, marketing, and programming on embedded and Windows environments. For more information, contact us at:   jobs@TriangleMicroWorks.com   We are currently hiring for the following positions in Raleigh, North Carolina:  • C#/C++ Software Engineer • Technical Sales Associate • Technical Support Engineer • Software Test Engineer                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us/events
                                             Current Events   Upcoming Events      PacWorld Europe   Athens | Dates: 17 – 21 Jun, 2024     IEC 61850 Interoperability Testing Event (IOP)   Birmingham | Dates: 15 – 20 Sep, 2024     Past Events        Georgia Tech Protective Relay Conference   Atlanta | Dates: 23 – 26 Apr, 2024     DistribuTECH 2024   Orlando | Dates: 26 Feb – 29 Feb, 2024     SGF-IEC Week 2023   Amsterdam | Dates: 16 – 20 Oct, 2023     PAC World Americas Conference 2023   Raleigh | Dates: 29 Aug – 31 Aug, 2023     Charting a Course to a Digital Substation - A Guide to Transitioning from DNP3 to IEC 61850   San Diego, CA | Dates: 08 – 08 Feb, 2023     Distributech 2023   San Diego, CA, USA | Dates: 06 – 09 Feb, 2023     Virtual Isolation & Testing with IEC 61850   Virtual Event | Dates: 26 – 26 Jan, 2023     Hack the Power - Women in Cybersecurity   Columbia, MD, USA | Dates: 02 – 03 Nov, 2022     IEC 61850 Week 2022   Cardiff, UK | Dates: 17 – 21 Oct, 2022     PACWorld Americas   Raleigh, NC USA | Dates: 22 – 25 Aug, 2022     IEC 61850 Interoperability Testing Event   Milan, Italy | Dates: 14 – 22 Jul, 2022     DistribuTECH 2022   Dallas | Dates: 23 – 25 May, 2022     DistribuTECH 2022   Dallas | Dates: 23 – 25 May, 2022     The Business Case for IEC 61850 and Cybersecurity   DT2022 - Utility University -UU307 | Dates: 22 – 22 May, 2022     IEC 61850 Week 2021   Hybrid Event - Virtual/ Sweden | Dates: 18 – 22 Oct, 2021     Pac World Americas 2021   Virtual | Dates: 31 Aug – 01 Sep, 2021     IEC 61850 Global   Virtual | Dates: 26 – 29 Oct, 2020     Debugging GOOSE   12 NOON EDT | Dates: 24 – 24 Sep, 2020     Pac World Americas 2020   Virtual | Dates: 01 – 02 Sep, 2020     FREE IEC 61850 Training & Workshop   Raleigh | Dates: 31 Aug – 31 Aug, 2020     Simplifying Secure Routable GOOSE & Sampled Values   FREE Webinar | Dates: 30 – 30 Jul, 2020     A Holistic Approach to Facilitate the IEC 61850 Process Webinar Series Part 1    Dates: 18 – 18 May, 2020     Dept of Energy (DOE) Solar Energy Technologies Office Peer Review Conference   Washington DC | Dates: 06 – 07 Apr, 2020     FREE Webinar - Selecting the RIght DER Protocol    Dates: 18 – 18 Mar, 2020     Working Group 10   Colorado | Dates: 10 – 14 Feb, 2020     FREE IEC 61850 Training Sessions @ DTECH 2020   San Antonio | Dates: 29 – 29 Jan, 2020     DTECH 2020   San Antonio | Dates: 27 – 30 Jan, 2020     DTECH 2020 Utility University   San Antonio | Dates: 27 – 27 Jan, 2020     Automate SCADA Testing of your Gateway, Data Concentrator or RTAC    Dates: 10 – 10 Dec, 2019     IEC 61850 Global   London | Dates: 14 – 18 Oct, 2019     IEC 61850 Interoperability Testing Meeting 2019   Charlotte | Dates: 20 – 27 Sep, 2019     PAC World Americas Conference   Raleigh | Dates: 20 – 22 Aug, 2019     DTECH 2019   New Orleans | Dates: 05 – 07 Feb, 2019     DTECH 2019 Utility University Course   New Orleans | Dates: 04 – 04 Feb, 2019     IEC 61850 Global   Berlin | Dates: 16 – 18 Oct, 2018     Cigre Session 2018   Paris | Dates: 26 Aug – 31 Aug, 2018     PAC World Americas Conference   Raleigh | Dates: 21 – 23 Aug, 2018     Free IEC 61850 Training & Workshop   Raleigh | Dates: 20 Aug, 2018     DNP3 Tutorial at IEEE T&D 2018   Denver | Dates: 16 Apr, 2018     IEC TC57 WG10 Sub Group Engineering Meeting   Brussels | Dates: 10 – 12 Apr, 2018     DistribuTECH 2018   San Antonio | Dates: 23 – 24 Jan, 2018     DT2018 Utility University Course   San Antonio | Dates: 22 – 22 Jan, 2018     IEC Technical Committee 57 Working Group 10 Meeting   New Orleans | Dates: 23 – 27 Oct, 2017     IEC 61850 IOP   New Orleans | Dates: 13 – 18 Oct, 2017     IEC 61850 Europe   Amsterdam | Dates: 26 – 27 Sep, 2017     PAC World Conference 2017   Raleigh | Dates: 29 – 30 Aug, 2017     Free IEC 61850 Training at PAC World Americas 2017   Raleigh | Dates: 28 – 28 Aug, 2017     DistribuTECH 2017   San Diego | Dates: 31 Jan – 02 Feb, 2017     TMW Happy Hour at DistribuTECH   San Diego | Dates: 30 – 30 Jan, 2017     IEC 61850 Europe   Amsterdam | Dates: 18 – 20 Oct, 2016     PAC World Americas 2016   Raleigh | Dates: 30 Aug – 31 Aug, 2016     Cigre Session 2016   Paris | Dates: 22 – 26 Aug, 2016     IEC TC57 WG10 Meeting   Rome | Dates: 05 – 09 Jun, 2016     DNP3 Tutorial at IEEE T&D 2016   Dallas | Dates: 02 May, 2016     IEC TC57 WG10 Meeting   Cathedral City | Dates: 15 – 19 Feb, 2016     DistribuTECH 2016   Orlando | Dates: 09 – 11 Feb, 2016     DTM Workshop at DistribuTECH   Orlando | Dates: 08 – 11 Feb, 2016     DNP Technical Committee Face-to-Face Conference   Las Vegas | Dates: 16 – 20 Nov, 2015     IEC TC57 WG10 Meeting   Brussels | Dates: 05 – 09 Oct, 2015     IEC 61850 IOP   Brussels | Dates: 26 Sep – 02 Oct, 2015     PAC World Americas 2015   Raleigh | Dates: 01 – 02 Sep, 2015     DTM Workshop at Triangle MicroWorks    Dates: 31 Aug – 01 Sep, 2015     PAC World Conference 2015   Glasgow | Dates: 29 Jun – 02 Jul, 2015     IEC TC57 WG10 Meeting   Regina, Canada | Dates: 01 – 05 Jun, 2015     WG 10 / TC88 JWG25 Meeting   Sweden | Dates: 09 – 13 Feb, 2015     DistribuTECH 2015   San Diego | Dates: 03 – 05 Feb, 2015     EPRI DNP3 Secure Authentication Workshop   Knoxville | Dates: 06 Nov, 2014     DNP Technical Committee Face-to-Face Conference   Reno | Dates: 20 – 24 Oct, 2014     IEC 61850 Europe   Prague | Dates: 15 – 16 Oct, 2014     PAC World Americas 2014   Raleigh | Dates: 23 – 25 Sep, 2014     Cigre Technical Exhibition   Paris | Dates: 24 – 29 Aug, 2014     IEC TC57 WG10 /  WG17 / WG18 Meeting   Quebec City | Dates: 02 – 06 Jun, 2014     Distributech 2014   San Antonio | Dates: 28 – 30 Jan, 2014                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us/privacy-policy
                                             Privacy Policy Triangle MicroWorks is committed to maintaining your confidence and trust with respect to the privacy of the personally identifiable information we collect from you. This privacy policy sets forth our practices regarding collection, use and disclosure of information collected. Collection and Registration of Personal Information We may request users of this Site to provide personal information. The information collected directly from a user may include contact information such as the user's name, e-mail address, and postal address. In order to access certain content and to make use of the full functionality of the Site, we ask you to register by completing and submitting a registration form. See our terms of service . What Personal Data Is Collected? We collect the following personal information through different pages on our website:  Contact details including first and last name, email address, phone number, and address  Technical support details including which products you are using, how you are using our products, issues you are facing, questions that you have  Sales details including which products you would like to purchase, quotation details, and project timeline   When Is Your Personal Data Collected? We collect the personal information when:  You register for access to our customer portal  You submit a technical support request with one of our products  You submit a sales request for one of our products   How Is E-mail Handled?  We use information you send us by e-mail only for the purpose for which it is submitted (e.g., to answer a question, to send information). In addition, if you do choose to provide information, it is only used to respond to your request.  For What Purposes Is Your Personal Data Used? We collect personal information for the following purposes:  To provide software trials for download to customers and potential customers  To provide technical support to customers and potential customers  To answer sales related questions for customers and potential customers   Disclosure of Personal Data to Third Parties  Access to your personal data is restricted to our employees, agents, service representatives, and approved third parties that provide commercial services.  We never sell your personal data.  We only distribute your information to third parties for the specific purposes listed below or in response to subpoenas, court orders, or legal process, or to establish or exercise our rights to defend against legal claims, or otherwise required by law. We share your personal information with the following third parties in a secure way:  Third party email services are used to send newsletters, product release notices, and other important announcements about Triangle MicroWorks events and news.  Third party credit card services are used to process credit card payments. Personal information is only shared with these third party service providers if you request to make a payment by credit card.   Cookies We use cookies to help understand how visitors use our website and use this information to improve our website and product offerings.  You can block your browser from using cookies.  Here is a link to learn more about cookies .  We use cookies from the following third party service providers:  Google Analytics tracking code is a snippet of JavaScript that is used on our website to track visitors to our website. The information gathered by Google Analytics is used to track how visitors use our website and improve our website. Data is retained for 50 months. Google offers a browser add-on to prevent your data from being used by Google Analytics.  Here is more information and a link to download the add-on .  Here is information about how Google Analytics protects your data including information about how cookies are used.   Access And Modification You have the rights to access your personal data, modify your personal data, or object to the use of your personal data, subject to applicable laws.  You can send personal data requests or any questions about our privacy policy to sales@trianglemicroworks.com .                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/contact-us
                                                      Sign up for our newsletter!  Fill out form below:       Contact Us  Click for more information on:       Request Product Information or Price Quotation  Request Technical Support  Career Opportunities  About Us    Triangle MicroWorks, Inc. 2840 Plaza Place, Suite 205 Raleigh, North Carolina, 27612-6343, USA  Phone: +1 919.870.5101 Fax: +1 919.870.6692                                                              Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: http://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-6
                                                  Request a Quote          Source Code Library Video Playlist               IEC 60870-6 (TASE.2/ICCP) Our IEC 60870-6 (TASE.2/ICCP) Source Code Library provides a quick and cost effective way to implement a Client, Server, or Bi-Directional TASE.2/ICCP device or application.  The library was designed from the ground up to fully support IEC 60870-6 services and efficiently use resources.  We offer our Library with .NET Components and in C++ for maximum flexibility across many different applications.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Features  Services Supported  .NET Components  What's New       Overview    Features    Services Supported    .NET Components    What's New        Related Products           Iron  Distributed Test Manager (DTM)  SCADA Data Gateway          Overview    The IEC 60870-6 Source Code Library from Triangle MicroWorks was designed from the ground up with highly optimized code and advanced protocol parsing techniques.  This allows our customers to integrate our code into embedded systems or workstations with low overhead and great performance.  Because it is ANSI C code, our library is processor and operating system independent.  Also, IEC 62351 security has been implemented for MMS, TLS, and IEC 60870-6 in the library.    IEC 60870-6 Support    Support for Blocks 1, 2, and 5, including DataSets, DataSet TransferSets, and SBO Controls  Full IEC 60870-6 services out of the box (not just MMS)    Designed for Performance    Highly optimized code and advanced protocol parsing techniques  Designed from the ground up to operate efficiently in embedded systems  Enables Client applications to scale up to hundreds of Server connections      Supported Platforms  .NET Components     Supports exception handling and full Object Oriented development  Supported on Windows and Linux  Supports .NET 4.x framework and .NET 6  Packaged together with C++ and ANSI-C source code libraries     C++    Supported on Linux and embedded environments  Packaged together with ANSI-C source code library                                                                                          Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: http://www.trianglemicroworks.com/products/testing-and-configuration-tools/scl-navigator-pages/overview
                                         SCL Navigator           Request a Quote          SCL Navigator Video Playlist           Download Evaluation                 SCL Navigator is a Windows™ application for creating, viewing, editing, and verifying IEC 61850 System Configuration Language (SCL) files with an intelligent editor.  SCL Navigator makes it easy to:   View SCL files with simplified XML view or as an interpreted data model  Create IED Capability Description (.ICD) files with standard IEC 61850 logical nodes and data objects (Ed. 1, Ed. 2 or Ed. 2.1)  Manage data templates with the option to edit, reuse, or clean templates  Verify SCL Files with results that are highlighted in the editor  Compare models to verify changes to SCL as files are updated and propagated through the IEC 61850 Engineering Process                           Overview  What's New  System Requirements       Overview    What's New    System Requirements        Related Products           Test Suite Pro (TSP)  Distributed Test Manager (DTM)                    (Click the button above to see the role SCL Navigator plays in the IEC 61850 Engineering Process)  Verify SCL Files    SCL verification is built in to help make SCL files more inter-operable. There are 2 methods to verify SCL files in Navigator. First, Triangle created an SCL Verify feature in Navigator to go beyond what the XML files do. SCL verify identifies missing parameters and non-compliant SCL. The tool also checks the data model against the standard and verifies that data type template references can be resolved. It also verifies that control blocks, external references, and datasets can be resolved.  Warnings and errors are displayed to distinguish high priority issues. The location of the warning or error is highlighted in the XML view for convenient troubleshooting.  The second method to validate IEC 61850 SCL configuration files is based on rules developed by the Rise Clipse initiative using Object Constraint Language (OCL).  To download the free SCL Reader version of Navigator which includes RiseClipse Verify (previously called OCL Verify):    Download an evaluation license of SCL Navigator  Type in “ freetool ” as the product license key  You will have access to the full version of SCL Navigator for 14 days.  After 14 days, you will still have access to the free SCL Reader with RiseClipse Verify.         SCL verification is built in to help make SCL files more inter-operable. (Click Image for Larger View)   An EDF and CentraleSupélec software  RiseClipse is currently developed by the computer science department of CentraleSupélec and by EDF R&D . This collaboration is done in the context of the RISEGrid Institute .          Create a Clean Data Model    Data models for an IED Capability Description (ICD) template can be created with standard IEC 61850 data objects. Logical nodes & data objects from -7-3, -7-4, -7-410 (Hydro), -7-420 (DER), -90-7 (DER), and IEC 61400-25 (Wind) can be added to the data model. Mandatory or optional objects can be added according to the IEC 61850 standard (both Edition 1 & 2).  The tool manages the Data Type Template section with the option to edit data templates, reuse templates, or clean the templates section to remove unused sections. This allows you to build an efficient data model that does not lead to oversized SCL files during the engineering process.        Build a clean data model using templates and standard IEC 61850 Data Objects (Click Image for Larger View)          View Files as XML or Data Model    SCL files can be viewed and edited with an intelligent, simplified XML editor. This view shows important XML tags that are needed in different parts of the data model.  There is also the option to see the SCL file as an interpreted data model. This allows SCL files to be viewed and edited on a line by line basis (with the XML view) but also verified by viewing how the tool builds the data model.       View SCL Files as XML Files or as Data Models (Click Image for Larger View)                            Model Comparison    Model compare allows verification of changes to SCL as files are updated and propagated through the IEC 61850 Engineering Process.  Significant time and effort is wasted when incorrect files are used.  SCL Navigator allows an independent verification of files so that these expenses can be reduced.  As ICD files are updated, the changes can be identified and validated, and the ICD file can be compared to IED instances in the SCD file to verify that the correct files were processed.       Import/Export DataType Templates and Custom Libraries     Users can export their DataTypeTemplates to a generic XML file or a Name Space Definition (NSD) file. A NSD file is a machine-processable definition of a 61850 data model and is commonly used by the standard to make the data model updates available for tools. Exported NSD files from Navigator follow this process and can be used in other tools to convey the user specific 61850 data model.  Navigator also supports importing NSD files. When Navigator imports a NSD file, it creates a custom user library that can be use to create or update SCL files. These custom user libraries can be created off Navigator's own exported NSD files or NSD files published by IEC with the latest updates to the data model.         Export 61850 DataTypeTemplates and Import Custom Libraries (Click Image for Larger View)        Ready to Get Started?       Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: http://www.trianglemicroworks.com/products/testing-and-configuration-tools/dtm-pages
                                         Distributed Test Manager (DTM)                       Request a Quote          DTM Video Playlist           Download Evaluation                Distributed Test Manager (DTM) is a Windows® application that can simulate the SCADA communications in the substation and perform automated tests to confirm system or individual device behavior.   Some benefits of system simulation using DTM include:   Reduce Risk & Cost: Incorporate system level simulation earlier in the design process to discover any issues before selecting IEDs. In addition, system testing before deployment significantly reduces the cost and risk during factory acceptance testing and commissioning.  Reduce Integration Time: Discover and resolve compatibility issues prior to commissioning by simulating components of the system to validate candidate IEDs.         Distributed Test Manager  (Click image for larger view)                    Overview  Gateway Testing  Network Load Testing  Substation Communication Testing  Cybersecurity Testing  Architecture  What's New  System Requirements       Overview    Gateway Testing    Network Load Testing    Substation Communication Testing    Cybersecurity Testing    Architecture    What's New    System Requirements        Related Products           61850 Test Suite Pro (TSP)  SCL Navigator  Communication Protocol Test Harness (TH)             Verify HMI: Simulate the substation or system to test the HMI before purchasing hardware. Future HMI changes can be verified via this simulation before being deployed.    Verify Fault Behavior: Use simulation to test fault conditions and verify system response without risks to a deployed system.  Support Training: Safely train employees on HMI and test equipment usage in a simulated environment.           Supported Protocols           IEC 61850  GOOSE  IEC 60870-5      ICCP/TASE.2  DNP3  Modbus          DTM Use Case Examples:   Automate testing of RTUs, IEDs, gateways, and SCADA systems  Network load testing for SCADA systems  Substation communication testing  Network and cybersecurity testing   KEY FEATURES are outlined below:  Easily Simulate 100s of Devices    Sample use cases include Gateway Testing and Network Load Testing .  Easily simulate different types of equipment so the simulated network mimics the actual substation network.               DTM allows multiple devices to be created and managed from a single interface:       Create up to hundreds of simulated devices  Model and test multi-protocol and multi-vendor systems  Add, subtract, or move devices to different computers on the network  Configure IEC 61850 devices based on SCL Files  Automatically configure network adapters for the IP addresses configured in the substation.  Mix simulated and real devices in the same test            Gateway Testing Using DTM to Simulate 100s of IEDs  (Click image for more information)      Distribute Processing across Multiple Computers    DTM can be configured to run on a single computer or multiple computers across a network.                The advantages of distributing a test across a network include:        Simulate more devices  Perform more complicated, computing-intensive simulations  Test network hardware including switches, routers, and radios  Validate security             Run on Single or Multiple Computers  (Click image for larger view)      Simultaneously Support Multiple Communication Protocols    DTM allows you to mix and match multiple protocols within a single simulation, so the simulated network accurately depicts your real network.               DTM uses our Protocol Libraries to enable the simulation of systems using many different protocols:        IEC 61850 - implements Client, Server, GOOSE, and Sampled Values  IEC 60870-5 (-101 and -104) - implements Master, Outstation, and Peer to Peer devices  IEC 60870-6 (ICCP/TASE.2) - implements bi-directional Client/Server  DNP3 - implements Master, Outstation, and Peer to Peer devices  Modbus - implements Master, Outstation, and Peer to Peer devices                Simultaneously Support Multiple Protocols  (Click image for larger view)      Simulate Some or All Devices in a System    With DTM you can mix and match physical and simulated networks, allowing you to see how a physical device is going to behave in your network.  For example, you can start with a fully simulated network, and gradually replace simulated devices with physical devices. Or, if a piece of equipment needs to be replaced during a test, you can replace a physical device with a simulated device. The possibilities are endless.               DTM has built-in capability to model device logic through InSight:       Use scripting to simulate device behavior or logic  Use flow charting to model and visualize device behavior                DTM Simulated Devices Alongside Real Devices in UCA Users Group Booth Demo at CIGRÉ 2014  (Click image for larger view)      Easily Configure Devices    DTM supports multiple methods of configuring devices, including using your actual devices' SCL files or DNP3 Device Profile, and even CSV files to make configuration as easy as possible. IP addresses can be automatically added and removed from your computer based on the device requirements resulting in more realistic testing scenarios with less setup time.               DTM supports a variety of configuration file formats:       SCL Files (including .ICD, .CID, .SCD)  DNP3 Device Profiles  Comma Separated Value (CSV) Files with DNP3, IEC 60870-5, or Modbus data points              Easily Configure Devices Using Multiple Types of Configuration Files  (Click image for larger view)      Visualize Entire Systems    With DTM's built-in Insight™ capability, you can create custom displays, front panel simulations, single line diagrams, and more so that you can visually monitor your system while it is under test, eliminating the need to spend hours after the test poring over data to figure out what happened during the test.               DTM has built in Insight™ capability with flow charts and customized GUIs       Create customized GUI's to facilitate testing and visualization  Emulate a display, single line diagram, or system representation  Use flow charting to model and visualize a testing procedure or device behavior              Visualize Entire Systems  (Click image for larger view)        Manipulate Device Data       For more rigorous testing, you need to create a variety of data changes so you can see how the system reacts. DTM makes creating these data changes easy, using a variety of methods including manual data changes, automatic changes, table driven data events, and scripting.               DTM allows device data models to be manipulated through a variety of techniques:       Manually - manipulate an individual data attribute on any client or server manually using a custom user interface  Automatically - enable automatic data changes on specific attribute(s) at a user specified interval  Table Driven - playback data states with Comma Separated Value (CSV) files on multiple devices simultaneously  Scripting - via Flow Chart in a graphical scripting environment or via JavaScript                    AutoDataChange Allows Periodic Changes to Data Points Based on configuration  (Click image for larger view)                 Change Data via Scripts  (Click image for larger view)                    Change Data Manually  (Click image for larger view)      Change Data via CSV File  (Click image for larger view)         Create and Manage Groups of Tests    The DTM Test Manager allows you to define, organize, and execute groups of tests. Tests can be run sequentially or individually, and results can be grouped to easily find ones that have failed.                Features of the DTM Test Manager:       Simple 10 button interface  All tests are managed by a single Java Script file that can be created and edited using the Edit Test Scrip button in DTM  Easily add prompts for tests that require user intervention  JavaScript allows virtually any behavior to be incorporated into a test                  Manage Multiple Tests Simultaneously (Click image for larger view)      Ready to Get Started?    Download Evaluation>>       Request Pricing>>                                                                                            Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: http://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          Overview   Simulate Master or Outstation devices including protocol services like polling, controls, reads, writes  Create simulated databases for Outstations that support multiple data types for different SCADA protocols  View data from real devices (IEDs, RTUs, PLCs) through periodic polling, reads, or unsolicited messages  Monitor protocol communications in monitor mode (listen only) for serial or TCP/IP network traffic  Analyze SCADA messages from Master and/or Outstations with the built-in protocol analyzer  Create Custom Functional Tests with any .NET programming language, Python, or Tcl/Tk scripts  Test a device with valid protocol messages just like a real utility SCADA network  Perform conformance test procedures for DNP3 or Modbus with automated test scripts  Test IEC 62351 security features like TLS and Secure Authentication    Ready to Get Started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: http://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/overview
                                         61850 Test Suite Pro                    Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation                 Test Suite Pro is a Windows® application with the most comprehensive set of tools on the market today to cover all of your IEC testing & troubleshooting needs in the lab, during the commissioning process, and ongoing maintenance in the Substation. It can be run on a Laptop, or left running on a Lab or Substation Computer ready to visualize your IEC 61850 data, modeling, and communications. Test Suite Pro supports Edition 1, Edition 2 and Edition 2.1 of the IEC 61850 standard.  Test Suite Pro is useful for monitoring, testing, and troubleshooting the following use cases:   Utility/System Integrator   IEC 61850 Engineering Process - the video “TMW Tools in the IEC 61850 Engineering Process” shows how Test Suite Pro works with our other tools from concept to Substation Maintenance.  Virtual Isolation Testing - Publish simulated GOOSE messages to test Protection, Automation, and Control (PAC) functions in a live substation without affecting other functions that are not involved in the test.  Deploying or modifying SCADA Systems involving a large number of devices.  Individual IED testing and validation.   Equipment Developer   Testing your new Client or Server implementation of IEC 61850, whether it was developed in-house or using a Source Code Library .  Help prepare for formal conformance testing.     Test Suite Lite is a lower cost, less featured option. Click here for a comparison of features to other 61850 testing tools in the market today.                         Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit              Basic Layout and Key Startup Information:  The screenshot below shows the basic layout of Test Suite Pro.   System Status Tools on the left bring any Errors or Warnings found to the Users attention.  The user may acknowledge any Errors or Warnings that are expected for the test conditions so that any new Errors or Warnings are noticed.   Displays on the right are used to visualize the data and are created by dragging Data Objects from the IEC 61850 model shown in the Advanced Client and Data Miner Tools.   Extensive help is available via tooltips when hovering over column headings or visual elements.  The context sensitive help for a Tool or Display may be accessed by clicking the “?” in each window. The online Help also contains a Quickstart that builds upon a preconfigured workspace to monitor & troubleshoot the example Substation Simulation workspace provided with Distributed Test Manager (DTM) . This is an excellent way to learn about IEC 61850 communications and it can all run on a single computer!   KEY FEATURES are outlined below and in the tabs to the left:  Automated Testing with Test Sequencer    This tool allows the user to create a display and build a repeatable set of commands to run against the workspace. Tests are managed through the Test Explorer with provides a view of all Test Plans and Groups in the Workspace. The Test Builder, Commands, Step Details and Execution Log are utilized to create and run tests.      Test Sequencer to set up Automated & Repeatable Testing     Visualize signal flow and timing on the network    The Signal Flow Diagram replaces the old wire connections shown on prints with animated GOOSE signals captured on a timeline that can be replayed one event at a time.  Tool tips on the signal lines show time stamps for the events on that signal.  After visually inspecting test results in Signal Flow, an automated test can be generated for Test Sequencer to easily verify the results during the next test cycle.       Visualize Signal Flow and Timing      View timing between signals    With Test Suite Pro's built-in Logic Analyzer tool, you can quickly and easily understand the timing between events.      View Timing Between Signals with Built-in Logic Analyzer      Monitor Missing/Duplicate/Extra GOOSE Streams from the SCL File    Monitoring GOOSE message streams has never been easier! The built-in GOOSE Tracker tool matches streams on the wire with their definitions in the SCL file so  you can quickly and easily monitor for:     Missing/duplicate/extra GOOSE streams from the SCL File  Dropped states  Missed retransmissions  Timeouts  Simulated messages  Dataset definitions in each GOOSE message match those defined in the SCL file          Monitor Missing/Duplicate/Extra GOOSE Streams from the SCL File      Find Out-of-Date SCL Configuration in Devices    System errors due to IEDs with out-of-date configuration files used to require tedious and time consuming comparisons. But not any longer!  The Test Suite Pro Compare Model tool automatically performs a discovery of each IED and compares it to the SCL file, and notifies you of any conflicts.    View differences at each level in the model hierarchy  Group and Filter by category        Find Out-of-Date Configuration in Devices      Troubleshoot Potential SCL File Issues    Finding obscure SCL file errors is a tedious process and can lead to erratic behavior. Test Suite Pro provides an easy, intuitive way to discover the source of these errors.   The SCL Verify tool can be configured to report only SCL file errors that are known to cause a problem in the target system.     Group results by Severity, Test, or Message  Select an error to show the corresponding line in the SCL Viewer window        Troubleshoot Potential SCL File Issues      Easily Find and View Data Across the Substation        The Data Miner tool integrated within 61850 Test Suite Pro allows you to:   View objects in an intuitive grid with user-selectable columns  Easily group, sort, and filter data  Quickly view Data Objects for desired use cases with custom or factory-defined presets  Once you've found the Data Objects of interest, just drag them to one of the five displays below to show the corresponding Data Attribute values.        Easily Find and View Data Across the Substation      View Data from across the Substation with Custom Displays   A Display selection bar at the bottom of the right hand pane of the Test Suite Pro makes it easy to view data.     Display options include (click topic for more information):     Signal Flow  Logic Analyzer  Custom Mode  Group Mode  Data Monitor    Report Viewer  GOOSE Viewer  SCL Viewer  Server Data Simulation        View Data from across the Substation with Custom Displays (Click image for larger view)     Online Help  Go to the 61850 Test Suite Pro online help to learn more about all the capabilities of 61850 Test Suite Pro.  Ready to Get Started?    Download Evaluation>>      Request Pricing>>                                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/dnp-scl-pages/overview
                                                  Request a Quote          Source Code Library Video Playlist               DNP3 Source Code Library  Our ANSI C source code library provides a quick and cost effective way to implement a DNP3 master or outstation.  A .NET component version is also available.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  DNP3 Starter Kit  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    DNP3 Starter Kit    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          Overview   Conforms to DNP3 Level 4+ Subset Definitions requirements.  Supports any physical communication network including RS 232/485 and TCP/IP.  Can be used in event-driven or non-event-driven environments.  Supports binary (input and output), analog (input, deadbands, and output), counter (running and frozen), time objects, octet strings, virtual terminal, device attributes, data sets, and internal indications.  Supports File Transfer objects for transferring large data blocks (e.g., configuration or oscillographic (COMTRADE) data).  Supports Unsolicited Responses and/or polled Report-By-Exception (RBE) for change event objects.  Supports function codes for confirm, read, write, select, operate, direct operate, direct operate NoAck, freeze, freeze NoAck, freeze and clear, freeze and clear NoAck, cold/warm restart, assign class, file transfer, enable/disable unsolicited response, activate config, record current time, and delay measurement.  Supports qualifier codes for start-stop range (0x00, 0x01), all points (0x06), limited quantity(0x07, 0x08), indexed (0x17, 0x28), and free format (0x5B).  Supports DNP3 Secure Authentication.  Extensive, built-in (but removable) diagnostics including a protocol analyzer used to visually decipher protocol messages. The diagnostic and analyzer strings can be directed to any target system display device, even a serial port or RAM buffer.  Large response messages can consist of multiple data link frames and multiple application layer fragments.  Provides support for statistics of communication protocol errors such as bad CRC, invalid synchronization byte, and incorrect frame sequence number to help identify faulty communication lines.                                                                                  Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/overview
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                  Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                    Overview  The IEC 61850 Source Code Library from Triangle MicroWorks was designed from the ground up with highly optimized code and advanced protocol parsing techniques.  This allows our customers to integrate our code into embedded systems or workstations with low overhead and great performance.  Because it is ANSI C code, our library is processor and operating system independent.  The library incorporates a merged schema that supports SCL files and model definition files for Edition 1, Edition 2 and Edition 2.1 of the IEC 61850 standard.  Also, IEC 62351 security has been implemented to support authentication and TLS encryption in the IEC 61850 in the library.       IEC 61850 Support   Components available to implement Client, Server, GOOSE, or Sampled Values  Full IEC 61850 services out of the box (not just MMS)  Implements Edition 1, Edition 2 and Edition 2.1 of the IEC 61850 standard based on SCL file   Designed for Performance   ANSI-C Library uses highly optimized code and advanced protocol parsing techniques  Designed from the ground up to operate efficiently in embedded systems  Enables Client applications to scale up to hundreds of Server connections                C Library    ANSI-C Library is designed to be processor and operating system independent  Designed for use in embedded systems and applications that require high performance  Supports any ANSI-Standard C compiler  Library includes target layers for Windows and Linux ®  Customers have implemented library on many platforms including real time OS (RTOS)    C++ Library    Supports Windows and Linux ®  C++ wrapper built on top of C Library  Underlying ANSI-C source code is included to allow for customization     .NET Components    Supports Windows and Linux  Supports .NET 4.x framework and .NET 6  Built on top of C and C++ Libraries  Underlying ANSI-C and C++ source code is included     Java Components    Supports Windows and Linux  Built on top of C and C++ Libraries  Underlying ANSI-C and C++ source code is included    Applications    Intelligent Electronic Devices (IEDs)  Remote Terminal Units (RTUs)  Metering Devices  Protocol Translators / Gateways  Protection, Control, Switchgear  Human Machine Interfaces (HMIs)    IEC 61850 Solutions  View all of Triangle MicroWorks IEC 61850 solutions here   Linux® is the registered trademark of Linus Torvalds                                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/modbus-scl-pages/overview
                                                  Request a Quote          Source Code Library Video Playlist               Modbus Source Code Libraries Modbus is an application layer messaging protocol, positioned at level 7 of the OSI model that provides client/server communication between devices connected on different types of buses or networks. Modbus is a request/reply protocol and offers services specified by function codes. Modbus function codes are elements of Modbus request/reply PDUs.  Triangle MicroWorks’ Software Libraries provide a cost-effective means of supporting industry-standard protocols in your device. Incorporating our  Software Libraries in your products will shorten development time, freeing internal resources to work on company proprietary aspects of your products.  Triangle MicroWorks Software Libraries are available in two formats: .NET Protocol Components for incorporation in Windows .NET-based products, and ANSI-Standard C Source Code Libraries for all other platforms.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    ANSI C Source Code    .NET Components    What's New        Related Products           Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)          Overview    Conforms to Modbus Application Protocol Specification V1.1.  Supports any physical communication network including RS 232/485 (for RTU and ASCII), Modbus Plus, and TCP.  Can be used in event-driven or non-event-driven environments.  Supports binary data (coils and discrete inputs) and analog data (holding registers and input registers).  Supports function codes for read, write, and read/write multiple registers.  Simple configuration for big-endian or little-endian byte order.  Extensive, built-in (but removable) diagnostics including a protocol analyzer used to visually decipher protocol messages. The diagnostic and analyzer strings can be directed to any target system display device, even a serial port or RAM buffer.  Records communication protocol errors such as "Unsupported function code", “Data base errors”, “Address range errors”, “Exception response, FC = xxx, Exception Code = xxx”.  No royalty fees per unit sold.                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/goose
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                GOOSE Features  The GOOSE (Generic Object Oriented Substation Event) component implements an additional IEC 61850 protocol used for protection functions and other applications requiring high speed multicast peer-to-peer communications capabilities.  The protocol uses a publisher/subscriber model, in which a publisher transmits unacknowledged data to subscribers.      Application interface designed to free application developers from protocol details  Integrated with data model features, including control blocks  Any valid DataSet can be used in the GOOSE Control Block  Implements IEC 61850 GOOSE protocol very efficiently  Full support for External References defined in SCL Files  Support for routable GOOSE  Click here to learn more on how to secure routable GOOSE and Sampled Values                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-scl-pages/overview
                                                  Request a Quote          Source Code Library Video Playlist             IEC 60870-5 Source Code Libraries  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                   Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          IEC 60870-5 (-101, -102, -103, -104)  IEC 60870-5 is a general protocol definition developed by the International Electrotechnical Commission (IEC) Technical Committee 57. It is an outline for the structure of a protocol and can only be implemented with a companion standard (profile) to specify options such as one of five link layer formats. Triangle MicroWorks has implemented protocol stacks for all of the currently defined profiles: 101, 102, 103, and 104.  The 101 profile has been adopted by the Substation Committee of the IEEE Power Engineering Society (1379-2000) as a recommended practice for RTU-IED communication.  Source Code Library Overview  Triangle MicroWorks’ Software Libraries provide a cost-effective means of supporting industry-standard protocols in your device. Incorporating our Software Libraries in your products will shorten development time, freeing internal resources to work on company proprietary aspects of your products. Triangle MicroWorks Software Libraries are available in two formats: .NET Protocol Components for incorporation in Windows .NET-based products, and ANSI-Standard C Source Code Libraries for all other platforms.  Triangle MicroWorks’ IEC 60870-5 libraries include these features:     Clock synchronization commands using bitmapped time/date information objects are compensated for transmission and processing delays.  Supports any applicable physical communication network including RS 232/485 and TCP/IP.  Supports both balanced and unbalanced link layer configuration using zero, one, or two octet address fields (IEC 60870-5-101 only).  Can be used in event-driven or non-event-driven environments.  Extensive, built-in (but removable) diagnostics including a protocol analyzer used to visually decipher protocol messages. The diagnostic and analyzer strings can be directed to any target system display device, even a serial port or RAM buffer.  Provides support for statistics of communication protocol errors such as bad checksum, incorrect synchronization byte, and invalid frame length to help identify faulty communication lines.  Security features to support IEC 62351 specification including TLS encryption and Secure Authentication     Learn more about IEC 60870-5  Learn more about the IEC 60870-5 communication protocol  with our training videos                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/sampled-values
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                Sampled Values Features  The Sampled Values (IEC 61850-9-2) component is used to transmit high speed streams of data set samples encoded in multicast Ethernet frames. The protocol uses a publisher/subscriber model, in which a publisher transmits unacknowledged data to subscribers.   .NET components for Sampled Values should only be used for testing purposes on Windows Operating Systems due to the timing constraints.      Application interface designed to free application developers from protocol details  Integrated with data model features, including control blocks  Supports 9-2 LE Profile  Support for Routable Sampled Values                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-6/overview
                                                  Request a Quote          Source Code Library Video Playlist               IEC 60870-6 (TASE.2/ICCP) Our IEC 60870-6 (TASE.2/ICCP) Source Code Library provides a quick and cost effective way to implement a Client, Server, or Bi-Directional TASE.2/ICCP device or application.  The library was designed from the ground up to fully support IEC 60870-6 services and efficiently use resources.  We offer our Library with .NET Components and in C++ for maximum flexibility across many different applications.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Features  Services Supported  .NET Components  What's New       Overview    Features    Services Supported    .NET Components    What's New        Related Products           Iron  Distributed Test Manager (DTM)  SCADA Data Gateway          Overview    The IEC 60870-6 Source Code Library from Triangle MicroWorks was designed from the ground up with highly optimized code and advanced protocol parsing techniques.  This allows our customers to integrate our code into embedded systems or workstations with low overhead and great performance.  Because it is ANSI C code, our library is processor and operating system independent.  Also, IEC 62351 security has been implemented for MMS, TLS, and IEC 60870-6 in the library.    IEC 60870-6 Support    Support for Blocks 1, 2, and 5, including DataSets, DataSet TransferSets, and SBO Controls  Full IEC 60870-6 services out of the box (not just MMS)    Designed for Performance    Highly optimized code and advanced protocol parsing techniques  Designed from the ground up to operate efficiently in embedded systems  Enables Client applications to scale up to hundreds of Server connections      Supported Platforms  .NET Components     Supports exception handling and full Object Oriented development  Supported on Windows and Linux  Supports .NET 4.x framework and .NET 6  Packaged together with C++ and ANSI-C source code libraries     C++    Supported on Linux and embedded environments  Packaged together with ANSI-C source code library                                                                                          Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                           The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/90-5-synchrophasor
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                Synchrophasor Features  Support for routable Sampled Values and GOOSE (Part 90-5 of the IEC 61850 standard) is built into our GOOSE and Sampled Values Libraries. This new capability allows Sampled Values and GOOSE messages to be routed across wide area networks so that substations in different locations can share voltage, current, phase, and frequency data from phase measurement units. Sharing these accurate measurements and event data enables the implementation of Wide Area Measurement and Protection and Control (WAMPAC) systems to help stabilize large, interconnected power systems      Easily configured through System Configuration description Language (SCL)  Configuration based on Data Set and enabled by Control Block  Uses GOOSE or Sampled Values library components with a change to the transport layer in order to route messages                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/training/free-webinars-2
                                               Securing Grid Communications: Unleashing IEC 61850 Routable GOOSE & Sampled Values   Free Webinar - Watch Now!  Previously held Wednesday, March 20, 2024 - 10 AM ET  Problem:   The IEC 61850 standard introduced two communication protocols known as the Generic Object-Oriented Substation Event and Sampled Values, or GOOSE and SV. Initially designed for communication within a single substation, new applications quickly emerged, necessitating the transmission of GOOSE and SV messages over longer distances. This rendered a flat network architecture impractical.   Solution:    In response to these evolving requirements, routable authenticated and encrypted variants of GOOSE and Sampled Values have been introduced as part of the IEC 62351-9 specification.  As a result of this enhanced security, wide area protection schemes that were previously unachievable due to security concerns are now possible.  What you will learn:   What is Secure GOOSE & Sampled Values  Why do I need to secure GOOSE & Sampled Values – Review Use Cases  Centralized Remedial Action  Transfer Trip Schemes  Synchrophasor Delivery  FLISR – Fault Location Isolation and Service Restoration  SV Authentication in the Substation    How to use a KDC Server, such as Garibaldi, to manage and distribute secure keys  Review how a KDC Client uses the keys to encrypt, decrypt, sign, and authenticate messages for both GOOSE & SV  Talk about the difference between Authentication & Encryption and their unique use cases  Where are we today: Review the evolving security ecosystem and the adoption within the utility industry                Instructors:   Jackson Moore  Applications Engineer Triangle MicroWorks, Inc.    Jackson received degrees in Electrical Engineering and Computer Engineering from North Carolina State University and has a background in power systems engineering. Prior to joining Triangle, Jackson worked as a Microgrid Systems Engineer, where he aided in the design and development of control systems for multi-source microgrids. In his role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges facing the energy automation sector.      Herb Falk Senior System Architect PCITek, Inc.   Mr. Falk has over 40 years of experience working in the automation, information exchange, standardization activities, and systems integration.  Previous employment was Westinghouse Numa-Logic and SISCO where he was responsible for 61850 and CIM integration projects and products.  Mr. Falk has been involved with IEC 61850 technology since 1982 and cyber security since 1993.  Mr. Falk is an editor of IEC 61850-8-1, editor of several cyber security standards, and is the Vice President of Testing for the UCA International Users Group (UCA Iug) . His work for His work for the UCA Iug has involved the coordination of IEC 61850 interoperability testing in 2011, 2013, 2015, and 2017.       Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/home/2024/06/17/default-calendar/pacworld-europe
                                                                                                     0 1 2 3 4 5 6 7                     OUR PRODUCTS SUPPORT THE FOLLOWING INDUSTRY STANDARD COMMUNICATION PROTOCOLS:     DNP3 | IEC 61850 | Modbus | GOOSE | IEC 60870-5 (101, 102, 103, 104) | 9-2 (Sampled Values) | IEC 60870-6 (ICCP/TASE.2) | 90-5 (Synchrophasor)                     Software Libraries  Add support for industry standard Communication Protocols directly in your device. Available as:  • ANSI Standard C Source Code Libraries  • C++ (IEC 61850 and IEC 60870-6 only)  • .NET Communications Protocol Components  •  Learn More>          Testing & Simulation Tools  We provide a variety of test tools that can be used to test and validate devices and networks. We also provide tools to help configure your devices.  •  Learn More>                  SCADA Data Gateway  The SCADA Data Gateway can be used as a Data Concentrator or Data Translator or both at the same time. Check out the features of this advanced, yet affordable gateway.  •  Learn More>              Training Videos and Workshops   Check out our extensive training video library for communication protocol training, product overviews and demos. Keep up with the latest training classes being offered by industry experts  •  Learn More>                 SIGN UP to receive updates about   SCADA communication protocols,  trainings and the latest product releases                      Missed our last webinar? Watch NOW!  Securing Grid Communications:  Unleashing IEC 61850 Routable GOOSE & Sampled Values               UPCOMING EVENTS    PacWorld Europe   Dates: 17 – 21 Jun, 2024  Location: Athens, Greece                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/home/2024/09/15/default-calendar/iec-61850-interoperability-testing-event-(iop)
                                                                                                     0 1 2 3 4 5 6 7                     OUR PRODUCTS SUPPORT THE FOLLOWING INDUSTRY STANDARD COMMUNICATION PROTOCOLS:     DNP3 | IEC 61850 | Modbus | GOOSE | IEC 60870-5 (101, 102, 103, 104) | 9-2 (Sampled Values) | IEC 60870-6 (ICCP/TASE.2) | 90-5 (Synchrophasor)                     Software Libraries  Add support for industry standard Communication Protocols directly in your device. Available as:  • ANSI Standard C Source Code Libraries  • C++ (IEC 61850 and IEC 60870-6 only)  • .NET Communications Protocol Components  •  Learn More>          Testing & Simulation Tools  We provide a variety of test tools that can be used to test and validate devices and networks. We also provide tools to help configure your devices.  •  Learn More>                  SCADA Data Gateway  The SCADA Data Gateway can be used as a Data Concentrator or Data Translator or both at the same time. Check out the features of this advanced, yet affordable gateway.  •  Learn More>              Training Videos and Workshops   Check out our extensive training video library for communication protocol training, product overviews and demos. Keep up with the latest training classes being offered by industry experts  •  Learn More>                 SIGN UP to receive updates about   SCADA communication protocols,  trainings and the latest product releases                      Missed our last webinar? Watch NOW!  Securing Grid Communications:  Unleashing IEC 61850 Routable GOOSE & Sampled Values               UPCOMING EVENTS    IEC 61850 Interoperability Testing Event (IOP)   Dates: 15 – 20 Sep, 2024  Location: Birmingham, Alabama, USA                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/overview
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway           Overview            A Highly Configurable Tool            Create a standalone ICCP Client  Create a standalone ICCP Server  Create a bidirectional (Peer to Peer) ICCP Client/Server  Use either .XML or .CSV files to define models in both the Client and Server  Dynamically change data in the server for testing purposes  View entire Object Model in both client and server  Poll individual points in the Client  Keeps history of Dataset TransferSets for each Client  Create and delete Dynamic Datasets              Ready to Get Started?     Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/sclwebcheck
                                                  SCL Verify  A free tool to check your SCL Files          First Name  *      Please enter your First Name.          Last Name  *      Please enter your Last Name.          Company  *      Please enter your Company Name.          Email Address  *       Please enter a valid email address  Please enter your Email Address.          Country  *      Please enter your company's Country.          SCL file to test  *       Please choose a file to upload.         Return messages:   All  Hide Warnings and Statistics          I agree to the terms of service document.   Please select the checkbox to proceed.         Check File          The full version of SCL Verify in SCL Navigator provides more comprehensive sorting and filtering of test results. You may download a free trial of SCL Navigator to perform SCL checking on your local PC.  We do not share uploaded SCL Files with anyone outside of Triangle MicroWorks.  Please understand that information sent to Triangle MircoWorks, Inc. is used internally to improve our products and services. Triangle MicroWorks, Inc. may also publish summary statistics from submitted information to showcase the most common aspects of SCL Files and highlight ways to improve the usage of SCL Files. No personal information or specific information from your SCL File will be shared externally with any third party. Please read our Terms of Service document to learn more about our policies.                SCL Verify           SCL Verify is a free tool that helps verify both syntactical and IEC 61850 specific aspects of SCL Files.  Among the many checks performed, the tool verifies the Data Model against the following standards:       Standard Object Models defined in IEC 61850 7-3, 7-4  Object Models defined in IEC 61400-25 (Wind Turbines)  IEC 61850-7-410 (Hydroelectric Power Plants)  IEC 61850-7-420 (Distributed Energy Resources)            The goal of SCL Verify is to promote the interoperability of IEC 61850 devices and systems.  SCL Verify is provided as a community service by Triangle MicroWorks, Inc. which makes no guarantees about the suitability of the service for any purpose.                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                             The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dnp3-forge-pages
                                                  Request a Quote          DNP3 Forge Video Playlist          Download Evaluation              DNP3 Forge The DNP3 Forge supports the creation and validation of DNP3 XML Device Profile documents.                   Overview  DNP3 Parameters  Device Profile Document  Multiple Devices  Validation and Test Scripts  What's New  System Requirements       Overview    DNP3 Parameters    Device Profile Document    Multiple Devices    Validation and Test Scripts    What's New    System Requirements        Related Products          Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)         Overview   Create/Edit DNP3 XML Device Profiles (included in free version).  Supports January 2010, July 2012, November 2013, and November 2014 versions of the DNP3 XML Device Profile Schema.  Allows the version of a profile to be changed.  Displays DNP3 Device Profile contents in an easy to navigate and searchable tree.  Displays properties of selected element with details.                                                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/garibaldi
                                             Garibaldi: IEC 61850 Security Management Tool                  Request a Quote             Request Evaluation                 Garibaldi is a Key Distribution and System Management tool for IEC 61850.  Garibaldi can:    Manage large numbers of multicast group's key and policy distribution  Manage system enrollment to allow revocation  Operate in centralize or de-centralized environments      for more information  KEY FEATURES include:                                 Related Products           Test Suite Pro  SCL Navigator  Distributed Test Manager (DTM)              Key Distribution and Access Control    Garibaldi Key Support Functions Include:    IEC 61850 Key Management  Multi-domain routing  PKI Management/Certificate Authority(s)  User Identification  Logging        Garibaldi Security Key Support Functions     Security Domains    Supports deployments that are:  Centralized  Decentralized (Mesh)  Garibaldi provides the perfect mix between resilience and standalone security requirements.      Garibaldi Supports Centralized and De-centralized (Mesh) Deployments      IEC 61850 Security    Garibaldi provides security functionality to IEC 61850 as shown in the adjacent table. IEC 61850 key and policy distribution for GOOSE, R-GOOSE, SV and R-SV via:   Distribution and management  Individual keys generated per DataStream (increases security)  Distribution and management per IEC 62351-9          Security Functions  Messages  Policy    Authentication of peers communicating with each other  GOOSE/SV  Garibaldi    Prevention of message tampering  GOOSE/SV  Garibaldi    Confidentiality (e.g., Encryption)  GOOSE/SV  Garibaldi    System membership  GOOSE/SV  Garibaldi    Determining rights of usage  SCL  Garibaldi    Revocation of membership or rights    Garibaldi          Member Validation and Policy Delivery    Membership    Determined by SCL  Manual entry  Digital Certifications   Policy Distribution    Delivery typically 24 hours   PULL  PUSH  Multicast PUSH  Key Delivery Assurance (KDA)   Delivers:   Group Keys  Encryption algorithm  Authentication/MAC Algorithm         Garibaldi Member Validation and Policy Delivery                    Ready to Get Started?    Request an Evaluation License>>      Request Pricing>>                                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/helinks-pages/overview/
                                             Helinks STS: IEC 61850 System Configuration Tool                  Request a Quote          Helinks Video Playlist           Request Information                 Helinks STS is a System Configuration tool for IEC 61850.  HELINKS STS is a tool set for IEC61850-based Substation Automation System Engineering (SAS Engineering). It supports a device-independent top down engineering process according to part 4 and part 6 of the IEC 61850 Standard.  The Helinks STS tool set includes:   System Specification Tool  System Configuration Tool  Various utilities for SCL Handling.   Helinks STS:   Supports top down and bottom up engineering processes  Focuses on multi-vendor, IED independent engineering  Allows you to perform complex engineering tasks with a single click  Solve detailed SCL problems with the embedded native SCL Editor  Standardized solutions based on libraries give a boost to engineering efficiency and harmonization  Implements IEC 61850 Edition 1 and Edition 2    for more information  KEY FEATURES include:                                 Related Products           Test Suite Pro  SCL Navigator  Distributed Test Manager (DTM)              Bay and Function Library    When creating or editing a Single Line Diagram, the built-in bay and function library makes it easy to select the correct bay equipment. Each bay can have different power equipment which can be chosen from the “Power Equipment Library” tab      Easy Configuration via Bay and Function Libraries     Excel ® Import/Export of Signal List    Signal lists can be imported from or exported to Microsoft Excel sheets.      Import/Export Point Lists Using Excel     SCL Import/Export    Helinks STS provides an easy means of importing from SCL files.  When an SCL file is selected for import, recognizes the format of the SCL file and allows the user to select appropriate options for the import.  Configuration files can also be exported to SCL files.        SCL Import Options     Directly Edit and Validate SCL Files    Helinks STS includes a text-based XML editor to allow direct edits to SCL files. Once edited, Helinks STS can validate the resulting file.      Helinks STS Text-based XML Editor can be used to modify SCL/SCD/ICD Files      Signal Engineering    The signals tab for a logical node contains the IEC 61850 address of each signal and their status. You can check out the signals for the bay by clicking on the screen or you can see the signals for the whole substation by moving back to the single line diagram and clicking on the substation name.      Logical Node Signals Tab         Communication Specification    Communication exchange between IEDs can be specified within HELINKS STS. After implementation, the signals used in the exchange are updated to the ones for the IED's data models. The “Applications” option from the dashboard is used for creating the application section.  In the Applications section, users have the ability to specify peer to peer applications such as GOOSE and SV or client server applications such as MMS reports. After implementating the IEDs in the system, the communication can be configured, and control blocks are created by Helinks STS.      Helinks STS can configure signal communications      Automatic generation of IEC 61850 communication configurations (MMS Reports, GOOSE)    Helinks STS supports easy creation of control blocks for any specified communication application.  You can configure publishing, subscription, or both, and you can choose to configure communication exchange for all IEDs.       Helinks Communication Configuration Settings                 Ready to Get Started?    Request an Evaluation License>>      Request Pricing>>                                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/sales/sales-request?product=SDG
                                                          Sales Request Please complete the form below and we will contact you shortly.  If you prefer, you may email us directly at sales@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Your contact information will not be sold or used outside of our organization as explained in our privacy policy .    First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Company:   Click to add    *Required          (?)                Street:   Click to add            (?)                City:   Click to add    *Required          (?)                State:   Click to add            (?)                Country:   Click to add    *Required          (?)                Zip:   Click to add            (?)                Phone:   Click to add            (?)                Fax:   Click to add            (?)                Web Site URL:   Click to add            (?)                Product Interest?    Source Code Libraries  .NET Protocol Components  SCADA Data Gateway  Communication Protocol Test Harness  61850 Test Suite Pro  Training  Distributed Test Manager (DTM)  SCL Navigator  61850 System Tool Kit  Helinks STS  Garibaldi   *Required  Please select a value for Product Interest?.       Protocol Support?    DNP3  IEC 61850  IEC 60870-5-101  IEC 60870-6 (TASE.2/ICCP)  Modbus  IEC 60870-5-102  IEC 60870-5-103  IEC 60870-5-104   *Required  Please select a value for Protocol Support?.      How did you hear about us?   Please Select  Search Engine  Social Media  Referral  E-Newsletter  Other     If other, please tell us how:   Click to add            (?)               Comments, Questions and/or Additional Information:          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.                                                                             Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                              The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/features
                                                  Request a Quote          SDG Video Playlist           Download Evaluation-Linux             SCADA Data Gateway     Key Features                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)                Web-based configuration interface     Multiple Views  Enhanced protocol and status logging  Point mapping integrated into point list  Special views for errors, warnings and performance  Support for searching and filters  Support for paging with large point lists         Web-based Configuration  (Click image for larger view)       User access management     Restricts User Account capabilities with Role Based Access Control (RBAC)  Automatic logoff timeouts  Feature can be enabled/disabled         User Access Management  (Click image for larger view)         Audit Logging     Audit Logging to track users' actions  Support for searching and exporting of logs  Additional audits coming soon         Audit Logging  (Click image for larger view)        Configuration Workspace     Consolidates application configuration files into a single directory (i.e. Workspace)  A Workspace can be selected form the System configuration page  Allows for a more flexible management of multiple configurations         Workspace Support for Different Sets of Configuration Files  (Click image for larger view)         Protocol/System logging     Logs protocol specific communications  Logs system diagnostic information  Allows filtering and searching  Can be exported to a file  Always logs Errors and Exceptions to disk with size management         Protocol/System Logging with Filters  (Click image for larger view)         Drag and Drop point mapping     Map multiple data points in one action  Makes it easy to specify point mappings between protocols  Support for SDO to MDO and MDO to MDO mapping         Drag and Drop Mapping  (Click image for larger view)            Special Views     Provides a concise place to see performance metrics, warnings, errors and overall system health  Supports reset to clear/acknowledge certain errors         Special Views  (Click image for larger view)        Point mapping display     Integrated into the point list  Tooltip to support additional information  Shows mapping of all point relationships         Improved Point Mapping Displays  (Click image for larger view)        Point List     Paging  Filtering  Searching  Sorting         Search and Filter Point List from Web Interface  (Click image for larger view)      Ready to Get Started?      Download Evaluation >>       Request Pricing>>                                                                                          Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/equation-editor
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     Equation Editor  The integrated Equation Editor supports creation of new data-points based on raw data or even combinations of raw data points and/or other equation points.                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)                                                                                             Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/protocol-analyzer
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     Protocol Analyzer                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)          The built in Protocol Analyzer displays and decodes each message:   Filters allow display of only desired level of information.  Display any selected channels, sessions, or sectors.  Scroll back buffer allows copying to the Windows™ clipboard and examination of previous messages.  Search function allows finding a specific protocol exchange quickly.  Protocol analyzer log can be saved as a text file for later analysis.                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/supported-protocols
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     Supported Protocols                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)                            Features  Windows  Linux    DNP3        • Serial  ✔  ✔    • TCP/IP  ✔  ✔    • Secure Authentication (SAv5)  ✔  ✔     IEC 61850          • 7-410 (Hydro)  ✔  ✔     • 7-420 (DER)  ✔  ✔     • 61400-25 (Wind)  ✔  ✔     IEC 60870-6 (TASE.2/ICCP)         • Block 1  ✔  ✔     • Block 2  ✔  ✔     • Block 5  ✔  ✔     IEC 60870-5-101 (Basic Telecontrol Tasks)        • Secure Authentication  ✔  ✔     IEC 60870-5-103 Master - Protection Equipment        IEC 60870-5-104 - TCP/IP network access for -101          • Secure Authentication  ✔  ✔     Modbus        • Serial ASCII/RTU  ✔   ✔     • Modbus Plus  ✔  ✔    • Modbus TCP  ✔   ✔    ODBC Client        OPC UA Client and Server (OPC Certified)        OPC        • OPC Data Access Server 1.0a, 2.00, 2.05a, and 3.0 (OPC Certified™)   ✔  N/A    • OPC Data Access Client 2.00 and 2.05a (OPC Certified™)   ✔  N/A    • OPC Alarms and Events 1.00, 1.02, and 1.10 Server (OPC Certified™)  ✔  N/A    • OPC Alarms and Events 1.00, 1.02, and 1.10 Client  ✔  N/A     • OPC XML Data Access Client • OPC XML Data Access Server   ✔ ✔  N/A N/A                    More Detailed Protocol Information:      ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication       ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication                                                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/iccp-tase-2
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     Support for ICCP/ TASE.2                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)          Single Solution - With the SCADA Data Gateway, you can translate an ICCP interface directly to other SCADA protocols like DNP3, IEC 60870-5 (IEC 101, IEC 104), IEC 61850, or Modbus. There is no need to have two separate OPC drivers (one for ICCP and one for the other SCADA protocol).   Software Only Solution - The SCADA Data Gateway can be run on any Windows PC. This allows the translation to ICCP to be performed on either a SCADA server or an embedded PC in the field.   Map ICCP Data and Controls - The SCADA Data Gateway supports mapping ICCP points and controls directly to other SCADA protocols like DNP3, IEC 101, IEC 104, IEC 61850, or Modbus.   Large Number of Points - High server point count and 250 connections can be supported.   Millisecond Time Stamps - Millisecond time stamps are supported for extended ICCP data types.   Security - Authentication and TLS encryption is supported for ICCP.                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/wind-integration
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     Wind Integration with IEC 61400-25                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)            The SCADA Data Gateway supports the integration of wind power plants through the international standard IEC 61400-25. This allows monitoring and control of wind resources through standard communication protocols.   Single Solution -  translate directly between SCADA protocols like DNP3, IEC 60870-5 (IEC 101, IEC 104), IEC 61850, or Modbus with better reliability rather than using multiple OPC drivers   Protocol Mapping - support for monitoring and control point mappings as defined by IEC 61400-25-4 including protocols  DNP3, IEC 60870-5-104, and IEC 61850   IEC 61850 Data Models - support for wind data models including common data classes, data objects, and logical nodes   Large Number of Points - Up to 30,000 points and 250 connections can be supported    Security - Authentication and TLS encryption is supported for IEC 61850, DNP3, and IEC 60870-5-104                 Customer Case Study  Learn how Triangle MicroWorks helped Senvion integrate large scale wind farms into the grid using communication protocols based on the IEC 614000-25 standard. Benefits of this approach include:   Increased reliability  Greater interoperability  Reduced project time    Download case study                                                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/iec-60870-5-secure-authentication
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     IEC 60870-5 Secure Authentication                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)          The SCADA Data Gateway supports  Secure Authentication as defined in the IEC 60870-5-7 standard which is based on the cyber-security standard IEC 62351 (Parts 5 and 8).  TLS encryption is also supported by following the defined cipher suites, algorithms, and connection duration defined by IEC 62351-3.     Support for Secure Authentication for IEC 101 and IEC 104  Can be deployed in the substation or control center to provide a secure link for telecontrol  Supports both symmetric and asymmetric keys as well as x.509 certificates  Supports both serial and IP based networks  Supports Role Based Access Control (RBAC) - multiple users and roles (engineers, operators, viewers, admin)  Configure privileges(view, read, security, etc.) for each role  Users can be added, modified, or removed  Secure Authentication testing can be performed using the Communication Protocol Test Harness            Security for SCADA The SCADA Data Gateway can be used to integrate with IEC 101 or IEC 104 systems which support or are upgrading to Secure Authentication based on IEC 62351.  The following use cases are supported:    Integrating an existing SCADA system with IEDs or RTUs which support Secure Authentication  Integrating existing IEDs with a SCADA system which supports Secure Authentication  Adding Secure Authentication on both sides to form secure channel from SCADA to substation  Data concentrator for IEC 101/104 systems that support Secure Authentication  Translating from IEC 61850 or other protocols to IEC 101 or IEC 104 with security           Example of Aggressive Mode Secure Authentication Challenge Reply Sequence                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/dnp3-secure-authentication
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     DNP3 Secure Authentication                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products            Distributed Test Manager (DTM)          The SCADA Data Gateway supports  Secure Authentication as defined in the IEEE 1815-2012 (SAv5) and IEEE 1815-2010 (SAv2).  TLS encryption is also supported by following the defined cipher suites, algorithms, and connection duration defined by IEC 62351-3.     Can be deployed in the substation or control center to provide a secure link for SCADA  Supports both symmetric and asymmetric keys as well as x.509 certificates  Supports both serial and IP based networks  Supports Role Based Access Control (RBAC) - multiple users and roles (engineers, operators, viewers, admin)  Configure privileges(view, read, security, etc.) for each role  Users can be added, modified, or removed  Supports remote key and user management  Secure Authentication testing can be performed using the Communication Protocol Test Harness  To learn more about DNP3 Secure Authentication, watch our DNP3 training videos on our website. Example of Secure Authentication Challenge Reply Sequence                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/protocol-mapping-testing2
                                                   Request a Quote          DTM Video Playlist           Download Evaluation               Gateway Testing with Distributed Test Manager(DTM)      Use Case        Gateways are commonly used to distribute controls and data across different parts of SCADA systems. These devices are often used to concentrate thousands of data points from hundreds of devices and translate messages across multiple protocols. Because of their large scope, testing gateways can be time consuming and error prone.      Today, many protocol mapping tests for gateways are performed manually. Each point is tested, one by one, to ensure data is mapped correctly. This manual approach may be required in final site acceptance testing, but preliminary configuration testing in the lab can be automated by simulating the field devices and SCADA system with valid protocol messages.                Gateway Testing using DTM  (Click image for larger view)                    Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager(DTM)                   This allows the real gateway to be tested within a simulated test system, which can automatically run through all data points and document the results of the protocol mapping. This approach is faster and more reliable than manual testing. In addition, it can allow testing to be performed earlier rather than waiting for the final system to be installed and configured.       Training Video                To learn how to automate testing of your SCADA Gateway, Data Concentrator or Protocol Converter, watch this short training video for step by step instructions.         Rapidly deploy accurate simulations of each IED within your SCADA network  Quickly specify thousands of point-to-point gateway tests across single or multiple protocols  Execute thousands of individual tests and view detailed results in real-time  Use both default and custom Gateway Test Actions  Quickly scale up test coverage by exporting Gateway Test Actions for several points to Excel and replicating to a large number of points         Example: IEC 61850 to DNP3 Mapping Verification           This example demonstrates how to test the mapping of a SCADA gateway or RTU in an automated test environment.  In this example, Distributed Test Manager (DTM) simulates multiple IEC 61850 IEDs and a single DNP3 SCADA Master. This simulates the real environment that the gateway can be tested within. Automated tests are generated by repeatable actions. DTM runs these automated tests in order to validate the configuration of the gateway and document the results. In this example, the Triangle MicroWorks SCADA Data Gateway is the device under test.  This example illustrates how DTM can be used to test partial systems without requiring all components in place and configured.  It also shows that this type of testing can be performed without expensive test equipment to generate signal changes.  This same test can be performed for standard SCADA protocols like ICCP/TASE.2, IEC 60870-5 (-101 or -104), DNP3, IEC 61850, and Modbus.       This video demonstrates using the Distributed Test Manager to validate mappings.      Ready to get started?      Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/system-requirements
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     System Requirements                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)          Windows Support: The Triangle MicroWorks SCADA Data Gateway (SDG) supports the following versions of Windows TM :    Windows 10 IoT  Windows 11  Windows 10 (64-bit)  Windows Server 2016 (64-bit)  Windows Server 2019 (64-bit)   Minimum system requirements for the host computer:    1.5 GHz processor; recommended: 2.4 GHz multi-core processor  1 GB RAM; recommended: 4GB RAM  Windows 8 or higher  Microsoft .NET 4.6 Framework     Linux Support:     Runs on x86 64-bit Single Board Computers to large Servers  Works on Virtual Machines  Ubuntu 18.04 and 20.04  Red Hat Enterprise 8.0  Minimum requirements:    X86 processor 64 bit capable  2GB ram 4GB preferred  0.5GB disk min plus OS requirements    Note:  SDG supports virtual machines (VM)     The minimum requirements are for small SDG configurations with limited data throughput requirements.                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/what's-new
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway     What's New                 Overview  Features  Equation Editor  Protocol Analyzer  Supported Protocols  |---ICCP TASE.2-test  |---Wind Integration-test  ICCP TASE.2  Wind Integration  IEC 60870-5 Secure Authentication  DNP3 Secure Authentication  Automate Gateway Testing  System Requirements  What's New       Overview    Features    Equation Editor    Protocol Analyzer    Supported Protocols    ICCP TASE.2    Wind Integration    IEC 60870-5 Secure Authentication    DNP3 Secure Authentication    Automate Gateway Testing    System Requirements    What's New        Related Products           Distributed Test Manager (DTM)          Linux & Windows Support: Current Release: v5.02.01 Release Date: March 2024  Newest Features in this Release    Upgraded OpenSSL from 1.1 to 3.07  ODBC: Added Support for MySQL driver  Improved Installation and Configuration  Improved configuration of 61850 Reporting  Improved method of creating and configuring security certificates  Improved Equations creations and configuration  Improved Linux Support and Stability  Improved searching features and useability  Improved Modbus write capabilities and SDG Engine stability  Streamlined and Improved ICCP configuration  Removed requirements for 7 layer parameters when 7 layer stack validation is disabled on for 61850 and ICCP   Watch a 5 minute video on how to migrate to SCADA Data Gateway v5.01+   Windows Support only: Current Release: v4.02.0037 Release Date: October 2022  Newest Features in this Release    61850 and ICCP denial of service vulnerability fixed    Key Additions to Recent Releases    v5.02 (Linux & Windows support)    Improved the reliability of service restart on all computers.  Fixed several reported Security Vulnerabilities.  Updated the OpenSSL version to 1.1.1t.  Added support for Ubuntu 20.  Add MySQL driver support for ODBC Client.  Updated OPC libraries to latest version due to third party vulnerabilities.  New warnings were added for self-signed certificates.  Added new equation: countchange().  Added support options, aliases, and descriptions in import/export mappings.  Enhanced the startup logging with improved messages for the Monitor and Engine services.  Improved the Linux install, IP address and password validation with install scripts.  Allow Engine to continue running even if not licensed.  Many other security, reliability, and performance enhancements.   v5.01.03 (Linux & Windows support)   Addresses vulnerability ICSA-22-249-01.  Upgraded installation software to latest version to eliminate reported vulnerabilities.     v5.01.02 (Linux & Windows support)   Updated the SCADA Data Gateway to require https encryption. The application can no longer be configured to use http .  Created a new GTWSettings command line utility can be used to modify some parts of the configuration including setting the host IP and setting the public and private certificates.  Improve security and verification of files during workspace upload.  Added a notification to the Gateway screen when the self-signed TMW sample certificates are being used.  Improve several warning/ error messages, adding more details for the specific issues.     v5.01.01 (Linux & Windows support)   Improved the functionality of the Migration tool that is used to upgrade configuration files from previous versions of the SCADA Data Gateway  Modified the initial configuration to use 1270.0.1 instead of localhost to improve performance  Corrected deep OPC logging  Improved installation experience to help with the initial setup and configuration of the new SCADA Data Gateway.  Update openssl to 1.1.1n.  The new SDG was modified to limit the IP addresses that can access the web server to local addresses if https is disabled.  A new icon to access the web browser has been added to the desktop after installation.   v5.01 (Linux & Windows support)     Import and Export of Gateway configuration, partial or total, has been added to the web interface.  Strong password requirements added to the API  Improved mapping point selection and filters for the 61850 Server<  Added support for m104 Channel Redundancy  Added json IP configuration information to the installer  An “Auto Trust” checkbox was added to the UA client GUI for auto trusting server certificates  Added audit logging option to the Gateway  Simulation code for 61850 client updated to allow use in Linux  Session address will be incremented automatically when adding a new MDO  Turn off read-only attribute on OPC  AE item dialog so that names can be entered manually  Improved ICCP configuration and model editing including dynamic modification of datasets and auto MDO mapping  Removed restriction for remote and local link address to be different in ICCP configuration  Added VCC folder to ICCP Server  Added OPC  XML Client functionality (Windows Only)  Improve OPC Logging (Windows Only)  Enhanced OPC  AE to be more configurable (Windows Only)  Add OPCUA _PUBLISH_OFF option for all MDOs  Add new Channel Action functionality for MDNP channel actions to allow pause between sessions  Improve the stability of the SDG with internal library components  Improve OPC UA and Classic Server to show “ RW ” status    v5.0 (Linux support only)   Web based configuration interface with support for multiple views for configuration and logging  User access management with support for roles  Audit Logging for User access control (more audits coming soon)  Workspace support for management of different configuration files (INI, CSV , SCD )  Improved Protocol/System logging with more filters and built in file logging  Drag and drop of multiple items for mapping  Client/Server runtime model for configuration (gets rid of the run as service and run
    as GUI required in SDG 4)  Special Views for consolidating performance metrics, warnings, errors into a more
    concise location  Improved display of point mapping integrated into the point list panel  Added the ability to search and filter the point list from the web interface  Compatible with INI/ CSV configurations from previous versions of SDG (there couldThe requested
    be some minor changes required related to file paths in the INI file)   4.02.0021 (Windows support only)      Upgraded openssl to 1.1.1o.  Upgraded to VS 2017 runtimes.  Added auto creation of OPC UA Server certificate.      4.01.0180 (Windows support only)      Added OPC client (UA and classic) capability to add multiple points at a time including whole branches and all items.  Added capability to create self-signed certificates that can be used for OPC UA client and server.  Improved OPC UA interoperability.  Added INT 64 (I8) and UINT 64 (UI8) data type support for both OPC Classic and OPC UA.  Add Char (I1) and UChar (UI1) support to OPC Classic.  Added support for Enumeration Types in OPC UA client and server.  Added capability to disable any type ID for S104 devices.  Add capability to set the log to file size.     4.01.0131 (Windows support only)   Improved ICCP configuration and model editing including allowing dynamic modification of datasets and auto MDO mapping  Added VCC folder to ICCP Server  Added OPC XML Client functionality  Improve OPC Logging  Enhanced OPC AE to be more configurable  Add OPCUA_PUBLISH_OFF option for all MDOs  Add new Channel Action functionality for MDNP channel actions to allow pause between sessions  Improve the stability of the SDG with internal library components  Improve OPC UA and Classic Server to show “RW” status   4.01.0058 (Windows support only)    Improved ICCP configuration and model editing including allowing dynamic modification of datasets and auto MDO mapping  Improved transitioning from GUI to Service and back to GUI  Added User Defined Folders for organizing internal MDOs and equations  Added a new strcmp equation for comparing strings  Enhance both Slave and Master modbus dual registers to support int32, uint32, and float32  Add capability to manually switch to a specific redundant channel for M104 redundancy   v4.01.0047 (Windows support only)    Improved ICCP configuration and model editing including dynamic modification of datasets and auto MDO mapping  Added VCC folder to ICCP Server  Added OPC XML Client functionality  Improved OPC Logging  Enhanced OPC AE to be more configurable  Added OPCUA_PUBLISH_OFF option for all MDOs  Improved the stability of the SDG with internal library components  Improved OPC UA and OPC Classic Server to show Read/Write permissions  Added new Channel Action functionality for MDNP channel actions to allow pause between sessions   v4.00.0140 (Windows support only)    The SDG now allows you to create, edit, and delete model points and domains individually through the GUI interface. This is an alternative to having to define a model file and read it in. This means the SDG will allow modifications to the ICCP client model without having to read in a file.  Improved ICCP mapping capability to automatic mapping and automatic addition to control blocks including automatic redefinition of datasets  Added a “Show Config” option on ICCP servers to show the full model configuration that remote clients have setup  Improved logging to file to provide a means to not fill up the disk and limit the number of logs that are generated  Improved ODBC queries when mapped to equations to only update once when multiple arguments from the same query are used in the equation  Improved 61850 server side controls when mapped to other protocol MDO controls to handle asynchronous updates   v4.00.0123 (Windows support only)    Improved OPC UA and OPC Classic logging  Added GUI configuration for the OPC UA server for all settings, including application certificates, user names/passwords and user certificates  Added support for Windows IoT  64 bit support and continued support for 32 bit with support of up to 200,000 server points  Fully support V3 OPC Classic for both Client and Server  OPC UA (Client and Server)  Added GOOSEMonitor device for monitoring GOOSE streams  Enhanced description and tracing for IEC 61400-25 alarms  Improved GUI performance  Added TLS security to 60870-5, DNP, and Modbus TCP protocols   v3.06.0027 (Windows support only)    Added support for redundant TCP/IP connections for IEC 60870-5-104 controlled station/outstation channels in the SDG (according to section 10 of IEC 60870-5-104 standard)  Added capability for using Modbus dual registers (for 32 bit) in equations  Improved parsing of point mappings configuration file (.CSV)  Improved performance of processing IEC 61400-25 alarms  Enhanced description and tracing for IEC 61400-25 alarms  Added .INI configuration parameter for OPC Classic asynchronous write functionality  Added .INI configuration parameter to update OPC tags on value change only  Enhanced ICCP/TASE.2 client writes and reads   v3.05 (Windows support only)    Added support for Secure Authentication for IEC 60870-5 (-101 and -104) according to the IEC 60870-5-7 standard based on the cyber-security standard IEC 62351 (Parts 3, 5, and 8)  Added capability to subscribe to GOOSE messages using external references from an SCL file when configuring an IEC 61850 server  Added capability to select which IED model to use from SCD file while when creating an IEC 61850 client connection to an IED or for creating a server model in the SDG  Added numerous INI parameters for enhanced customization for IEC 101/104, Modbus, DNP3, OPC, and IEC 61850 protocols  Added Read Range, Dual Register (32 bit integer/floating point), and Writeable Period Actions for Modbus Masters and Dual Registers (32 bit integer/floating point) for Modbus Slaves   v3.04.0025 (Windows support only)    New licensing options including direct support for Virtual Machines without a USB license  Read more about the license changes before upgrading  Significantly enhanced IEC 61850 functionality including: - Dataset validation for Report Control Blocks - Retry enable for Report Control Blocks - Manual Enable/Disable of Report Control Blocks - Manual reads of remote Server objects directly from MDO points - Support for Write-able points, including SP, CF, and SV functional constraints - Complete support for controls in Servers (controls already supported in Clients) - Better, more flexible support for handling IEC 61400-25 controls - Support for simultaneous close of IEC 61850 Clients   Provided several customization enhancements for IEC 60870-5, Modbus, and DNP protocols including more flexible control over when and how quality and time stamps change  Added better support and also greater customization for IEC 60870-5 -101 and -104 Slave reset requests  Added additional support to allow more flexible protocol to protocol mapping including new control mappings between IEC 61850 and IEC 60870-5 control types and OPC   v3.03.849 (Windows support only)    Improved connect/disconnect process and thread management to further improve stability  Added a configurable channel start delay to stagger the startup of channels  Improved SOE Queue interface to allow applications to capture Sequence of Events data from the SCADA Data Gateway  Added change event detection in equations to allow equations defined in the SCADA Data Gateway to be triggered based on input values changing  Enhanced shutdown performance for large gateway configurations  Added new OPC Alarms and Events options  Added full stack addressing for IEC 61850 and IEC 60870-6 (ICCP/TASE.2) for 7 layer stack address filtering and to allow multiple servers to use one port  Improved mapping support for IEC 61850 and IEC 60870-6 (ICCP/TASE.2) including better support of controls  Added support for Normalized Fixed Point ASDU Types MMENA and CSENA in IEC 60870-5-101 and -104   v3.03.782 (Windows support only)    InSight/OPC viewer component option added to SCADA Data Gateway  InSight allows users to create custom displays for data visualization  OPC viewer acts as an OPC Client to acces OPC Server data  Performance improvements and new INI parameters for OPC Server  Added IEC 62351 security for IEC 61850 Servers and Clients  Additional improvements to thread pool management for improved performance  Performance improvements for ODBC  Added the ability for pre-filter diagnostics for performance improvements  Added additional "ift" (if-then) equation  Fixed miscellaneous point mappings  Performance improvements when using numerous IEC 61850 Clients   v3.03.729 (Windows support only)    Added a thread pool to distribute load to multiple threads and improve utilization of multi-core processors  Added support for Dynamic Data Sets with array indexes  Optimized memory allocation further to improve the stability and robustness of the Gateway  Improved startup time and re-connect times for many protocols and scenarios that the Gateway supports  Added multiple features and capabilities to the IEC 61850, DNP3, OPC, and other protocol components of the Gateway (for example, the Gateway now supports Double Bit Binary Inputs for DNP3 and has increased support for Report Control blocks in IEC 61850)  Added mapping ability between IEC 60870-6 (ICCP/TASE.2) command mdo's and ODBC mdo's  Updated OpenSSL library to version 1.0.1g that protects against Heart Bleed vulnerability  Added diagnostics related to database updates and input processing  Added ability for polled point sets to be read as an array in 61850, significantly improving the reading of large polled point sets   v3.01.673 (Windows support only)    Added enhancements to ODBC Client, ChannelActiveControl for IEC 61850 Client   v3.01.661 (Windows support only)    Made improvements in memory handling for 61850 clients  Added protocol analyzer diagnostic messages for an OPC AE Client  Added additional statistics MDOs for 61850 clients for the request and response queues   v3.00   First public release with bidirectional support for IEC 60870-6 (ICCP/TASE.2) and IEC 61850 Server  Added IEC 61850 Ed. 2 support.   v2.54   Added IEC 61850 client, ICCP Server, and ODBC client   v2.48   Added support for OPC AE client   v2.42   Added support for DNP3 Data Sets and Device Attributes   v2.39   Successfully completed OPC certification and interoperability testing   v2.29   Successfully completed third-party DNP3 Conformance Testing   v2.24   Added support for mapping Slave points to other Slave points                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                             content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dtm-pages/protocol-mapping-testing
                                                   Request a Quote          DTM Video Playlist           Download Evaluation               Gateway Testing with DTM      Use Case        Gateways are commonly used to distribute controls and data across different parts of SCADA systems. These devices are often used to concentrate thousands of data points from hundreds of devices and translate messages across multiple protocols. Because of their large scope, testing gateways can be time consuming and error prone.      Today, many protocol mapping tests for gateways are performed manually. Each point is tested, one by one, to ensure data is mapped correctly. This manual approach may be required in final site acceptance testing, but preliminary configuration testing in the lab can be automated by simulating the field devices and SCADA system with valid protocol messages.                Gateway Testing using DTM  (Click image for larger view)                    Overview  Gateway Testing  Network Load Testing  Substation Communication Testing  Cybersecurity Testing  Architecture  What's New  System Requirements       Overview    Gateway Testing    Network Load Testing    Substation Communication Testing    Cybersecurity Testing    Architecture    What's New    System Requirements        Related Products           61850 Test Suite Pro (TSP)  SCL Navigator  Communication Protocol Test Harness (TH)                   This allows the real gateway to be tested within a simulated test system, which can automatically run through all data points and document the results of the protocol mapping. This approach is faster and more reliable than manual testing. In addition, it can allow testing to be performed earlier rather than waiting for the final system to be installed and configured.       Training Video                To learn how to automate testing of your SCADA Gateway, Data Concentrator or Protocol Converter, watch this short training video for step by step instructions.         Rapidly deploy accurate simulations of each IED within your SCADA network  Quickly specify thousands of point-to-point gateway tests across single or multiple protocols  Execute thousands of individual tests and view detailed results in real-time  Use both default and custom Gateway Test Actions  Quickly scale up test coverage by exporting Gateway Test Actions for several points to Excel and replicating to a large number of points         Example: IEC 61850 to DNP3 Mapping Verification           This example demonstrates how to test the mapping of a SCADA gateway or RTU in an automated test environment.  In this example, Distributed Test Manager (DTM) simulates multiple IEC 61850 IEDs and a single DNP3 SCADA Master. This simulates the real environment that the gateway can be tested within. Automated tests are generated by repeatable actions. DTM runs these automated tests in order to validate the configuration of the gateway and document the results. In this example, the Triangle MicroWorks SCADA Data Gateway is the device under test.  This example illustrates how DTM can be used to test partial systems without requiring all components in place and configured.  It also shows that this type of testing can be performed without expensive test equipment to generate signal changes.  This same test can be performed for standard SCADA protocols like ICCP/TASE.2, IEC 60870-5 (-101 or -104), DNP3, IEC 61850, and Modbus.       This video demonstrates using the Distributed Test Manager to validate mappings.      Ready to get started?      Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/images/default-source/product-images/sdg/web-based-config.png?sfvrsn=b29bfcea_0

URL: https://www.trianglemicroworks.com/testing-and-configuration-tools/61850-test-suite-pages
The resource you are looking for has been removed, had its name changed, or is temporarily unavailable.
URL: https://www.trianglemicroworks.com/download/product/?id=0c83fbf4-f597-4c0e-a0fe-238bcb005735&SRURL=6NP8S4mnwvW%2bRLgmoIILgKZfU5i0OCL7YNmHdaTAlpPhdgl92UK0earLL4YCBFYD6SBlSMNJh99SvIW%2f%2bn3RIA%3d%3d&ext=
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:11:55         
URL: https://www.trianglemicroworks.com/testing-and-configuration-tools/dnp3-forge-pages
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:11:59         
URL: https://www.trianglemicroworks.com/download/product/?id=547c85fd-e381-4ead-822a-47110d614c4f&SRURL=6NP8S4mnwvW%2bRLgmoIILgKZfU5i0OCL7YNmHdaTAlpPhdgl92UK0earLL4YCBFYD6SBlSMNJh99SvIW%2f%2bn3RIA%3d%3d&ext=
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:04         
URL: https://www.trianglemicroworks.com/testing-and-configuration-tools/iron-pages/overview
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:07         
URL: https://www.trianglemicroworks.com/download/product/?id=7f258b16-a405-4148-a3f9-af08887f428e&SRURL=6NP8S4mnwvW%2bRLgmoIILgKZfU5i0OCL7YNmHdaTAlpPhdgl92UK0earLL4YCBFYD6SBlSMNJh99SvIW%2f%2bn3RIA%3d%3d&ext=
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:11         
URL: https://www.trianglemicroworks.com/download/product/?id=4fdcbd72-296b-488d-bb19-6552f520e45f&SRURL=6NP8S4mnwvW%2bRLgmoIILgKZfU5i0OCL7YNmHdaTAlpPhdgl92UK0earLL4YCBFYD6SBlSMNJh99SvIW%2f%2bn3RIA%3d%3d&ext=
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:15         
URL: https://www.trianglemicroworks.com/source-code-libraries/iec-61850-scl-pages/-net-components
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:18         
URL: https://www.trianglemicroworks.com/download/product/?id=73c5b278-2620-4059-b912-0016910dd2a1&SRURL=6NP8S4mnwvW%2bRLgmoIILgKZfU5i0OCL7YNmHdaTAlpPhdgl92UK0earLL4YCBFYD6SBlSMNJh99SvIW%2f%2bn3RIA%3d%3d&ext=
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:22         
URL: https://www.trianglemicroworks.com/source-code-libraries/dnp-scl-pages/-net-components
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:25         
URL: https://www.trianglemicroworks.com/download/product/?id=07a2ff27-8dae-4b1b-a1f5-c38dbfbbf1b0&SRURL=6NP8S4mnwvW%2bRLgmoIILgKZfU5i0OCL7YNmHdaTAlpPhdgl92UK0earLL4YCBFYD6SBlSMNJh99SvIW%2f%2bn3RIA%3d%3d&ext=
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:29         
URL: https://www.trianglemicroworks.com/source-code-libraries/iec-60870-6/-net-components
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:33         
URL: http://www.trianglemicroworks.com/video/product-features
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:38         
URL: http://www.trianglemicroworks.com/training/protocol-training
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:49         
URL: https://www.trianglemicroworks.com/docs/default-source/referenced-documents/dtech-2016-demo-overview.png?sfvrsn=2
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:12:55         
URL: https://www.trianglemicroworks.com/training/training/free-webinars-2/securing-the-grid
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:13:01         
URL: https://www.trianglemicroworks.com/training/previous-webinars/61850-common-vision
       Upgraded to Sitefinity 14.4.8100.0  Go to site  Show details 0  0       Upgrade to Sitefinity 14.4.8100.0 failed    Initialization failed   Show details 0  0      Upgrading to Sitefinity 14.4.8100.0  System is initializing...  Show details 0  0      Please wait a moment  System is initializing...  Show details 0  0     Hide details   Upgrading your project to Sitefinity 14.4.8100.0  Please wait a moment    Upgraded to Sitefinity 14.4.8100.0  Go to site    Upgrade to Sitefinity 14.4.8100.0 failed  Initialization failed     0 error(s)   0 warning(s)        System is initializing...   Expand   09:13:08         
URL: http://www.trianglemicroworks.com/training/previous-webinars/scada-data-gateway-tips-techniques
                                               Tips & Techniques  Gateway Configuration Made Easy!   Free Webinar - Watch Now!  Previously held Thursday, November 16th, 2023 at 10am ET     Configuring your SCADA gateway is a crucial step in establishing communications between the SCADA Master and all other intelligent electronic devices (IEDs) in your substation. A well-configured SCADA gateway ensures the seamless flow of data and control signals between your field devices and the SCADA system. In this webinar, we will briefly review the fundamentals of our SCADA Data Gateway, then focus on advanced features with a live demonstration.  The goal of this webinar is to help simplify the configuration process and make your gateway set up more efficient.   WHAT YOU WILL LEARN:  Brief review of the following basic features from our previous webinar:    Restrict User Account capabilities with Role Based Access Control (RBAC)  Track users' actions using Audit Logging  Drag and drop to map multiple data points in one action  Import and export configurations and mappings  For a more detailed review of these and other fundamentals, watch our previously held webinar "Introducing SCADA Data Gateway v5.1 - Learn how to use the enhanced security & configuration functionality for new & existing users"   Learn about advanced configuration techniques such as:    Create protocol channels including OPC UA, IEC 61850, IEC 60870-5-104 and DNP3  Special Views to see performance metrics and system health  Define databases and mappings across multiple devices simultaneously  Use equations to accomplish complex mappings, such as scaling and alarms  Translate control operations across multiple protocols  Configure and view Sequence of Events (SOE) data logs     WHO SHOULD ATTEND:    Protection & Control Engineers/ Managers  System Integrators  Software Engineers/Managers  Security Professionals             Instructors:   Matthew Green Senior Application Engineer Triangle MicroWorks, Inc.   Matt is a Senior Applications Engineer at Triangle Microworks, bringing a decade of experience to the role. Matt serves as a Technical Trainer, providing direct customer support, conducting product demonstrations, providing one-on-one or group trainings as well as hosting webinars. Additionally, Matt works as a Quality Assurance Tester specializing in the company’s simulation tools and Gateway.    Jackson Moore Application Engineer Triangle MicroWorks, Inc.   Jackson received B.S. degrees in Electrical Engineering and Computer Engineering from North Carolina State University and has a background in power systems engineering. Prior to joining Triangle Microworks, Jackson spent five years as a Microgrid Systems Engineer, where he designed and developed load management control systems for multi-source microgrids ranging from 1MW to 30MW. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges our clients face.             Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/substation-design-testing
                                               IEC 61850 Substation Design & Testing  2 Part Webinar Series   Part 1 - IEC 61850 Substation Design Workshop  Previously held on August 10, 2023 at 10am EDT -  Watch Now!   Part 2 - Test Your Design - Simulating and Validating an IEC 61850 Substation Design  Previously held on September 21, 2023 at 10am EDT - Watch Now!    PROBLEM:  Substation design and testing is a notoriously complex and difficult task. Traditional design and documentation methods are tedious, requiring meticulous attention to detail and are often difficult to validate. New designs are costly to develop, difficult to test, and any templated approach must be manually tracked and maintained.   SOLUTION:  From its inception, the IEC 61850 standard has been designed as both a communication protocol , and an accompanying engineering process . This engineering process was designed to incorporate best practices and lessons learned from the previous century of utility substation engineering, while adding modern features such as standardization, interoperability, scalability, precise documentation, and ease of simulation, all while reducing engineering efforts and associated costs. However, to realize the promise of IEC 61850, powerful modern software tools are required. This two-part webinar series will use the following tools to Create , Simulate , and Validate an IEC 61850 Substation design:  Helinks STS, a power system engineering tool for specifying and implementing templated IEC 61850 substations.  Distributed Test Manager (DTM) , a simulation tool capable of simulating and validating IEC 61850 substation designs.   Part 1 - IEC 61850 Substation Design Workshop   What you will learn:    Describe the "Top-Down" IEC 61850 engineering process.  Demonstrate the engineering process in action, using Helinks STS to:   Rapidly create a complete Substation Specification Description (SSD) file from predefined bay templates  Convert our specification into a reality, by tying our SSD to instantiated devices, thereby creating our final product, the Substation Configuration Description (SCD) file       Part 2 - Test Your Design - Simulating and Validating an IEC 61850 Substation Design   What you will learn:    Simulate the substation SCD created in Part 1 using Distributed Test Manager (DTM)  Validate and visualize the signals required for the protection schemes which make up our substation        Sign up for each session below.  Click on the date:     Please note these webinars may be watched as a series or independently. You do not need to watch Part 1 to understand what is covered in Part 2.         Instructors:   Christoph Brunner President it4Power   Christoph Brunner is Utility Industry professional with over 30 years of industry experience across several areas within the Utility Industry and technologies from the Automation Industry. He is a well-known expert on IEC 61850 as he has been involved in the development of the standard since the beginning. He is president of it4power in Switzerland and has worked as a project manager at ABB Switzerland Ltd in the business area Power Technology Products in Zurich / Switzerland where he was responsible for the process close communication architecture of the substation automation system. He is convenor of the working group (WG) 10 of the IEC TC57 and member of WG 17, 18 and 19 of IEC TC57. He is IEEE Fellow, member of IEEE-PES and IEEE-SA. He is active in several working groups of the IEEE-PSRC (Power Engineering Society – Relay Committee) and member of the PSRC main committee and the subcommittee H. He is guest professor at Shangdong University of technology and international advisor to the board of the UCA international users group.    Jackson Moore Application Engineer Triangle MicroWorks, Inc.   Jackson received B.S. degrees in Electrical Engineering and Computer Engineering from North Carolina State University and has a background in power systems engineering. Prior to joining Triangle Microworks, Jackson spent five years as a Microgrid Systems Engineer, where he designed and developed load management control systems for multi-source microgrids ranging from 1MW to 30MW. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges our clients face.             Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/virtualisolation
                                               Virtual Isolation & Testing with IEC 61850   Free Webinar - Watch Now! Previously held Thursday, January 25, 2024 at 10am ET   PROBLEM:  In a traditional substation with copper wires, testing is straightforward. You can isolate the section to test attempting to avoid effecting the entire system, hook up the test equipment and physically perform a manual test. However, this involves a tremendous amount of manual effort, intervention & modification of the entire system which is ultimately inefficient and creates a high risk of multiple safety issues.  SOLUTION:  With the introduction of IEC 61850 into a substation, there are many features defined by the standard to eliminate the manual effort with Virtual Isolation.  This will increase efficiency & accuracy of testing while greatly reducing safety risks.  The transition from physical isolation to virtual isolation requires new tools to visualize what signals are being processed, injected by test equipment, or blocked.  Automated test sequences performed by these tools can be validated on a simulated substation and prove that no unexpected results occur.  Portions of the simulated substation can be replaced by critical protection equipment to verify they will respond correctly to signals generated by both the test tool and other IEDs in the simulated substation.    What you'll learn:    How to increase confidence in Virtual Isolation testing before entering the substation  IEC 61850 features for managing virtual isolation  Different testing modes of an IEC 61850 Logical Node  How to manage reception of simulated GOOSE streams  Understanding the impact of blocked behavior    How LGOS is used to monitor the status of GOOSE subscriptions  Real life examples of how to perform virtual isolation testing using 61850 Test Suite Pro on a simulated substation in Distributed Test Manager (DTM)   Monitor real and simulated signal flow  Set test mode in IEDs  Configure IEDs to receive a simulated GOOSE stream  Publish simulated GOOSE Streams  Identify what signals are being processed, injected by test equipment, or blocked  Automate test setup and validation       Who should attend this course:    Protection & Control Engineers/ Managers  Maintenance personnel  System Integrators  Software engineers/managers  Substation Field Technicians  Anyone that is open to adopting a standardized process to save time & money               Instructors:    Christoph Brunner President it4Power    Christoph Brunner has graduated as electrical engineer at the Swiss Federal Institute of Technology in 1983. He is Utility Industry professional with over 30 years of industry experience with both knowledge across several areas within the Utility Industry and of technologies from the Automation Industry. He is a well-known expert on IEC 61850 as he has been involved in the development of the standard since the beginning. He is president of it4power in Switzerland, a consulting company to the power industry. As such, he has been consultant in many projects for substation automation and projects involving IEC 61850. He has worked as a project manager at ABB Switzerland Ltd in the business area Power Technology Products in Zurich / Switzerland where he was responsible for the process close communication architecture of the substation automation system. He is convenor of the working group (WG) 10 of the IEC TC57 and member of WG 17, 18 and 19 of IEC TC57. He is IEEE Fellow, member of IEEE-PES and IEEE-SA. He is active in several working groups of the IEEE-PSRC (Power Engineering Society – Relay Committee) and member of the PSRC main committee and the subcommittee H. He is guest professor at Shangdong University of technology and international advisor to the board of the UCA international users group.   Jackson Moore  Applications Engineer  Triangle MicroWorks, Inc.   Jackson received B.S. degrees in Electrical Engineering and Computer Engineering from North Carolina State University and has a background in power systems engineering. Prior to joining Triangle Microworks, Jackson spent five years as a Microgrid Systems Engineer, where he designed and developed load management control systems for multi-source microgrids ranging from 1MW to 30MW. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges our clients face.       Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/intro-v5.1
                                               Introducing SCADA Data Gateway v5.1 More Secure & Easier to Configure   Learn how to use the enhanced security & configuration functionality for new & existing users    Free Webinar  - Watch Now! Previously held Thursday, October 27th, 2022 at 10 AM EDT     Triangle MicroWorks' SCADA Data Gateway has been used globally in more than 70 countries for over 20 years. The latest release v5.1 is built on our extremely reliable platform which includes support for both Windows and Linux with many new features to increase ease of configuration and flexibility. This version of the SCADA Data Gateway has been redesigned to incorporate a web-based user interface and now provides for user management and user roles. The user interface has a familiar look and feel from previous versions, so the learning curve is minimal. In addition, the browser-based interface allows for remote configuration and monitoring of gateway operations.  This webinar will teach you how to use the enhanced security and configuration functionality for both new & existing users. We will review & demo the new security features in detail as well as the enhanced configuration features to help minimize the learning curve of using the redesigned gateway. We will wrap up with identifying the available resources to help with the migration from the legacy gateway v4 to v5.1 for anyone that is a current user of the SCADA Data Gateway.    WHAT YOU'LL LEARN:     Review the similarities and differences between v4 and v5.1 SCADA Data Gateway  Detailed review of enhanced security features including a demo of managing user roles and audit logging.  Learn how to configure and manage workspace. Demos will include step by step of the following:  How to create a devices  Import, export, drop/drop multiple data points  Search & Sort features  Search node feature  Special views to monitor performance, health and warnings  Log filters  And more…    Learn how to migrate from v4 to v5.1 easily .    WHO SHOULD ATTEND:    Protection & Control Engineers/ Managers  System Integrators  Software Engineers/Managers  Security Professionals               Instructors:     Matthew Green Senior Applications Engineer Triangle MicroWorks, Inc.    Matt is a senior applications engineer here at Triangle MicroWorks. He’s been with Triangle for 6 years and specializes QA testing with all our simulation tools and gateway. Matt also works as a technical trainer on Triangle's software tools, creates training videos, and participates in product demos for customers and conferences.     Check out and view recordings of our previous webinars!                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/charting-a-course-to-a-digital-substation
                                               Charting a Course to a Digital Substation  How to Integrate IEC 61850 in Your Existing Solutions  3 Part Series Webinar  Part 1 - An Introduction & Comparison of DNP3 and IEC 61850  Previously held on April 13th at 10am EDT -  Watch Now!   Part 2 - IEC 61850 SCL and the Engineering Process  Previously held on May 4th at 10am EDT - Watch Now!   Part 3 - Next Steps - Strategies for Integration of DNP3 and IEC 61850  Previously held on May 24th at 10am EDT - Watch Now!    DNP3 and IEC 61850 are two widely used communication protocols in the electrical power industry. While both protocols serve similar purposes, they have distinct differences in their design, implementation, and capabilities.  DNP3 (Distributed Network Protocol 3) is a communication protocol used for vertical SCADA communication, typically used from a control center through a data concentrator to various types of data acquisition and control equipment . It provides control of process equipment and event-driven data acquisition with quality flags.  The IEC 61850 standard similarly includes a vertical SCADA protocol, based on MMS, and expands its scope with the following:   A semantic data model  An integrated engineering process based on XML files for system design  High-speed GOOSE messaging to replace control wiring between IEDs  Sampled Measured Value messaging to replace the wiring of analog data and eliminate redundant sensors such as CTs and PTs   This 3-part webinar series will cover the following:   Review of the similarities and differences between DNP3 and IEC 61850 at both a protocol and functional level  Demonstrations of both IEC 61850 and DNP3 in action using real-world examples  Introduction to the top-down IEC 61850 engineering process   Review of strategies for migration and integration of IEC 61850 into a DNP3-based system    Part 1 - An Introduction and Comparison for DNP3 and IEC 61850   What you will learn:    Overview of design principles and architecture of traditional power system SCADA communication protocols, like DNP3/IEEE 1815  Compare and contrast DNP3 with IEC 61850 at a protocol level:   Event driven transmission  Client/Server vs Master/Outstation  Configuration and control of where data is sent  Object-oriented structure vs individual points   Compare and contrast DNP3 with IEC 61850 at a functional level:   Review semantic data models and related concepts  ICD files vs a points list  Control wiring is replaced by GOOSE messages, allowing animated signal flow diagrams instead of prints   Demonstrations of both protocols in action using real-world examples     Part 2 - IEC 61850 SCL and the Engineering Process   What you will learn:    Introduction to the Top-Down IEC 61850 Engineering Process  Review recent and upcoming extensions to the IEC 61850 standard  A detailed look at the recently enhanced “Specification Stage” of the Engineering Process, including:   Specification independent of devices  Defining detailed data requirements  Defining protection and control schemes  Simulation and validation of design during the specification stage   Best practices when creating SCD files  Live demonstration of concepts using a simulated IEC 61850 substation    Part 3 - Next Steps: Strategies for Integration of DNP3 and IEC 61850    What you will learn:   Multiprotocol Device Approach   GOOSE replaces wires  DNP3 communications to the SCADA Master   Applying the IEC 61850 Engineering Process to a DNP3-based system with traditional control wiring   Leverage the IEC 61850 Top-Down Engineering Process in brownfield systems using communication media that does not support 61850  Document traditional control wiring   Using the IEC 61850 Mapping information contained in a DNP XML Device Profile   Auto-configure gateways  Configure the DNP3 Point Map for an IED or the Master it reports to, based on the Report Control Blocks created during the IEC 61850 Engineering Process  Configure the DNP3 Report by Exception Class assignments for an IED based on the Report Control Blocks created during the IEC 61850 Engineering Process        Credit: Murrstock - stock.adobe.com  Copyright: ©Murrstock - stock.adobe.com   Sign up for each session below.  Click on the date:     Please note these webinars may be watched as a series or independently. You do not need to watch Part 1 to understand what is covered in Part 2 or 3.        Instructors:   Christoph Brunner President it4Power   Christoph Brunner has graduated as electrical engineer at the Swiss Federal Institute of Technology in 1983. He is Utility Industry professional with over 30 years of industry experience with both knowledge across several areas within the Utility Industry and of technologies from the Automation Industry. He is a well-known expert on IEC 61850 as he has been involved in the development of the standard since the beginning. He is president of it4power in Switzerland, a consulting company to the power industry. As such, he has been consultant in many projects for substation automation and projects involving IEC 61850. He has worked as a project manager at ABB Switzerland Ltd in the business area Power Technology Products in Zurich / Switzerland where he was responsible for the process close communication architecture of the substation automation system. He is convenor of the working group (WG) 10 of the IEC TC57 and member of WG 17, 18 and 19 of IEC TC57. He is IEEE Fellow, member of IEEE-PES and IEEE-SA. He is active in several working groups of the IEEE-PSRC (Power Engineering Society – Relay Committee) and member of the PSRC main committee and the subcommittee H. He is guest professor at Shangdong University of technology and international advisor to the board of the UCA international users group.  Jackson Moore Application Engineer Triangle MicroWorks, Inc.   Jackson received B.S. degrees in Electrical Engineering and Computer Engineering from North Carolina State University and has a background in power systems engineering. Prior to joining Triangle Microworks, Jackson spent five years as a Microgrid Systems Engineer, where he designed and developed load management control systems for multi-source microgrids ranging from 1MW to 30MW. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges our clients face.             Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/easy-button-for-iec-61850-security
                                               The Easy Button for IEC 61850 Security  Authentication & Encryption for Routable GOOSE & Sampled Values    Free Webinar - Watch Now! Previously held on Wednesday, June 29, 2022 at 10 AM EDT     PROBLEM:   Layer 2 GOOSE has long allowed for event-based communication within a substation. As emerging 61850 use cases require communications to extend beyond a single substation, the need for a secure and routable solution has emerged. Secure Routable GOOSE (R-GOOSE) and Routable Sampled Values (R-SV) satisfies the security and communications requirements of the utility industry.  SOLUTION:   Secure R-GOOSE/R-SV requires that publishers and subscribers share a common symmetric key, which is distributed via a mechanism specified in IEC 62351-9. The engineering process of IEC 61850 allows for the configuration of publication groups, destination addresses for the publications, and addresses for key manage­ment for that group. The System Configuration Tool (SCT) stores this information in an SCL file, which can be leveraged by the Garibaldi Key Distribution Center (KDC) to simplify security configuration.  When integrated with an IEC 61850 device, Triangle MicroWork's KDC Client Source Code Library enables direct communication with a KDC Server, such as Garibaldi, for key exchange. These keys are then used by the IEC 61850 Source Code Library in GOOSE and Sampled Values messages that are configured for authentication or encryption. Leveraging the IEC 61850 & KDC Source Code Li­braries will provide ‘out of the box ’ support for Secure R-GOOSE/R-SV.  WHAT YOU'LL LEARN:     Why it's important to use R-GOOSE & R-Sample Values and the need for security and review of real-life use cases  Learn about Key & Policy Exchange creating a Security Management Plane  Learn about Pull and Push Re-synchronization as defined by IEC 62351-9  Discuss IEC 61850 Multicast Security in Real Time Plane  How to configure Garibaldi Key Distribution Center using SCL to save time & minimize configuration mistakes  Learn the impacts of security on a simulated system using Distributed Test Manager (DTM)  Learn how Authentication and Encryption is used to protect against Spoofing, Snooping, and other Man in the Middle Attacks    WHO SHOULD ATTEND:    System Integrators   Software engineers   IT Network engineers  IT/OT Cyber security personnel  Anyone interested in IEC 61850 automation & security                 Instructors:     Mark Adamiak Principle Consultant Adamiak Consulting LLC    Mark Adamiak is an independent consultant for the electric power industry.  Mark started his career in the utility business with American Electric Power (AEP) and in mid-career, joined General Electric where his activities have ranged from advanced development, product planning, application engineering, and system integration in the Protection and Control industry.  Mr. Adamiak is an original member of the IEC61850 WG, a Life Fellow of the IEEE, a registered Professional Engineer in the State of Ohio and a GE Edison award winner.  Mark was the Principal Investigator for the EPRI IntelliGrid project to develop a reference architecture for the Smart Grid. In 2012, Mr. Adamiak was elected to the US National Academy of Engineering.   Herb Falk Managing Director Outside The Box Consulting Services   Mr. Falk has over 40 years of experience working in the automation, information exchange, standardization activities, and systems integration.  Previous employment was Westinghouse Numa-Logic and SISCO where he was responsible for 61850 and CIM integration projects and products.  Mr. Falk has been involved with IEC 61850 technology since 1982 and cyber security since 1993.  Mr. Falk is an editor of IEC 61850-8-1, editor of several cyber security standards, and is the Vice President of Testing for the UCA International Users Group (UCA Iug) . His work for His work for the UCA Iug has involved the coordination of IEC 61850 interoperability testing in 2011, 2013, 2015, and 2017.    Jackson Moore  Applications Engineer  Triangle MicroWorks, Inc.  Jackson received B.S. degrees in Electrical Engineering and Computer Engineering from North Carolina State University and has a background in power systems engineering. Prior to joining Triangle Microworks, Jackson spent five years as a Microgrid Systems Engineer, where he designed and developed load management control systems for multi-source microgrids ranging from 1MW to 30MW. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges our clients face.          Check out and view recordings of our previous webinars!      Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                           Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/automate-testing
                                               Automate Testing of your SCADA Gateway, Data Concentrator or Protocol Converter   Free Webinar   This webinar features the new Action Based Test Manager now available in DTM v1.6.      Previously held Wednesday, June 28th, 2023    PROBLEM:  Data Concentrators and Gateways are commonly used to aggregate device communications in complex SCADA systems. Often mapping thousands of data points from hundreds of IEDs, and frequently converting between various communication protocols, configuration is notoriously complex.    To ensure correct configuration, verification testing of point mapping and behavior is commonly required for customer acceptance. Due to scale and complexity, this testing has traditionally been performed manually. Time consuming and prone to error, each point must be tested one by one.  Complexity increases further when protocol conversion is utilized, as characteristics or capabilities unique to one protocol can be translated in unexpected ways which are difficult to test using traditional methods.  SOLUTION:   Using the Gateway Test Action Manager Module, a powerful new module within Distributed Test Manager (DTM), this testing can now be largely automated. With DTM, data mapping and protocol conversion are tested faster and more reliably than possible via traditional methods. Test coverage can be quickly scaled to thousands of points across datatype, protocol, and devices, and configured tests can be run repeatedly for regression testing, an impracticality using conventional validation methods. Without the need for physical IEDs, testing may now be performed at any stage in the engineering process.  WHAT YOU'LL LEARN:     Rapidly deploy accurate simulations of each IED within your SCADA network  Quickly specify thousands of point-to-point gateway tests across single or multiple protocols, i.e. DNP3 , IEC 61850 , IEC 60870-5 -101, IEC 60870-5-104 , IEC 60870-6(TASE.2/ICCP) , and Modbus , etc.  Execute thousands of individual tests and view detailed results in real-time  Use both default and custom Gateway Test Actions  Quickly scale up test coverage by exporting Gateway Test Actions for several points to Excel and replicating to a large number of points              Instructors:     Matthew Green Senior Applications Engineer Triangle MicroWorks, Inc.    Matt is a senior applications engineer here at Triangle MicroWorks. He’s been with Triangle for 6 years and specializes QA testing with all our simulation tools and gateway. Matt also works as a technical trainer on Triangle's software tools, creates training videos, and participates in product demos for customers and conferences.     Jackson Moore  Applications Engineer  Triangle MicroWorks, Inc.  Jackson received B.S. degrees in Electrical Engineering and Computer Engineering from North Carolina State University and has a background in power systems engineering. Prior to joining Triangle Microworks, Jackson spent five years as a Microgrid Systems Engineer, where he designed and developed load management control systems for multi-source microgrids ranging from 1MW to 30MW. In his current role of Application Engineer, Jackson serves as a bridge between our customers and our development team, seeking to understand and solve the unique and complex challenges our clients face.   WHO SHOULD ATTEND:    Software engineers  Software engineering managers  Substation Field Technicians   Anyone who wants to save time & money when deploying substations        Check out and view recordings of our previous webinars!        Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/61850-tools-techniques
                                               IEC 61850 Tools & Techniques:  Simulation of a Specification for a Digital Substation   Free Webinar - Watch Replay Now! Previously held on Thursday, December 2, 2021 - 10 AM EST     Increased energy needs and the demand for renewable energy resources continues to place pressure on the utility industry to evolve & adapt. As an industry that is historically slow to change, flexible power systems, efficient project realization, and interoperability are the critical requirements for solving the energy needs of tomorrow.    Over the last decade, several projects and solutions have been developed to address this growing need. Leading among these is OSMOSE ( O ptimal S ystem- M ix O f Flexibility S olutions for E uropean Electricity), a consortium led by RTE and composed of 33 partners from around Europe. This webinar will focus on using a vendor neutral tool, Distributed Test Manager, to utilize the improvements to interoperability and efficiency in the IEC61850 engineering process, as learned from OSMOSE. Specifically, on bridging the gap in the development lifecycle of digital substation between the specification stage and the simulation stage.  WHAT YOU'LL LEARN:     Overview of the OSMOSE Project & their goals specific to interoperability  A user’s perspective on the benefits of a specification, using the extensions defined in OSMOSE, and the ability to do a simulation of the specification.  An overview of the IEC 61850 specification aspect of the Engineering Process and its benefits  A live simulation of an IEC 61850 design specification in Distributed Test Manager (DTM)  Learn how to verify flexible power system concepts via simulation before investing in hardware, using IEC 61131 Structured Text and Function Block Diagrams.                    Instructors:    Christoph Brunner President,  it4Power   Christoph Brunner is Utility Industry professional with over 30 years of industry experience across several areas within the Utility Industry and technologies from the Automation Industry. He is a well-known expert on IEC 61850 as he has been involved in the development of the standard since the beginning. He is president of it4power in Switzerland and has worked as a project manager at ABB Switzerland Ltd in the business area Power Technology Products in Zurich / Switzerland where he was responsible for the process close communication architecture of the substation automation system. He is convenor of the working group (WG) 10 of the IEC TC57 and member of WG 17, 18 and 19 of IEC TC57. He is IEEE Fellow, member of IEEE-PES and IEEE-SA. He is active in several working groups of the IEEE-PSRC (Power Engineering Society – Relay Committee) and member of the PSRC main committee and the subcommittee H. He is guest professor at Shangdong University of technology and international advisor to the board of the UCA international users group.     Thomas Sterckx Expert - IEC 61850 Engineering Elia Engineering   Thomas has been working at Elia for more than 7 years, with a focus on IEC61850 engineering and modeling the last years. He is also a member of the IEC61850 WG10 and Cigre B5.68.     Matthew Green Senior Applications Engineer Triangle MicroWorks, Inc.    Matt is a senior applications engineer here at Triangle MicroWorks. He’s been with Triangle for 6 years and specializes QA testing with all our simulation tools and gateway. Matt also works as a technical trainer on Triangle's software tools, creates training videos, and participates in product demos for customers and conferences.                Check out and view recordings of our previous webinars!      Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/strategies-for-upgrading-to-ed2
                                               Strategies for Upgrading to IEC 61850 Edition 2.1  Learn How to Prepare & Test   Free Webinar - Watch Replay Now! Previously held on Thursday, October 7, 2021 - 10 AM EDT   In July, Triangle MicroWorks hosted and participated in a webinar including UCA International User Group (UCAIug), DNV Energy Systems, and Elewit from Red Electrica Group to present the upcoming changes involved with IEC 61850 Ed. 2.1. With these changes comes new conformance test procedures and policies for Ed 2.0 & 2.1 conformance testing. Click here to watch the recorded webinar from July.      In this upcoming webinar, we will briefly review those topics with a focus on migration strategies and how to prepare for the update to Ed 2.1. The best way to prepare for a successful migration is by testing in advance of implementing the update to Ed 2.1. Testing can help you understand any challenges, minimize the risks involved and uncover any issues early on for planning purposes.  WHAT YOU'LL LEARN:     Brief review of July’s webinar on upcoming changes & recommended migration strategies to IEC 61850 Ed 2.1  An overview of how to test for IEC 61850 Ed 2.1 to prepare your systems  Learn how to test Report Control Block Reservation with Distributed Test Manager (DTM) and inspect clients to ensure conformance  Learn how to simulate & test IEC 61850 Ed 2.1 servers for conformance to the new Ed 2.1 changes  Learn how to test each server editions (Ed 1, Ed 2, and Ed 2.1) and validate the difference and behaviors with Test Suite Pro(TSP)                 Instructor:    Joel Greene Director, IEC 61850 Development Triangle MicroWorks , Inc.   Joel has worked with communication protocols for over 15 years with the last 6 years in the power industry. He is the lead engineer for IEC 61850 development & products at Triangle MicroWorks and is heavily involved with 61850 industry groups. He also works as the co-editor of IEC 61850 7-2. He is a member of the UCA Test Committee and Test Procedures Working Group and involved in IEEE Standard Association.           Check out and view recordings of our previous webinars!      Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/fder-solutions
                                               DER SOLUTIONS:  Enabling Interoperable SCADA Communications for PV Inverters through Embedded Controllers  Free Webinar - Watch Replay NOW! Previously held on Thursday, August 26, 2021 - 11 AM - 12 noon EDT    PROBLEM: The percentage integration of Distributed Energy Resources( DER ), specifically photovoltaic ( PV ) inverters in the field, has increased significantly in the past 5 years. Regardless of the size of the PV plants and the inverters (residential vs . commercial), it is becoming crucial that these devices have the capability to communicate with peers (other smart devices) and with components that are at a hierarchy above the inverters (e.g., supervisory control and data acquisition ( SCADA ) systems, distributed controllers, and data managers).  SOLUTION:   National Renewable Energy Laboratory’s ( NREL ’s) with support from Triangle MicroWorks , developed standard SCADA software code for inverters’ embedded controllers that will enable interoperability with other components in the system. This was achieved by using code from two different protocols: Distributed Network Protocol 3 ( DNP3 ) and International Electrotechnical Commission ( IEC ) 61850 . The developed code is aimed to be deployed in simple embedded controllers. It will be tested in the NREL ’s Energy Systems Integration Facility. The tested code will then be made available through Triangle MicroWorks ( TMW ’s) software platform.  The focus of this webinar is to show part of the larger work, specifically how to enable IEC 61850 server & client communication on a PV inverter.   WHAT YOU'LL LEARN:    Overview & objective of the project for interoperable  SCADA protocol, specifically IEC 61850, for PV inverters  Review SmartGrid support functions on PV inverters, i.e. VoltVAR , etc. and how SCADA can update these points on a PV inverter  Learn how to design an ICD file using IEC 61850 7-420 modeling with SCL Navigator to implement SmartGrid support functions so you can configure the settings in real time as required for SmartGrid integration according to IEEE 1547  Where to find a full ICD file for support of SmartGrid functions  Demonstration of data exchange between BeagleBone board & simulated 61850 Client using Test Suite Pro ( TSP )   Import ICD file into 61850 Client & Server to update & change points.  Client & Server communications  Measurements using MMXU from server to client  One inverter function update from client to server      How to access the technical report & get involved in future projects                 Instructors:   Joel Greene Director, IEC 61850 Development Triangle MicroWorks , Inc.   Joel has worked with communication protocols for over 15 years with the last 6 years in the power industry. He is the lead engineer for IEC 61850 development & products at Triangle MicroWorks and is heavily involved with 61850 industry groups. He also works as the co-editor of IEC 61850 7-2. He is a member of the UCA Test Committee and Test Procedures Working Group and involved in IEEE Standard Association.    Kumaraguru  Prabakar Senior Research Engineer National Renewable Energy Laboratory ( NREL )   Kumaraguru received the M.S. Degree from Arizona State University, Tempe , AZ, in 2011, Ph.D. degree from The University of Tennessee, Knoxville, TN, in 2016. and the M.B.A. degree from the University of Colorado, Boulder in 2021. He is a Senior Research Engineer with Power Systems Engineering center, the National Renewable Energy Laboratory, Golden, Colorado. He leads research projects targeting improvements in distribution system protection, and interoperability of distribution system assets. He is a Technical contributor in multiple microgrid controller evaluation projects, and advanced distribution management systems evaluation projects. His research work focuses on the controller hardware-in-the-loop, power hardware-in-the-loop, and remote hardware-in-the-loop experiments.     Deepthi  Vaidhynathan Research Engineer National Renewable Energy Laboratory ( NREL )   Deepthi received an M.S. Degree in Electrical Engineering from the University of Colorado at Boulder, CO in 2015. She is a Research Engineer with the Computational Science Center at the National Renewable Energy Laboratory, Golden, Colorado. Her interests are in energy system integration, grid modeling and simulation, performance optimization for scientific software.        Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/Gateway
                                               What is a Data Gateway in the SCADA world?   Free Webinar - Watch Replay Now! Presented Thursday, March 25, 2021 - 10 AM to 11AM EDT   A gateway is a powerful tool that combines the ability to collect, interpret, translate, and deliver data coming from multiple downstream sources. It simplifies workflow, integrates data from multiple data sources, and adds a layer of security within one application.  Both data concentration and protocol translation are necessary functions in the utility industry to be able to work with the wide variety of protocols used around the globe. Our industry is reliant on gateways and protocol converters to manage & simplify these complex functions. But how is it different than a router? How can this be done across different SCADA protocols with a gateway?  This webinar will look at how data gateways are used in SCADA. We will look at the functions a gateway provides and discuss configurations for data concentrators and protocol converters. Using our SCADA Data Gateway, we’ll demonstrate how to configure a gateway in real world scenarios using protocols like DNP3 , IEC 60870-101 & 104 , IEC 61850 , IEC 60870-6(TASE.2/ICCP) , Modbus and OPC.  You’ll learn how to set up:   Data concentration of SCADA protocols  Different data types by protocol  Protocol conversion and data mapping  Data collection using:  Polling  Event polling  Data Sets  Dynamic Data Sets    Commands for different protocols and how they are mapped     Who should attend this course:    Protection & Control Engineers/ Managers  System Integrators  Software Engineers/Managers  Security Professionals                Instructors:   Matt Green  Senior Applications Engineer Triangle MicroWorks, Inc.    Matt is a senior applications engineer here at Traingle MicroWorks. He’s been with Triangle for about 6 years and specializes QA testing with all our simulation tools and gateway. Matt is also one of our lead product trainers and presenter for webinars and customer demos.       Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/virtual-isolation-testing-61850
                                               Virtual Isolation & Testing with IEC 61850   Free Webinar - Watch Replay Now! Previously held on Thursday, February 18, 2021 - 10 AM to 11am EST  PROBLEM:  In a traditional substation with copper wires, testing is straightforward. You can isolate the section to test attempting to avoid effecting the entire system, hook up the test equipment and physically perform a manual test. However, this involves a tremendous amount of manual effort, intervention & modification of the entire system which is ultimately inefficient and creates a high risk of multiple safety issues.  SOLUTION:  With the introduction of IEC 61850 into a substation, there are many features defined by the standard to eliminate the manual effort with Virtual Isolation.  This will increase efficiency & accuracy of testing while greatly reducing safety risks.  The transition from physical isolation to virtual isolation requires new tools to visualize what signals are being processed, injected by test equipment, or blocked.  Automated test sequences performed by these tools can be validated on a simulated substation and prove that no unexpected results occur.  Portions of the simulated substation can be replaced by critical protection equipment to verify they will respond correctly to signals generated by both the test tool and other IEDs in the simulated substation.    What you'll learn:    How to increase confidence in Virtual Isolation testing before entering the substation  IEC 61850 features for managing virtual isolation  Different testing modes of an IEC 61850 Logical Node  How to manage reception of simulated GOOSE streams  Understanding the impact of blocked behavior    How LGOS is used to monitor the status of GOOSE subscriptions  Real life examples of how to perform virtual isolation testing using 61850 Test Suite Pro on a simulated substation in Distributed Test Manager (DTM)   Monitor real and simulated signal flow  Set test mode in IEDs  Configure IEDs to receive a simulated GOOSE stream  Publish simulated GOOSE Streams  Identify what signals are being processed, injected by test equipment, or blocked  Automate test setup and validation       Who should attend this course:    Protection & Control Engineers/ Managers  Maintenance personnel  System Integrators  Software engineers/managers  Substation Field Technicians  Anyone that is open to adopting a standardized process to save time & money                Instructors:    Joel Greene Director, IEC 61850 Development Triangle MicroWorks, Inc.   Joel has worked with communication protocols for over 20 years with the last 10 years in the power industry. He is the lead engineer for IEC 61850 development & products at Triangle MicroWorks and is heavily involved with 61850 industry groups. He also works as the co-editor of IEC 61850 7-2. He is a member of the UCA Test Committee and Test Procedures Working Group and involved in IEEE Standard Association.   Matt Green  Senior Applications Engineer Triangle MicroWorks, Inc.    Matt is a senior applications engineer here at Traingle MicroWorks. He’s been with Triangle for about 6 years and specializes QA testing with all our simulation tools and gateway.       Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/free-webinars
                                               A Holistic Approach to Industrial Cybersecurity  Detecting Cyber Attacks in Substation Networks    Free Webinar - Watch Replay Now!    Previously held on Thursday, December 10, 2020 - 10AM to 11AM EST   Security is a critical requirement in modern SCADA systems. Interest & awareness of cybersecurity has increased with the looming deadlines by North American Electric Reliability Corporation Critical Infrastructure Protection (NERC CIP) standards increasing the urgency for the electric power industry to address this need. The good news is existing standard communication protocols like IEC 60870-5-104 and IEC 61850 have security capabilities built into to them and are used by SCADA devices today. This existing infrastructure can be leveraged as a major step towards meeting this requirement.   PROBLEM:  Energy companies typically operate across vast, geographically dispersed sites and rely on a high volume of complex devices and systems from multiple vendors. Such conditions can make accurately identifying and managing all assets in these OT environments exceedingly challenging. This coupled with the challenge of testing all the system components prior to deployment can further delay that the appropriate security requirements are in place in the final system.  SOLUTION:  Understanding the security in existing SCADA protocols like IEC 60870-5-104 and IEC 61850 along with utilizing the right tools can help overcome security challenges posed by IT/OT convergence and the expansion of remote workforces. Many challenges can be overcome by verifying secure communications are configured correctly at the system level.  By simulating system components, configuration issues can be identified earlier to ensure security requirements will be satisfied in the final system.  What you'll learn:   Understand the history of malware, specifically Industroyer , the technical details on how it operates and its objectives in the OT network.  Using Claroty Continuous Threat Detection (CTD) Platform:  Learn how to extend the same controls IT security teams utilize for minimizing risk in IT environments to OT environments.  Utilize Passive, Active, and AppDB scanning capabilities to provide comprehensive OT visibility and asset management controls.  Learn how to automatically profile all assets, communications, and processes in OT networks, generate a behavioral baseline that characterizes legitimate traffic and weeds out false positives, and alerts users in real-time to anomalies and both known and zero-day threats.  Automatically map and virtually segment OT networks into Virtual Zones, which are logical groups of assets that communicate with one another under normal circumstances.  Learn how to compare each asset in an OT environment to an extensive database of vulnerabilities and exposures (CVE) data from the National Vulnerability Database allowing you to identify, prioritize, and remediate vulnerabilities in OT environments more effectively.    How to simulate messages from IEC 60870-5-104 and IEC 61850 including MMS & GOOSE, using Distributed Test Manager (DTM ) to show how substation network monitoring software can be used to learn valid packets and then detect bad packets and invalid IEC 62351 authentication messages.  How to simulate a cyber attack on a substation including full network IP discovery followed by malicious controls.       Who should attend this course:    Cyber security personnel  Compliance Managers  Protection & Control Engineers/ Managers  System Planners  Maintenance personnel  Utility IT personnel  Operations personnel                  Instructors:   Sharon Brizinov Vulnerability Research Team Lead Claroty   Sharon is an experienced security researcher, who specializes in vulnerability research, malware analysis, network forensics, and ICS security. Sharon's work is highly appreciated in the cyber community, and in the last few years he was invited to speak at leading cybersecurity conferences such as DEFCON and ICS-Village.   Claroty bridges the industrial cybersecurity gap between information technology (IT) and operational technology (OT) environments. Organizations with highly automated production sites and factories that face significant security and financial risk especially need to bridge this gap. Armed with Claroty’s converged IT/OT solutions, these enterprises and critical infrastructure operators can leverage their existing IT security processes and technologies to improve the availability, safety, and reliability of their OT assets and networks seamlessly and without requiring downtime or dedicated teams. The result is more uptime and greater efficiency across business and production operations.  Matthew Green Senior Applications Engineer Triangle MicroWorks, Inc.   Matt is a senior applications engineer here at Traingle MicroWorks. He’s been with Triangle for about 6 years and specializes QA testing with all our simulation tools and gateway.             Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/debugging-goose-streamline-troubleshooting-system-validation
                                               Debugging GOOSE  S treamline Troubleshooting & System Validation  Free Webinar - Watch Replay NOW!   Previously held on Wednesday, September 16, 2020 - 12 NOON to 1PM EDT   One of the major advantages provided by IEC61850 is the utilization of GOOSE messages. GOOSE messaging provides broadcast communication from one IED to many other IEDs. The GOOSE service has been implemented in order to replace the traditional signal wiring in the substation providing several advantages:   A GOOSE message can carry more information such as quality and time stamp.  Continuous supervision of GOOSE messages provides immediate notification of broken links.  Scheme redesign can be made in software and does not need expensive physical rewiring.  Reduce cost associated with traditional wiring.  Achieve more comprehensive automated testing.    PROBLEM:  As hard-wired schemes are converted to GOOSE-based schemes, test personnel need new tools to document, test, and troubleshoot these schemes.  SOLUTION:  Having the right tools can simplify your testing & troubleshooting while improving efficiency & productivity. Test Suite Pro provides a comprehensive set of tools to support your testing activities. Based on the SCD file, Test Suite Pro can guide you through the process to identify and solve problems in the substation design and realization. When you do not have the SCD file of the system, Test Suite Pro will discover the system using the standard IEC 61850 services and visualize the result for you.  What you'll learn:     How to quickly get an overview of the system status.  How to verify a device or system is configured as designed in the SCD file by comparing the SCD file against the actual configuration of the system.  How to track GOOSE messages on the network to identify streams that are present and check their configuration, the presence of simulated streams and streams that are missing.  How to analyze the status of GOOSE subscriptions in each IED based on LGOS.  How to automate testing of control operations and observe the result, i.e. for regression testing.  How to visualize signal flow between IEDs and logical nodes within an IED.  How to show timing between signals exchanged by GOOSE or reporting with a virtual logic analyzer.  How to verify as part of unit testing that a device is sending and receiving the GOOSE messages as configured in the SCD file using GOOSE Tracker and GOOSE Publisher.        Who should attend this course:    System Integrators  Software engineers  Substation Automation Engineers  Protection Engineers  Substation Field Technicians  Anyone that wants to save time & money                  Instructors:   Christoph Brunner President it4Power   Christoph Brunner has graduated as electrical engineer at the Swiss Federal Institute of Technology in 1983. He is Utility Industry professional with over 30 years of industry experience with both knowledge across several areas within the Utility Industry and of technologies from the Automation Industry. He is a well-known expert on IEC 61850 as he has been involved in the development of the standard since the beginning. He is president of it4power in Switzerland, a consulting company to the power industry. As such, he has been consultant in many projects for substation automation and projects involving IEC 61850. He has worked as a project manager at ABB Switzerland Ltd in the business area Power Technology Products in Zurich / Switzerland where he was responsible for the process close communication architecture of the substation automation system. He is convenor of the working group (WG) 10 of the IEC TC57 and member of WG 17, 18 and 19 of IEC TC57. He is IEEE Fellow, member of IEEE-PES and IEEE-SA. He is active in several working groups of the IEEE-PSRC (Power Engineering Society – Relay Committee) and member of the PSRC main committee and the subcommittee H. He is guest professor at Shangdong University of technology and international advisor to the board of the UCA international users group.  Jim Coats President Triangle MicroWorks, Inc.   Jim received both his Bachelors and Masters degrees in Electrical Engineering from Georgia Institute of Technology and spent the next 8 years developing embedded software for computer integrated manufacturing applications and medical electronics products.  In 1994 he started Triangle MicroWorks, Inc. to provide communication protocol software to the Electrical Utility Industry and continues as President with a primary role of product research and design.  Jim is currently Chief Financial Officer of the DNP3 Users Group and participates in IEC Technical Committee 57, Working Group 10.             Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/simplifying-secure-routable-goose-sampled-values
                                               Simplifying Secure Routable GOOSE & Sampled Values   Free Webinar - Watch Replay Now!  Applications, Architecture, Configuration & Simulation  Thursday, July 30, 2020 - 11 AM to 12 NOON EDT  Smart Grid concepts have driven the need to migrate to microprocessor based controls and digital communications. There are several use cases that require interoperable communications within and between utilities as well as industrial complexes such as: - Transfer-Trip - Remedial Action Schemes (RAS)/System Integrity Protection Schemes(SIPS) - Load Shed - Blackstart - Dynamic Grid Formation - Actionable Synchrophasor Exchange  All of the these use cases require point-to-multipoint secure communications. Some of the use cases can be extended to integrate between the utility enterprise and the industrial complex.  The routable version of the Generic Object Oriented Substation Event (R-GOOSE) and routable Sampled Values (R-SV) profiles satisfy the security and communication requirements of the utility industry. These profiles allow IP multicast address and advanced route path determination to prevent network flooding. The release of Amendment 1 to IEC 61850 Edition 2 has supplanted IEC  TR 61850-90-5, providing enhancements and defect correction.  PROBLEM:  Security configuration and maintenance, in many cases, is viewed as complex and represents an impediment for adoption and deployment. The use of R-GOOSE/R- SV requires that the pairing of published information to subscribers of that information (e.g. a publication group) share a common symmetric key which utilizes a key distribution mechanism as specified in IEC 62351-9.  SOLUTION:  The engineering process of IEC 61850 allows for the configuration of publication groups, destination addresses for the publications, and addresses for key management for that group. The System Configuration Tool (SCT) stores this information in an SCL file that can be used by the Garibaldi Key Distribution Center to simplify security configuration by: - Managing large numbers of multicast group keys and policy distribution - Manages system Enrollment & Revocation - Operates in centralized and de-centralized environments     What you'll learn:    Why it's important to use routable GOOSE and the need for security  The concepts of key delivery resiliency  Leveraging IEC 61850 SCL to minimize security and system simulation configuration time  Learn how to simulate system design and impact of security on a system using Distributed Test Manager ( DTM )  How to configure Garibaldi (Key Distribution Center) using SCL to save time & minimize configuration mistakes  Learn about the separation of security management planes and real time operation planes  How in-service testing equipment can be used in a secure environment     Who should attend this course:    System Integrators  Software engineers  IT Network engineers  IT/OT Cyber security personnel  Anyone interested in IEC 61850 automation & security                Instructors:   Herb Falk Senior System Architect PCITek  Mr. Falk has over 40 years of experience working in the automation, information exchange, standardization activities, and systems integration.  Previous employment was Westinghouse Numa-Logic and SISCO where he was responsible for 61850 and CIM integration projects and products.  Mr. Falk has been involved with IEC 61850 technology since 1982 and cyber security since 1993.  Mr. Falk is an editor of IEC 61850-8-1, editor of several cyber security standards, and is the Vice President of Testing for the UCA International Users Group (UCA Iug) . His work for His work for the UCA Iug has involved the coordination of IEC 61850 interoperability testing in 2011, 2013, 2015, and 2017.    Joel Greene Director, IEC 61850 Development Triangle MicroWorks, Inc.   Joel has worked with communication protocols for over 15 years with the last 6 years in the power industry. He is the lead engineer for IEC 61850 development & products at Triangle MicroWorks and is heavily involved with 61850 industry groups. He also works as the co-editor of IEC 61850 7-2. He is a member of the UCA Test Committee and Test Procedures Working Group and involved in IEEE Standard Association.   Mark Adamiak Principle Consultant Adamiak Consulting LLC  Mark Adamiak is an independent consultant for the electric power industry.  Mark started his career in the utility business with American Electric Power (AEP) and in mid-career, joined General Electric where his activities have ranged from advanced development, product planning, application engineering, and system integration in the Protection and Control industry.  Mr. Adamiak is an original member of the IEC61850 WG, a Life Fellow of the IEEE, a registered Professional Engineer in the State of Ohio and a GE Edison award winner.  Mark was the Principal Investigator for the EPRI IntelliGrid project to develop a reference architecture for the Smart Grid. In 2012, Mr. Adamiak was elected to the US National Academy of Engineering.       Check out our other webinars:      Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/a-holistic-approach-to-facilitate-the-iec-61850-process---part-1
                                               A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series - Part 1 & 2  Part 1 - IEC 61850 Specification& Design  Monday, May 18, 2020 - 11 AM EDT  PROBLEM:  The lifetime of substation equipment is getting shorter. Substation equipment that was replaced every 40+ years in the past is now worn out or obsolete in 15 years.  As a result, the need to optimize the process of designing an electrical power system has become critical to continue to meet increased demand while minimizing costs & resources.  SOLUTION:  The IEC 61850 61850 Engineering Process can meet that need today. Understanding the IEC 61850 Engineering Process and using with the right tools allows you to learn the approach from a standard method, not from different vendors. Adapting a standard set of tools that are not vendor specific allows you to work across multiple suppliers for specification & design along with verification early in the process. Here are just some benefits when you adopt a standard process & vendor neutral tools: - Allows for easier transfer of skills set - Hire employees that understand your process - Reuse of design - Engineering efficiency - Saves money - Saves time - Interoperability between vendors    In this webinar you will learn how to:   Facilitate the IEC 61850 Engineering Process throughout the system lifecycle including design, Factory Acceptance Testing, commissioning and maintenance process.  Create IED Capability Description (.ICD) files with standard IEC 61850 logical nodes and data objects.  Compare models to allow verification of SCL changes as files are updated & propagated through the IEC 61850 Engineering Process. As ICD files are updated, the changes can be identified and validated, and the ICD file can be compared to IED instances in the SCD file to verify that the correct files were processed.  Manage data templates with the option to edit, reuse, or clean templates.  Verify SCL Files against the standard with a vendor neutral tool called SCL Navigator .  Create a system specification & implement a design for it with a vendor neutral 61850 tool called Helinks .                  Specific knowledge, skills and/or capabilities that attendees should have to take this course:   None   Who should attend this course:    System Integrators  Substation Design Engineers  Protection Engineers  SCADA Architects  Substation Field Technicians  Anyone that is open to adopting a standardize process to save time & money   Instructor:   Joel Greene Director, 61850 Development Triangle MicroWorks, Inc.   Christoph Brunner President it4Power              Check out our other webinars:                Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                           Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/a-holistic-approach-to-facilitate-the-iec-61850-engineering-process-webinar-series-part-2
                                               A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series - Part 1 & 2   Part 2 - IEC 61850 Simulation & Testing  Thursday, June 25, 2020 - 11 AM to 12 NOON EDT  On May 18th, Triangle MicroWorks hosted Part 1 of a 2 part webinar series "A Holistic Approach to Facilitate the IEC 61850 Process - Part 1 - IEC 61850 Specification & Design ". If you missed, a recording is available - click here  to watch now.    On June 25th, Triangle MicroWorks will be hosting " Part 2 - IEC 61850 Simulation & Testing " to complete the series on understanding the IEC 61850 Engineering Process and how our tools make this process more efficient. See below for details and sign up today!  PROBLEM:  The lifetime of substation equipment is getting shorter. Substation equipment that was replaced every 40+ years in the past is now worn out or obsolete in 15 years.  As a result, the need to optimize the process of designing an electrical power system has become critical to continue to meet increased demand while minimizing costs & resources.  SOLUTION:  The IEC 61850 61850 Engineering Process can meet that need today. Understanding the IEC 61850 Engineering Process and using with the right tools allows you to learn the approach from a standard method, not from different vendors. Adapting a standard set of tools that are not vendor specific allows you to work across multiple suppliers for specification & design along with verification early in the process. Here are just some benefits when you adopt a standard process & vendor neutral tools: - Allows for easier transfer of skills set - Hire employees that understand your process - Reuse of design - Engineering efficiency - Saves money - Saves time - Interoperability between vendors    Part 1 showed us how to create IED Capability Description (.ICD) files and verify SCL FIles against the standard. We also showed how to create a system specification and implement a design for that system using vendor neutral tools. If you missed it, you can watch here .  What you'll learn in Part 2:   You will learn the 2nd half of the IEC 61850 Engineering Process with a focus on Factory Acceptance Testing, Commissioning and Maintenance process.  Test the designed SCD file created by Helinks STS from Part 1 .  Learn how Distributed Test Manager (DTM) can simulate all the IEDs in the SCD file (created by Helinks STS from Part 1 ) and create automated tests to confirm individual device behavior.  How to use these automated repeatedly in the substation lifecycle to validate design updates and behavior of the corresponding IEDs prior to installation in the substation.  Use a vendor neutral diagnostic tool, Test Suite Pro (TSP) , to monitor, troubleshoot and confirm the behavior of the new substation and simulated substation in DTM.                  View Replay  Specific knowledge, skills and/or capabilities that attendees should acquire by taking this course:   None   Who should attend this course:    System Integrators  Software engineers  Software engineering managers  Substation Field Technicians  Anyone that is open to adopting a standardize process to save time & money   Instructor:   Matt Green Senior Applications Engineer & QA Tester Triangle MicroWorks, Inc.   Jim Coats President Triangle MicroWorks, Inc.          Check out our other webinars:      Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/reduce-travel-increase-productivity-with-scada-simulation-tools-for-substation-maintenance-commissioning
                                               Quarantined...Working Remotely?   Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Free Webinar - Watch Replay Now!  Thursday, April 9, 2020 - 11 AM to 12 NOON EST    PROBLEM:  With the onset of COVID-19, we are facing new challenges everyday beyond anything we have ever planned for. The utility industry plays a vital role to keep moving forward and ensure our infrastructure can meet the needs of these new & future challenges. As we transition to working remotely, we need the right tools in place to allow us to continue the every day work that keeps the lights on.  SOLUTION:  Remote testing & simulation can drastically reduce your need for travel, to be onsite, and increase productivity so you can maintain existing/ new project schedules as well as respond quickly to new situations. Remote workforces need tools that are user friendly, work with existing hardware & operating systems (i.e. Windows, laptops, servers), are low cost  & easy set up so they can quickly start using them. The goal of this webinar is to show you how SCADA testing can be done remotely that you would normally do onsite to maintain and commission a substation. You will learn about remote testing strategies & a tool called Distributed Test Manager(DTM) that can be used from a remote set up. It will cover the following protocols: DNP3, Modbus, IEC 60870-5, IEC 61850, and ICCP.  Remote Testing Strategies you’ll learn:   Learn how to simulate different types of equipment so the simulated network mimics the actual substation network.  Mix and match multiple protocols within a single simulation, so the simulated network accurately depicts your real network.  Review multiple methods of configuring devices, including using your actual devices' SCL files or DNP3 Device Profile, and even CSV files to make configuration as easy as possible.  Learn how to create custom displays, single line diagrams, and more so that you can visually monitor your system while it is under test, eliminating the need to spend hours after the test poring over data to figure out what happened during the test.  For rigorous testing, demonstrate how to create a variety of data changes so you can see how the system reacts. This can be done easily, using a variety of methods including manual data changes, automatic changes, table driven data events, and scripting.  Show you how to define, organize, and execute groups of tests. Tests can be run sequentially or individually, and results can be grouped to easily find ones that have failed.                   View Replay  Specific knowledge, skills and/or capabilities that attendees should acquire by taking this course:   None   Who should attend this course:    Software engineers  Software engineering managers  Substation Field Technicians  Anyone who wants to minimize travel time  Anyone who wants to increase productivity while working remote   Instructor:    Matthew Green Senior Applications Engineer Triangle MicroWorks, Inc.             Check out our other webinars:                Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                           Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/triangle-microworks-webinar-series
                                               Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Free Webinar  Tuesday, December 10, 2019 - 1PM to 2PM EST    PROBLEM:  Most protocol mapping tests for gateways are performed manually. Each point is tested, one by one, to ensure data is mapped correctly from each device across the system. This manual process is: - Time Consuming - Error Prone - Repetitive    SOLUTION:  Automatically test protocol and data type mapping in a gateway and document results with the Distributed Test Manager (DTM) . This is accomplished by simulating changes in the downstream IEDs and verifying they are received upstream in the SCADA Master. The following benefits will be achieved: - Saves Time - Saves Money - More Reliable - Perform testing earlier by not waiting for the final system to be installed and configured  Learning Objectives:   Learn how to simulate IEC 61850 & DNP3 network traffic to test point mapping within your data concentrator  Walk thru an example of testing our SCADA Data Gateway using Distributed Test Manager (DTM) simulating multiple downstream IEC 61850 IEDs and an upstream DNP3 SCADA Master.  Learn how a script running in DTM automates the testing of protocol mapping in the Gateway, Data Concentrator or RTAC and documents the results.   Test partial systems by simulating missing physical devices and without purchasing expensive test equipment to generate signal changes.                 View Replay   Specific knowledge, skills and/or capabilities that attendees should acquire by taking this course:   None   Who should attend this course:    Software engineers  Software engineering managers  Substation Field Technicians   Anyone who wants to save time & money when deploying substations    Instructor:    Matthew Green SQA Engineer Triangle MicroWorks, Inc.         Check out our other webinars:      Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/test-suite-lite
                                         61850 Test Suite Lite                    Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation                Test Suite Lite is a Windows® application with a subset of the tools that are available in the full version of Test Suite Pro. Test Suite Lite offers a lower cost option with a limited number of features to enable the user to perform the basic diagnostic functions necessary for 61850 testing.  The list below compares the features available in Test Suite Lite & Test Suite Pro to a typical 61850 test tool.  KEY DIFFERENCES between Test Suite Lite and Test Suite Pro are outlined below:                        Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit                  Features  Test Suite Lite  Test Suite Pro  Typical 61850 Test Tool    Browse 61850 Data Model          Hierarchical view of the Full IED Model ( Advanced Client )  ✔   ✔   ✔    Flat view with prefiltered selection of key process Data Objects from all IEDs that can be grouped or filtered using factory or user defined presets ( Data Miner )    ✔  ✔    ✘    MMS          Number of servers that can be connected to (* unlimited, typical maximum of 50 based on computer performance)  *  *  1    Number of servers that can be simulated ( IED Simulator ) (* unlimited, typical maximum of 50 based on computer performance)  5  *  1    Support control services based on control model configuration of the Data Object   ✔  ✔    ✔    Visualize Buffered/Unbuffered reports ( Report Viewer )   ✔  ✔    ✔    Compare SCL to the online Modem ( Compare Model )   ✔  ✔    ✘    Support Log Services   ✔  ✔    ✔    Browse/Stream Logs ( Log Services )   ✘  ✔    ✘    Support File Services   ✔  ✔    ✔    Browse/Download Files ( File Services )   ✘  ✔    ✘    GOOSE          Publish/Receive GOOSE   ✔  ✔    ✔    Publish Simulated GOOSE ( GOOSE Publisher )  ✘  ✔   ✘    Consolidated view of GOOSE messages Received ( GOOSE Viewer )   ✔  ✔    ✘    Track all the GOOSE streams on Network for Validity ( GOOSE Tracker )   ✘  ✔    ✘    Visualize GOOSE Subscription status in all IEDs ( LGOS )   ✘  ✔    ✘    Data Visualization          View Attribute Values as Tiles ( Group Display )   ✔  ✔    ✔    Customize layout of tiles to represent one line diagram ( Custom Display )   ✔  ✔    ✘    Signal Visualization          Record and View Signals as Time Chart ( Logic Analyzer )   ✘  ✔    ✘    Record and View Signals in a Signal Flow diagram   ✘  ✔    ✘    Testing          Support configuration of Mod and LPHD.Sim and observation of Beh for all LNs for virtual isolation during tests in a live system ( Mod/Beh/Sim )   ✘  ✔    ✘    Create a repeatable test sequence ( Test Sequencer )   ✘  ✔    ✘    Network Sniffer          View all IEC 61850 network activity ( Output )  ✔   ✔   ✔     Analyze IEC 61850 packet data   ✘   ✘  ✔          Ready to Get Started?    Download Evaluation>>      Request Pricing>>                                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/docs/default-source/referenced-documents/maximizing-iec-61850-value-with-tools.pdf
                                         SCL Navigator           Request a Quote          SCL Navigator Video Playlist           Download Evaluation                 SCL Navigator is a Windows™ application for creating, viewing, editing, and verifying IEC 61850 System Configuration Language (SCL) files with an intelligent editor.  SCL Navigator makes it easy to:   View SCL files with simplified XML view or as an interpreted data model  Create IED Capability Description (.ICD) files with standard IEC 61850 logical nodes and data objects (Ed. 1, Ed. 2 or Ed. 2.1)  Manage data templates with the option to edit, reuse, or clean templates  Verify SCL Files with results that are highlighted in the editor  Compare models to verify changes to SCL as files are updated and propagated through the IEC 61850 Engineering Process                           Overview  What's New  System Requirements       Overview    What's New    System Requirements        Related Products           Test Suite Pro (TSP)  Distributed Test Manager (DTM)                    (Click the button above to see the role SCL Navigator plays in the IEC 61850 Engineering Process)  Verify SCL Files    SCL verification is built in to help make SCL files more inter-operable. There are 2 methods to verify SCL files in Navigator. First, Triangle created an SCL Verify feature in Navigator to go beyond what the XML files do. SCL verify identifies missing parameters and non-compliant SCL. The tool also checks the data model against the standard and verifies that data type template references can be resolved. It also verifies that control blocks, external references, and datasets can be resolved.  Warnings and errors are displayed to distinguish high priority issues. The location of the warning or error is highlighted in the XML view for convenient troubleshooting.  The second method to validate IEC 61850 SCL configuration files is based on rules developed by the Rise Clipse initiative using Object Constraint Language (OCL).  To download the free SCL Reader version of Navigator which includes RiseClipse Verify (previously called OCL Verify):    Download an evaluation license of SCL Navigator  Type in “ freetool ” as the product license key  You will have access to the full version of SCL Navigator for 14 days.  After 14 days, you will still have access to the free SCL Reader with RiseClipse Verify.         SCL verification is built in to help make SCL files more inter-operable. (Click Image for Larger View)   An EDF and CentraleSupélec software  RiseClipse is currently developed by the computer science department of CentraleSupélec and by EDF R&D . This collaboration is done in the context of the RISEGrid Institute .          Create a Clean Data Model    Data models for an IED Capability Description (ICD) template can be created with standard IEC 61850 data objects. Logical nodes & data objects from -7-3, -7-4, -7-410 (Hydro), -7-420 (DER), -90-7 (DER), and IEC 61400-25 (Wind) can be added to the data model. Mandatory or optional objects can be added according to the IEC 61850 standard (both Edition 1 & 2).  The tool manages the Data Type Template section with the option to edit data templates, reuse templates, or clean the templates section to remove unused sections. This allows you to build an efficient data model that does not lead to oversized SCL files during the engineering process.        Build a clean data model using templates and standard IEC 61850 Data Objects (Click Image for Larger View)          View Files as XML or Data Model    SCL files can be viewed and edited with an intelligent, simplified XML editor. This view shows important XML tags that are needed in different parts of the data model.  There is also the option to see the SCL file as an interpreted data model. This allows SCL files to be viewed and edited on a line by line basis (with the XML view) but also verified by viewing how the tool builds the data model.       View SCL Files as XML Files or as Data Models (Click Image for Larger View)                            Model Comparison    Model compare allows verification of changes to SCL as files are updated and propagated through the IEC 61850 Engineering Process.  Significant time and effort is wasted when incorrect files are used.  SCL Navigator allows an independent verification of files so that these expenses can be reduced.  As ICD files are updated, the changes can be identified and validated, and the ICD file can be compared to IED instances in the SCD file to verify that the correct files were processed.       Import/Export DataType Templates and Custom Libraries     Users can export their DataTypeTemplates to a generic XML file or a Name Space Definition (NSD) file. A NSD file is a machine-processable definition of a 61850 data model and is commonly used by the standard to make the data model updates available for tools. Exported NSD files from Navigator follow this process and can be used in other tools to convey the user specific 61850 data model.  Navigator also supports importing NSD files. When Navigator imports a NSD file, it creates a custom user library that can be use to create or update SCL files. These custom user libraries can be created off Navigator's own exported NSD files or NSD files published by IEC with the latest updates to the data model.         Export 61850 DataTypeTemplates and Import Custom Libraries (Click Image for Larger View)        Ready to Get Started?       Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                           The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-scl-pages/secure-authentication
                                                  Request a Quote          Source Code Library Video Playlist             IEC 60870-5 Source Code Libraries  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                   Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          Secure Authentication for IEC 60870-5 (-101 and -104)  The IEC 60870-5 Source Code Library supports Secure Authentication for -101 and -104.  Cyber-security Features    End to end cryptographic authentication at the application layer which goes beyond VPN tunnels or TLS  Based on IEC 62351 security standard (Parts 3, 5, and 8)  TLS is supported for  IP based networks  Legacy support for networks or devices that do not support encryption  Configuration of which requests are considered critical  Authentication can be performed in either direction (Controlling or Controlled Station)  Supports IEC 101 and IEC 104    Support for Multiple Users    Supports Role Based Access Control (RBAC) - multiple users and roles (engineers, operators, viewers, admin)  Configure privileges(view, read, security, etc.) for each role  Users can be added, modified, or removed    Legacy Support    Support low bandwidth and/or serial networks  Low overhead for Outstations that may not have processing capability for public/private certificates or encryption     Additional tools for Secure Authentication   Test Harness   The Communication Protocol Test Harness supports Secure Authentication  A powerful tool for testing the implementation of the IEC 60870-5 Source Code Libraries    Distributed Test Manager (DTM)    Distributed Test Manager (DTM) supports Secure Authentication  A powerful tool that can simulate the SCADA communications in the substation and perform automated tests to confirm system or individual device behavior   SCADA Data Gateway     For projects where Secure Authentication needs to be added to an existing system  The SCADA Data Gateway is a flexible protocol translation application for easily deploying Secure Authentication in a control center or substation     Watch our IEC 60870-5 training videos to learn more about Secure Authentication    Example of Aggressive Mode Challenge-Reply Sequence for IEC 60870-5:                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/secure-routable-goose-sampled-values
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                  What is Secure GOOSE & Sampled Values? GOOSE is a mechanism for transmitting time-critical information, such as alarms or status changes, between intelligent electronic devices (IEDs) in a substation. Sampled Values provides synchronized sampling and transmission of analog data, such as voltage and current, from sensors to protection and control devices in substations. Securing GOOSE and Sampled Values refers to the implementation of enhanced security measures in the transmission of real-time events and analog measurements throughout the power system, namely authentication and optionally encryption: When using KDC, each GDOI (Group Domain of Interpretation as defined in RFC-6407) has one publisher and one or more subscribers.  The KDC Server is responsible for assigning cryptographic keys to each GDOI and sending them to each KDC Client, which then matches the GDOIs to the correct publisher or subscriber stream on the device. Authentication (mandatory for routable versions) - GOOSE and Sampled Values are authenticated by  utilizing HMAC to validate that the messages were signed with the key supplied by the KDC Server, this verifies that the message was not corrupted in transit. Encryption (optional) - Implement encryption to protect the confidentiality of the data transmitted. Encryption ensures that unauthorized parties cannot decipher the content of the messages.  Why do I need to Secure GOOSE & Sampled Values? Confidentiality of Data: Use Case: Protect       sensitive information in real-time operational data or critical system parameters . Integrity Assurance: Use Case:  Verify data integrity and prevent tampering during       transmission. Perfect Forward Secrecy : Use Case: Breaking the key for one set of key pulls does not provide key materials       that allow you to break future exchanges since key exchanges are       protected via Diffie-Hellman Exchanges. Information Isolation: Use Case: Each group consisting of a publisher and its subscribers has its       own set of keys and policies therefore compromising one group does not       compromise the other groups. Secure Communication in Shared Networks: Use Case: Ensure       confidentiality in shared network environments with non-critical systems       or external entities. Compliance with Security Standards: Use Case: Meet       industry standards and regulations for power system security. Prevention of Man-in-the-Middle Attacks: Use Case: Guard       against unauthorized interception and alteration of communication. Maintaining Data Integrity in Wide-Area Networks: Use Case: Secure       communication over extended distances to ensure data integrity. Protection Against Insider Threats: Use Case: Mitigate       risks associated with malicious actions from within the organization. Ensuring System Resilience: Use Case: Enhance       infrastructure resilience against cyber threats for continuous and       reliable power grid operation.     How do I secure GOOSE & Sampled Values? The KDC, or Key Distribution Center , provides a mechanism for encryption and authentication of both GOOSE and Sampled Value messages. KDC is made up of two components, a KDC Server such as Garibaldi, which manages and distributes secure keys, and a KDC Client such as the one provided with Triangle MicroWorks’ IEC 61850 Source Code Library, which receives the keys and uses them to encrypt, decrypt, sign, and authenticate messages. This technology can be applied to both Routable and Layer 2 GOOSE and Sampled Value communications.               When using KDC, each GDOI (Group Domain of Interpretation as defined in RFC-6407) has one publisher and one or more subscribers.  The KDC Server is responsible for assigning cryptographic keys to each GDOI and sending them to each KDC Client, which then matches the GDOIs to the correct publisher or subscriber stream on the device.       What does TMW support? KDC Client Source Code Library Supports failover to a secondary KDC Server. Autonomously performs pull requests for clients as keys expire. Fully compliant with IEC-62351-9 and RFCs 2407, 2408, 2409, 6407, and 8052. Cryptographic algorithms supported include any combination of: Authentication = HMAC-SHA256-128, HMAC-SHA256-256, AES-GMAC-128, AES-GMAC-256 Encryption = none, AES-128-GCM, AES-256-GCM, AES-128-CBC, AES-256-CBC Garibaldi Key Distribution and Access Control Security Domains IEC 61850 Security Member Validation and Policy Delivery   Ready to Get Started? Request information & pricing for the KDC Client Library>>  Request evaluation license for Garibaldi>>                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us/careers/c-c-software-engineer
                                             C#/C++ Software Engineer   POSITION DESCRIPTION:  The Software Engineer position is responsible for software development and support of applications used for testing and/or simulating communications between intelligent electrical substation equipment. This is a great opportunity for a Software Engineer to work as a member of a team of industry experts who have developed products that are used globally.  As a Software Engineer on this team, you will have a chance to contribute toward the way the electrical utility grids are controlled and monitored around the world.  RESPONSIBILITIES:   Implement new features in Windows O/S based applications using current programming frameworks and techniques.  Learn communication protocol specifications used in the electrical utilities industry.  Work under the supervision of senior developers.  Write test fixtures and unit tests to verify software performance.  Help with support questions from customers related to product issues.      EDUCATION, EXPERIENCE and SKILLS REQUIRED:   Minimum of five years of experience working with C# and C++ software development.  Strong C# skills using Microsoft Visual Studio are required.  Experience using WPF, WinForms, and LINQ technologies is required.  Additionally, strong skills in C++ is preferred.  Some experience with C programming preferred.  Experience with industrial communications protocols is preferred.  Experience with DNP3, IEC 61850, IEC 60870-5, ICCP, or SCADA is preferred.  Bachelor’s or higher level degree in computer science or engineering is required.  Ability to communicate clearly and concisely, verbally and in writing, is required.  Must have excellent interpersonal skills.   Location: Raleigh, NC   About employer: Triangle MicroWorks is a fast paced, growing, privately-held company which provides communication protocol software for customers in the electric power industry.                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us/careers/technical-sales-associate
                                             Technical Sales Associate   POSITION DESCRIPTION:  The Technical Sales Associate is responsible for ensuring our sales, support, and quality assurance teams deliver the best possible solutions to our customers. Triangle MicroWorks desires a person who is a self-starter, energetic, and positive. Applicants must be highly organized, customer focused, and capable of managing multiple tasks in a dynamic environment.  RESPONSIBILITIES:  Customer Activities (80%)    Emails and phone calls with customers to answer initial product questions  Assist customers who are evaluating software products  Help customers find the right solution for their needs  Provide product demonstrations for customers  Work with developers to identify customer solutions and product enhancements  Assist technical support team and quality assurance team  Expected to become proficient in the use of multiple, technical software products   Marketing Activities (20%)    Assist with creating product demonstrations  Create content for website, email, and marketing materials   EDUCATION, EXPERIENCE and SKILLS REQUIRED:   Minimum of three to five years of experience with sales, marketing, or support of technical products and services  Bachelor’s or higher level degree in a technical field  Ability to communicate clearly and concisely, both verbally and in writing  Strong computer skills including Word, PowerPoint, CRM software, Excel, and software tools used in typical high tech business environments  Personable, confident, energetic professional who has successfully worked as part of a team  Must have excellent interpersonal skills and experience working directly with customers   ADDITIONAL PREFERRED REQUIREMENTS:   Background in communication protocols  Experience with engineering software (design, simulation, testing)  Experience with sales and marketing of technical software applications  Work experience within the electric utility industry is a plus   About employer: Triangle MicroWorks is located in Raleigh, NC.  We are a fast paced, growing, privately-held company which provides communication software for customers in the electric power industry for substation automation, SCADA systems, distributed energy resources (wind, solar, storage), and distribution automation.                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us/careers/technical-support-engineer
                                             Senior Technical Support Engineer   POSITION DESCRIPTION:  We are looking for a Senior Technical Support Engineer to provide product support assistance to our customers. You will diagnose and troubleshoot software problems and help our customers install applications and programs.  Our products are software applications that test and simulate data communications between electronic equipment used in electric utility substations.  Applicants to this position must have experience supporting complex software that is used in computer networks and industrial communications systems.  Experience in the electrical utilities industry is a plus.  Senior Technical Support Engineer responsibilities include assisting customers with configuration of our software products, investigating problems customers may be experiencing with the products, assisting with licensing problems, and answering general questions about the capabilities of our products. The position works directly with customers and with the internal engineering team to solve problems of varying complexity.  You will use email, virtual meeting applications, remote desktop, and phone to work with clients.  You must be able to provide clear written and verbal instructions.  If you’re naturally a helper, enjoy assisting people with product issues and are able to explain technical details simply, we’d like to meet you. Ultimately, you will be a person our customers trust. They will rely on you to provide timely and accurate solutions to their technical problems.  RESPONSIBILITIES:    Learn our products including the details of how to use them and concepts of what they do  Research and identify solutions to software and hardware issues  Diagnose and troubleshoot technical issues, including network configuration, licensing issues, product configuration, etc.  Work to reproduce reported problems in software products  Ask customers targeted questions to quickly understand the root of the problem  Track product issues through to resolution, within agreed time limits  Talk clients through a series of actions, either via phone or email, until they’ve solved a technical issue  Properly escalate unresolved issues to appropriate internal teams (e.g. software developers)  Provide prompt and accurate feedback to customers  Refer to internal database or external resources to provide accurate technical solutions  Ensure all issues are properly logged  Prioritize and manage several open issues at one time  Follow up with clients to ensure their applications are fully functional after troubleshooting  Document technical knowledge in the form of notes and manuals; Technical Writing Experience a plus  Maintain positive relationships with clients     EDUCATION, EXPERIENCE and SKILLS REQUIRED:   Proven work experience as a Technical Support Engineer, or similar role working with complex software based products  Must have previously provided product support for software applications, devices, or systems that utilized complex software.  Hands-on experience with Windows/Linux OS environments  Good understanding of computer systems and computer networking  Excellent problem-solving and communication skills  Ability to provide step-by-step technical help, both written and verbal  Some programming and/or computer scripting skills are preferred  Experience in utilities industry is highly desired    Location: Raleigh, NC   About employer: Triangle MicroWorks is a fast paced, growing, privately-held company which provides communication protocol software for customers in the electric power industry.                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us/careers/software-test-engineer
                                             Software Test Engineer   POSITION DESCRIPTION:  We are looking for an engineer to test software to target SCADA communication protocols by following specifications used in the electrical utilities industry.  RESPONSIBILITIES:   Test software by creating automated test scripts to test the user interface of communications gateways and test tools to verify requirements and defect corrections.  Write manual test scripts with detailed instructions and expected results to test communications gateways and test tools.  Use both written requirements and personal knowledge of the communications protocols and substation operation to craft the test scripts.  Create written test plans incorporating both automated test scripts and manual test scripts using a test plan creation tool.  Execute test plans to test software applications and communications libraries.  Perform manual steps indicated in manual test cases and record results. Use automated software testing tools to execute automated test scripts and review the results of automated test scripts.  Update written defect reports which have been marked as corrected by software engineers with the results of test script execution.  Review QA checklists to prepare tested software for release to customers.  Provide customer support related to product issues by setting up and using substation simulation software to emulate customer environment, communication traffic and recreate conditions of reported problems.  Review user manuals and other product documentation for accuracy. Recommend changes for readability and markup corrections.  Test software against instructions provided in user manuals.  Write test fixtures and unit tests to verify low level communications libraries, verify conformance to published specifications.  Write test software using a test framework such as NUnit test and integrate the test software into an automated test execution system.    EDUCATION, EXPERIENCE and SKILLS REQUIRED:   Bachelor’s degree in Computer Science or Engineering plus 5 years of experience in the job offered or related software testing position.  Must have 3 years of experience working with at least one of the following SCADA communications protocols: DNP3, IEC 60870-5 or IEC 61850.   Must have 1 year of experience testing industrial communications gateways.   In the alternate will accept 7 years of experience in the job offered or related software testing position.  Must have 3 years of experience working with at least one of the following SCADA communications protocols: DNP3, IEC 60870-5 or IEC 61850.   Must have 1 year of experience testing industrial communications gateways.     Location: Raleigh, NC   About employer: Triangle MicroWorks is a fast paced, growing, privately-held company which provides communication protocol software for customers in the electric power industry.                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/about-us/terms-of-service
                                                Terms
of Service     LAST
UPDATED: July 22, 2014     YOU AND
TRIANGLE MICROWORKS, INC. (“WE”, “US”, AND “OUR”) AGREE THAT YOUR ACCESS TO AND
USE OF THE WEB SITE LOCATED AT WWW.TRIANGLEMICROWORKS.COM , IS SUBJECT TO YOUR
AGREEMENT TO THESE TERMS OF SERVICE. TO
MAKE THESE TERMS OF SERVICE EASIER TO READ, THE FOREGOING REFERENCED WEB SITE IS
REFERRED TO AS THE “WEB SITE” AND THE SERVICES PROVIDED BY US THROUGH THE WEB
SITE ARE REFERRED TO AS THE “SERVICES.” YOUR USE OF THE WEB SITE OR THE
SERVICES IS
SUBJECT TO YOUR AGREEMENT TO THESE TERMS OF SERVICE, WHICH WILL BECOME A
BINDING AGREEMENT BETWEEN YOU AND US (THE "AGREEMENT"). WE ARE
WILLING TO ALLOW YOU ACCESS TO THE WEB SITE AND PROVIDE THE SERVICES ONLY UPON
THE CONDITION THAT YOU ACCEPT ALL OF THE TERMS OF THIS AGREEMENT. PLEASE READ
THESE TERMS CAREFULLY. AFTER READING THE TERMS, IF YOU AGREE TO THE TERMS OF
SERVICE, PLEASE INDICATE YOUR DECISION BY CLICKING THE CHECKBOX ON THE ACCOUNT
SIGN UP/REGISTRATION FORM NEXT TO THE STATEMENT THAT READS “I have read and
agreed to the Terms of Service.” IF YOU DO NOT AGREE, YOU WILL NOT BE ABLE TO ESTABLISH
AN ACCOUNT AND USE OUR SERVICES. IF YOU HAVE PREVIOULY SET UP AN ACCOUNT, YOU
MAY BE REQUIRED TO AGREE TO THESE TERMS OF SERVICE PRIOR TO BEING ALLOWED
ACCESS TO ANY OF THE SERVICES. IN ALL CASES, BY ACCESSING, VIEWING OR USING OUR
SITE, YOU AGREE TO THE TERMS OF SERVICE.    The Services
are available only to persons who can form legally binding contracts under
applicable law. Without limiting the foregoing, the Services are not available
to individuals under the age of 18. If you are under 18, then you are not
permitted to use the Services.      Services  You may use the Web Site and the Services
solely for your own personal, non-commercial use. You may not modify, copy,
distribute, re-publish, transmit, display, perform, reproduce, publish, reuse,
resell, license, create derivative works from, transfer, or sell any
information, content, material, software, products or services obtained from
the Web Site or through the Services, including the results of the SCL
Consistency and Completeness Check. We reserve all of our statutory and common
law rights against any person or entity who violates this paragraph. You may
not frame any pages of the Web Site or any content contained therein, whether
in whole or in part, without prior written consent from us. Any rights not
expressly granted herein are reserved.     You agree that your use of the Web
Site and the Services are subject to all applicable local, state and federal
laws and regulations. You also agree:  ·  not to use the Web Site or the Services
or submit content to the Web Site if you are under the age of 18;  ·  not to access the Web Site using a
third-party's account/registration without the express consent of the account
holder;  ·  not to use the Web Site of the Services
for illegal purposes;  ·  not to use the Web Site or Services to
engage in commercial activities that are competitive with our products or
services;  ·  not to copy any content from the Web
Site for republication in print or online, including the SCL Consistency and
Completeness Check;  ·  not to attempt to gain unauthorized
access to other computer systems from or through the Web Site or the Service;  ·  not to interfere with another
person's use and enjoyment of the Web Site or the Services or another entity's
use and enjoyment of the Web Site or the Service;  ·  not to upload or transmit viruses or
other harmful, disruptive or destructive files;  ·  not to disrupt, interfere with, or
otherwise harm or violate the security of the Web Site or the Service, or any
system resources, accounts, user IDs, passwords, servers or networks connected
to or accessible through the Web Site or affiliated or linked sites (including
those of our partners); and/or  ·  not to upload or transmit to the Web
Site unauthorized data or any information for which you are not the owner or
authorized by the owner to share.     User Provided Content  At times, the Web Site may allow you and others to upload IEC
61850 SCL files (.ICD, .CID, .IID, .SCD) for testing ("SCL Files").
You agree not to upload any SCL File that:  ·  unfairly interferes with any third
party's uninterrupted use and enjoyment of the Web Site or the Service;  ·  contains copyrighted content without
the express permission of the owner of the copyrights in the content;  ·  infringes any copyright, trademark, patent,
trade secret, or other intellectual property right;  ·  contains viruses or other harmful,
disruptive or destructive files;  ·  links to any commercial or other
website; and/or  ·  is not otherwise in compliance with
these Terms of Service.   Each time you provide an SCL File to
the Web Site, you represent and warrant that you have the right to provide such
SCL File, which means:  ·  you are the author of the SCL File,
or  ·  the SCL File is not protected by
copyright law, or  ·  you have express permission from the
copyright owner to use the SCL File in connection with the Web Site; and  ·  you have the right to grant us the
license set out in these Terms of Service;  ·  your use of the Web Site and SCL
File does not violate these Terms of Service.     You grant us a royalty-free,
perpetual, irrevocable, non-exclusive right and license to use, copy, modify,
archive, store, transmit, reproduce and create derivative works from all SCL
Files you provide to us in any form, media, software or technology of any kind
now existing or developed in the future solely as necessary to provide the
Services to you and for our internal testing purposes. You grant all rights
described in this paragraph in consideration of your use of the Web Site,
without compensation of any sort to you. We do not claim ownership of SCL Files.     We specifically disclaim any duty,
obligation, or responsibility, to review, screen or test any SCL File. In
addition, we do not represent or warrant that any other content or information
accessible via the Web Site is accurate, complete, reliable, current or
error-free. We assume no responsibility or liability for any errors or
omissions in the content provided via the Web Site.     You acknowledge and agree that the Web
Site contains proprietary information, methods, tools and content, that is
protected by intellectual property and other laws, and may not be used except
as provided in these Terms of Service without advance, written permission of us.
All designs, text, graphics, interfaces, and images (and the selection and
arrangements thereof), and software, hypertext markup language
("HTML"), scripts, active server pages, and other content and
software used in the Web Site are reserved.      Termination of access
to the Web Site  We reserve the right, in our sole
and absolute discretion, to modify, suspend, or discontinue at any time, with
or without notice, the Services or the availability of the Web Site (or any
part thereof).     Your intellectual
property rights  Except as set forth in this
Agreement, we make no claims on any work submitted to us for review. Your SCL
Files will remain your intellectual property. You represent and warrant that: (i)
you have obtained all necessary third party rights, including, without
limitation, copyrights, for any SCL Files or portions thereof that belong to
third parties, which are necessary for us to provide Services in connection
with the SCL Files: (ii) the SCL Files will not contain any illegal material
that could give rise to any liability to us or which might adversely affect our
public image, reputation or goodwill; and (iii) the SCL Files do not include
any infringing, misappropriated or content that you do not have the rights to
use. It is your responsibility to determine if it is necessary for you to
obtain, and for obtaining, any licenses required to use third party content that
is part of the SCL Files. We are not storing SCL Files for your benefit and you
will not be able to download files provided to us.     Indemnification  You
agree to indemnify, defend and hold us and our directors, officers, agents,
contractors, partners and employees, harmless from and against any loss,
liability, claim, or demand, including reasonable attorneys' fees, arising out
of or related to:  (i) your breach of any representations or warranties made by you hereunder or
your breach of any term of this Agreement;  (ii) your use of the Services or the Web Site in violation of this Agreement;
or  (iii) your SCL Files and/or your violation of any law or the rights of a third
party.    You
hereby agree not to sue, assist in or be a voluntary party to assist in or be a
voluntary party to, except as required by law, any action, suit, or proceeding
against us for any claims, actions, suits, damages, liability, losses or
expenses of whatever kind or however arising out of or relating to your use of
the Web Site or the Services.     Our intellectual
property rights  You
acknowledge that we own all right, title and interest in and to the Web Site and
the Services (the " Intellectual Property"), and such Intellectual
Property is protected by U.S. and international intellectual property laws. You
also agree that you will not use any robot, spider, other automated device, or
manual process to monitor or copy any content from the Web Site.     Disclaimer of
warranties  We are not
responsible for the conduct, whether online or offline, of any user of the Web
Site. The Web Site and the Services may be temporarily unavailable from time to
time for various reasons. We assume no responsibility for any error, omission,
interruption, deletion, defect, delay in operation or transmission,
communications line failure, theft or destruction or unauthorized access to, or
alteration of, the Web Site. We are not responsible for any problems or
technical malfunction of any telephone network or lines, computer online
systems, servers or providers, computer equipment or software, or the failure
of email on account of technical problems or traffic congestion on the Internet
or at any web site, including injury or damage to any person's computer related
to or resulting from participating or downloading materials or SCL Files in
connection with the Web Site or the Services.  THE WEB SITE AND THE SERVICES ARE
PROVIDED "AS-IS" AND WE DISCLAIM ANY AND ALL WARRANTIES, WHETHER
EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION IMPLIED WARRANTIES OF TITLE,
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR NON-INFRINGEMENT. WE
CANNOT GUARANTEE AND DO NOT PROMISE ANY SPECIFIC RESULTS FROM USE OF THE WEB
SITE AND/OR THE SERVICES.     Without limiting the generality of
the foregoing, you acknowledge and agree that we do not guarantee: (i) the
quality, accuracy or results of the Service; or (ii) that our testing of the SCL
Files provided by you will be accurate or error free. Regardless of whether we
detect errors in your SCL Files, we are not responsible for the use of your SCL
Files by you or any third party.     Limitation of
liability  IN NO
EVENT WILL WE BE LIABLE TO YOU OR ANY THIRD PERSON FOR ANY INDIRECT,
CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, SPECIAL OR PUNITIVE DAMAGES, INCLUDING
DAMAGES FOR ANY LOST PROFITS OR LOST DATA ARISING FROM YOUR USE OF THE WEB SITE
OR THE SERVICES, EVEN IF WE ARE AWARE OR HAVE BEEN ADVISED OF THE POSSIBILITY
OF SUCH DAMAGES. TO THE FULLEST EXTENT PERMITTED BY
LAW, WE DISCLAIM ANY WARRANTIES FOR SERVICES RECEIVED THROUGH OR ADVERTISED ON
THE WEB SITE. NOTWITHSTANDING ANYTHING TO THE CONTRARY CONTAINED HEREIN,
OUR LIABILITY TO YOU FOR ANY CAUSE WHATSOEVER, AND REGARDLESS OF THE FORM OF
THE ACTION, WILL AT ALL TIMES BE LIMITED TO TEN DOLLARS ($10). THE PARTIES
AGREE THAT THESE LIMITATIONS ARE AN ESSENTIAL PART OF THIS AGREEMENT AND SHALL
SURVIVE ANY REMEDY’S FAILURE OF ESSENTIAL PURPOSE. CERTAIN STATE LAWS DO NOT
ALLOW LIMITATIONS ON IMPLIED WARRANTIES OR THE EXCLUSION OR LIMITATION OF
CERTAIN DAMAGES. IF THESE LAWS APPLY TO YOU, SOME OR ALL OF THE ABOVE
DISCLAIMERS, EXCLUSIONS OR LIMITATIONS MAY NOT APPLY TO YOU, AND YOU MAY HAVE
ADDITIONAL RIGHTS .     Links and Third Party Service  As a courtesy to you, the Web Site may
offer links to other websites. We are not responsible for the contents of any
website pages created and maintained by organizations independent of us.
Visiting any such third-party website pages is at your own risk. We have no
control of these third-party website pages, nor can we guarantee the accuracy,
completeness, or timeliness of information in third-party website pages. Your
use of such information is voluntary, and your reliance on such information
should be made only after independent review. References to commercial products
or services within any such third-party website pages do not constitute or
imply an endorsement by us. If you receive any products or services from a
third party web site, your use of such services or receipt of such products
will be subject to the terms and conditions of the applicable third party web
site. By using the Web Site and Services you acknowledge that we are responsible
neither for the availability of, nor the content located on or through any
third-party website pages .     Customer
Communications  By
signing up for an Account with us, you are agreeing in to receive
communications from us regarding our products and services, including customer
newsletters, special offers and exclusive discounts. You may opt out of these
communications at any time by following the "unsubscribe" link at the
bottom of any email.     Confidentiality  We will treat all SCL
Files you provide to us in confidence and will disclose the SCL Files only to
our employees and agents that are providing the Services. We may publish
summary statistics from submitted SCL Files to showcase the most common aspects
of SCL Files and highlight ways to improve the usage of SCL Files. The content of your SCL Files will not be
provided to any third party.    Trademarks  Triangle MicroWorks ™ is
our trademark. Such trademark and other marks, logos, and names used on or in
connection with the Web Site may not be used in connection with any product or
service that is not under our ownership or control. Furthermore, such
trademarks may not be used in any manner that is likely to cause confusion
among customers or in any manner that disparages or discredits us. All other
trademarks not owned by us, which appear on the Web Site are the property of
their respective owners, who may or may not be affiliated with, connected to,
or sponsored by us.     Consideration  You acknowledge that these Terms of
Service are supported by reasonable and valuable consideration, the receipt and
adequacy of which are hereby acknowledged. Without limiting the foregoing, you
acknowledge that such consideration includes, without limitation, your use of
the Web Site and receipt or use of data, content, products and/or Services
provided through the Web Site.     Jurisdiction,
Arbitration, Applicable Law and Limitations  The Web Site is created and
controlled Triangle MicroWorks, Inc. in the State of North Carolina, U.S.A.  YOU AGREE THAT DISPUTES BETWEEN YOU
AND US WILL BE RESOLVED BY BINDING, INDIVIDUAL ARBITRATION AND YOU WAIVE YOUR
RIGHT TO PARTICIPATE IN A CLASS ACTION LAWSUIT OR CLASS-WIDE ARBITRATION. You agree that this Agreement will be governed by and
construed in accordance with the laws of the United States of America and the
State of North Carolina, without regard to its conflicts of law provisions. You
agree that any dispute or claim arising out of, or in connection with, this
Agreement shall be finally settled by binding arbitration in Raleigh, North
Carolina, in accordance with N.C. Gen. Stat. § 1-569.1 et seq . (the
“Uniform Arbitration Act”) and the then-current rules and procedures of the
American Arbitration Association by one (1) arbitrator appointed by the
American Arbitration Association. The arbitrator shall apply the law of the
State of North Carolina, without reference to rules of conflict of law or
statutory rules of arbitration, to the merits of any dispute or claim. Judgment
on the award rendered by the arbitrator may be entered in any court of
competent jurisdiction. The parties agree that, any provision of applicable law
notwithstanding, the arbitrator shall have the authority to award the
prevailing party its costs and reasonable attorneys’ fees . You agree that
service of process upon you in any such action may be made if delivered in
person, by courier service, by telegram, by telefacsimile or by first class mail,
and shall be deemed effectively given upon receipt .   Use of the Web Site is not authorized
in any jurisdiction that does not give effect to all provisions of this
Agreement. We make no claims or assurances that the Web Site and Services are appropriate
or may be utilized or accessed outside of the United States. In the event that
the arbitration or class action terms are determined to be unenforceable or
invalid for any reason, you agree that all legal proceedings arising out of or
in connection with this Agreement, or Services available on or through the Web
Site must be filed in a federal or state court located in Raleigh, North
Carolina within one year of the time in which the events giving rise to such
claim began, or your claim will be forever waived and barred. You expressly
submit to the exclusive jurisdiction of said courts and consent to
extraterritorial service of process.     General  a.  Enforceability. If any provision in
this Agreement is found to be void, invalid or otherwise unenforceable, then that
provision shall be deemed to be superseded by a valid, enforceable provision or
that matches the intent of the original provision as closely as possible. The
remainder of the Agreement shall continue to be enforceable and valid according
to terms contained herein.  b.  Entire Agreement. This Agreement sets forth the entire
agreement between you and us pertaining to your use of the Web Site and the Services.
We reserve the right, at our sole discretion, to change, modify, add, or delete
portions of this Agreement at any time without further notice. If we do this,
we will post the changes to this Agreement on this page and will indicate at
the top of this page the new effective date (i.e., the LAST UPDATED date). Your
continued use of the Services or the Web Site after any such changes
constitutes your acceptance of the revised Agreement. If you do not agree to
abide by this Agreement or any future revised Agreement, do not use or access
the Services or the Web Site. It is your responsibility to regularly review
this Agreement. If any provision of this Agreement is held invalid or
unenforceable, such provision shall be revised to the extent necessary to cure
the invalidity or unenforceability, and the remainder of this Agreement shall
continue in full force and effect. Our failure to exercise any right or
provision of this Agreement shall not constitute a waiver of such right or
provision .  c.  No Waiver. The failure of us to
exercise or enforce any right or provision of this Agreement shall not
constitute a waiver of said right or provision. Neither party hereto shall be
deemed to be in default of any provision of this Agreement or for failure in
performance resulting from acts or events beyond the reasonable control of such
party and arising without its fault or negligence, including, but not be
limited to, acts of God, civil or military authority, interruption of electric
or telecommunication services, civil disturbances, acts of war or terrorists,
strikes, fires, floods or other catastrophes.  d.  Headings & Construction. The
section titles in this Agreement are for your convenience only and carry no
contractual or legal effect whatsoever. The language in this Agreement shall be
interpreted in accordance with its fair meaning and shall not be strictly
interpreted for or against either party.  e.  Contact Us. For questions regarding the
terms of this Agreement, contact us at http://www.trianglemicroworks.com/support , or Triangle MicroWorks, Inc., 2840 Plaza Place, Suite 205,
Raleigh, NC 27612.   Copyright © Triangle MicroWorks, Inc. All Rights Reserved.                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-6/tase-2-features
                                                  Request a Quote          Source Code Library Video Playlist               IEC 60870-6 (TASE.2/ICCP) Our IEC 60870-6 (TASE.2/ICCP) Source Code Library provides a quick and cost effective way to implement a Client, Server, or Bi-Directional TASE.2/ICCP device or application.  The library was designed from the ground up to fully support IEC 60870-6 services and efficiently use resources.  We offer our Library with .NET Components and in C++ for maximum flexibility across many different applications.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Features  Services Supported  .NET Components  What's New       Overview    Features    Services Supported    .NET Components    What's New        Related Products           Iron  Distributed Test Manager (DTM)  SCADA Data Gateway          Library Features    Servers can be built in weeks, rather than months with our unique approach to integrating application data with IEC 60870-6 and MMS. Our library was designed from the ground up to operate efficiently, which makes it a great choice for embedded system Server implementations.          Full IEC 60870-6 Library           Configuration of RFC 1006 layer parameters            Implementation and Configuration of entire seven layer stack            Support and Configuration of MMS parameters            Supports security both weak (MMS Authentication) and strong (IEC 62351)            Support for blocks 1,2 and 5, including DataSets, DataSet TransferSets, and SBO Controls            Support for ICCP/TASE.2 security            All libraries support client only, server only, and bidirectional applications          The library supports both 1996.8 and 2000.8 versions of IEC ICCP/TASE.2 protocol.                   Supported Features          Read Services          Write Services           Reporting Services            Controls            Dynamic Data Sets            Discovery         For a full list of supported IEC 60870-6 services, see IEC 60870-6 Services .            Simple and Powerful Integration           Additional lower level interfaces available for specialized device integration requirements          Integrates with application at object level, not transaction level          Advanced data modeling techniques simplify generation of conformant servers              Additional Standards Built In           IEC 62351 security specification has been implemented for MMS, TLS, and IEC 60870-6                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-6/services-supported
                                                  Request a Quote          Source Code Library Video Playlist               IEC 60870-6 (TASE.2/ICCP) Our IEC 60870-6 (TASE.2/ICCP) Source Code Library provides a quick and cost effective way to implement a Client, Server, or Bi-Directional TASE.2/ICCP device or application.  The library was designed from the ground up to fully support IEC 60870-6 services and efficiently use resources.  We offer our Library with .NET Components and in C++ for maximum flexibility across many different applications.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Features  Services Supported  .NET Components  What's New       Overview    Features    Services Supported    .NET Components    What's New        Related Products           Iron  Distributed Test Manager (DTM)  SCADA Data Gateway          IEC 60870-6 Services           Supported IEC 60870-6 Services          Read services  Fully Supported    Read at any level in the hierarchy  Return configurable errors when a DataAttribute is read                Write services  Fully Supported    Write at any level in the hierarchy  Return configurable errors when a DataAttribute is written                Reporting Services  Fully Supported    DataSet Transfer Sets  Any Valid Data Set can be used in a Transfer Set  Supports all trigger options  All optional items included in report                Discovery  Fully Supported    Supports Server Device Discovery by Client  Server Object Model and Supported Services                Control Models  Fully Supported    All Control Types are supported  Select Before Operate  Checkback Name                Dynamic DataSets  Fully Supported    Persistant  Non-Persistant                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-6/-net-components
                                                  Request a Quote          Source Code Library Video Playlist           Download Evaluation              IEC 60870-6 (TASE.2/ICCP) Our IEC 60870-6 (TASE.2/ICCP) Source Code Library provides a quick and cost effective way to implement a Client, Server, or Bi-Directional TASE.2/ICCP device or application.  The library was designed from the ground up to fully support IEC 60870-6 services and efficiently use resources.  We offer our Library with .NET Components and in C++ for maximum flexibility across many different applications.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Features  Services Supported  .NET Components  What's New       Overview    Features    Services Supported    .NET Components    What's New        Related Products           Iron  Distributed Test Manager (DTM)  SCADA Data Gateway          .NET Components    Based on Triangle MicroWorks, Inc. industry-proven Source Code Library design  Supports Windows and Linux  Supports all .NET Languages (C#, J#, Managed C++, VB .NET, etc.) and tools  Compatible with .NET 4.0 framework and .NET Core  Documentation including API and user manual  Source code version includes corresponding ANSI-Standard C and C++ Source Code Libraries  Includes built-in simple database with save/restore capabilities; also supports user-defined database  Ideal for quick development of products and tools requiring TASE.2 / ICCP support  Scalable for large implementations  Typical product integration time of less than one week                                                                                     Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-6/what's-new
                                                  Request a Quote          Source Code Library Video Playlist               IEC 60870-6 (TASE.2/ICCP) Our IEC 60870-6 (TASE.2/ICCP) Source Code Library provides a quick and cost effective way to implement a Client, Server, or Bi-Directional TASE.2/ICCP device or application.  The library was designed from the ground up to fully support IEC 60870-6 services and efficiently use resources.  We offer our Library with .NET Components and in C++ for maximum flexibility across many different applications.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Features  Services Supported  .NET Components  What's New       Overview    Features    Services Supported    .NET Components    What's New        Related Products           Iron  Distributed Test Manager (DTM)  SCADA Data Gateway          What's New  .NET Components Current Release:  V5.0 Release Date:  May 2024   C++ Library  Current Release:  V5.0 Release Date: May 2024   Newest Features in Latest .NET Release   Added support for dotnetcore for both Windows and Linux  Upgraded to openssl 3.0  Improved example client and server code samples and provided sample certificates for testing  Added capability to use TLS only or MMS only certificate based security  Added ReadNodeArray functionality for reading arbitrary lists of Nodes   Newest Features in Latest C++ Release   Improved Examples for client and server and provided sample certificates for testing  Added capability to use TLS only or MMS only certificate based security  Added ReadNodeArray functionality for reading arbitrary lists of Nodes   Key Additions to Recent .NET Releases  v4.5       Fix certificate based security     Add Examples for client and server    v4.4.4           Upgrade openssl to 1.1.1p.      Fix C library denial of service vulnerability for uninitialized pointer   v4.4.3           StartLibrary now takes no arguments. Each server can now turn on/off the 7 layer stack restriction instead of this setting being application wide.     To turn on 7 layer stack for each server individually use the API: SetlsMultiPlex(Boolean).   v4.4.2        All SetXXX methods on types derived from Tase2DataAttribute and the Value class types no longer accept the “ autoWrite ” argument and no longer support this API . If a write needs to occur, one of the write methods on the Tase2Client must be called.     v4.4.1      Added Begin/End dataset read callbacks  Added SetVersion to the Tase2Client Disconnect API  Added optional abort parameter to the Tase2Client Disconnect API       v4.4.0       Added SetVersion to the Tase2Client API .     v4.3.1    Added WriteNodeAlternateAccess  API on Tase2Client for writing nodes as alternate access  Added ReadNodeAlternateAccess  API on Tase2Client for reading nodes as alternate access    v4.1.3    Added SBOSelectTimeout to the API for server side support  Added 64 bit support  Enhanced seven layer stack support   v4.0.0    Added 3 new extended types: DiscreteQTimeTagExtended , RealQTimeTagExtended , StateQTimeTagExtended    v3 .6.0    Enhanced the connect/disconnect code to be more robust when connections are lost  Added n SSL Method to strong security configuration  Added ability in server to keep track of multiple connections in bidirectional mode                                                                                  Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/scl-navigator-pages/what's-new
                                                  Request a Quote          SCL Navigator Video Playlist           Download Evaluation              What's New - SCL Navigator Current Release: v1.4 Release Date: May 2024   Newest Features in this Release   Added the ability to drag from Logical Nodes Library and drop in the Substation level.  Improved management of LN for Ed 2 or Ed 2.1 SCL files containing Ed 1 IED(s).  Added the ability to create custom libraries.  Added a new "Compare ISD" tool.  Removed the proprietary "SCL verify" tool and replaced it with the standard tool "RiseClipse" Verify.  Added support for the creation of FSD (Functional Specification Description) files.  Added support for the creation of ISD (IED Specification Description) files.                  Overview  What's New  System Requirements       Overview    What's New    System Requirements        Related Products           Test Suite Pro (TSP)  Distributed Test Manager (DTM)          Key Additions to Recent Releases  v1.3.6   Update to Sentinel runtime v9.x  Added a Quick Start Page on start up  Enhanced Compare Model functionality between SCD and other files.  Implemented addition of Comments and Private Elements.  Added function to Import/Export SCL Templates via XML file  Added the ability to export Data Templates as a NSD file  Added the ability to import NSD files as a new User Logical Node Library.  Added functionality to generate Mapping files for DNP3 Points from 61850 SCL.  Updated to the latest RiseClipse Verify version.   v1.3.4   Update to the latest RiseClipse Verify version 1.2.4 (previously OCL Verify).  Update LN Library to the Latest NSD.  New line Editor.  Bug fixes and stability improvements.   v1.3.3   Added OCL Verify, the tool can now compare the SCL files to a set of OCL and NSD Rules.  Added a utility to manage IEDs IPs:  This utility is used to quickly reassign IP Addresses, Subnet Masks, and Gateway Addresses to multiple IEDs simultaneously.   v1.3.2   Added the ability to delete DAinstancesinbulk even without value elements   v1.3.1   Added Model Comparison to the SCL Navigator. The comparison tool will review two identified files and evaluate any SCL Element and report states of Missing, Matched or Modified.   v1.2.1   Added sAddresss Import and Export functionality   v1.2   Stability improvements   v1.1.1   Added support for the Edition 2.1 Schema (2007B4)  Added schema conversion to convert the 2007B (Edition 2) schema to the 2007B4 (Edition 2.1) schema  Implemented an expert mode and normal mode. The normal mode provides greater guidance for new users and the expert mode allows full control over the SCL File.  Added on-the-fly model validation which validates the schema, lists possible values for an attribute, and highlights values as default, optional, or mandatory as the model is updated  Improved the search capability  Improved configuration of datasets and control blocks  Added capability to add multiple instance values for an object   v1.0 - the first release of SCL Navigator  SCL Navigator V1.0 is a major new release with many new features to make editing, viewing, and checking SCL files easier. Here are highlights of this release:   Updated editing interface to use a simplified XML view  Added a new view to show SCL file as an interpreted data model  Added a new editor to manage data type templates with the capability to edit and reuse templates for data objects and logical nodes  Added new capability to automatically clean data templates to remove unused template information from the data type templates section  Improved SCL Verify with the capability to highlight specific errors and warnings in the editor window  Extended logical node library to include -90-7 DER models   SCL Forge has been repackaged as SCL Navigator    SCL Forge has been repackaged into SCL Navigator which will be released separately from 61850 Test Suite starting in January 2018   SCL Forge Upgrade Information   SCL Forge licenses with maintenance expiration dates of September 1, 2017 or later can be upgraded to SCL Navigator for free  Contact support for a new product key to upgrade  Full details about upgrading to SCL Navigator can be found in the TMW Customer Portal by downloading the "What's New" document for SCL Navigator V1.0  Or contact sales@TriangleMicroWorks.com for information about upgrading   Prior releases of SCL Forge are listed below: v3.9.1 - Improved checking of SCL files with SCL Verify  v3.8.0 - Added support for private and text fields in EnumType elements  v3.7.0 - New licensing options including a network-based license and support for VM’s  Updated SCL File Checking with new tests and better debugging capability Added ability to filter SCL File Checking results based on test type, severity, IED, and other parameters   v3.6.1 - The Object Model now shows the relationships between a point's Timestamp, Quality, and Value in the Object Model  Added the ability to sort by columns in the Flat View pane   v3.5.59 - Added SCL File Consistency and Completeness Check  v3.5.54 -  Added ResvTms to Settings Group items in Services section Updated Schema to 1.6 for Ed. 1 Updated Ed. 1 and Ed. 2 templates used to build new SCL files Added ability to add an LN under AccessPoint  v3.5.15 - Added support for Ed.2, including Tracking Nodes, Security, Object Model changes, and Function Constraints.  Added new rules for custom CDCs.                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/scl-navigator-pages/system-requirements
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               System Requirements                 Overview  What's New  System Requirements       Overview    What's New    System Requirements        Related Products           Test Suite Lite  SCL Navigator  Distributed Test Manager (DTM)          SCL Navigator supports the following versions of Windows TM :   Windows Server 2012 (64-bit)  Windows 10 (32-bit and 64-bit)  Windows Server 2016 (64-bit)   Windows Server 2019 (64-bit)    Note:  SCL Navigator supports virtual machines (VM)  Minimum system requirements:   1.5 GHz processor; recommended: 2.4 GHz multi-core processor  2GB RAM; recommended: 4GB RAM  Windows 7 or higher (not Embedded Compact)  Microsoft .NET 4.6.2 Framework                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dtm-pages/network-load-testing
                                                   Request a Quote          DTM Video Playlist           Download Evaluation             Load Testing with DTM      Use Case          Network load testing can include testing a single device or an entire system.  In either case, load testing requires that many pieces of the system to be present.  This can be challenging to setup in a lab, especially if you do not have all of the system components. It is difficult to perform load testing with tools designed to simulate a single device because repeating the test consistently in order to verify results is almost impossible. DTM solves this challenge by simulating up to hundreds of devices across a network and coordinating test scenarios with all the simulated devices from a single interface.  There are many different scenarios for load testing a device in a SCADA system. The Device Under Test might be a single IED or RTU and the desire is to connect as many clients as possible and make sure the device functions correctly. Alternatively, the device might be a gateway, data concentrator, or HMI in which case the test environment could simulate multiple IEDs/RTUs communicating with the Device Under Test.                         Overview  Gateway Testing  Network Load Testing  Substation Communication Testing  Cybersecurity Testing  Architecture  What's New  System Requirements       Overview    Gateway Testing    Network Load Testing    Substation Communication Testing    Cybersecurity Testing    Architecture    What's New    System Requirements        Related Products           61850 Test Suite Pro (TSP)  SCL Navigator  Communication Protocol Test Harness (TH)                   The goal is to put the device under test in conditions in excess of what they are likely to encounter in the field and to see where performance limitations exist.  DTM can be configured to create a simulated system that stresses the device under test beyond what it would see in the field.             DTM Capabilities Used   Test network or software application under load with hundreds of simulated devices across a real network  Generate messaging traffic with standard communication protocols found in SCADA systems  Distribute the simulation of devices across multiple host PCs or VMs on the network  Create different levels of traffic with configurable data changes driven automatically, through scripting, or with CSV file playback          Example: Load Testing a Device   This example shows a typical scenario for load testing a gateway. In this test system, the gateway is communicating with multiple IEC 61850 servers and mapping the data and controls between IEC 60870-5-104 and IEC 61850.  There are four DTM Hosts, each running on a separate PC.  Each DTM Host is simulating different devices in the system.  The three hosts on the bottom are simulating IEC 61850 Servers and the host on top is simulating an IEC 60870-5-104 Master.  Each host is running on a separate PC, which distributes the simulation across the network. The Device Under Test connects to each of the distributed devices with different IP addresses and port numbers.  Data changes in the IEC 61850 devices can be driven automatically or through custom scripts to generate specific traffic. Data changes can also be driven by playing back captured CSV files from real systems.  Data change time steps can be configured to a specific number of seconds or follow the captured time steps from the CSV file. The data transition rate can be increased to stress the gateway.  In this way, load testing scenarios can be created to assist measurements of maximum throughput, latency, reliability, and other performance factors for the gateway. In addition to performance testing, custom scripts can be written to test the mapping of data through the gateway.     Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                            Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dtm-pages/substation-communication-testing
                                                   Request a Quote          DTM Video Playlist           Download Evaluation               Substation Communication Testing with DTM      Use Case        Substation automation has increased the complexity of communications in substations. Part of this complexity arises from IEC 61850 which has introduced many new communication paths with GOOSE, Sampled Values, and MMS based messages that flow between different equipment in the substation. Implementing IEC 61850 on the process bus is moving hard-wired connections to network based messaging which requires a new form of verification versus traditional substations. This new virtual wiring can be verified by simulating the communications of different components in the system together and checking that the right messages are being transmitted and received throughout the system. Testing the communication configuration of the system early in the design process can help reduce risk during the deployment phase                         Overview  Gateway Testing  Network Load Testing  Substation Communication Testing  Cybersecurity Testing  Architecture  What's New  System Requirements       Overview    Gateway Testing    Network Load Testing    Substation Communication Testing    Cybersecurity Testing    Architecture    What's New    System Requirements        Related Products           61850 Test Suite Pro (TSP)  SCL Navigator  Communication Protocol Test Harness (TH)                              DTM Capabilities Used   Simulate the communications and data model for each Client, Server, Publisher, or Subscriber in a substation  Load IED configurations based on SCL Files (IEC 61850), DNP3 XML Device Profiles, or point lists  Import an entire IEC 61850 substation configuration based on SCD File  Simulated IEC 61850 Clients subscribe to Control Blocks (Reports, Logs, GOOSE) or issue commands to Servers  Simulated IEC 61850 Servers publish and subscribe GOOSE messages  Simulated DNP3 or IEC 60870-5 Masters can read, poll, or request reports from Outstations  Simulate gateways and protocol translators with scripting and simulated data devices          Example:  Simulating an IEC 61850 Substation  This example shows a simulation of multiple bay controllers, protection devices, and an HMI in an IEC 61850 based automated substation.  The station bus has MMS based Client/Server connections with Reports, Controls, and Logs. The process bus has broadcast GOOSE messages and Sampled Values streams which are subscribed to by different components.  The Object Models, Data Sets, Control Blocks, and subscription configurations need to be verified to ensure that the right information is available to different devices across the substation.  DTM is simulating the communications and Object Models of the Bay Controllers and protection IEDs. The simulation can be run on a single PC or distributed across multiple hosts to test the communication configuration of the substation with a real station bus and process bus.  IED configurations are loaded from SCL files (.ICD or .SCD) including the Object Models, services, and communications. The HMI is a simulated IEC 61850 Client which subscribes to Control Blocks in the IEDs and issues controls to the IEDs.  Test cases can be built on top of these simulated devices to test specific substation automation processes.  In this example, GOOSE message publishing and subscribing configurations are tested by generating specific data changes which test GOOSE signals all the way through the Publisher's Object Model, Data Set, and Control Block to the Subscriber's External Reference. Also in this example, the IEC 61850 Client is testing the Report Control Blocks that are configured in the IEDs.  IED behavior can be modeled with scripting capability in DTM which is highly configurable. Variables in scripts can be driven by the transition of specific Data Attributes in the Object Model. Scripts can also directly set Data Attributes in the Object Model. GOOSE publishing and subscribing can tie directly into scripting functions based on configuration of Control Blocks and Data Sets in SCL Files. In this way, scripts can be created to monitor and verify entire data paths in the system. Data changes can be driven by playing back captured CSV files from real systems.  Data change time steps can be configured to a specific number of seconds or follow the captured time steps from the CSV file.   Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dtm-pages/cybersecurity
                                                   Request a Quote          DTM Video Playlist           Download Evaluation             Cybersecurity Testing with DTM        Use Case      Security is critical in modern SCADA systems. Many different security capabilities are now built into the standard communication protocols used by SCADA devices. It is important to verify that secure communications are configured correctly at the system level.  Frequently, the first time all of the system components are deployed together is when the system is constructed. By simulating system components with DTM, configuration issues can be identified earlier to ensure security requirements will be satisfied in the final system.           DTM Capabilities Used   Simulate the communications of multiple devices in a substation or other power system  Coordinate simulation across network with real network equipment (routers, switches) to create realistic network test scenario  Generate messaging traffic with standard communication protocols found in SCADA systems  Simulate both sides of SCADA system (Client/Server, Master/Outstation) with a single tool  Separate the simulated SCADA traffic from DTM test coordination traffic to create "clean" SCADA traffic on the network under test                         Overview  Gateway Testing  Network Load Testing  Substation Communication Testing  Cybersecurity Testing  Architecture  What's New  System Requirements       Overview    Gateway Testing    Network Load Testing    Substation Communication Testing    Cybersecurity Testing    Architecture    What's New    System Requirements        Related Products           61850 Test Suite Pro (TSP)  SCL Navigator  Communication Protocol Test Harness (TH)             Ready to get started?     Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dtm-pages/architecture
                                                   Request a Quote          DTM Video Playlist           Download Evaluation              DTM Architecture  The DTM architecture was designed to be as flexible as possible in order to support a wide range of use cases. Because SCADA systems are inherently distributed, a tool for simulating and validating SCADA systems must be distributed. DTM is a distributed system made up of a variety of applications that can be configured to satisfy a wide range of requirements. The main components are:   DTM Host – a service that runs all the protocol clients/servers (i.e. IEC61850, IEC60870, DNP, etc.). Each DTM Host is an application, usually run as a Windows Service, which can host multiple protocol specific clients and/or servers. Multiple copies of the DTM Host application can be run on a single computer, or spread across any number of computers on your network.  DTM Administrator – Provides the user interface to manage the DTM network. This application can be run from any computer that has access to the network on which the DTControl and DTHost applications are running.  Additional Components – additional applications coordinate activities between the different components of DTM.                  Overview  Gateway Testing  Network Load Testing  Substation Communication Testing  Cybersecurity Testing  Architecture  What's New  System Requirements       Overview    Gateway Testing    Network Load Testing    Substation Communication Testing    Cybersecurity Testing    Architecture    What's New    System Requirements        Related Products           61850 Test Suite Pro (TSP)  SCL Navigator  Communication Protocol Test Harness (TH)                Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dtm-pages/what's-new
                                                   Request a Quote          DTM Video Playlist           Download Evaluation              What's New - Distributed Test Manager Current Release: V1.6.1 Release Date:  October 2023                Overview  Gateway Testing  Network Load Testing  Substation Communication Testing  Cybersecurity Testing  Architecture  What's New  System Requirements       Overview    Gateway Testing    Network Load Testing    Substation Communication Testing    Cybersecurity Testing    Architecture    What's New    System Requirements        Related Products           61850 Test Suite Pro (TSP)  SCL Navigator  Communication Protocol Test Harness (TH)          Newest Features in this Release   Various Bug Fixes     Key Additions to Recent Releases  V1.6   Introducing the new Test Action functionality allowing for Gateway test automation. This new feature is designed to help test communications gateways and protocol converters as well as other communication scenarios.  The new Test Action Manager will automate test cases where a device or system under test is expected to react data changes in connected devices by sending data messages to other connected devices. For example, the Test Action Manager can control data values in numerous DTM simulated IEC 61850 servers that are connected to a real communications gateway which in turn is connected to a DTM simulated IEC 60870-5-104 controlling station.  The Test Action Manager can then conduct hundreds or thousands of simulated data changes in the servers while monitoring the controlling station for expected data and quality updates.  All of the testing can be set up using CSV files without writing any scripts due to prepackaged Test Actions that come with DTM 1.6.   V1.5.1   Added a turnkey mode and additional options to the DTM Test Manager.  Allow Terminals as children of Power Transformers in the functional node simulation support.  Added support for control block reservations before enable for IEC61850 client channels.  Added IEC 60870-5-101/104 Read, Reset Process, and Test command support.  Add new FindDataAttributeFromMMSNameEx to support accessing attributes where two nodes at the same parent have the same name (but different functional constraints).  Clean up TASE2 command and model view support.   V1.5.0   Implemented logical node simulation and related support for the substation simulation.  Added IEC 61131 support.  Implemented 7-layer addressing for simulated IEC 61850 devices.  IEC61850: implemented Quality and Beh.  Added IEC 61850 LGOS and LSVS support to allow monitoring of GOOSE and Sampled Values performance from a client.  Improved performance of DTHost.  Added ability to specify the resource type when importing resources.  Added ability to capture data for subsequent replay via CSV files.  Changed the return type for DTExec from void to a value (0 or -1) to assist users who use scripting and batch files.  Added DTMAdmin configuration option to display time in local format instead of UTC.  Added new sample workspaces. Updated existing ones.   V1.4.1    Updated DTM Interface with a new DTM Start Page for easy access to predefined workspaces, technical documentation, training videos and technical support.    Added MDNP Support for File Transfer    Improved performance of devices simulation and GUI    Added IEC 60870-5-101/104 Secure Authentication functionality    V1.4   Redesigned the "Import Substation" feature to allow easier configuration and simulation of the devices in your substation. Three new configuration modes have been added to control how devices are imported into a workspace.  Added a new Test Manager module to allow users to run their customized test scripts automatically. This feature can be used to automate and executed your entire Test Plan.  Added automatic management for DTHosts with "one-per computer" and "one-per-device" modes. "User-Managed" mode is still available for users who need more control.  Added the ability to control DTM's functionality via scripting. This allows other engineering and test tools to take advantage of DTM's powerful simulation features.  Added several script methods to support IEC 61850 dynamic datasets and file transfer.  Added new devices addressing features. IP addresses can now be automatically added and removed from your computer based on the device requirements resulting in more realistic testing scenarios with less set up time. IPv6 capability has also been added.  V1.3.1   Added support for folders in the Load/Save workspace dialog to allow users to organize workspaces  Improved IEC 61850 controls, GOOSE configuration and logging  Various bug fixes (see release notes for details)    V1.3.0    Added a multi-device editor that can create or re-configure multiple devices, channels, adapters, and sessions across multiple hosts at the same time.  Added support for IEC 60870-6 (ICCP/TASE.2) communication protocol  Added 64-bit support for larger simulations  Updated user interface with more capabilities and improved ease of use  Added new point list capability to select and show specific points across multiple devices  Improved handling of resource files including capability to monitor IEC 61850 SCL files for changes  Enhanced many scripting functions to return improved information upon execution  Improved statistics for simulated devices  Improved multi-device editor including filtering and search options  Added phasor diagram and other enhancements for IEC 61850 Sampled Values  Added support for routable GOOSE for IEC 61850  Expanded IEC 61850 model view to filter data across multiple IEDs  Added capability to “zip” log files to send to technical support   V1.2.1   Added support to register JavaScript callback to specify behavior of IEC 61850 controls  Added ability to check for updates for DTM  Implemented an application level heartbeat to monitor DTHosts on the DTM Network   V1.2.0   Added a multi-device editor that can create or re-configure multiple devices, channels, adapters, and sessions across multiple hosts at the same time.  Improved simulation options for CSV file playback, auto-data changes, scripting, and commands.  Simulation options can be configured easily across the entire DTM workspace.  Improved auto data changes so the user can be select specific points to change based on DNP3 class, IEC 61850 data sets or functional constraint, IEC 60870-5 interrogation group, or custom  configuration.  Added ability to view all points points across devices in one point list (right click on top of workspace -> show -> point list).  Expanded support for IEC 61850 Sampled Values for both publishers and subscribers.  Made multiple improvements to IEC 61850 commands and browsing.  Added support for IEC 61850 File Transfer.  Added option to execute scripts in DTHost processes which allows the script to run on the same process on the same computer as the simulated device.  Added capability to multi-select nodes (devices, folders, scripts, etc.) in both the Workspace and Network views.  This allows invoking the same function across multiple nodes with one click.  Added ability to configure database for DNP3, IEC 60870, and Modbus Masters.  Added support of drag and drop of DTHost(s) to different computers in network tab.  Added option to automatically create and delete DTHosts as needed when loading a saved workspace.  Added support to show IEC 61850 data model in flat point list which allows more filtering capability.  Added scripting support for DNP3 and IEC 60870-5 controls.  Added support for 16 bit registers in Modbus.  Added support for DNP3 Pulse On/Off controls.  Please take note of a couple changes to DTM workspaces when upgrading to DTM Version 1.2.0:    The ‘yield’ JavaScript function in InSight has been deprecated. The new ‘delay’ function should be used instead.  Workspace commands have been reworked to improve usability and flexibility. Workspaces saved in DTM V1.1.0 or earlier may not load correctly in DTM Version 1.2.0 or later. Commands will need to be added back to the workspace in V1.2.0.   V1.1.0.19   Improved the organization and management of multiple simulated devices in DTM workspaces  Enhanced the ability to configure multiple devices from a CSV file or IEC 61850 SCL Files  Enhanced IEC 61850 Object Model Viewer  Added capability for an IEC 61850 Client in DTM to perform discovery of an IEC 61850 Object Model for real or simulated IEDs  Increased support for configuration of IEC 61850 Control Blocks for GOOSE, Sampled Values, Reports, and Logs  Enhanced CSV data playback capability to simulate data changes across multiple simulated devices for all supported protocols  Added automatic check for DTM software updates  Added capabilities to save/restore multiple layouts, save protocol analyzer logs, start/stop DTM Hosts automatically, and store workspaces in folders managed by a source control system   V1.0.13   Improved support of serial channels  Added capability to configure security parameters for all supported protocols  Improved stability when simulating multiple devices and when distributing simulation   V1.0.12    Added GOOSE Viewer to display IEC 61850 GOOSE messages  Added Report Viewer to display IEC 61850 Reports  Added capability for IEC 61850 External Reference event subscriptions in JavaScript   V1.0.11    Added support for DNP3 Secure Authentication                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dtm-pages/system-requirements
                                                   Request a Quote          DTM Video Playlist           Download Evaluation            DTM System Requirements      DTM supports the following versions of Windows TM :   Windows Server 2012 (64-bit)  Windows 10 (32-bit and 64-bit)  Windows Server 2016 (64-bit)   Windows Server 2019 (64-bit)                   Overview  Gateway Testing  Network Load Testing  Substation Communication Testing  Cybersecurity Testing  Architecture  What's New  System Requirements       Overview    Gateway Testing    Network Load Testing    Substation Communication Testing    Cybersecurity Testing    Architecture    What's New    System Requirements        Related Products           61850 Test Suite Pro (TSP)  SCL Navigator  Communication Protocol Test Harness (TH)          Minimum (Simulate up to 25 Devices)   Dual Core i3 Processor or equivalent  4GB RAM  10GB Free Disk Space  Microsoft .NET 4.6.2 Framework   Recommended (Simulate up to 200 Devices)   4 Core i5 Processor or equivalent  8GB RAM  10GB Free Disk Space  Microsoft .NET 4.6.2 Framework   Extreme Load Testing (Simulate up to 1000 Devices)   8 Core i7 Processor or equivalent  16 GB RAM  10GB Free Disk Space  Microsoft .NET 4.6.2 Framework                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/custom-functional-tests
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          Custom Functional Tests   Commands may be executed individually or all run as a sequence. Conditional command sequences can be used to automatically test specific DUT functionality.    Clicking on the button next to the command description executes the command. The command description may be user modified.  A “progress bar” gives graphic indication for time of next command execution when in repeat interval mode.  A Validate Points Command verifies the device returned the expected data values.  Drag & Drop (or cut/paste) to change Command Order or copy/move to another command window.  Use custom Tcl commands or any .NET programming language (VB, C#, J#, etc.) to provide functionality not covered by existing commands.                                                                                           Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/monitor-communications
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          Monitor Communications   Built-in Protocol Analyzer for all supported protocols (DNP3, IEC 60870-5, Modbus)  Analyze communication using extensive filter options and customized colors/fonts/indentation on both real-time and capture buffers.  Passively monitor serial or TCP/IP connections.  Monitoring serial communications requires a Serial Monitor Cable .  Parse and interpret a sequence of bytes captured outside the Test Harness.    A scroll back buffer with search function provides for easy examination of previous exchanges.  Save/restore or copy/paste protocol analyzer output for post-capture analysis.                                                                                     Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/perform-conformance-tests
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          Perform Conformance Tests   Conformance Test Scripts available for DNP3 and Modbus  Performs the official Conformance Test Procedures published by the Technical Committees of each protocol  Useful as a periodic regression test during continued product development or in preparation for third party conformance testing   1. Trigger necessary changes in device inputs by:     Voltage/current source such as  OMICRON CMC156/256  DNP Virtual Terminal message  Custom Tcl command  User dialog box    2. Steps in test procedure are listed in both the procedure window and protocol analyzer  Watch this video to learn more about automated conformance testing                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/view-data-from-devices
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          View Data from Devices   View data from remote or simulated devices.  Data window provides graphical view
    of database points and values.    Create custom views
    of critical data by
    hiding unnecessary rows.  Right click menus
    allow easy modification of
    channels, sessions, sectors, and slave session
    data points.  User-defined point descriptions,
    as well as
    all other configuration data, are saved in a
    workspace file. The entire setup can then be
    easily restored as required.  Load an actual field database point map
    including point descriptions from spreadsheet
    or text file.                                                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/secure-authentication
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          Testing DNP3 Secure Authentication The Test Harness supports DNP3 Secure Authentication as defined in the IEEE 1815-2010 and 1815-2012 standards.  DNP3 Secure Authentication is based on the IEC 62351-5 security standard.  To learn more about DNP3 Secure Authentication, watch our DNP3 SA training videos on our You Tube channel.  TLS encryption is also supported  in the Test Harness by following the defined cipher suites, algorithms, and connection duration defined by IEC 62351-3.     Support for DNP3 Secure Authentication SAv2 and SAv5  Can be used to simulate Secure Authentication messages to test DNP3 Masters and Outstations  Supports remote key management for multiple users through the DNP3 protocol  Supports both symmetric and asymmetric keys as well as x.509 certificates  DNP3 Secure Authentication is also supported in the DNP3 Source Code Library  Example of Secure Authentication Challenge Reply Sequence                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/iec-60870-5-secure-authentication
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          Testing IEC 60870-5 Secure Authentication The Test Harness supports  Secure Authentication as defined in the IEC 60870-5-7 standard which is based on the cyber-security standard IEC 62351 (Parts 5 and 8).  TLS encryption is also supported  in the Test Harness by following the defined cipher suites, algorithms, and connection duration defined by IEC 62351-3.     Support for Secure Authentication for IEC 101 and IEC 104  Can be used to simulate Secure Authentication messages to test both Controlling and Controlled Stations  Supports both symmetric and asymmetric keys as well as x.509 certificates  Supports both serial and IP based networks  Supports Role Based Access Control (RBAC) - multiple users and roles (engineers, operators, viewers, admin)  Configure privileges(view, read, security, etc.) for each role  Users can be added, modified, or removed  Secure Authentication is also supported in the IEC 60870-5 Source Code Library  Example of Aggressive Mode Secure Authentication Challenge Reply Sequence                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/simulate-devices
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          Simulate Devices   Simulate master or slave devices.  Simulate communications to a large number of devices. Multiple Slave sessions can be active at one time.    When simulating a Master device, easily configured command windows allow commands to be sent once or repeated at desired intervals.  A Functional front panel may be used to change data values, or a timed operation can simulate periodic data changes.  New simulators may be added to the Test Harness by duplicating one of the sample simulators provided.                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/what's-new
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation                The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          What's New Current Release: v3 .32 Release Date: April 2024  Newest Features in this Release    Added a channel diagnostic mask bit to control whether diagnostics messages are generated.  Improve the GUI to allow IEC 60870-5-104 Timer parameter to allow timer value larger than 65535ms.    Key Additions to Recent Releases  v3.31    Replaced “slave” with “Server”, “Outstation” or “Controlled” depending on the protocol. “Server” may be used for all protocols in generic high level menus.  Added new Filter Checkbox “Octets” to control the display of the individual octets transmitted or received in the Protocol Analyzer Window.  Added DNP3 IED Certification Procedure Subset Levels 1, 2 and 3 Version 3.2.1 January—2023 Section 9 Network and Section 10 Output Control Events tests to DNP3 Outstation Conformance Tests.  Added MESA-DER DNP3 Outstation Conformance Tests.  Updated information about winpcap and npcap for TH Monitor Mode.  Added ClientSourceIpPort to TCP Channels to allow Clients to specify the IP Port to use for originating TCP Connections.  Added TCL>tmwchannel to get the connect state for a particular channel. Also added Connected state to existing TCL>tmwchannels command for all channels in the Test Harness.  Improved TCL>m102ccinx command and help. Also modify default S102 database to ignore seconds and milliseconds in comparison of a time or time range since they are NOT included in the request.  Improved behavior of S101/4 Data window related to MMENA, PMENA, and MBONA points.  Provided more help and examples for TCL>m103genericdata write and exec commands.  Improvements to tmwtest -runplugin xxx improvements with functionality and naming of error and output log files.   v3.30    Add capability for TH to save protocol log file periodically.  Add drop down item to File menu “Open Wireshark Log” to allow Wireshark log file to be read into the Test Harness Protocol Analyzer Window for display.  Add functions that can be called from Test Plugin interface to save Test Results and Test Execution Results to specified files in addition to the GUI.  Implement updated DNP IED Certification Procedure Subset Levels 1, 2, and 3 Version 3.1 from April 2022 not including sections 9 Network and 10 Output Control Events.  Create DNP Plugin for simulating and testing Master and Outstation Dual End Point functionality including timer to disconnect after the specified idle time.  DNP cleaned up TCL sdnp/mdnp/dnprxcallback commands and help menu for consistency  Create SDNP DER Plugin for simulating MESA-DER Outstation.  SDNP Add associated point functionality AssocPoint property to Binary and Analog Output Points to cause the associated input point value to change when a control from Master changes the output value.  Improve 104 Monitor Mode functionality so it does not complain about CTSTA time stamp because it does not match time on Monitor Mode Test Harness.  S101/S104 Add associated control point functionality ControlIOA Property S101 and S104 Monitor Points. This allows more flexibility than ControlPointOffset which applies to the entire sector database. Points which do not have ControlIOA set still use the offset. This must be enabled per sector by setting ControlIOAEnabled to prevent unnecessary processing.   v3.29    Implemented DNP IED Certification Procedure Subset Levels 1, 2, and 3 Version 3.0 from December 2020 including some revisions to the tests through March 2021. This involved additional tests and changes to the Test GUI to allow testing an SDNP device for Level 1, 2, or 3 compliance.  Add TCL command tmwserial channel x dtr 1|0 rts 1|0 to control DTR and RTS on COM ports.  Added TCL commands i101rxcallback, i102rxcallback, i103rxcallback, i104rxcallback, and mbrxcallback commands for layers link and phys.  Allow TCL command “s101db callback” with no specified callback function to unregister a previously registered TCL callback function. Do this for all s10xdb, m10xdb, mdnpdb, sdnpdb, mmbdb, and smbdb functions.  Implement a DNP Dual End Point Plugin to configure a Master and/or Outstation and a GUI for sending Enable Unsolicited, Read Command, Generate Event, and Disconnect for demonstration and testing purposes.  Increase Maximum supported TX and RX Fragment Sizes for DNP to 65535 for 64 bit Test Harness, remaining at 8K for 32 bit Test Harness install.  Provide more complete Monitor Mode diagnostics of DNP control requests even though the controlled DNP point index does not exist in the SDNP Monitor session database.  Add support in TCL command mdnpassignclass for sending a DNP request to remove an SDNP point from class 0 as directed by TB2021-001. This includes the ability to create combinations of requests for multiple Class Object Headers and Data Object Headers.  For MDNP Secure Authentication V5 show plain text and MAC in the protocol analyzer window when the TH MDNP acts as a DNP3 Authority and creates certification data when using a Symmetric Key Change Method as is done in other situations by the MDNP SCL code.  Add SDNP Session configuration counterSizeVarPerPoint, anlginSizeVarPerPoint, and anlgOutSizeVarPerPoint to optionally switch variation in a response to indicate precision on a per point basis even though a specific (ie. non-zero) variation is defaulted or requested.  Enhance 103 General Command support for S103 DPI points. Previously only the default points indicated in the IEC-60870-5-103 specification as supporting General Commands had that capability. Added a Property bool GnrlCmdSupported on each S103 DPI point that can be set and viewed in either the TH Data Window or by the TCL command “s103dpi add/set/get gnrlCmd”. This property will also be saved and restored from Test Harness workspaces.  More completely implement parsing of responses into TCL arrays for M101 and M104 when datavariable is specified in a command.    v3 .28    Implemented IED Certification Procedure Subset Levels 1 and 2 Version 2.8 changes. This included wording and step numbering changes as well as additional steps in some of the tests.  Add M101 /4 File Transfer GUI User Plugin .    v3 .27.1    Allow “::” in addition to “*.*.*.*” for the IPV6 Ip Address to indicate that ANY device may connect to this channel.   v3 .26.00     Added Forms based example User Plugin and documented how to modify this to implement a custom plugin in the Test Harness User Manual  Updated the SDNP Conformance Test 8.5.6.2 to be tolerant of an event class not supported by the device under test.    v3 .25.00     Added DNP SA Attributes AuthCriticalResponse & AuthCriticalUnsolicited which allow the configuration of all responses or unsolicited responses are critical such that they will be challenged.  Support for DNP FC 11 and 12 with obj50v2 has been implemented in the library and test harness.  The device profile has been updated to show support for Object Group 50, Variation 2.  Allow diagnosticMask to be set in TCL open and modify channel/session/sector commands.  Allow write value for Device Attribute test to be configured in SDNP  CTS .  The 64-bit Test Harness no longer presents the option to configure Modbus plus.  Changed the DNP default file transfer directory to the public directory, C:/Users/Public/Documents/Triangle MicroWorks /Protocol Test Harness/ FileTransfer .  Updated DNP SA User Key Change Method property so last method used is displayed in the GUI .  mdnprecordtime added channel as parameter to support broadcast.  Also, mdnptimesync type lan will also support broadcast now.  Made improvements to the help and error messages for s103 commands related to reason, relTime and fault number.  Added tmwloadworkpace command to allow ability to load workspace from TCL command window.   v3 .24.00   The Test Harness has been updated to support both 32 and 64 bit installations. This includes updates to Iron Python, TCL library, Winpcap , and OPC Client.  Add IPV6 support to the DNP / Modbus Conformance Test Configuration GUIs .   v3 .23.00    Added support for TLS  v1 .2 in the Test Harness  Updated DNP3 Device Profile to Version 2.11 April 2016  Added support for x.509 certificates in DNP3 Secure Authentication g120v8 requests  Enhanced DNP3 Secure Authentication to "remove" expired users as directed by DNP3 Secure Authentication conformance tests  Added authuser set functionality to Secure Authentication for DNP3 and IEC 101/104  Enhanced DNP3 Conformance Test Scripts based on DNP Technical Bulletin TB 2017-001 and added new feature to allow forcing multi-fragment responses by way of multiple events or points  Enhanced DNP3 Conformance Tests to allow automating more tests, requiring less tester interaction    v3 .22.00    Improved performance of Test Harness database and queue  Added LoopPeriod  parameter to configure how often to check for data being received on a channel  Update to allow access to session keys for DNP3 and IEC 101/104 Secure Authentication from the TCL Command window  Enhanced Master Command GUI for DNP3 and IEC 101/104 Secure Authentication  Expanded Windows target layer support for resolving IP address from destination host name  Several additional fixes (see Test Harness What’s New Document for details)    v3 .21.00    Added support for Secure Authentication for IEC 60870-5 (-101 and -104) according to the IEC 60870-5-7 standard based on the cyber -security standard IEC 62351 (Parts 3, 5, and 8)  Enhanced the testing framework to support scripting to test DNP3 Secure Authentication  Enhanced TCL scripting capability  Improved support of UDP in DNP3 conformance testing  Added automatic Secure Authentication statistics for DNP3 , IEC 60870-5 -101 and -104  Improved handling of TCP connection loss for IEC 60870-5-104  Added capability to configure use of Aggressive Mode for Secure Authentication critical functions launched from the “Auto Request Mask” for DNP3 Master  Added capability for DNP3 channel to skip offline sessions for specified amount of time  Several improvements to DNP3 Secure Authentication  Improved DNP3 Dual End Point mode    v3 .20.00    Implemented several improvements to DNP Secure Authentication including all items suggested in TB2016 -002, addressing some deficiencies in DNP3 - SAv5  Updated sDNP  TH database, added Binary and Analog output Command event classes as separate configured items in the database  Added support for Modbus function code 22 – Mask Write Register  Improved support for UDP    v3 .19.00    Improved TLS configuration with GUI auto-filled with default certificate files that are shipped with the Test Harness  Added TLS Handshake Timeout property to channels for all protocols in the Test Harness  Added DNP3 Device Attributes to data window in the Test Harness with up to 253 variations  Added ability to have DNP3 Conformance Test Plugins interfaces write output to Test Harness command window and protocol windows  Additional improvements to DNP3 Conformance Tests (find details in Test Harness What’s New Document)    v3 .18.00    Added the ability to read a DNP3 binary configuration file (created based on DNP3 device profile with the Triangle MicroWorks tool DNP Forge) and use that file to configure a DNP3 Master or Outstation in the Test Harness  Additional documentation and examples describing how to generate events and commands  Support for newer versions of Windows TM that do not allow writing to program directory locations  Added documentation on how to use TCL help files in WindowsTM 8.1  Improved support of different language sets and screen resolutions in the session windows, channel open window, and splash screen  Improved DNP3 Conformance Test Scripts with enhancements for tolerance of timing issues and differently formatted responses (refer to DNP3 Slave Conformance Test Module What’s New document for details of changes) along with more informative error messages  Added support for multiple Omicron devices when using DNP3 Conformance Test Scripts    v3 .17.03    Updated licensing to SafeNet online licensing service  24 hour access to an online licensing service to manage licenses  Virtual Machines can be licensed directly without needing a USB license  Network license option is available to share licenses across multiple users and computers  Learn more about the new licensing    v3 .17.00    Updated DNP3 device profiles for secure authentication changes  Added protection against invalid malicious messages for DNP3 and IEC 60870-5  Added a shortcut key to save XML Protocol Analyzer log  Added support for 62351-3, specifically for TLS  Updated sessions to expose UseUTC to the TCL command window  Modified to use the newer version 4.1.3 winpcap  Additional protocol specific updates to address customer issues    v3 .16 - Added support for negative control point offset, improved input validation testing, and improved support of DNP3  SAv5 support  v3 .15 - Added the ability to automatically generate data changes via plug-ins  v3 .10 - Added support for TLS and SSL  v3 .04 - Added the ability to automatically load a workspace on start-up                                                                                     Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/test-harness-pages/system-requirements
                                                  Request a Quote          Test Harness Video Playlist          Download Evaluation            Test Harness System Requirements        The Test Harness is a powerful tool for testing DNP3, IEC 60870-5, and Modbus devices.                 Overview  Custom Functional Tests  Protocol Analyzer  Conformance Testing  View Data from Devices  DNP3 Secure Authentication  IEC 60870-5 Secure Authentication  Simulate Devices  What's New  System Requirements       Overview    Custom Functional Tests    Protocol Analyzer    Conformance Testing    View Data from Devices    DNP3 Secure Authentication    IEC 60870-5 Secure Authentication    Simulate Devices    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)          The current release of the Test Harness is supported on the following versions of Windows.    Windows Server 2012 (64-bit)  Windows 10 (32-bit and 64-bit)  Windows Server 2016 (64-bit)   Windows Server 2019 (64-bit)    Windows 11 (64 bit)    Note:  Supported on virtual machines (VM)    Hardware Requirements     1.5 GHz processor;  Recommended: 2.4 GHz multi-core processor or higher  2 GB RAM; Recommended: 4GB RAM or higher  Microsoft .NET 4.6.2 Framework                                                                                     Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                           The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/test-sequencer
                                                         Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Test Sequencer   This tool allows the user to create a display and build a repeatable set of commands to run against the workspace. Tests are managed through the Test Explorer which provides a view of all Test Plans and Groups in the Workspace. Advanced features make it simple to create and run tests as shown in the screenshot below.                   Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit          Ready to Get Started?     Download Evaluation>>       Request Pricing>>                                                                                            Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/signal-flow
                                                         Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation             Signal Flow Diagram   The Signal Flow diagram replaces the old wire connections shown on prints with animated GOOSE signals captured on a timeline that can be replayed one event at a time. Tool tips on the signal lines show time stamps for the events on that signal.  After visually inspecting test results in Signal Flow, an automated test can be generated for Test Sequencer to easily verify the results during the next test cycle.                     Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit            Charts can be displayed as a Sequence Of Events(SOE) Log and exported as a CSV file.    Ready to Get Started?     Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/logic-analyzer
                                                         Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Logic Analyzer  The Logic Analyzer can be used to chart real time values from multiple IEDs in a substation retrieved using GOOSE or Reports.  The data values are charted using data timestamps or GOOSE message timestamps from the IEDs for the best accuracy possible.  Multiple charts can be acquired and saved as part of the workspace for viewing later.  Each chart can record an unlimited number of data attributes and selected data attributes can be shown at the same time for comparison using time cursors to measure deltas between events.                  Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit             Charts can be displayed as a Sequence Of Events (SOE) Log and exported as a CSV file.   Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/goose-testing
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               GOOSE Testing  Each GOOSE Stream published by an IED is configured by a GOOSE Control Block. It identifies a Dataset that defines the Data Attributes to be included in a GOOSE Message .  Each time one of these Data Attributes changes causing a new Event , a GOOSE Message is Published with a new State number and retransmitted periodically for any Subscribers that missed it or came online after the Event occurred.  The Relationship between GOOSE Streams, GOOSE Messages, and new Events for a Data Attribute is best shown the GOOSE Viewer screenshot below :                  Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit                Troubleshoot GOOSE Publishers  The GOOSE Tracker Tool identifies duplicate, missing, extra, simulated, misconfigured, corrupt, and valid GOOSE Streams in the workspace.  Each row in the table is a GOOSE Stream, either defined in one of the SCL files loaded, or found on one of the selected network adapter cards.     The GOOSE Tracker contains a Status Column which indicates if any of the columns for a row contain an error or warning message.  Grouping by the Status Column brings any GOOSE Streams with an error or warning message to the User's attention.     The main GOOSE Tracker window has a column showing time between the last Retransmission. In order to preserve bandwidth on the network, this number grows over time from minTime immediately following a state change to maxTime at the steady state condition.  To see the time difference between the first 15 Retransmissions after a new GOOSE Message is Published, click the "Record ReTrans" button on the Ribbon Bar:     The following display for GOOSE Retransmissions will appear with columns showing the Publication time in the GOOSE Message Header and the time difference between the next 15 Retransmissions.      Determine if all GOOSE Subscribers are receiving Messages  Once any GOOSE Publisher issues discovered in the GOOSE Tracker Tool are resolved, the  LGOS Tool shown below can verify all Subscribers are receiving the GOOSE Streams defined in the SCD file.  The LGOS Logical Node provides the status of the subscription to a GOOSE stream. Each row in the chart below corresponds to one GOOSE Stream in the publishing IED. Each column in the chart represents a subscribing IED.  The intersecting cell graphic represents the LGOS status received from the Subscribing IED, hovering over it will show a tooltip with more detail.  The Legend displays all the possible subscription statuses.       Troubleshoot GOOSE Subscribers and Protection Schemes  The GOOSE Publisher Tool allows the user to manually set the GOOSE Message Header information and Data Attribute values for any GOOSE Stream defined in the workspace and publish both new states and retransmissions. The main use cases for the GOOSE Publisher Tool are:   By default, GOOSE Publisher is configured to send simulated GOOSE messages for Virtual Isolation Testing . This is used to test protection functions in a live substation without affecting other functions that are not involved in the test.  During Functional Testing, GOOSE Publisher can be used to simulate a GOOSE stream for a missing device.  If a Subscriber is not receiving messages from a GOOSE Publisher that passes all the checks in GOOSE Tracker, the same GOOSE Stream can be generated in GOOSE Publisher to test the Subscriber from a different source.  During IED validation testing, GOOSE Publisher can test the reaction of a Subscriber to invalid messages such as missing state numbers or missing sequence numbers.     Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/compare-model-tool
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Compare Model Tool      Quickly Find and Resolve System Errors Due to Out-of-date Configuration Files    System errors due to IEDs with out-of-date configuration files used to require tedious and time consuming comparisons. But not any longer!  The Test Suite Pro Compare Model tool automatically performs a discovery of each IED and compares it to the SCL file, and notifies you of any conflicts.                          Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit                Find Out-of-Date Configuration in Devices  (Click image for larger view)     View Differences at Each Level of the Model Hierarchy    Not sure where to look for configuration errors? With the Data Model Compare tool, you can quickly and easily view differences at each level in the hierarchy.      View Differences at Each Level of the Hierarchy  (Click image for larger view)     Group and Filter by Category      You can also group and filter by category, making it super easy to find differences between the SCL File and the actual configuration of the IED.       Group and Filter by Category  (Click image for larger view)      Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/scl-verify
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               SCL Verify  Verify SCL Files  SCL verification is built in to help check for SCL file interoperability issues. The SCL Verify tool identifies missing parameters or non-compliant SCL. The tool also checks to see if control blocks, external references, and datasets can be resolved. Warnings and errors are displayed to distinguish high priority issues. And the line number for the warning or error is highlighted in the SCL File                    Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit            Learn More about Testing SCL Files  Go to the 61850 Test Suite Pro online help to learn more about SCL testing.   Ready to get started?     Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/data-miner
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Find Data across the Substation IEC 61850 data models can be very large and complex. Finding the right data from the right IED, logical node, dataset, GOOSE, or report can be a lengthy process. The Data Miner tool makes it easy to find the data you need for testing:     Data models for multiple IEDs are parsed by the Data Miner to give a complete view of the substation  Powerful filtering and grouping helps to sort out the important data for your testing  Data models for IEDs can be sourced from discovery or by importing SCL files                  Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit                  Filtering Data  The Data Miner has powerful filters to help you quickly find the data you need. Custom filters can be saved to easily re-use filters that are often needed for testing.  For example, a filter can be created to show all protection related common data classes that are available through GOOSE messages.  Learn more about filtering in online help         Grouping Data  The Data Miner can also group data by data model parameters like IED, LN Prefix, LN Class, or common data class. This provides an easy way to segment data and more clearly view the data model across the entire substation.  Learn More about the Data Miner  Go to the 61850 Test Suite Pro online help to learn more about how the Data Miner helps you find the data you need for testing.   Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/test-client
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Full Featured Test Client  The Advanced Client tool provides all the client/server testing capabilities from the Hammer test tool (part of v3.9 of the 61850 Test Suite).  There is full support for testing client/server communications and viewing IED data models. An intuitive user interface makes it easy to test MMS services like reports, logs, controls, reads, and controls. And the entire IED data model is shown to allow troubleshooting and testing specific capabilities of an IED.                      Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit            Easy to Use Interface The user interface for the Advanced Client makes it easy to test IEDs:     Switch between any IED connection quickly  The full data model is shown for the selected IED  Testing options are shown in the ribbon bar based on what part of the data model is selected  Search, filter, or bookmark objects in the data model          Navigate Data Model  Powerful filtering and searching allows you to easily find the data you need. Other navigation options allow you to locate data, locate data in  datasets, and add bookmarks for important parts of the model.         Full Client Capabilities All client capabilities are supported for testing IEC 61850 services with IEDs.       Reports    Tracking Services      Logs    Datasets      Controls    Read/Write      File Services        Learn More  Go to the 61850 Test Suite Pro online help to learn about all the client test capabilities.   Ready to get started?     Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/ied-simulator
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Simulate an IED  An IED simulator is built into 61850 Test Suite Pro to make it easy to simulate any IED based on either an SCL file (.SCD, .CID, .ICD, etc.) or based on a discovered data model that has been read from a real IED.                   Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit               IEC 61850 Services  The IED Simulator simulates an IEC 61850 server based on the configuration from an SCL file or by discovering an IED data model. Supported services include reporting, logging, GOOSE, tracking services, dynamic datasets, controls, and file transfer.    Generate Data Changes for GOOSE or Reports  Data objects in the simulated IED server can be changed either manually, periodically, or through a custom script. The data changes can trigger reports, logs, or GOOSE to be generated by the simulated IED.   Learn more about simulated data changes   Learn More about the IED Simulator  Go to the 61850 Test Suite Pro online help to learn more about simulating an IED.       Ready to get started?     Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/viewing-data
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               View Substation Data  The Custom Display allows you to create single line displays with data acquired through reports, GOOSE, or polling from multiple IEDs across a substation. This allows you to visualize a test in a single view with updated data values. The custom display is saved as part of a workspace so that you can  re-load the display later as needed for specific test cases.                  Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit                  Quickly Create Custom Displays  Create displays easily by dragging data objects from the Data Miner onto the custom display window. Data objects can be added from multiple IEDs using different retrieval methods (GOOSE, Report, or Polled). This allows you to simplify testing with a single display that shows data from several different sources.            Retrieve Data through GOOSE, Reports, or Polling  Simply set the data retrieval method, and 61850 Test Suite Pro does all the hard work for you. The tool analyzes the IED data model and determines if the data can be retrieved from GOOSE or reports and then retrieves the data through the selected method. This saves you time by avoiding the manual process of enabling the correct report or subscribing to the right GOOSE control block. If the data is not available through GOOSE or reports, then polling can be used. If you want full control of how GOOSE or reports are used, then the Advanced Client tool provides full access to client functions.          View Data in Table  It is also possible to view data in a table format by using the Data Monitor display. This allows you to view lots of data in one place with data value, quality, and time stamps.  Learn More about Custom Displays  Go to the 61850 Test Suite Pro online help to learn more about how custom displays can be created.   Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/report-viewer
                                                         Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation             Report Viewer   The Report Viewer displays report messages. Any Report Control Block that has been enabled from the Advanced Client or Data Miner is displayed in this viewer.                    Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit              Ready to get started?     Download Evaluation>>       Request Pricing>>                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/log-services
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Log Services  The Test Suite Pro Log Services provide a quick and easy means to view log data. Logs provide a mechanism for  an IEC 61850 Server in an IED to maintain Sequence of Events (SOE) information. Logs are based on datasets, and store information exactly as in reporting; only the transmission method is different.  The client must query logs to get the events. Multiple clients can query the same log, and multiple Log Control Blocks can store events in the log.  Shown below are several of the ways that Test Suite Pro can retrieve Log information.      View Log Data                 Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit            The left column in the Log display shows the header information for each entry in the log. Selecting an entry shows the data contained in the log entry on the right.        Example of Log Viewer  (Click image for larger view)     Auto Refresh    In IEC 61850, the client must query the log to get events. Test Suite Pro can be configured to automatically refresh the logs by setting a Refresh Log time interval.      Set time interval to automatically refresh log  (Click image for larger view)     Manual Query by EntryID      The log can be queried from an EntryID. Simply select the desired EntryID from the list and press Query.      Log Query by EntryID  (Click image for larger view)     Query Logs by Time Interval      Logs can also be queried between two points in time by entering the start and end times.      Log Query by Time Interval  (Click image for larger view)          Ready to get started?     Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/file-services
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               File Services   IEC 61850 provides basic file transfer services: upload, download, delete, and directory. The directory structure is simplified as no assumptions about the underlying platform or file system are assumed.  Test Suite Pro provides the ability to test the four basic file services offered by IEC 61850:   Directory Listing – Ability for a Client to retrieve a directory listing of files on a Server.  File Delete – Ability for a Client to delete a file residing on the Server  File Upload – Ability for a Client to upload (or copy) a file to a Server.  File Download – Ability for a Client to download (or retrieve) a file from a Server.   Client File Services                 Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit            Test Suite Pro can view and navigate the server's file system using IEC 61850 File Services.  In addition, Test Suite Pro can utilize the following file services:   Download Service: The selected file can be downloaded and saved to the local disk  Delete Service: The file will be permanently deleted from the server  Upload Service: The selected file will be uploaded from the local file system to the server        Example of File Services Directory  (Click image for larger view)     IED Simulator File Services    Test Suite Pro Server has a complete implementation of all File Services by default. Specific configuration of File Services is available via the editing/creating of workspaces. On each specific IED a File Services Root directory can be configured along with flags indicating if the particular File Services should be available when initiated by a Client.      Server File Services Configuration  (Click image for larger view)          Ready to get started?      Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/online-help
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Online Help for 61850 Test Suite Pro  Online help is available to assist you learn all the capabilities of 61850 Test Suite Pro. The help pages are HTML based topics with links to help you navigate and search to find the right topic you need.   Go to Online Help for 61850 Test Suite Pro                  Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit          Offline Access  For offline access, the help pages are saved locally to your PC during installation of 61850 Test Suite Pro or the pages are available online. This allows you to access help pages when you are in the field without Internet access.       Search Help Topics    Find the topic you need quickly by searching the help pages.               Categories and Topics    The help pages are divided into testing categories and each page covers a specific topic.                                                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/whats-new
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               What's New - 61850 Test Suite Pro Current Release: v4.7.2 Release Date:  January 2024                Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit          Newest Features in this Release    KDC Implementation: Added the ability to configure a Routed GOOSE publisher or subscriber to communicate with a Key Distribution Center (KDC) Server in order to establish a secure, encrypted connection.    Hammer, Anvil, and SCL Forge have been repackaged for v4.0   Hammer and Anvil have been combined into a single application called 61850 Test Suite Pro  SCL Forge has been repackaged into SCL Navigator  which will be released separately from 61850 Test Suite starting in Q4 2017   Hammer/Anvil Upgrade Information   Contact sales@ TriangleMicroWorks .com for information about upgrading   Key Additions to Recent Releases  v4.7.1    Sequence of Events: Added a new tool to capture a sequence of events via GOOSE/Reports.    Logic Analyzer: Allow the user to export/import data points from the logic analyzer to a .csv file.    Advanced Client: Added a command to allow ‘Select and Operate’ on a data object.    Advanced Client: Reorganized the commands on the Ribbon to simplify the GUI.    Test Sequencer: Allow the user to change the IED’s associated with a test.    v4.7    Added an option to enforce the Strict/Legacy Parser. An alternate parser has been added to be able to ignore certain elements in the SCL file to successfully load the devices from the file.    Added columns for the # of Clients supported. The new column indicates the number of clients that can connect to the server at a given time.    Updates have been made to Connecting and the Connection Status Column. Devices will not auto connect by default, but can be set to auto connect in the Options. Toolbar controls have been added for Connecting all IEDs, Disconnecting all IEDs, and Testing all connections.    Added an option to Auto Scale the diagram in the Signal Flow to show all the nodes.    In the Signal Flow and Logic Analyzer, you can right click on the timeline to display or export an SOE log of all the events shown on the timeline .    Added the ability to change the background colors of the IED nodes in the Signal Flow.    Added an option to show Sequence of Events on the Signal Flow and the Logic Analyzer.    Added a tool in the GOOSE Tracker to capture ReTrans data for the GOOSE streams currently being tracked. The user can turn on GOOSE ReTrans in the sniffer.    Auto connect to servers is now disabled by default and can be configured on the File | Options | Client Configuration screen.    An option has been added to disable all background polling to retrieve data on the File | Options | Client Configuration screen.    Added an option to the File | Options | Display Settings screen to disable subscribing nodes when a data object is dragged to the Signal Flow.    The sniffer can now be started even if no workspace is loaded. When a workspace is loaded the adaptors used by the devices in the workspace are added to the sniffer. The selected adaptors can be changed by clicking on the ‘Start Sniffer’ button while holding CTRL.    v4.6.2   Signal Flow now displays if an IED is in Simulation Mode( LPHD . Sim = TRUE).  Nodes in a Signal Flow indicate the behavior of the Behavior(ON/TEST/ TESTBLOCKED /BLOCKED) of the corresponding Logical Node.  Links in the Signal Flow now indicate if a given input to a Logical Node is processed based on LGOS and Behavior of the subscribing Logical Node.  Simulated GOOSE signals are displayed in the Signal Flow based on LPHD . Sim and whether the subscribing Logical Node is receiving a simulated GOOSE signal.  Change in LPHD . Sim of the IED, LGOS of the subscribing IED and Behavior of the Logical Node can now be recorded in the Signal Flow.  Simulated GOOSE events can now be captured in the Signal Flow.   v4.5.1   Summary of issues available for user to review before using the other tools/displays to verify, debug, or just learn about the system design.  Workspace tool is now the new System Status. The connection status is more detailed to help the user debug server connection issues  Reorganized the Tools menu to group like tools together and show the number of underlying Errors/Warnings in each tool that TSP has automatically detected in the System Status Group.  Added LGOS tool.  It provides a consolidated view of all the LGOS Logical Nodes which indicate if the device is actually receiving the GOOSE streams configured in the SCD file.   Added MOD/ BEH / Sim tool which monitors Beh (Behavior) and allows users to change Mod and LPHD . Sim .   Test Suite Pro has been refined to support the following Test Methodology:  Connect TSP to Substation network or automatically generated simulation in DTM and load the SCL file.  Resolve or Acknowledge errors and warnings on the System Status group of tools.  Find signals in Data Miner to show in Signal Flows, One line diagrams, and Logic Analyzer Display to visually confirm the system behavior.  Create/perform Automated Functional testing of Protection and Control schemes with Test Sequencer.     Support to control a SMRT class Megger device from the Test Sequencer.    Introducing Test Suite Lite with limited functionality and lower price.    Workspaces can now be imported/exported in TSP .   Show the status of the GOOSE/Report/Sampled Value/Log control blocks based on whether the control block is Enabled/Disabled or Subscribed/ Unsubscribed in the Advanced Client and the IED Simulator.  Added indication on the Data Objects/Data Attributes to show how the current value was acquired( SCL/Report/GOOSE/Read).  Redesigned the presets for Data Miner/Advanced Client/IED Simulator.   v4.4   The Test Sequencer functionality was added to the Test Suite Pro. This tool allows the user to create a display and build a repeatable set of commands to run against the workspace. Tests are managed through the Test Explorer with provides a view of all Test Plans and Groups in the Workspace. The Test Builder, Commands, Step Details and Execution Log are utilized to create and run tests.  The Report Viewer display has been redesigned to allow the user to start and stop recording reports received by TSP in the Report Viewer.  The GOOSE Viewer display has been redesigned to allow the user to start and stop recording GOOSE messages received by the TSP GOOSE Viewer.  Created a testing API for use on test tools, 6TH , Kema , OMICRON, UTInnovation , etc.  Added Clear to the GOOSE Tracker to allow messages to be cleared without stopping and starting the tracker.  Added subscribe to any GOOSE message from the GOOSE Tracker.  Modifying a value in the dataset in the GOOSE Publisher the state will increment automatically.  Added a Search Dialog to the top right of the SCL Viewer.  Display Virtual parts of the model in italics on the Advanced Client.  Made updates to the Signal Flow including the ability to “Send to Test Sequencer as Steps” and added the ability to use the events in the Signal Flow to create a verify step in the Test Sequencer.  Updated TSP to constantly monitor the SCD for changes.  Updates to the Custom Display including Shift + Mouse Click to select multiple items and ability to turn off Grid Lines and labeling for each drawing.  Display Virtual parts of the model in italics on the Advanced Client.   v4.3.1   Signal Flow was added. The Signal Flow Diagram replaces the old wire connections shown on prints with animated GOOSE signals captured on a timeline that can be replayed one event at a time.  Tool tips on the signal lines show time stamps for the events on that signal. To determine exact timing between signals, see the Logic Analyzer (below).  GOOSE Publisher Tool has been redesigned for improved usability. Additional error filtering has been added.  GOOSE Tracker Tool has been redesigned for improved usability. Additional error filtering has been added. The methods used to identify duplicate messages have been improved.  Protocol Analyzer window was added.  This new window includes messages that were displayed in the Output window in the previous version, but now includes details of GOOSE and MMS messages sent and received by the IEDs in the workspace.  The default tool presets have been improved and custom presets now indicate when a change has been made.  More default presets have been added to some tools.  The method of saving workspaces has been reworked to improve performance and stability.  A Quick Start dialog has been added at startup so opening a workspace is faster.     v4.2.0.   Logic Analyzer display was added. This can be used to chart real time values from multiple IEDs in a substation and includes time cursors for measuring time deltas between events.  Added 32 bit release  Added Time Received in GOOSE Event Viewer. This helps in finding out of order events.  Added ability to copy Output window to the clipboard.  Added Max Client Connections setting in the server.  Added Time Synch checks when receiving GOOSE messages to ensure clocks are closed to synched.  IED Simulator now shows received ExtRef data under the Logical Node.Inputs section. As the data is received it is updated in the tree. This received data can then be mapped to another signal using the context menu. This allows the user to send GOOSE signals upon receiving GOOSE signals.  Added drag & drop and copy & paste capability to the Custom Display, the Logic Analyzer, the Data Monitor, and the Group Display.     v4.1.0   Added new GOOSE Publisher tool for testing and troubleshooting GOOSE subscribers  New capability to create multiple custom displays in one workspace  Option to disable IED simulator and GOOSE publisher in a workspace to reduce memory and time to load workspace  Enhance data source filter in Data Miner to include custom data objects that use standard common data classes and also indexed objects     v4.0     Version 4.0.2 is a major new release with many new features for making IEC 61850 testing easier  Combined Hammer and Anvil tools into a single application  Redesigned interface that is easier to use  Updated workspace approach for saving and loading configurations for multiple IED connections  Added Data Miner for finding data across multiple IEDs in a substation  Integrated filtering, grouping, and searching capabilities to navigate IED data models  Added Custom Display for easily visualizing data from multiple IEDs  Improved GOOSE and Sampled Values tracker for sniffing messages on the network  Updated support for 64 bit WindowsTM OS (32 bit Windows is only supported for v3.9 and earlier)  Published online product documentation available at trianglemicroworks.com/help/6tsp      v3.9.1     Last release of Hammer, Anvil, SCL Forge as 61850 Test Suite  Added GOOSE Subscriber capability - subscribe to any or even all the GOOSE Control Blocks in any SCL File and see the current status (active, timeout, unsubscribed ) for each GOOSE subscription  Added GOOSE Tracker - sniff for any GOOSE messages on the network. GOOSE found on the wire are compared to any SCL File to show which messages are expected and which are rogue  Improved checking in SCL Verify      v3.8.0 - Significantly faster discovery of Server Object Models  Better support for time stamps in File Services Added support for private and text fields in EnumType elements   v3.7.0 - New licensing options including a network-based license and support for VM ’s  Updated SCL File Checking with new tests and better debugging capability Added ability to filter SCL File Checking results based on test type, severity, IED, and other parameters Increased support of External References for Edition 1, Edition 2, and incomplete definitions for GOOSE and Sampled Values subscriptions Improved both the consistency of testing results and usability of the tool   v3.6.1 - Added the ability to sort by columns in the Flat View pane  Added the ability to sort by columns in the Flat View pane The Object Model now shows the relationships between a point's Timestamp , Quality, and Value in the Object Model  Reflect changes in GUI when Insight, Flow Charting, or JavaScript makes changes to Report Controls or GOOSE Controls Allow Clients and Servers created in JavaScript to be attached to the GUI in Insight Added negative testing capabilities via JavaScript . A script runner GUI was added that includes a suite of negative tests that can be run against any server. The GUI is flexible enough to allow any script written by a user to integrate their own scripts negative or positive Added support for Setting Groups Added Auto Scroll to Report, GOOSE, and Log viewers Added ability to poll Datasets Added ability to delete any dataset   v3.5.59 - Added SCL File Consistency and Completeness Check   Added menu option to update the datasets by reading them from the server    v3.5.54 -   Added “Perform Read During Discovery” option to a Client’s configuration  Added Open SSL Method to strong security configuration Added PDU -size to Service Errors Added support for Dynamic Data Sets with array indexes Changed Algorithm for looking up ReportControls from RptId EnableReportBlock now issues a read to ensure that its RptID is up to date Allow all items except RptEna and TimeofEntry to be editable Added “Enable raw” option to context menu option on Report Control Blocks to bypass automatic logic and simply issue a Write of the RptEna item Added EntryID tracking Added Simulation column in GOOSE Message Window (Ed. 2 requirement). Added Inclusion bits to configuration that controls OSI fields   v3.5.41 - Made Workspace files standalone   v3.5.23 - Added ability to discover and validate against an SCL file   v3.5.15 - Added support for Ed.2, including Tracking Nodes, Security, Object Model changes, and new Function Constraints.  Added Simulation Files.                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/system-requirements
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               System Requirements                 Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           Test Suite Lite  SCL Navigator  Distributed Test Manager (DTM)          61850 Test Suite Pro supports the following versions of Windows TM :   Windows Server 2012 (64-bit)  Windows 10 (32-bit and 64-bit)  Windows Server 2016 (64-bit)   Windows Server 2019 (64-bit)    Note:  61850 Test Suite Pro supports virtual machines (VM)  Minimum system requirements:   1.5 GHz processor; recommended: 2.4 GHz multi-core processor  2GB RAM; recommended: 4GB RAM  Windows 7 or higher (not Embedded Compact)  Microsoft .NET 4.6.2 Framework   Note: the minimum requirements are for small IEC 61850 SCL files.  Larger SCL files with large data models will require more resources to process.                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/iec-61850-services
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               Supported Services - 61850 Test Suite Pro            61850 Services Built In           All of these services are built into 61850 Test Suite Pro:               Read services  Fully Supported    Read at any level in the hierarchy  Return configurable errors when a DataAttribute is read                Write services  Fully Supported    Write at any level in the hierarchy  Return configurable errors when a DataAttribute is written                Reporting Services  Fully Supported    Ability to enable/disable Report Control Block with the click of a button  Ability to assign DataSets from a list of available ones  Un-buffered and buffered Report Control Blocks  Any Valid Data Set can be used in the Report Control Block  Supports all trigger options  All optional items included in report (Reserve Time, Owner, GI)  Set any Report Control parameter individually  Track last 10 entry id’s received in order to test servers ability to restore buffered reports  The Object Model is automatically updated as Information Reports are parsed  Each individual Information Report including header data can be examined or compared with one another                Discovery  Fully Supported    Supports the discovery of an object model on a Server                Control Models  Fully Supported  Intuitive GUI detects controls and their control models making it easy to test all controls models.  It also makes it easy to do negative testing by allowing individual data attributes to be written separately.  All five control models for IEC 61850:   Status-only  Direct Control with normal security  Direct Control with enhanced security  Select Before Operate with normal security  Select Before Operate with enhanced security                GOOSE  Fully Supported    Ability to enable/disable GOOSE Control Block with the click of a button  Ability to assign DataSets from a list of available ones  Any valid DataSet can be used in the GOOSE Control Block  Unicast and Multicast GOOSE messaging  Test and simulation flag support  The Object Model is automatically updated as GOOSE messages are parsed  Each individual GOOSE message including header data can be examined or compared with one another                Sampled Values (9-2 LE)  Fully Supported    Non-real-time implementation for testing message structure                Tracking Services  Supported    Tracking of Control Blocks  Tracking of Control Operations                Dynamic DataSets  Fully Supported    Creation/Deletion of Persistant  Creation/Deletion of Non-Persistant                Logging Services  Fully Supported    Ability to enable/disable Log Control Block with the click of a button  Ability to assign DataSets from a list of available ones  All Trigger options, integrity period, etc.  Any valid DataSet can be used in the Log Control Block  Display each individual Log message as well as compare the data in each  Ability to query the log                File Transfer Services  Fully Supported    File upload  File download  File delete  Directory browsing                Security  Fully Supported    IEC 62351 (parts -3, -4, and -6) security standard is supported for authentication and encryption (TLS)                               Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit          Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/61850-test-suite-pro-pages/viewing-data#custom_displays
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               View Substation Data  The Custom Display allows you to create single line displays with data acquired through reports, GOOSE, or polling from multiple IEDs across a substation. This allows you to visualize a test in a single view with updated data values. The custom display is saved as part of a workspace so that you can  re-load the display later as needed for specific test cases.                  Overview  61850 Tool Comparison  Test Sequencer  Signal Flow  Logic Analyzer  GOOSE Testing  Compare Model  SCL Verify  Data Miner  Test Client  IED Simulator  Custom Display  Report Viewer  Log Services  File Services  Online Help  What's New  System Requirements  IEC 61850 Services       Overview    61850 Tool Comparison    Test Sequencer    Signal Flow    Logic Analyzer    GOOSE Testing    Compare Model    SCL Verify    Data Miner    Test Client    IED Simulator    Custom Display    Report Viewer    Log Services    File Services    Online Help    What's New    System Requirements    IEC 61850 Services        Related Products           SCL Navigator  Distributed Test Manager (DTM)  IEC 61850 System Toolkit                  Quickly Create Custom Displays  Create displays easily by dragging data objects from the Data Miner onto the custom display window. Data objects can be added from multiple IEDs using different retrieval methods (GOOSE, Report, or Polled). This allows you to simplify testing with a single display that shows data from several different sources.            Retrieve Data through GOOSE, Reports, or Polling  Simply set the data retrieval method, and 61850 Test Suite Pro does all the hard work for you. The tool analyzes the IED data model and determines if the data can be retrieved from GOOSE or reports and then retrieves the data through the selected method. This saves you time by avoiding the manual process of enabling the correct report or subscribing to the right GOOSE control block. If the data is not available through GOOSE or reports, then polling can be used. If you want full control of how GOOSE or reports are used, then the Advanced Client tool provides full access to client functions.          View Data in Table  It is also possible to view data in a table format by using the Data Monitor display. This allows you to view lots of data in one place with data value, quality, and time stamps.  Learn More about Custom Displays  Go to the 61850 Test Suite Pro online help to learn more about how custom displays can be created.   Ready to get started?    Download Evaluation>>       Request Pricing>>                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                           The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/dnp-scl-pages/outstation-library-features
                                                  Request a Quote          Source Code Library Video Playlist               DNP3 Source Code Library  Our ANSI C source code library provides a quick and cost effective way to implement a DNP3 master or outstation.  A .NET component version is also available.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  DNP3 Starter Kit  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    DNP3 Starter Kit    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          Outstation Library Features   Database manager maps randomly organized Target Application data points (binaries, controls, integers, floats, etc.) into sequential points within DNP3 object groups.  Example Database Interface implementations are provided for testing, illustration, and as templates to be used for developing final Database Interface.  Report-by-Exception data can be automatically generated by configurable scans of Target Application data.  Supports Virtual Terminal objects for embedding other protocols.                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/dnp-scl-pages/master-library-features
                                                  Request a Quote          Source Code Library Video Playlist               DNP3 Source Code Library  Our ANSI C source code library provides a quick and cost effective way to implement a DNP3 master or outstation.  A .NET component version is also available.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  DNP3 Starter Kit  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    DNP3 Starter Kit    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          Master Library Features   An unlimited number of remote devices can be configured on an unlimited number of communication ports, and new remote devices can be added at runtime.  Multiple devices can be assigned to the same communication port to support multiple network communication topologies.  Database manager maps received DNP3 data objects into Target Application data points (binaries, controls, integers, floats, etc.).                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/dnp-scl-pages/secure-authentication
                                                  Request a Quote          Source Code Library Video Playlist               DNP3 Source Code Library  Our ANSI C source code library provides a quick and cost effective way to implement a DNP3 master or outstation.  A .NET component version is also available.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  DNP3 Starter Kit  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    DNP3 Starter Kit    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          DNP3 Secure Authentication  Cyber-security Features    End to end cryptographic authentication at the application layer which goes beyond VPN tunnels or TLS  Based on IEC 62351 security standard (Parts 3, 5, and 8)  TLS is supported for DNP3 IP based networks  Legacy support for networks or devices that do not support encryption  Configuration of which requests are considered critical  Authentication can be performed in either direction (Outstation or Master)    Support for Multiple Users    Supports Role Based Access Control (RBAC) - multiple users and roles (engineers, operators, viewers, admin)  Configure privileges(view, read, security, etc.) for each role  Users can be added, modified, or removed    Legacy Support    Support low bandwidth and/or serial networks  Low overhead for Outstations that may not have processing capability for public/private certificates or encryption    Additional tools for Secure Authentication   Test Harness     The Communication Protocol Test Harness supports Secure Authentication  A powerful tool for testing the implementation of the IEC 60870-5 Source Code Libraries    Distributed Test Manager (DTM)    Distributed Test Manager (DTM) supports Secure Authentication  A powerful tool that can simulate the SCADA communications in the substation and perform automated tests to confirm system or individual device behavior   SCADA Data Gateway   For projects where Secure Authentication needs to be added to an existing system  The SCADA Data Gateway is a flexible protocol translation application for easily deploying Secure Authentication in a control center or substation      Watch our training videos to learn more about DNP3 Secure Authentication    Example of Challenge-Reply Sequence for DNP3 Secure Authentication:                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/dnp-scl-pages/ansi-c-source-code
                                                  Request a Quote          Source Code Library Video Playlist               DNP3 Source Code Library  Our ANSI C source code library provides a quick and cost effective way to implement a DNP3 master or outstation.  A .NET component version is also available.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  DNP3 Starter Kit  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    DNP3 Starter Kit    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          ANSI C Source Code   Written in ANSI-Standard C Source Code, under a strict corporate coding standard.  Designed to be processor and operating system independent, using any ANSI-Standard C compiler.  Simple configuration for big-endian or little-endian byte order.  Can be used with or without a Real Time Operating System (RTOS).  Database interface supports any database, ranging from direct I/O input with no storage to complex, relational databases.  Includes sample applications and source code for Low-Level Target Interface for Linux ® and Windows.  Typical product integration times are less than three weeks.     Linux® is the registered trademark of Linus Torvalds                                                                              Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/dnp-scl-pages/dotnet-components
                                                  Request a Quote          Source Code Library Video Playlist           Download Evaluation              DNP3 Source Code Library  Our ANSI C source code library provides a quick and cost effective way to implement a DNP3 master or outstation.  A .NET component version is also available.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  DNP3 Starter Kit  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    DNP3 Starter Kit    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          .NET Components   Based on Triangle MicroWorks, Inc. industry-proven Source Code Library design.  Supports all .NET Languages (C#, J#, Managed C++,VB .NET, etc.) and tools.  Compatible with .NET 2.0 and later Frameworks.  Integrates with Visual Studio Help.  Source code version includes corresponding ANSI-Standard C Source Code Library.  Includes built-in simple database with save/restore capabilities; also supports user-defined database.  Ideal for quick development of products and tools requiring DNP3 support.  Scalable for large implementations.  Typical product integration time of less than one week.                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/dnp-scl-pages/dnp3-starter-kit
                                               DNP3 Starter Kit  Whether you are new to DNP3 or just need to quickly implement DNP3 in a new product, we can help you get a "jump-start" on your DNP3 implementation. The DNP3 Starter Kit includes everything you need to quickly and efficiently implement DNP3 in your device.  Choose from one or more of the following components to get the tools you need to quickly and easily complete your DNP3 implementation:    DNP3 Source Code Library   Our ANSI C Source Code Library provides a quick and cost effective way to implement a DNP3 master or outstation.  A .NET component version is also available.   >>Learn More      Communication Protocol Test Harness   The communication Protocol Test Harness is a complete test tool for testing your implementation   Simulate Master and/or Outstation devices  Monitor and analyze communication protocol message  Create custom functional tests  Perform DNP3 certification test procedures  and more!   >>Learn More                Request Quote                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                           The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/dnp-scl-pages/what's-new
                                                  Request a Quote          Source Code Library Video Playlist               DNP3 Source Code Library  Our ANSI C source code library provides a quick and cost effective way to implement a DNP3 master or outstation.  A .NET component version is also available.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  DNP3 Starter Kit  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    DNP3 Starter Kit    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          What's New Current Release: v3.32.0 Release Date:  March 2024  Newest Features in this Release    In WinIoTarg improvements to UDP/TCP receiving will avoid errors if the UDP broadcast message received in middle of the TCP octets.     Added TCP channel callback functions to retrieve IP addresses when the server is connected or disconnected.    Channels are no longer closed automatically when the last session on the channel is closed.    In WinIoTarg, added a channel diagnostic mask bit to control whether diagnostics messages are generated.   In LinIoTarg improvements to use linked list instead of array for keep channels listening on a particular port. That improvement will allow unlimited channels be configured to one specific TCP port.    MDNPResponseParser capture authChallengeData for g120v9 response.     In WinIoTarg, if the TCP listening thread is not started due to the port being occupied, the channel is now closed more quickly (previously could take up to 5 seconds).       Key Additions to Recent Releases  v3.31     "Slave" has been replaced by "Outstation", "Controlled Station", or "Server" depending on protocol in documentation, filenames, variable names, and function names. An effort has been made to maintain interface backward compatibility where possible.    Minor changes to allow SCL to build with VS2019 and VS2022 without modifications related to std::string and .NET Framework version.    Added support for OpenSSL Version 3.x.x. Open SSL Version 1.1.1 is still supported, however, according to openssl.org the End of Life date for the 1.1.1 series is September 11, 2023. Support for Version 1.0.2 has been removed.    In WinloTarg and LinLoTarg improvements to UDP functionality including Listen Mask and Range of Addresses Support as well as changes to isolate the differences between IPV4 and IPV6 handling.    In WinloTarg and Linlo Targ add clientSourceIpPort to allow TCP Client Channels to specify the IP Port to use for orignating TCP Connections.    Add support for TMWDIAG_ID_OCTETS but to determine whether the actual HEX Octets from a transmitted or received message are formatted and/or displayed in a diagnostic message.    Clean up some header comments in mdnpdata.h and sdnpdata.h so that they can be better parsed by customer software.    Include DNP MasterDatasets and DNPOutstationDatasets example application source code.    In WinloTarg and LinloTarg added ability to configure DNP Outstation to provide both TCP Server and UDP Request/Responses. This allows an Outstation to be configured to receive TCP connections OR to act as a UDP-ONLY outstation. It will respond to TCP requests over TCP and respond to UDP (non-broadcast) requests over UDP. This functionality was described and tested in DNP3 IED Certification Procedure Version 3.1 April 2022 and removed in a later version.    If the TCP listening thread is not started due to the port is occupied, it might take up to 5 seconds to close the channel. That has been improved in this release.    Clena up some header comments in mdnpdata and sdnpdata.h so that they can be better parsed by customer software.    Correct some “SUPPORT” #defines in sdnpdata.h to indicate the correct DNP3 Level for some Object/Variations. Changes were made to SDNPDATA_CNFG_SUPPORT_DOUBLE, SDNPDATA_SUPPORT_OBJ10_V1 and V2, SDNPDATA_SUPPORT_OBJ12_V3 and V5.and SDNPDATA_SUPPORT_OBJ21_v6. Double Precision Floating Point variations are beyond Level 4 and therefore now dependent on (SDNPDATA_CNFG_LEVEL_TMW && TMWCNFG_SUPPORT_DOUBLE).    Generate Binary Output and Analog Output Command Events on both success or failure if sendCommandEvent is configured for the SDNP Session.    Added support for sdnpdata_xxxOutSelect and sdnpdata_xxxOutOperate to return status PROCESSING for slow commands to allow SCL to perform other processing. In particular for a gateway implementation, it will allow commands to be forwarded downstream and responses processed. It also allows a channel supporting multiple sessions to process requests for other sessions while the command is still processing. DNP3 allows only a single command per association/session at a time to be in progress.    v3.30     In LinIoTarg speed up processing during TCP Channel closing.    In LinIoTarg function call to deprecated pthread_mutexattr_setkind_np() has been replaced with a call to pthread_mutexattr_settype().    Add support for multiple Polled Timers and disabling the Windows Target Layer multiple timers to allow application to run SCL in the context of its own threads.    Secure Authentication. Improve tmwcrypto.c support for multiple sessions when Simulated Crypto Database is being used during initial portation.    .NET Secure Authentication. Improve tmwcrypto.c TMWCRYPTO_ASYMTESTING if compiled in so that sample asymmetric key files can be found.   MDNP display a better diagnostic message if response contains an unsupported Object Group.  Improve diagnostic messages when control requests fail, making it clear the status value received in the request and the status value sent in the response.     .NET SCL. Improve SDNPSimPoint LocalMode Property performance.    .NET SCL. Add AssocPoint property to SDNPSimBinOut and SDNPSimAnlgOut which when set will cause the associated input point value to change when a control from Master changes the output value.    v3.29     Added new functionality to send a request message to remove static objects from class 0 assignment according to TB2021-001.    Discard a fragment that contains group 2 or 4 variation 3 (relative time) if it is not preceded in that same fragment by a Common Time Object 51 according to TB2018-001.    When a binary or double-bit input event is generated and the time is invalid/unsynchronized send variation 3 instead of variation 2 according to TB2017- 003a.    Add support to allow responses for counters and analogs to optionally switch variation to indicate the precision supported for each point even when a specific non-zero variation is requested.    Add configuration to SDNP Session to allow events to be sorted by timestamp as they were in the past and not by the order they are added to the event queue as is now required by TB2018-001.    In WinIoTarg add define to allow channel thread code to be compiled out.    Improved TLS example code and included sample TLS Certificates for ANSI C and .NET SCLs.    Added Object 50 Variation 4 Indexed Absolute time and long interval to Device Profile.    Combine Binary Input and Double-bit Input event objects into a single internal queue to better support the User Managed Event code to sort these together by when they occur.    Added configuration options and example code to disable or restrict File Transfer functionality at run time when File Transfer functionality is compiled in.    Reduce default compiled in features and functionality to Level 3 for ANSI C and .NET SDNP SCL. All functionality must be carefully chosen, tested and then documented in the Device Profile specific to your device. The .NET SDNP SCL will now have the same default functionality compiled in as the ANSI C SCL. The file tmwprvt.h will no longer be provided to turn on additional .NET functionality.     v3.28    In WinIoTarg potentially speed up waiting for a channel thread to exit when a channel with polledMode=false is closed.  In WinIoTarg improved diagnostics when configured for UDP-Only mode.  In LinIoTarg replaced deprecated stime() with clock_settime().  In LinIoTarg speed up new connection indication to protocol library when running with polledMode=true.  Enhance Big/Little Endian code in tmwtarg.c to support 64 bit IEEE 754 Double Floating Point conversion as was previously done for other data types without customer intervention.  Added diagnostic information for CROB and Analog Output Controls to show points and values sent by MDNP.  Added support for Object Group 50 Variation 4 Indexed Absolute Time and Long Interval read and write.  Enhanced MDNP SCL to support file transfer target layer interface even when simulated database is compiled out.  Enhanced MDNP SA SCL to reset the CSQ to zero when the master detects the Outstation is restarted. While not in the specification it allows the AES-GMAC algorithm if configured to continue to work properly over an Outstation restart.  Corrected MDNP SA issue when trying to send a g120v13 Update Key Change request the User Update Key cannot be retrieved the library could display more data in a diagnostic message than actually exists in the buffer.  Moved .NET rxLinkHdrCallbackEvent and rxTransportCallbackEvent from TMWChannel.h to DNPChannel.h since those two events only apply to DNP channels.  IED Certification Procedure Version 3.0 released in December 2020 specified new behavior for a response to a read request of a supported data type such as Binary Input that currently has no points in the database. The SDNP library will now reply with a NULL Response with Error IIN bits set. Previously it was considered optional to set the Error IIN bits.  IED Certification Procedure Version 3.0 added tests for devices that do not have a valid timestamp to support events with time variations. Normally, for SDNP you would compile out support for the events with time variations. However, to better allow the events with time variation support to be compiled in but in some instances not have a clock to provide timestamps; Added supportEventsWithTime configuration to SDNP defaulting to true. If false SDNP will send without time variations for events when events with time variations are requested. A timestamp of Jan 1 1970 can be passed to sdnpxxx_addEvent and will not be sent over DNP.   v3.27.1    Changed “configuration signature” to “configuration digest” for default Device Attribute Object 0 Variations 200 and 201 as directed by TB2013-004b. Affects Diagnostics and #defines only.  In LinIoTarg use localIpAddress when a TCP Client binds to the connect socket.  Allow “::” in addition to “*.*.*.*” for the IPV6 Ip Address to indicate that ANY MDNP device may connect to this channel.  Added session configuration unsolOfflineDelayOnce to allow unsolOfflineRetryDelay to be used once and then to revert back to unsolRetryDelay until the next time the retry count is exceeded.  Allow compile time code that can be configured per session to allow read requests between select operate requests. This is not compliant with the DNP Specification and would fail DNP Conformance tests. However, it was required for certain gateway customers to allow DNP Masters that don’t follow the rules.  Changed “configuration signature” to “configuration digest” for default Device Attribute Object 0 Variations 200 and 201 as directed by TB2013-004b. Affects Diagnostics and #defines only.  If file name is longer than configured maximum DNPCNFG_MAX_FILENAME, stop parsing the request sending an error response to the Master. Using a truncated filename should not work and it could access a different file than intended.  In LinIoTarg use localIpAddress when a TCP Client binds to the connect socket.   v3.26.00:    Decrement unsolicited pending count of events for an event class when an event is removed from the event queue because of event mode or other reasons.  Improved the sDNP SA example to show authority interaction and asymmetric key change methods. Enhanced tmwcrypto simulated crypto database to support multiple sessions and sectors to make it clear what configuration in the exampe code applies only to the simulated crypto database and is not used by the SCLs.  Changed “configuration signature” to “configuration digest” for default Device Attribute Object 0 Variations 200 and 201 as directed by TB2013-004b. Affects sDNP simulated database description.  Added virtual terminal and string point names to the generated sDNP Device Profile current values.  Add any missing pChannel and pSession to DNPDIAG_ERROR where they are available.  Created additional SDNP example, sdnpDer implementing DER Application Note 2018-001 DNP3 Profile for Communications with Distributed Energy Resources DERs). This serves as a starting point for developing DER devices that use DNP.   v3.25.00:       Created an implementation guide for SCLs.  Added support DNP File Transfer in Simulated Database for Linux.  The target layers for Linux and Windows have been updated to include File I/O functions to support DNP. They reside in mdnptarg.c and sdnptarg.c with their common prototypes defined in dnp/mdnptarg.h and dnp/sdnptarg.h. sdnpdata.c have been updated to call these functions to support file operations even when the simulated database is not enabled.  Support for Freeze and Freeze at Time FC 11 and 12 with obj50v2 has been implemented. The Device Profile has been updated to show support for these.  Added session configuration parameter unsolSendByClass. When set to true, only unsolicited events for the class whose timer expires or count is exceeded will be sent. Otherwise, unsolicited events for all classes will be sent.  Support User Managed Events for g33, g115, g120, and g122 events.  Support for start-stop qualifiers has been added to write operations for Object Groups 34, 110, & 114 and the Device Profiles updated accordingly.  Better support for optional the g120v7 event on another DNP Association with optional error text that does not fit in current or any response fragment.  Generate a session statistic when a Link Status Request is received.  Provide tmwcrypto_verifyUserCertificate function using OpenSSL to verify received X.509 Certificate.  The SCL sample target layers now support a common serial configuration structure.  The SCL has been updated to work with OpenSSL 1.1.x while retaining backward compatibility with version 1.0.x. For Windows builds a new define has been added, TMWCNFG_USE_OPENSSL_1_0_2 that should be defined when running with the legacy version.  Added event driven receive data support for Windows serial channel.  WinIoTarg now uses the Channel thread to check for received data eliminating the need for a separate CheckForInput thread/class.  The wintoolkit samples now include TLS support. The samples include certificates that are compatible with those used by the Test Harness and the default configurations for both Linux and Windows are set to use them. Only a single define USE_TLS which defaults to false needs to be updated to configure the sample to use TLS for both DNP and 101/4 Secure Authentication builds.  Updated Linux target layer to use consistent logging technique  Added TLS support to the Linux target layer.  LinIoTarg now creates a thread per channel (when not configured to run in polled mode). Once the channel has been opened, this thread will establish/listen for the channel connection and wait on the data socket for incoming data. This eliminates the latency when polling for received data. Also, locking has been reduced by eliminating the linked list of TCP Channels which required locking each time the list was accessed.  Updated linTCP_transmit to check the error code returned by send. If its EAGAIN indicating that the socket is busy, wait for the socket to become writable and retry the send instead of disconnecting immediately.  The Linux target layer has been updated to provide a basis for porting a target layer to another OS: • The dependency on threads has been removed. • The Linux target layer will conditionally support poll and select. The Linux target layer will default to use epoll/poll which is more efficient than select and correctly handles file descriptors greater than 1024. However, select will continue to be supported by using a conditional so that it can be tested with Linux and provide a porting example for OS's that only support select. • A tmwtargcnfg.h file has been added to both the Linux and Windows target layer implementations. This configuration file defines which features a given target layer supports. This will allow customers porting the Linux target layer to other OS's to easily identify code that may be unnecessary for their application and remove it instead of wasting time porting it. Target layer features currently defined in this file are: 1. RS232 2. TCP 3. UDP (Only required for DNP) 4. TLS 5. EPOLL    In addition, SampleIoTarg has been created. This Sample target layer is a target layer example that can be compiled and linked with the wintoolkit C example programs that has no OS dependencies. It can be used as a baseline target layer to help port the library to another OS or RTOS.  It can be built by specifying build=sample from the Linux command line.    v3.24.00:    Implement DNP String and Vterm changes spelled out in TB2015-001 including adding support for Extended String Object Groups 114 & 115.  Support for Frozen Analog Input Event Object Groups 31 and 33 has been added.  Modified handle in .NET libraries to be an IntPtr to avoid warnings with 64 bit build and to provide better 64 bit functionality.  Made improvements to target layer timer examples.  Implemented event driven receive data in winiotarg so channels don’t have to be polled. This is also available in liniotarg.  Updated the Linux target layer, replaced LINIOCNFG_EVENT_DRIVEN conditional with polledMode configuration.  Made improvements to target layer timer examples.  Implemented event driven receive data in winiotarg so channels don’t have to be polled. This is also available in liniotarg.  Updated the Linux target layer, replaced LINIOCNFG_EVENT_DRIVEN conditional.  Support for epoll has been added to the Linux target layer. This support is controlled by the preprocessor conditional LINIOTARG_SUPPORT_EPOLL. If this is not defined, select is used. The select option has been left in place so we continue to have an example that can be ported to OS’s that do not support epoll.  Added configuration to sDNP SCL to disable sending the initial Null Unsolicited Response (UR) at startup and therefore not require it to be application confirmed by the master. This is non-standard behavior differs from the DNP specification, but facilitates interoperability with a non-compliant master that does not process the Initial Null UR correctly, but does handle subsequent URs containing DNP events.  Improved sDNP user manual and sample code with respect to event generation. Updated slave examples to include add event for DNP, 101,102, 103, and 104.  Include files to allow SCL customers to generate Windows project files for different versions of Visual Studio.  Support for 64-bit versions of the .NET Protocol Components has been added.  Added configuration for the generation of diagnostic messages per sector, session, and channel to allow maximum flexibility. Maximum performance can be achieved by configuring each mask to zero so no diagnostic messages are generated, thus no messages would be forwarded to the .NET code. This also would allow a user to enable diagnostic messages on a single sector, session, etc. in a large configuration on a problem connection and avoid having to filter out diagnostic messages from all other connections.   v3.23.00:     Added support for TLS v1.2 in the Windows target layer  Included support for 64 bit builds in the example project and solution files provided for Visual Studio  Updated Device Profile Current Values section for Outstations to schema Version 2.11 April 2016  Added x.509 certificates and support for DNP Secure Authentication g120v8 requests  Updated simulated database for Outstation example to support DNP Technical Bulletin TB2017-001 Supervisory Mode  Resolved warnings when compiling on Linux with strict checking turned on  Improved Secure Authentication Version 2 to better support the expiration and updating of User Session Keys (in the same way as described in the SAv5 specification)  Added capability in Master to notify the application via a callback if commands or secure authentication requests time out  Improved the algorithm in the Master for managing multiple sessions in a single channel  Several other enhancements (see release notes)   v3.22.00:   Improved performance of database interface and other library functions  Restructured directories and sample code for Windows and Linux target layer libraries  Added support for IPv6 in Linux target layer  Expanded Windows target layer support for resolving IP address from destination host name  Updated Linux sample makefiles to generate debug information by default  Added multiple enhancements to Secure Authentication library and documentation   v3.21    I mproved Linux ® makefiles and Windows ® solution and project files  Improved integration with OpenSSL library  Added parsing of Object 91 response for DNP3 Master  Added GetRxBuf() method for DNP3 Master so that application can view the entire received message  Added capability to configure use of Aggressive Mode for Secure Authentication critical functions launched from the “Auto Request Mask” for DNP3 Master  Added capability for DNP3 channel to skip offline sessions for specified amount of time  Several improvements to DNP3 Secure Authentication  Improved DNP3 Dual End Point mode     v3.20.00:    Implemented several improvements to DNP3 Secure Authentication including all items suggested in TB2016-002, addressing some deficiencies in DNP3-SAv5  Added a configurable feature that would allow the user to issue two separate BRM commands, holding off other automatic behavior  Added statistic indicating Link Status was received  Made improvements to DNP3 Device Profile     v3.19.00:    Added support for Assign Class Function Code for Virtual Terminal object group  Added Session Statistic event to indicate a Link Status response is received  Added configuration to disable default behavior required on TCP by the DNP specification to disconnect and reconnect when a link status request times out  Modified code to allow use of either UNICODE or MBCS character set     v3.18.00:    Updated the DNP3 libraries to include DNP3 Device Profile November 2014 version 2.10  Improved the handling of DNP messages with the wrong link address  DNP Secure Authentication improvements including a configurable 4-32 byte challenge data object, a new device attribute for the version of Secure Authentication that is supported by the Outstation, corrected session statistics, and improved aggressive mode support  Exposed DNP Unsolicited class mask for .NET components (propertyUnsolnitialClassMask)  Added multiple new DNP3 device attributes in the default simulated database  Improved support of Linux ® networking layer for UDP  Added ability to disable function codes for device specific addresses as described in AN2014-001  Implemented DNP device attribute time type described in TB2013-004  Updated the code to use C99 standard types for 16 bit and 32 bit types for improved Linux ® implementations  Improved handling of unsolicited messages under special cases  Improved support of DNP3 points set to local mode     v3.17.00:    Additional support for DNP3 device profile, including secure authentication, control broadcast configuration, and other fields  Added cryptography interface for Secure Authentication V2 (the Library already has this interface for SAv5)  Enhanced the tolerance for restart in Master and Outstation with Secure Authentication, including initialization of keys  Added compile time choice of using either standard or Microsoft “safe” string functions that include the length of the buffer to be written to (ie strcpy or strcpy_s)  Added more checking to discard improperly formatted messages     v3.16.01 - Corrected an issue where DNP3 transport layer would reject a message   v3.16.00:    Addressed all items associated with ICS-CERT Advisory (ICSA-13-240-01)  Improved input verification  Made improvements to DNP3 Secure Authentication interface  Enhanced Assign Class functions  Added configuration option to disable support for receiving broadcast messages     v3.14 - Enhanced DNP3 SAv5 implementation   v3.07 - Added sample Linux ® and Windows ® target implementation   v3.05.01 - Added updates to DNP3 SA approved by DNP3 TC   v3.01.01 - Added ability to keep time on a per-session basis   v3.01.00 - Added support for DNP3 SAv2   v3.00.44 - Updates to Data Sets   v3.00.43 - Added support for Data Sets and Object 0 (Device Description)   v3.00.38 - Added support for Double Bit data types and Self Address Discovery mechanism   v3.00.36 - Added support for File Transfer Event Mode   v3.00.35 - Added memory alignment support   v3.00.32 - Added support for multiple threads                                                                              Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/services-supported
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                 The Most Complete IEC 61850 Stack           Supported Capabilities          Read services  Fully Supported                Write services  Fully Supported                Reporting Services  Fully Supported    Un-buffered and buffered Report Control Blocks  Any valid data set can be used in Report Control Blocks  Supports both data attributes and data objects in data sets  All trigger options (data change, quality change, data update, integrity, GI)                Discovery  Fully Supported    Supports Discovery of Server Object Model by Client                Control Models  Fully Supported  All control models for IEC 61850:   Status-only  Direct Control with normal security  Direct Control with enhanced security  Select Before Operate with normal security  Select Before Operate with enhanced security                GOOSE  Fully Supported    Any valid data set can be used in GOOSE Control Blocks  Supports both data attributes and data objects in data sets  Multicast GOOSE messaging  Supports both Layer 2 and routable GOOSE  Test and simulation flag support at transport level                Sampled Values  Fully Supported    Supports 9-2 LE Profile  Encoded in multicast Ethernet frames  Supports both Layer 2 and routable Sampled Values                Data Sets  Fully Supported    Any combination of Data Attributes and/or Data Objects  Preconfigured  Persistent  Non-Persistent  Application Association Specific                External References  Fully Supported    Support for External References in SCL description  Server will automatically resolve external references and subscribe to GOOSE or Sampled Values                Logging Services  Fully Supported    All Trigger options, integrity period, etc.  Any valid data set can be used in Log Control Blocks                File Transfer Services  Fully Supported    File upload  File download  File delete  Directory browsing                Settings Groups  Fully Supported                Service Tracking  Supported                Security  Fully Supported    IEC 62351 (parts -3, -4, and -6) security standard is supported for authentication and encryption (TLS)  IEC 62351- 9 security standard for Secure Routable GOOSE and Routable Sampled Values - upgrade is available                                                                                     Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/library-configurations
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                IEC 61850 Library Components             Available Library Components          IEC 61850 Server  The Server Source Code Library is optimized for high performance in a variety of device-level implementations including embedded systems with limited resources.                IEC 61850 Client   The Client  Source Code Library can be used in embedded or non-embedded environments to provide very efficient, high-speed client services.                 GOOSE  The GOOSE (Generic Object Oriented Substation Event) Publisher/Subscriber component implements an additional protocol used for protection functions and other applications requiring high speed multicast peer-to-peer communications capabilities.                  Sampled Values  The Sampled Values (IEC 61850-9-2) component is used to transmit high speed streams of data set samples encoded in multicast or unicast Ethernet frames. The protocol uses a publisher/subscriber model, in which a publisher transmits unacknowledged data to subscribers.                             Library components are available in ANSI-C, C++, Java or .NET Components .                                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/benefits
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                 Benefits of Using Our IEC 61850 Library          The Most Complete IEC 61850 Stack         Full IEC 61850 Service s Out of The Box (Not Just MMS)         Edition 2 and Edition 2.1 features including Tracking, Mixed Edition Systems, IEC 62351 Security         Support Edition 1, Edition 2, and Edition 2.1 with a single library implementation  based on SCL file          Training Videos, Design Review, and Example Code Included        A Proven Solution         Our IEC 61850 Stack has been implemented in hundreds of customer applications around the world         Customer applications have been highly successful in interoperability testing        More Efficient Code        Our Library was designed from the ground up to operate efficiently in small embedded systems         Advanced parsing technology (derived from computer science compiler techniques) which is much more efficient and compact         Our code has no run-time memory allocation and allows better task scheduling        High Performance        Efficient code has performance benefits for small or large systems, for both Servers and Clients          Significant performance advantages can be measured in both throughput and processor utilization         Our stack can operate in real-time environments          Client applications can scale to accommodate hundreds of server connections        Faster Development & Integration        Reduce development effort and free up resources to work on proprietary aspects of your products         Spend less time keeping track of work by IEC 61850 Technical Committees with our regular library releases that include the latest updates to the standard         Servers can be built in weeks, rather than months with our unique approach to integrating application data with IEC 61850        Full support of Substation Configuration Language (SCL)        Full boot-time configuration of data models         Integration directly from SCL files         Free license for SCL Navigator tool for creating and editing SCL        Dedicated Support        The success of our Source Code Library customers is our priority         Support includes access to engineers who develop the libraries and participate on the IEC 61850 Technical Committees         First year of the Maintenance and Enhancement plan is included with license         Web-based video training to learn IEC 61850 and data modeling          Object Model Consultation and Design Review of your completed implementation        Flexible Licensing Options        Two licensing options are available:          Per Product License - this option is a fixed price for each end-product with no royalty fees (the majority of our customers find this option best for their needs)  Royalty Based License - this option has royalty fees based on the number of units sold (a good option for customers with several low volume products)          First year of the Maintenance and Enhancement plan is included with either license option                                                                                           Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/server-features
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                Server Library Features    Servers can be built in weeks, rather than months with our unique approach to integrating application data with IEC 61850. Our library was designed from the ground up to operate efficiently, which makes it a great choice for embedded system Server implementations.  The efficiency of the library also allows for scaling in larger applications.           Full IEC 61850 Server Library           Full IEC 61850 Services out of the box          More than just an MMS protocol stack                  Supported Features          Read Services          Write Services           Reporting Services            Logging            Controls            Dynamic Data Sets            File Services            Sampled Values            Discovery             Tracking Services             Mixed-Edition Systems         For a full list of supported IEC 61850 services, see IEC 61850 Services .             Simple and Powerful Integration           Support for total runtime configuration via local SCL file          Additional lower level interfaces available for specialized device integration requirements          Integrates with application at object level, not transaction level          Advanced data modeling techniques simplify generation of conformant servers               Supported Object Models           IEC 61850-7-3 (Common Data Classes)          IEC 61850-7-4 (Logical Nodes and Data Objects)          IEC 61400-25 (Wind)          IEC 61850-7-410 (Hydroelectric)          IEC 61850-7-420 (DER)               Security           IEC 62351 (parts -3, -4 and -6) security standard is supported for authentication and TLS   encryption    IEC 62351-9 security standard for Secure Routable GOOSE and Routable Sampled Values - click HERE to learn more about how to secure Routable GOOSE and Sampled Values                                                                                                         Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/client-features
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                Client Features  Our IEC 61850 Client Library was designed from the ground up to fully support all IEC 61850 services.  The a pplication interface is easy to use, in both embedded and integrated environments.  Client implementations with our stack can accommodate hundreds of server connections without performance limitations.           Full IEC 61850 Client Library           Full IEC 61850 Services out of the box          More than just MMS implemented with custom user code                  Supported Features          Read Services          Write Services           Reporting Services            Logging            Controls            Dynamic Data Sets            File Services            Sampled Values            Discovery             Tracking Services             Mixed-Edition Systems         For a full list of supported IEC 61850 services, see IEC 61850 Services .             Simple and Powerful Integration           Additional lower level interfaces available for specialized device integration requirements          Integrates with application at object level, not transaction level          Advanced data modeling techniques simplify generation of conformant clients               Supported Object Models           IEC 61850-7-3 (Common Data Classes)          IEC 61850-7-4 (Logical Nodes and Data Objects)          IEC 61400-25 (Wind)          IEC 61850-7-410 (Hydroelectric)          IEC 61850-7-420 (DER)               Security           IEC 62351 (parts -3, -4 and -6) security standard is supported for authentication and TLS   encryption    IEC 62351-9 security standard for Secure Routable GOOSE and Routable Sampled Values - click HERE to learn more about how to secure Routable GOOSE and Sampled Values                                                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/data-models-supported
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                 IEC 61850 Data Models           Data Models Supported     Our IEC 61850 Source Code Library supports the following data models:       IEC 61850-7-3 (Common Data Classes) - Ed. 1, Ed. 2 and Ed. 2.1  IEC 61850-7-4 (Logical Nodes and Data Objects) - Ed. 1, Ed. 2 and Ed. 2.1  IEC 61400-25 (Wind Turbines)  IEC 61850-7-410 (Hydroelectric Power Plants)  IEC 61850-7-420 (Distributed Energy Resources)      Models for the library are XML based which can be extended to support additional IEC 61850 data models                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/-net-components
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                .NET Components    Based on Triangle MicroWorks, Inc. industry-proven Source Code Library design  Supports Windows and LInux  Supports all .NET Languages (C#, J#, Managed C++, VB .NET, etc.) and tools  Compatible with .NET 4.x framework and .NET Core  Integrates with Visual Studio Help  Source code version includes corresponding ANSI-Standard C and C++ Source Code Libraries  Includes built-in simple database with save/restore capabilities; also supports user-defined database  Ideal for quick development of products and tools requiring IEC 61850 support  Scalable for large implementations  Typical product integration time of less than one week  .NET components for Sampled Values should only be used for testing purposes on Windows Operating Systems due to the timing constraints.                                                                                  Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/eval-kit
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                Library Evaluation Kit  Evaluate the  Library:    Learn how the API is used to access the data model and services  See how the example database is integrated into application  Parse data model and control blocks in SCL Files  Serve reports and publish GOOSE based on SCL configuration    Evaluation Kit Includes:    Windows ® DLL for MMS Server and GOOSE stack  Example Visual Studio ® Project  Example server application which loads data model, generates reports, and publishes GOOSE  Example SCL File for server application    Note: the evaluation kit only supports Windows®, however the IEC 61850 ANSI-C library from Triangle MicroWorks is designed to be processor and operating system independent for Windows®, Linux®, or Real-Time Operating Systems (RTOS).  Download         Learn more - download the user manual   Download Eval Kit - registration is required but easy  Note:  a product key is required to activate the evaluation kit on your PC.  Contact support for a free license.        Linux® is the registered trademark of Linus Torvalds                                                                                 Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-61850-scl-pages/what's-new
                                                  Request a Quote          Source Code Library Video Playlist           Download C Library Eval Kit          Download .NET Evaluation              IEC 61850 Source Code Library  Our IEC 61850 Source Code Library provides a quick and cost effective way to implement an IEC 61850 Server, Client, GOOSE, or Sampled Values.  The library was designed from the ground up to fully support IEC 61850 services and efficiently use resources.  We offer our Library Components in ANSI-C, C++, Java or .NET Components for maximum flexibility across many different applications.  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to our customers to support all their conformance  & certification needs. We will work with you, as needed, to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Capabilities  Library Components  Benefits  Server Features  Client Features  GOOSE  9-2 Sampled Values (SV)  Secure Routable GOOSE and SV  90-5 Synchrophasor  Data Models Supported  .NET Components  Evaluation Kit  What's New       Overview    Capabilities    Library Components    Benefits    Server Features    Client Features    GOOSE    9-2 Sampled Values (SV)    Secure Routable GOOSE and SV    90-5 Synchrophasor    Data Models Supported    .NET Components    Evaluation Kit    What's New        Related Products           Test Suite Pro (TSP)  SCL Navigator                What's New  ANSI-C Library Current Release:  V12.2.0 Release Date: February 2024   C++ Library Current Release: V5 .7 Release Date: February 2024   .NET Component Current Release: V5 .7 Release Date: February 2024   Java Wrappers  Current Release: V5 .7 Release Date: February 2024  Newest Features in Latest ANSI-C Library Release    Fixed security vulnerability. A specifically crafted report with illegal fields could cause the server to crash.  Added support for SynchSourceID in the SmapledValueControlBlock Options field (smvOpts).   The Services section of the SCL file is now fully supported when defined under the AccessPoint Element.     Key Additions to Recent Releases of ANSI-C Library     V12.1    -Made KDC Client run in the same process as the stack -Added support for KDC Client  -Added logging support in KDC Client  -Improve premake support for openssl  v3 .x/ v1 .1.1 - Support for building static and as a shared library  V12.0.8    - Security Update per ICSA -22-249-01 -New build system based on premake5  - KDC support for Routable GOOSE/ SampledValues  -Upgraded to Opnssl support for v3 .x - Separate layer 2 GOOSE/ SAV and Routable GOOSE/ SAV in the build so they are independent of each other. -Added MapTimeStampstoSubVal flag on the Client and Server Configuration class. This defaults to true. If true 't' timestamps are mapped to subVal and subQ . - Routable threading model has been re-designed to reflect the layer2 (DL) design.  V11 .2.1   -Security Update per ICSA -22-249-01   V11 .2   -Updates to support authentication and encryption of Routable GOOSE and SAV   V11 .1.1   - Updates to support the first Amendment of Edition 2 of the core parts of IEC 61850 - Update schema to 2007B4 - LGOS support and LPHD . sim processing for GOOSE subscriptions - Callbacks interface to inform server application of writes to Control Blocks - Persist logs via file mapping - Allow TLS and MMS authentication independently - Update Logging - allow journal events, log events on LCB enable and disable - Update to openSSL 1.1.1 - but restrict from using TLS 1.3, as this is not compliant to 62531 - Update R-GOOSE to Amd1 of part 8-1  V11 .0.5.2  - Updates status value for BSC - Segmentation of reports does not depend on OptFlds - Add Routable ( 90-5 ) to SAV example application - TASE .2 allow writing DSTS enable without alternate access - Removed eval handler for 't' attributes - Improve 64 bit handling - Store scaleFactor , units, multiplier for extRefs - Provide method to find related DAs for extRefs - Updated error codes for ConfirmEditSGValues - Add API to set trace level and mask at runtime - New API to start multiple servers from SCD file without reparsing the SCL - Support application managing GOOSE enable outside the stack context - Allow MMS authentication without TLS - Improve tracing for connect errors - Support IEC61850 64 bit datatype - Added installer for NDIS drivers - Add Journal entry and ReasonCode support - Update sample certificates for MMS and TLS - Reinitialize  CBBs and services when creating a new stack context - Reorg 2 lines of code in jour_if.c because VS2017 optimizer can't make 2+2 equal 4  V11 .0.4  - Addressed conformance issues:  - Clear GoEna and set NdsCom when GOCB configuration is not complete - Release ResvTms ( sBr23 ) - Automatic reservation of RCBs (TISSUE 1477) - Improved microsecond timers - Added support for using system clock for timestamps on Windows ® - Added support for TCP  keepalive setting configuration at runtime on Windows ® - Added support for SE FC in Edition 1 SCL Files - Updated GOOSE retransmission scheme to use monotonic clock  v11 .0.3  - Added a common subscribe function for routable GOOSE (R-GOOSE) and traditional multicast GOOSE - Implemented several Technical Issues (TISSUES) including TISSUE 1369 and 1091 - Improved handling of GOOSE messages after TAL expiration - Improved timing resolution for Windows® implementations - Updated examples for implementing IEC 61850 servers - Update to Linux ® static allocation of semaphores and events - Improvements to time calculations with microsecond resolution - Several other enhancements - see release notes for full list  v11 .0.1 - Fixed issue with failure to disable report control blocks upon loss of two-party- assocation - Improved GOOSE by fixing timeout miscalculation and unbind error - Improved compiler warnings - New macro to allow a modifier for functions likely to be exposed  v11 .0 - This major release improves the library for more streamlined IEC 61850 implementations and builds on the 20 year track record of dependable code and high performance. - A new build process is provided that more closely follows typical customer use. The library builds completely from the application directory to simplify integration into a larger project. Multiple configuration files have been condensed into a single directory with one header file and the application makefile . -The code base has been significantly refactored in order to streamline library implementations and make debugging easier. - The target layer has been abstracted and consolidated. Linux and WindowsTM targets are provided and OS specific code has been collected to make porting easier. The Linux target has improved threading and semaphore handling. - A new consistent tracing mechanism has been added.  Applications can filter on messages for different library modules (like SCL parsing, transport, ACSI services, etc.) and can filter by message severity. - Most examples have been simplified to more clearly demonstrate particular functionality. v10 .5   Initial support for routable Sampled Values and GOOSE (according to Part 90-5 of IEC 61850 standard)  has been added. This new capability allows Sampled Values and GOOSE messages to be routed across wide area networks so that substations in different locations can share voltage, current, phase, and frequency data from phase measurement units. Sharing these accurate measurements and event data enables the implementation of Wide Area Measurement and Protection and Control ( WAMPAC ) systems to help stabilize large, interconnected power systems.   v10 .4   Allow integrity & GI when there is connection but rptEna =False Made changes to support Ed1 versus Ed2 Buffer integrity reports while there is no connection Improved checking on MMS context pointers  TLS / SSL method support for PNNL Added support for extRefs Addressed IEC 61850 TISSUES from WG10   v10 .3 Added updates for Schema 3.1 Added support for CRL updating Improved parsing of SCL  namespaces Improved Client API reporting callbacks Added optional compatibility with legacy IEC 62351 implementations Added support for mixed Ed. 1/Ed. 2 systems Added support for GOOSE Test flag Added improvements to Tracking nodes Updated classes and primitives for conformance testing Added updates for Schema 3.1 Removed security checks for GOOSE/ SAV (due to revisions to schema and IEC 62351)   v10 .0  Added support for IEC 61850 Ed. 2 Created new XML modeling scheme Added Ed2 Class models, Reporting and Logging Models, and Service Tracking Added Analog control support ( AnalogueValue structure as ctlVal ) Added IEC 62351 security Changed to Ed2 + Ed1 schema, adding nIedSclVersion and nSclVersion , and changing control block instances Changed to Ed2 + Ed1 log control blocks and log definitions from SCL , including domain specific Added fileDirectory change (handle case of file argument to fileDirectory service request) Added settings Group Control Block support Added support for arrays of SDOs Allow for array elements in datasets Added support which allows all Client write requests to automatically trigger invoking evaluation handlers and reporting                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/modbus-scl-pages/outstation-library-features
                                                  Request a Quote          Source Code Library Video Playlist               Modbus Source Code Libraries Modbus is an application layer messaging protocol, positioned at level 7 of the OSI model that provides client/server communication between devices connected on different types of buses or networks. Modbus is a request/reply protocol and offers services specified by function codes. Modbus function codes are elements of Modbus request/reply PDUs.  Triangle MicroWorks’ Software Libraries provide a cost-effective means of supporting industry-standard protocols in your device. Incorporating our  Software Libraries in your products will shorten development time, freeing internal resources to work on company proprietary aspects of your products.  Triangle MicroWorks Software Libraries are available in two formats: .NET Protocol Components for incorporation in Windows .NET-based products, and ANSI-Standard C Source Code Libraries for all other platforms.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    ANSI C Source Code    .NET Components    What's New        Related Products           Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)          Outstation Library Features   Interoperability is maximized by making it easy to attach virtually all possible interoperability configuration settings to run-time variables or function calls.  Supplies data to an unlimited number of host devices through an unlimited number of communication ports.  Example Database Interface implementations are provided for testing, illustration, and as templates to be used for developing final Database Interface.                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/modbus-scl-pages/master-library-features
                                                  Request a Quote          Source Code Library Video Playlist               Modbus Source Code Libraries Modbus is an application layer messaging protocol, positioned at level 7 of the OSI model that provides client/server communication between devices connected on different types of buses or networks. Modbus is a request/reply protocol and offers services specified by function codes. Modbus function codes are elements of Modbus request/reply PDUs.  Triangle MicroWorks’ Software Libraries provide a cost-effective means of supporting industry-standard protocols in your device. Incorporating our  Software Libraries in your products will shorten development time, freeing internal resources to work on company proprietary aspects of your products.  Triangle MicroWorks Software Libraries are available in two formats: .NET Protocol Components for incorporation in Windows .NET-based products, and ANSI-Standard C Source Code Libraries for all other platforms.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    ANSI C Source Code    .NET Components    What's New        Related Products           Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)          Master Library Features   An unlimited number of remote devices can be configured on an unlimited number of communication ports,and new remote devices can be added at runtime.  Multiple devices can be assigned to the same communication port to support multiple network communication topologies.  Database manager maps received Modbus data into Target Application data points (coils, discrete input registers, holding registers, input registers, etc.).                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/modbus-scl-pages/ansi-c-source-code
                                                  Request a Quote          Source Code Library Video Playlist               Modbus Source Code Libraries Modbus is an application layer messaging protocol, positioned at level 7 of the OSI model that provides client/server communication between devices connected on different types of buses or networks. Modbus is a request/reply protocol and offers services specified by function codes. Modbus function codes are elements of Modbus request/reply PDUs.  Triangle MicroWorks’ Software Libraries provide a cost-effective means of supporting industry-standard protocols in your device. Incorporating our  Software Libraries in your products will shorten development time, freeing internal resources to work on company proprietary aspects of your products.  Triangle MicroWorks Software Libraries are available in two formats: .NET Protocol Components for incorporation in Windows .NET-based products, and ANSI-Standard C Source Code Libraries for all other platforms.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    ANSI C Source Code    .NET Components    What's New        Related Products           Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)          ANSI C Source Code   Written in ANSI-Standard C Source Code, under a strict corporate coding standard.  Designed to be processor and operating system independent, using any ANSI-Standard C compiler.  Simple configuration for big-endian or little-endian byte order.  Can be used with or without a Real Time Operating System (RTOS).  Database interface supports any database, ranging from direct I/O input with no storage to complex, relational databases.  Includes sample applications and source code for Low-Level Target Interface for Linux and Windows (see Design Details for Implementation).  Typical product integration times are less than three weeks.                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/modbus-scl-pages/dotnet-components
                                                  Request a Quote          Source Code Library Video Playlist           Download Evaluation              Modbus Source Code Libraries Modbus is an application layer messaging protocol, positioned at level 7 of the OSI model that provides client/server communication between devices connected on different types of buses or networks. Modbus is a request/reply protocol and offers services specified by function codes. Modbus function codes are elements of Modbus request/reply PDUs.  Triangle MicroWorks’ Software Libraries provide a cost-effective means of supporting industry-standard protocols in your device. Incorporating our  Software Libraries in your products will shorten development time, freeing internal resources to work on company proprietary aspects of your products.  Triangle MicroWorks Software Libraries are available in two formats: .NET Protocol Components for incorporation in Windows .NET-based products, and ANSI-Standard C Source Code Libraries for all other platforms.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    ANSI C Source Code    .NET Components    What's New        Related Products           Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)          .NET Components   Based on Triangle MicroWorks, Inc. industry-proven Source Code Library design.  Supports all .NET Languages (C#, J#, Managed C++,VB .NET, etc.) and tools.  Compatible with .NET 2.0 Framework.  Integrates with Visual Studio Help.  Source code version includes corresponding ANSI-Standard C Source Code Library.  Includes built-in simple database with save/restore capabilities; also supports user-defined database.  Ideal for quick development of products and tools requiring Modbus support.  Scalable for large implementations.  Typical product integration time of less than one week.                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/modbus-scl-pages/what's-new
                                                  Request a Quote          Source Code Library Video Playlist               Modbus Source Code Libraries Modbus is an application layer messaging protocol, positioned at level 7 of the OSI model that provides client/server communication between devices connected on different types of buses or networks. Modbus is a request/reply protocol and offers services specified by function codes. Modbus function codes are elements of Modbus request/reply PDUs.  Triangle MicroWorks’ Software Libraries provide a cost-effective means of supporting industry-standard protocols in your device. Incorporating our  Software Libraries in your products will shorten development time, freeing internal resources to work on company proprietary aspects of your products.  Triangle MicroWorks Software Libraries are available in two formats: .NET Protocol Components for incorporation in Windows .NET-based products, and ANSI-Standard C Source Code Libraries for all other platforms.   For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                 Overview  Outstation Library Features  Master Library Features  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    ANSI C Source Code    .NET Components    What's New        Related Products           Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)          What's New Current Release: v3.32 Release Date:  March 2024  Newest Features in this Release    Added TCP channel callback functions to retrieve IP addresses when the server is connected or disconnected.      Channels are no longer closed automatically when the last session on the channel is closed.    Store the last exception Function Code so the user can find out what the Function Code is not accepted by the server device when needed.    In WinIoTarg, added a channel diagnostic mask bit to control whether diagnostics messages are generated.     In LinIoTarg improvements to use linked list instead of array for keep channels listening on a particular port. That improvement will allow unlimited channels be configured to one specific TCP port.     In WinIoTarg, if the TCP listening thread is not started due to the port being occupied, the channel is now closed more quickly (previously could take up to 5 seconds).    Key Additions to Recent Releases  v3.31    “Slave” has been replaced by “Outstation”, “Controlled Station”, or “Server” depending on protocol in documentation, filenames, variable names, and function names. An effort has been made to maintain interface backward compatibility where possible.  Minor changes to allow SCL to build with VS2019 and VS2022 without modification related to std::string and .NET Framework version.  Added support for OpenSSL Version 3.x.x. OpenSSL Version 1.1.1 is still supported, however according to openssl.org the End Of Life date for the 1.1.1 series is September 11 2023. Support for Version 1.0.2 has been removed.  In WinIoTarg and LinIoTarg add clientSourceIpPort to allow TCP Client Channels to specify the IP Port to use for originating TCP Connections.  Add support for TMWDIAG_ID_OCTETS bit to determine whether the actual Hex Octets from a transmitted or received message are formatted and/or displayed in a diagnostic message.  Allow Modbus TCP to be configured to use UDP Only instead of a TCP connection.   v3.30    In LinIoTarg speed up processing during TCP Channel closing.  In LinIoTarg function call to deprecated pthread_mutexattr_setkind_np() has been replaced with a call to pthread_mutexattr_settype().  Add support for multiple Polled Timers and disabling the Windows Target Layer multiple timers to allow application to run SCL in the context of its own threads.   v3.29    In WinIoTarg added define to allow channel thread code to be compiled out.  Improved TLS example code and include sample TLS Certificates for ANSI C and .NET SCLs.   v3.28    In WinIoTarg potentially speed up waiting for a channel thread to exit when a channel with olledMode=false is closed.  In LinIoTarg replaced deprecated stime() with clock_settime().  In LinIoTarg speed up new connection indication to protocol library when running with polledMode=true.  Support .NET txPhysCallbackEvent, rxPhysCallbackEvent, txApplCallbackEvent, and rxLinkCallbackEvent callback events in TMWChannel class for all protocols. Move .NET rxLinkHdrCallbackEvent and rxTransportCallbackEvent callback events out of TMWChannel class to DNPChannel since those two events only apply to DNP channels and were misleading.   v3.27.1    In LinIoTarg use localIpAddress when a TCP Client binds to the connect socket.   v3.26   General bug fixes and code optimization.   v3.25    Created an implementation guide for SCLs.  The SCL now supports a common serial configuration structure.  Added event driver support to Windows serial channel.  WinIoTarg now uses the Channel thread to check for received data eliminating the need for a separate CheckForInput thread/class.  Update Linux target layer to use consistent logging technique.  LinIoTarg now creates a thread per channel (when not configured to run in polled mode). Once the channel has been opened, this thread will establish/listen for the channel connection and wait on the data socket for incoming data. This eliminates the latency when polling for received data. Also, locking has been reduced by eliminating the linked list of TCP Channels which required locking each time the list was accessed.  The Linux target layer has been updated to provide a basis for porting a target layer to another OS:  • The dependency on threads has been removed. • The Linux target layer will conditionally support poll and select. The Linux target layer will default to use epoll/poll which is more efficient than select and correctly handles file descriptors greater than 1024. However, select will continue to be supported by using a conditional such that it can be tested with Linux and provide a port example for OS's that only support select. • A tmwtargcnfg.h file has been added to both the Linux and Windows target layer. This configuration file defines which features a given target layer supports. This will allow customers porting the Linux target layer to other OS's to easily identify code that may be unnecessary for their application and remove it instead of wasting time porting it. Target layer features currently defined in this file are: 1. RS232 2. TCP 3. UDP (Only required for DNP) 4. TLS 5. EPOLL    In addition, SampleIoTarg has been created. This Sample target layer is a target layer example that can be compiled and linked with the wintoolkit C example programs that has no OS dependencies. It can be used as a baseline target layer to port the library to another OS or RTOS.  It can be built by specifying build=sample from the Linux command line.    v3.24   Made improvements to target layer timer examples.  Implemented event driven receive data in winiotarg so channels don’t have to be polled. This is also available in liniotarg.  Updated the Linux target layer, replaced LINIOCNFG_EVENT_DRIVEN conditional with polledMode configuration.  Support for epoll has been added to the Linux target layer. This support is controlled by the preprocessor conditional LINIOTARG_SUPPORT_EPOLL. If this is not defined, select is used. The select option has been left in place so we continue to have an example that can be ported to OS’s that do not support epoll.  Include files to allow SCL customers to generate Windows project files for different versions of Visual Studio.  Support for 64-bit versions of the .NET Protocol Components has been added.  Added configuration for the generation of diagnostic messages per sector, session, and channel to allow maximum flexibility. Maximum performance can be achieved by configuring each mask to zero so no diagnostic messages are generated, thus no messages would be forwarded to the .NET code. This also would allow a user to enable diagnostic messages on a single sector, session, etc. in a large configuration on a problem connection and avoid having to filter out diagnostic messages from all other connections.  Updated database wrapper header files and removed the /clr flag from the modbusManaged and utilsManaged projects.   v3.23   Added support for TLS v1.2 in target layer  Included support for 64 bit builds in the example project and solution files provided for Visual Studio  Improved management of Transaction IDs by Master when responses are severely delayed   v3.22   Improved performance of database interface and other library functions  Restructured directories and sample code for Windows® and Linux® target layer libraries  Added support for IPv6 in Linux target layer (Modbus TCP)  Expanded Windows target layer support for resolving IP address from destination host name  Updated Linux sample makefiles to generate debug information by default   v3.20   Allow compile time choice of using either standard or Microsoft “safe” string functions that include the length of the buffer to be written to (ie strcpy or strcpy_s)  Added more checking to discard improperly formatted messages   v3.21   Improved Linux makefiles and Windows® solution and project files   v3.20   Allow compile time choice of using either standard or Microsoft “safe” string functions that include the length of the buffer to be written to (ie strcpy or strcpy_s)  Added more checking to discard improperly formatted messages   v3.19 - allow use of either UNICODE or MBCS character set  v3.18 - Support for C99 standard types for 16 bit and 32 bit types  v3.16 - Corrected GNU-Wall compiler warnings  v3.09 - Modifications to conform to new Modbus certification tests.  v3.07 - Added sample Linux and Windows target implementation.  v3.01.01 - Added ability to keep time on a per-session basis.  v3.00.35 - Added memory alignment support.  v3.00.32 - Added support for multiple threads.  v3.00.25 - Added ability for Master to queue commands.                                                                              Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-scl-pages/outstation-library-features
                                                  Request a Quote          Source Code Library Video Playlist             IEC 60870-5 Source Code Libraries  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                   Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          Outstation Library Features   Interoperability is maximized by making it easy to attach virtually all possible interoperability configuration settings to run-time variables or function calls.  Supplies data to an unlimited number of host devices through an unlimited number of communication ports.  Supports Multiple Application Layer ASDU addresses (sectors). This virtual device capability can supply unique database profiles, or database profiles with common components, to host stations.  Database manager maps randomly organized Target Application data points (binaries, controls, integers,floats, etc.) into IEC 60870-5-101 ASDU type information objects.  Example Database Interface implementations are provided for testing, illustration, and as templates to be used for developing final Database Interface.  Fully supports ASDU types with CP24Time2a (24-bit time tag), including proper handling of clock synchronization commands, and through spontaneous clock synchronization responses upon hourly rollover.  Includes support for ASDU types with CP56Time2a (56-bit time tag). These include ASDU types 30 through 40 defined by the approved “Addendum to IEC 60870-5-101 Concerning the Extension of Time Tags.”  Spontaneous response data can be automatically generated by configurable scans of Target Application data.                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-scl-pages/master-library-features
                                                  Request a Quote          Source Code Library Video Playlist             IEC 60870-5 Source Code Libraries  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                   Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          Master Library Features    An unlimited number of remote devices can be configured on an unlimited number of communication ports,and new remote devices can be added at runtime.  Communicates with devices on the same communication port that have different ASDU field sizes (ASDU address, cause of transmission, and information address), or with devices on separate communication ports that have different link address sizes.  Database manager maps received IEC 60870-5 data objects into Target Application data points (binaries,controls, integers, floats, etc.).                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-scl-pages/ansi-c-source-code
                                                  Request a Quote          Source Code Library Video Playlist             IEC 60870-5 Source Code Libraries  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                   Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          ANSI C Source Code   Written in ANSI-Standard C Source Code, under a strict corporate coding standard.  Designed to be processor and operating system independent, using any ANSI-Standard C compiler.  Simple configuration for big-endian or little-endian byte order.  Can be used with or without a Real Time Operating System (RTOS).  Database interface supports any database, ranging from direct I/O input with no storage to complex, relational databases.  Includes sample applications and source code for Low-Level Target Interface for Linux and Windows (see Design Details for Implementation).  Typical product integration times are less than three weeks.                                                                                Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-scl-pages/dotnet-components
                                                  Request a Quote          Source Code Library Video Playlist           Download Evaluation            IEC 60870-5 Source Code Libraries  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                   Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          .NET Components   Based on Triangle MicroWorks, Inc. industry-proven Source Code Library design.  Supports all .NET Languages (C#, J#, Managed C++, VB .NET, etc.) and tools.  Compatible with .NET 2.0 Framework.  Integrates with Visual Studio Help.  Source code version includes corresponding ANSI-Standard C Source Code Library.  Includes built-in simple database with save/restore capabilities; also supports user-defined database.  Ideal for quick development of products and tools requiring IEC 60870-5 support.  Scalable for large implementations.  Typical product integration time of less than one week.                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/source-code-libraries/iec-60870-scl-pages/what's-new
                                                  Request a Quote          Source Code Library Video Playlist             IEC 60870-5 Source Code Libraries  For almost 30 years, Triangle has provided the diagnostic tools and protocol expertise to  our customers to support all their conformance  & certification needs. We will work with you to ensure that your implementation of our source code libraries will pass all industry related certification and conformance testing.                   Overview  Outstation Library Features  Master Library Features  Secure Authentication  ANSI C Source Code  .NET Components  What's New       Overview    Outstation Library Features    Master Library Features    Secure Authentication    ANSI C Source Code    .NET Components    What's New        Related Products           Communication Protocol Test Harness (TH)  Distributed Test Manager (DTM)          What's New Current Release: v3.32 Release Date:March 2024  Newest Features in this Release    Added TCP channel callback functions to retrieve IP addresses when the server is connected or disconnected.  Channels are no longer closed automatically when the last session on the channel is closed.   In WinIoTarg improvements to UDP/TCP receiving will avoid errors if the UDP broadcast message received in middle of the TCP octets.    In WinIoTarg, added a channel diagnostic mask bit to control whether diagnostics messages are generated.   In LinIoTarg improvements to use linked list instead of array for keep channels listening on a particular port. That improvement will allow unlimited channels be configured to one specific TCP port.   In WinIoTarg, if the TCP listening thread is not started due to the port being occupied, the channel is now closed more quickly (previously could take up to 5 seconds).       Key Additions to Recent Releases  v3.31    “Slave” has been replaced by “Outstation”, “Controlled Station”, or “Server” depending on protocol in documentation, filenames, variable names, and function names. An effort has been made to maintain interface backward compatibility where possible.  Minor changes to allow SCL to build with VS2019 and VS2022 without modification related to std::string and .NET Framework version.  Added support for OpenSSL Version 3.x.x. OpenSSL Version 1.1.1 is still supported, however according to openssl.org the End Of Life date for the 1.1.1 series is September 11 2023. Support for Version 1.0.2 has been removed.  In WinIoTarg and LinIoTarg add clientSourceIpPort to allow TCP Client Channels to specify the IP Port to use for originating TCP Connections.  Add support for TMWDIAG_ID_OCTETS bit to determine whether the actual Hex Octets from a transmitted or received message are formatted and/or displayed in a diagnostic message.  In S14 Simulated Database include MIT Sequence and Quality when a Freeze Command causes an event to be generated.   v3.30     In LinIoTarg speed up processing during TCP Channel closing.    Add support for multiple Polled Timers and disabling the Windows Target Layer multiple timers to allow application to run SCL in the context of its own threads.    Secure Authentication. Improve tmwcrypto.c support for multiple sessions when Simulated Crypto Database is being used during initial portation.    .NET Secure Authentication. Improve TMWCRYPTO_ASYMTESTING if compiled in so that sample asymmetric key files can be found.    M104 Improve diagnostics to display the time when a time sync is sent.    S101/104 Add feature to allow disabling of Type IDs at run time.    S104 Close TCP connection and Listen again when CRPNA command is received.    S104 When a StopDT is received do not queue any more spontaneous responses containing events until a StartDT is received. This allows better enforcing of event queue sizes and removing duplicate events from the queue if configured to do so.    .NET SCL. Add associated control point functionality ControlIOA Property to S14SimPoints. This allows more flexibility than sector database property ControlPointOffset. Points which do not have ControlIOA set still use the common offset. This per point functionality must be enabled per sector by setting ControlIOAEnabled to prevent unnecessary processing.     v3.29     In WinIoTarg added define to allow channel thread code to be compiled out.    Improved TLS example code and included sample TLS Certificates for ANSI C and .NET SCLs.    In .NET SCL when CRPNA (Reset Process) is received reset the sector to cause MEINA to be sent even when .NET event S14Database::UseSimControlDatabase is set to false.    Reduced default compiled in functionality for ANSI C and .NET IEC 60870-5 SCL. All functionality, must be carefully chosen, tested and then documented in the CIGuide (and PICS for Secure Authentication) specific to your device. The .NET SCL will now have the same default functionality compiled in as the ANSI C SCL. The file tmwprvt.h will no longer be provided to turn on additional .NET functionality.    v3.28    In WinIoTarg potentially speed up waiting for a channel thread to exit when a channel with polledMode=false is closed.  In LinIoTarg replaced deprecated stime() with clock_settime()  In LinIoTarg speed up new connection indication to protocol library when running with polledMode=true.  Support .NET txPhysCallbackEvent, rxPhysCallbackEvent, txApplCallbackEvent, and rxLinkCallbackEvent callback events in TMWChannel class for all protocols. Move .NET rxLinkHdrCallbackEvent and rxTransportCallbackEvent callback events out of TMWChannel class to DNPChannel since those two events only apply to DNP channels and were misleading.   Add M104 and M101 C Sample code and .NET Application sample to demonstrate how to use File Transfer in the libraries.    v3.27.1    Separate 104 and 101 file transfer .NET database event interface from rest of the database, so that the decision to implement these can be independent from implementing the rest of 104 and 101 database.  In LinIoTarg use localIpAddress when a TCP Client binds to the connect socket.  When S104 and S101 is configured to send clock sync events when the timestamp in an event has an hour different from the previous event sent, only do this if the timestamp being sent in this new event is 24bit timestamp, not when no timestamp or 56 bit timestamp (which already has the hour in it) is sent.  Add balancedRetryTimeout configuration parameter to allow delayed command responses sent faster when in balanced mode.  Add class1DataForClass2Poll configuration to enable/disable sending class 1 data in response to a class 2 poll if there is no class 2 data to send. That behavior is specified in 101 but some masters do not support the 101 spec completely.  Separate 101 file transfer .NET database event interface from rest of the database, so that the decision to implement these can be independent from implementing the rest of 101 database.   v3.26    Simplified Secure Authentication (SA) CSQ algorithm removing aggressiveModeTxSequence.    Removed compiled out and unfinished SA Remote Key Update code.   v3.25    Created an implementation guide for SCLs.  In addition, Smaplelo Targ has been created. This Sample target layer is a target layer example that can be complied and linked with the wintoolkit C example programs that has no OS dependencies. It can be used as a baseline target layer to port the library to another OS or RTOS. It can be built by specifying build = sample from the Linux command line.     Return the intermediate response statuses during command processing when ActTerm is ON.    Added configuration to M104 to resolve an issue with the Secure Authentication Specification where M104SA sends a CTSTA (Test With Time) as the first message before beginning aggressive mode. If the time on the master and slave is out of sync this command may fail. In m104 - initTestCTSNA allows CTSNA (Test Without Time) to be sent if supported instead.    The SCL has been updated to work with OpenSSL 1.1.x while retaining backward compatibility with version 1.0.x. For Windows builds a new define has been added, TMWCNFG_USE_OPENSSL_1_0_2 that must be defined when running with the legacy version.    The wintooltkit samples now include TLS support. The samples include certificates that are compatible with those used by the test harness and the default configurations for both Linux and Windows are set to use them. Only a single define USE_TLS which defaults to false needs to be updated to configure the sample to use TLS for both DNP and 101/4 Secure Authentication builds.    WinIoTarg now uses the Channel thread to check for received data eliminating the need for a separate CheckForInput thread/class.    Update Linux target layer to use consistent logging technique    LinIoTarg now creates a thread per channel (when not configured to run in polled mode). Once the channel has been opened, this thread will establish/listen for the channel connection and wait on the data socket for incoming data. This eliminates the latency when polling for received data. Also, locking has been reduced by eliminating the linked list of TCP Channels which required locking each time the list was accessed.    Updated linTCP_transmit to check the error code returned by send. If its EAGAIN indicating that the socket is busy, wait for the socket to become writable and retry the send instead of disconnecting immediately.   The Linux target layer has been updated to provide a basis for porting a target layer to another OS:  A tmwtargcnfg.h file has been added to both the Linux and Windows target layer. This configuration file defines which features a given target layer supports. This will allow customers porting the Linux target layer to other OS's to easily identify code that may be unnecessary for their application and remove it instead of wasting time porting it. Target layer features currently defined in this file are: 1. RS232 2. TCP 3. UDP (Only required for DNP) 4. TLS 5. EPOLL   v3.24     Made improvements to target layer timer examples.    Implemented event driven receive data in winiotarg so channels don’t have to be polled. This is also available in liniotarg.    Updated the Linux target layer, replaced LINIOCNFG_EVENT_DRIVEN conditional with polledMode configuration.    Support for epoll has been added to the Linux target layer. This support is controlled by the preprocessor conditional LINIOTARG_SUPPORT_EPOLL. If this is not defined, select is used. The select option has been left in place so we continue to have an example that can be ported to OS’s that do not support epoll.    Include files to allow SCL customers to generate Windows project files for different versions of Visual Studio.    Support for 64-bit versions of the .NET Protocol Components has been added.    Added configuration for the generation of diagnostic messages per sector, session, and channel to allow maximum flexibility. Maximum performance can be achieved by configuring each mask to zero so no diagnostic messages are generated, thus no messages would be forwarded to the .NET code. This also would allow a user to enable diagnostic messages on a single sector, session, etc. in a large configuration on a problem connection and avoid having to filter out diagnostic messages from all other connections.    Updated database wrapper header files and removed the /clr flag from the 870Managed and utilsManaged projects.    Updated simulated databases to use this diagnostic function, i14diag_securityStatisticString, instead of each maintaining its own copy.      v3.22    Improved performance of database interface and other library functions  Restructured directories and sample code for Windows® and Linux® target layer libraries  Expanded Windows target layer support for resolving IP address from destination host name  Updated Linux sample makefiles to generate debug information by default  101/104: Added multiple enhancements to Secure Authentication library and documentation  101/104: Updated IEC 60870-5 (101/104) Master CI Guide to better reflect capabilities of library  104: Added support for IPv6 in Linux target layer   v3.21    101/104: Added support for Secure Authentication for IEC 60870-5 (-101 and -104) according to the IEC 60870-5-7 standard based on the cyber-security standard IEC 62351 (Parts 3, 5, and 8)  101/102/103/104: Improved Linux makefiles and Windows solution and project files  101/104: Improved integration with OpenSSL library  101/104: Added call to the user callback function for master when ACT CON response is received in response to CICNA and CCINA requests  101/104: Added call to the user callback function for master when ACT TERM response is received from each sector in response to CICNA and CCINA broadcast requests, not just the final ACT TERM response  104: Improved handling of TCP connection loss for controlled station (outstation)   v3.20    101/104: Improved support for creating a redundant channel  101: Corrected an error in Delay Acquisition, where the time was calculated incorrectly  101/104: Fixed issue with The Double Transmission feature  101/104: Corrected an issue when using unique functions for cdc and crc status functions and using the automatic generation of 101/104 protocol events with COT Remote when a control is received  101: Added support in .NET library for case when user overrides control database but not monitored point database    v3.19    101/104: Modify code to allow use of either UNICODE or MBCS character set  101/102/104: Changed master 101/102/104 to ignore the COT "test bit". Master will indicate the COT test bit was set in the diagnostics messages, then mask off that bit and process the COT normally  101/104: Improved Master 101/104 to better enforce rules for responses to Select requests  104: Add configuration for time tagged requests with time in the future to be allowed.  104: Validate time tagged requests with time in the future not just expired time and expand  maximum allowed “age” in the past or in the future of allowed commands.  101: Added feature to allow controlled station to support multiple requests of the same Type Id to be processed simultaneously    v3.18    Added more checking to discard improperly formatted messages  Allow compile time choice of using either standard or Microsoft “safe” string functions that include the length of the buffer to be written to (ie strcpy or strcpy_s)  Only generate cyclic and background data when online. This was done to improve cyclic behavior when the connection is lost  101 and 104: Enhanced examples to show the different addEvent possibilities and use the sector time instead of the system time  104: Added validating state for conformance testing - disconnect if I or S Frame is received when link is stopped    for IEC 60870-5-101: v3.12 - Added support for Peer (dual mode) session with the same address and support for Double Transmission.
v3.07 - Added sample Linux and Window target implementation.
v3.01.01 - Added ability to keep time on a per-session basis.
v3.00.45 - Added support for Gasunie PIDs.
v3.00.38 - Added ability to display Structured IOAs.  for IEC 60870-5-102: v3.07 - Added sample Linux and Window target implementation. v3.01.01 - Added ability to keep time on a per-session basis. v3.00.25 - Added ability for Master to queue commands.   for IEC 60870-5-103: v3.07 - Added sample Linux and Window target implementation. v3.01.01 - Added ability to keep time on a per-session basis. v3.00.45 - Added support for Areva private ASDUs and for Siemens SIPROTEC private ASDU 205. v3.00.29 - Enhanced support for multiple threads. v3.00.25 - Added ability for Master to queue commands. v3.00.22 - Added support for ASDU 124 Procomoe specification.   for IEC 60870-5-104: v3.12 - Added support for Double Transmission. v3.07 - Added sample Linux and Window target implementation. v3.01.01 - Added ability to keep time on a per-session basis. v3.00.45 - Added support for Gasunie PIDs. v3.00.38 - Added ability to display Structured IOAs. v3.00.35 - Added memory alignment support. v3.00.32 - Added support for multiple threads. v3.00.25 - Added ability for Master to queue commands.                                                                                  Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/features
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway            Features Built Into Iron           Built on Triangle MicroWorks IEC 60870-6 (TASE.2 / ICCP) Stack           Iron was implemented using our IEC 60870-6 Stack , which provides lots of built-in functionality:       Implementation and Configuration of entire seven layer stack  Support for Blocks 1, 2, and 5, including DataSets, DataSet TransferSets, and SBO Controls  Support and Configuration of MMS parameters  Supports both ACSE password security and MMS/TLS certificate based security  Configuration of RFC 1006 layer parameters       For a full list of IEC 60870-6 features, see IEC 60870-6 Services .               Creating/Deleting DataSets           Iron makes it easy to build, create, and delete both persistent and dynamic DataSets.               Control Models           All TASE.2 / ICCP controls are supported, including select before operate in both the client and server.                                                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/data-simulation
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway          Data Simulation Iron allows you to simulate data changes in the Server Data Model either manually or automatically:    Manual - change object data values via the user interface with intelligent editing  Automatic - Change data values automatically with the push of a button    Manual Data Simulation    Easily set  Data Attribute  values directly in the Server Data Model Tree.            Example:  Select the Data Attribute            Then set the value              Automatic Data Simulation Iron allows you to simulate data changes in the Server Data Model. Start changing data values automatically with the push of a button. Values are incremented by Step between Simulation Min and Simulation Max. Use the tree to display different sections of the object model and select data values to be changed automatically. Manual data changes are supported as well in the server.                                                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/iec-60870-6-services
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway           IEC 60870-6 Services           60870-6 Services Built In           Because Iron was designed with our IEC 60870-6 Stack , all of these services are built into the tool:               Read services  Fully Supported    Read at any level in the hierarchy  Return configurable errors when a DataAttribute is read                Write services  Fully Supported    Write at any level in the hierarchy  Return configurable errors when a DataAttribute is written                Reporting Services  Fully Supported    DataSet Transfer Sets  Any Valid Data Set can be used in a Transfer Set  Supports all trigger options  All optional items included in report                Discovery  Fully Supported    Supports Server Device Discovery by Client  Server Object Model and Supported Services                Control Models  Fully Supported    All Control Types are supported  Select Before Operate  Checkback Name                Dynamic DataSets  Fully Supported    Persistant  Non-Persistant                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/working-with-transfer-sets
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway           Working with Transfer Sets  Iron allows you to easily create, delete, enable and view DataSet Transfer Sets:   Add  DataSet Transfer Set in a Server         Get Next Available  DataSet Transfer Set from a Client         Associate a DataSet with the  DataSet Transfer Set          Enable the  DataSet Transfer Set to begin receiving Reports        View Reports                                                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/model-support
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway           Model Support  Iron allows for the configuration of the model by reading .csv files or .xml files in both clients and servers.  Edit Model for any DataSet:   Model editor can load .csv or .xml files:                                                                                     Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/real-time-data-monitor
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway          Real Time Data Monitor Iron allows a Client to poll any Data Member in the Data Model:   Poll any part of the Data Model    Specify the polling period     View polled nodes in Data Monitor Window                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/report-monitoring
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway          Report Monitoring  Iron displays reports sent from the server individually to allow for easy data verification.                                                                                     Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/what's-new
                                                  Request a Quote          Iron Video Playlist           Download Evaluation              Iron - IEC 60870-6 (TASE.2/ICCP) Test Tool  Iron is a Windows™ application and diagnostic tool that simulates both bidirectional (peer to peer) and unidirectional test applications which fully implement blocks 1, 2, and 5 of the IEC 60870-6 ICCP (TASE.2) protocol. Iron fully simulates blocks 1, 2, 5 for both clients and servers as well as supports versions 1996.8 and 2000.8. It is easily configured, supports both XML and CSV model files, supports building client and server models via GUI, supports discovery, and has multiple simulation options for changing server side data.  Support for Read, Write, Reporting, Discovery, Controls, and Dynamic DataSets are built into the tool.                   Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Distributed Test Manager (DTM)  IEC 60870-6 (TASE.2/ICCP) Source Code Library  SCADA Data Gateway          What's New Current Release: v4.5 Release Date:  October 2023  Newest Features in this Release   Upgrade to Microsoft 2019 runtimes  Upgrade to openssl 3.0   Key Additions to Recent Releases  v4.4     Add capability to dynamically modify datasets    Improve dataset creation (new dataset create/modify dialog window)   v4.3    Added new functionality to dynamically configure points and domains in a client data model (in the same way that a server data model can be configured)    Added capability for client to send write requests to non-controllable points for performing negative tests   v4.2    Added Simulation editing capability  Added Persistence of DSTransferSet settings  Added capability to modify Domain names in the ICCP Server  Added capability of setting time and date of Server Time Tags   v4.1   Add four new data types which support time in milliseconds: - Tase2DiscreteQTimeTagExtended - Tase2DiscreteExtended - Tase2StateQTimeTagExtended - Tase2RealQTimeTagExtended   Improved user interface  Added support for polling persistence and dataset polling  Added support for DSTransferSet persistence and restoration    v4.0.3   Improved Bi-Directional support  Added reconnect capability  Added Tase2 Application Tree to display all Tase2 applications                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/iron-pages/system-requirements
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               System Requirements                 Overview  Features  Data Simulation  IEC 60870-6 Services  Working with Transfer Sets  Model Support  Real Time Data Monitor  Report Monitoring  What's New  System Requirements       Overview    Features    Data Simulation    IEC 60870-6 Services    Working with Transfer Sets    Model Support    Real Time Data Monitor    Report Monitoring    What's New    System Requirements        Related Products           Test Suite Lite  SCL Navigator  Distributed Test Manager (DTM)          Iron supports the following versions of Windows TM :   Windows Server 2012 (64-bit)  Windows 10 (32-bit and 64-bit)  Windows Server 2016 (64-bit)  Windows Server 2019 (64-bit)   Note:  Iron supports virtual machines (VM)  Minimum system requirements:   1.5 GHz processor; recommended: 2.4 GHz multi-core processor  2GB RAM; recommended: 4GB RAM  Windows 7 or higher (not Embedded Compact)  Microsoft .NET 4.6.2 Framework                                                                                  Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dnp3-forge-pages/dnp3-parameters
                                                  Request a Quote          DNP3 Forge Video Playlist          Download Evaluation              DNP3 Forge The DNP3 Forge supports the creation and validation of DNP3 XML Device Profile documents.                   Overview  DNP3 Parameters  Device Profile Document  Multiple Devices  Validation and Test Scripts  What's New  System Requirements       Overview    DNP3 Parameters    Device Profile Document    Multiple Devices    Validation and Test Scripts    What's New    System Requirements        Related Products          Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)         DNP3 Parameters   Customized editor for each DNP3 Device Profile parameter.  Allows user to specify capabilities, current value, and/or methods that can be used to set each parameter on the device.  Editors are designed to enforce consistency and the creation of valid DNP3 XML Device Profiles.  Displays a detailed description and additional pertinent information for selected parameters.                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dnp3-forge-pages/device-profile-document
                                                  Request a Quote          DNP3 Forge Video Playlist          Download Evaluation              DNP3 Forge The DNP3 Forge supports the creation and validation of DNP3 XML Device Profile documents.                   Overview  DNP3 Parameters  Device Profile Document  Multiple Devices  Validation and Test Scripts  What's New  System Requirements       Overview    DNP3 Parameters    Device Profile Document    Multiple Devices    Validation and Test Scripts    What's New    System Requirements        Related Products          Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)         Device Profile Document   Display a DNP3 XML Device Profile in the standard format.  Can be used to print/distribute human readable versions in the standard format.  Displays the correct document based on the schema version of the document being viewed.  Import/Export Point Lists from/to CSV (Comma Separated Value) file format.                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dnp3-forge-pages/multiple-devices
                                                  Request a Quote          DNP3 Forge Video Playlist          Download Evaluation              DNP3 Forge The DNP3 Forge supports the creation and validation of DNP3 XML Device Profile documents.                   Overview  DNP3 Parameters  Device Profile Document  Multiple Devices  Validation and Test Scripts  What's New  System Requirements       Overview    DNP3 Parameters    Device Profile Document    Multiple Devices    Validation and Test Scripts    What's New    System Requirements        Related Products          Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)         Multiple Devices   Easily load multiple Device Profile Documents.  Layout the display to best meet the information.  Allows the comparison of two device profiles.   Determine parameters  that have been changed (i.e. are not the same).  Determine  compatibility between two devices (i.e. display parameter options that overlay).                                                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dnp3-forge-pages/validation-and-test-scripts
                                                  Request a Quote          DNP3 Forge Video Playlist          Download Evaluation              DNP3 Forge The DNP3 Forge supports the creation and validation of DNP3 XML Device Profile documents.                   Overview  DNP3 Parameters  Device Profile Document  Multiple Devices  Validation and Test Scripts  What's New  System Requirements       Overview    DNP3 Parameters    Device Profile Document    Multiple Devices    Validation and Test Scripts    What's New    System Requirements        Related Products          Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)         Validation and Test Scripts   Includes a scripting engine (Python) that provides access to all open DNP3 XML Device Profiles.  Includes a variety of built in test scripts as well as allowing the user to generate their own custom scripts.  Compare supported DNP Data Types against another DNP3 XML Device Profile or DNP Level 1, 2, 3, 4 requirements.  Use Scripts to validate consistency of related fields and create validation tests to meet requirements for a specific utility, region, or country.                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dnp3-forge-pages/what's-new
                                                  Request a Quote          DNP3 Forge Video Playlist          Download Evaluation              DNP3 Forge The DNP3 Forge supports the creation and validation of DNP3 XML Device Profile documents.                   Overview  DNP3 Parameters  Device Profile Document  Multiple Devices  Validation and Test Scripts  What's New  System Requirements       Overview    DNP3 Parameters    Device Profile Document    Multiple Devices    Validation and Test Scripts    What's New    System Requirements        Related Products          Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)         What's New Current Release: v1.4 Release Date:  November 2016  Newest Features in this Release    Aded IEC 61850 to DNP3 mapping based on IEEE 1815.1 standard including writing to XML Device Profile  Added the ability to save Device Profiles as PDF files  Expanded support for creating “current value only” Device Profiles  Several improvements for editing Device Profile parameters       Key Additions to Recent Releases v1.3 - Support for version 2.09 (Nov 2013) and 2.10 (Nov 2014)  Device Profile Specifications  DNP3 Device Profile validation DNP3 Device Profile Comparison Tool Added ability to create Binary Configuration Files for use with TMW DNP3 Source Code Library based on content of the device profile Improved handling of empty/unused device profile sections                                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/dnp3-forge-pages/system-requirements
                                                  Request a Quote          61850 Test Suite Pro Video Playlist          Download Evaluation               System Requirements                 Overview  DNP3 Parameters  Device Profile Document  Multiple Devices  Validation and Test Scripts  What's New  System Requirements       Overview    DNP3 Parameters    Device Profile Document    Multiple Devices    Validation and Test Scripts    What's New    System Requirements        Related Products           Test Suite Lite  SCL Navigator  Distributed Test Manager (DTM)          DNP3 Forge supports the following versions of Windows TM :   Windows Server 2012 (64-bit)  Windows 10 (32-bit and 64-bit)  Windows Server 2016 (64-bit)   Windows Server 2019 (64-bit)    Note:  DNP3 Forge supports virtual machines (VM)  Minimum system requirements:   1.5 GHz processor; recommended: 2.4 GHz multi-core processor  2GB RAM; recommended: 4GB RAM  Windows 7 or higher (not Embedded Compact)  Microsoft .NET 4.6.2 Framework                                                                                    Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/helinks-pages/helinks-information-request
                                                       Helinks STS (IEC 61850 Configuration Manager) Information Request    Please complete the form below and we will contact you shortly.  If you prefer, you may email us directly at sales@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Your contact information will not be sold or used outside of our organization as explained in our privacy policy .  Contact Information    Your Contact Information       First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Phone:   Click to add            (?)                        Your Company    Company:   Click to add    *Required          (?)                Street:   Click to add            (?)                City:   Click to add    *Required          (?)                State:   Click to add            (?)                Country:   Click to add    *Required          (?)                Zip:   Click to add            (?)                Web Site URL:   Click to add            (?)                 How did you hear about us?   Please Select  Search Engine  Social Media  Referral  E-Newsletter  Other     If other, please tell us how:   Click to add            (?)              Additional Information  Please provide any specific questions or areas of interest below.          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.                                                                             Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/testing-and-configuration-tools/garibaldi-pages/garibaldi-information-request
                                                       Garibaldi (IEC 61850 Security Manager) Information Request    Please complete the form below and we will contact you shortly.  If you prefer, you may email us directly at sales@TriangleMicroWorks.com . To help us respond to your request more efficiently, please include all of the information requested below.  Your contact information will not be sold or used outside of our organization as explained in our privacy policy .  Contact Information    Your Contact Information       First Name:   Click to add    *Required          (?)                Last Name:   Click to add    *Required          (?)                Email:   Click to add    *Required          (?)                Phone:   Click to add            (?)                        Your Company    Company:   Click to add    *Required          (?)                Street:   Click to add            (?)                City:   Click to add    *Required          (?)                State:   Click to add            (?)                Country:   Click to add    *Required          (?)                Zip:   Click to add            (?)                Web Site URL:   Click to add            (?)                 How did you hear about us?   Please Select  Search Engine  Social Media  Referral  E-Newsletter  Other     If other, please tell us how:   Click to add            (?)              Additional Information  Please provide any specific questions or areas of interest below.          RadEditor - HTML WYSIWYG Editor. MS Word-like content editing experience thanks to a rich set of formatting tools, dropdowns, dialogs, system modules and built-in spell-check.   RadEditor's components - toolbar, content area, modes and modules          Toolbar's wrapper           16px               Content area wrapper RadEditor hidden textarea Your browser does not support inline frames or is currently configured not to display inline frames.   RadEditor's bottom area: Design, Html and Preview modes, Statistics module and resize handle.  It contains RadEditor's Modes/views (HTML, Design and Preview), Statistics and Resizer   Editor Mode buttons Statistics module Editor resizer            RadEditor - please enable JavaScript to use the rich text editor.    RadEditor's Modules - special tools used to provide extra information such as Tag Inspector, Real Time HTML Viewer, Tag Properties and other.             Would you like a 30-day demo license?  Yes No                                                                   Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/supported-protocols/wind-integration-test
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway                    ICCP TASE.2-test  Wind Integration-test       ICCP TASE.2-test    Wind Integration-test        Related Products           Test Suite Pro (TSP)  Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)  Iron (60870-6/ICCP/TASE.2 Test Tool)            Wind Integration with IEC 61400-25 The SCADA Data Gateway supports the integration of wind power plants through the international standard IEC 61400-25. This allows monitoring and control of wind resources through standard communication protocols.   Single Solution -  translate directly between SCADA protocols like DNP3, IEC 60870-5 (IEC 101, IEC 104), IEC 61850, or Modbus with better reliability rather than using multiple OPC drivers   Protocol Mapping - support for monitoring and control point mappings as defined by IEC 61400-25-4 including protocols  DNP3, IEC 60870-5-104, and IEC 61850   IEC 61850 Data Models - support for wind data models including common data classes, data objects, and logical nodes   Large Number of Points - Up to 30,000 points and 250 connections can be supported    Security - Authentication and TLS encryption is supported for IEC 61850, DNP3, and IEC 60870-5-104                 Customer Case Study  Learn how Triangle MicroWorks helped Senvion integrate large scale wind farms into the grid using communication protocols based on the IEC 614000-25 standard. Benefits of this approach include:   Increased reliability  Greater interoperability  Reduced project time    Download case study                                                                                       Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/products/scada-data-gateway/supported-protocols/iccp-tase-2-test
                                                  Request a Quote          SDG Video Playlist          Download Evaluation              SCADA Data Gateway                    ICCP TASE.2-test  Wind Integration-test       ICCP TASE.2-test    Wind Integration-test        Related Products           Test Suite Pro (TSP)  Distributed Test Manager (DTM)  Communication Protocol Test Harness (TH)  Iron (60870-6/ICCP/TASE.2 Test Tool)          Support for ICCP / TASE.2  Single Solution - With the SCADA Data Gateway, you can translate an ICCP interface directly to other SCADA protocols like DNP3, IEC 60870-5 (IEC 101, IEC 104), IEC 61850, or Modbus. There is no need to have two separate OPC drivers (one for ICCP and one for the other SCADA protocol).   Software Only Solution - The SCADA Data Gateway can be run on any Windows PC. This allows the translation to ICCP to be performed on either a SCADA server or an embedded PC in the field.   Map ICCP Data and Controls - The SCADA Data Gateway supports mapping ICCP points and controls directly to other SCADA protocols like DNP3, IEC 101, IEC 104, IEC 61850, or Modbus.   Large Number of Points - High server point count and 250 connections can be supported.   Millisecond Time Stamps - Millisecond time stamps are supported for extended ICCP data types.   Security - Authentication and TLS encryption is supported for ICCP.                                                                                      Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/ucaiug
                                               Migration Strategies for using IEC 61850 Edition 2 Amendment 1 (Ed 2.1) UCA International User Group (UCAIug) Supporting Users   Free Webinar - Watch Replay Now! Previously held on Thursday, July 8, 2021 - 10 AM - 11AM EDT    Over the years, IEC 61850 started out with Edition 1 and has continued to evolve and improve with updates including  Edition 2 and 2.1 (Edition 2 Amendment 1). Ed 2.1 is the latest release and as a result the UCAIug IEC 61850 Test Procedure Working Group (TPWG) has been actively developing IEC 61850 Edition 2.1 conformance test procedures and new policies for Ed 2.0 & 2.1 conformance testing.  With updated conformance test procedures and timeframes, many new questions are being asked. What are the key drivers to motivate utilities to upgrade to Ed 2.1? What are the issues related to migrate a substation automation system towards Edition 2.1?  The UCAIug has published a white paper documenting best practices and new policies to rollout Ed2.1. The goal of this webinar is review highlights of the white paper, provide the IEC vs User perspective and review of testing and migration strategies for Ed 2.1.    What you'll learn:    Why use Ed 2.1 and highlight several improvements & benefits from Amendment 1, such as:    Ed 2 technical issues are addressed  Improvements to support Cyber Security  Support for both backward & forward compatibility for future proofing  Routable GOOSE & Sampled Values  Introduction of new logical nodes, i.e. DER  More widely applicable Sampled Values usage with IEC 61869-9 including DC support    The UCA International User Group (UCAIug) perspective and how they are helping users, specifically:  New releases & conformance testing policies  Resources for Ed 2.1 certification    DNV will review what is being tested in Ed 2.0 vs Ed 2.1 along with timing of Ed 2.1 certification.  Triangle MicroWorks, Inc. will review recommended vs non-recommended migration strategies.  Red Electric de Espana will discuss their motivation & strategy to migrate to Ed 2.1, and their real-world experience along with common issues they faced.                 Instructors:   Herb Falk  Vice President of Testing UCAIug    Mr. Falk has over 40 years of experience working in the automation, information exchange, standardization activities, and systems integration.  Previous employment was Westinghouse Numa-Logic and SISCO where he was responsible for 61850 and CIM integration projects and products.  Mr. Falk has been involved with IEC 61850 technology since 1982 and cyber security since 1993.  Mr. Falk is an editor of IEC 61850-8-1, editor of several cyber security standards, and is the Vice President of Testing for the UCA International Users Group (UCAIug) . His work for the UCAIug has involved the coordination of IEC 61850 interoperability testing in 2011, 2013, 2015, and 2017.    Richard Schimmel Verification Manager for Protocol test lab DNV Energy Systems - Arnhem, The Netherlands   Richard has over 30 year experience at DNV with 20 of those years working with IEC 61850. He also has extensive experience work with IEC 60870-6, ICCP/TASE.2 and other standards. Richard works with UCAIug as the main editor for conformance test procedures for server, client, GOOSE performance, ICT & SCT tools. He is also an editor and member of IEC TC57 WG10/WG17.   Joel Greene Director, IEC 61850 Development Triangle MicroWorks, Inc.   Joel has worked with communication protocols for over 15 years with the last 6 years in the power industry. He is the lead engineer for IEC 61850 development & products at Triangle MicroWorks and is heavily involved with 61850 industry groups. He also works as the co-editor of IEC 61850 7-2. He is a member of the UCA Test Committee and Test Procedures Working Group and involved in IEEE Standard Association.   Carlos Rodriguez del Castillo Project Manager of EPICS Project (Centralized Protection &Control System) Elewit, Red Electrica Group  Carlos Rodríguez del Castillo received his Telecommunication Engineering degree from the Universidad de Ingenieros de Sevilla in 1997. After graduation Carlos moved to Madrid and joined ELIOP, S.A. in 1998 where he worked for seven years in SCADA development projects. Carlos has been involved in the design and implementation of multiple SCADA communication system solutions with legacy protocols, and has a field experience in working with standards. In 2005 Carlos joined Red Eléctrica de España as substation engineer focused in the substation automation system. He has been working with IEC 61850 since 2008 making it a reality in Red Eléctrica. In September 2019, Carlos moved to Elewit, the Innovation company of Red Eléctrica, as Project manager of EPICS Project (Centralized Protection and Control System) and he is also working as IoT partner in the cycle Innovation department of Elewit. He is member of IEC TC57 WG10 since 2010.          Check out and view recordings of our previous webinars!                                                               Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/training/previous-webinars/troubleshooting---conformance-testing-with-dnp3
                                               Troubleshooting & Conformance Testing with DNP3  Beginner and Advanced Topics   Free Webinar - Sign up now!    Since its introduction in 1993, Distributed Network Protocol (DNP3) has become a powerful standardized communication protocol that provides significant advantages for SCADA operations to communicate with remote devices over a large distance. DNP3 has achieved widespread acceptance and has been implemented across many industry applications, particularly electrical and water utilities. As a result, there is an ongoing need to monitor and resolve DNP3 communications issues in the field effectively and quickly.  Problem:  For device vendors, SCADA communication testing is difficult and time consuming.  Interoperability problems in the field cause serious support issues. For Utilities, connection problems and configuration issues slow down device integration times and create downtime.  Solution:  Having the right tools can simplify your testing & troubleshooting while improving efficiency & productivity by allowing you to prevent many issues that would typically occur in the field. Communication Protocol Test Harness is a powerful tool for monitoring and troubleshooting all your DNP3 testing and activities. This webinar is designed to review the common use cases when working with DNP3 ranging from troubleshooting connection vs protocol issues to preparing for conformance testing.    What you’ll learn:   How to troubleshoot & resolve common connection problems  Identify & resolve configuration issues using Test Harness and Wireshark    How to troubleshoot & resolve common protocol issues  Learn to monitor communications with Test Harness or Wireshark  Learn how to connect and configure a compliant Master or Outstation    How to generate messaging traffic for Event & Unsolicited configurations  How to prepare for conformance testing with the official Conformance Test Procedures published by the DNP Technical Committee  Watch how Tcl/Tog scripting and C# plugins can be used to develop full custom tests               Instructors:   Matt Green  Senior Applications Engineer Triangle MicroWorks, Inc.    Matt is a senior applications engineer here at Traingle MicroWorks. He’s been with Triangle for about 6 years and specializes QA testing with all our simulation tools and gateway. Matt is also one of our lead product trainers and presenter for webinars and customer demos.  Who should attend this course:    Protection & Control Engineers/ Managers  System Integrators  Software Engineers/Managers        Check out and view recordings of our previous webinars:      Introducing SDG v5.1  Easy Button for IEC 61850 Security  IEC 61850 Tools and Techniques  Strategies for Upgrading to IEC 61850 Edition 2.1  DER Solutions  UCA International User Group (UCAIug) Supporting Users  What is a Data Gateway in the SCADA world?  A Holistic Approach to Industrial Cybersecurity  Debugging GOOSE Streamline Troubleshooting & System Validation  Simplifying Secure Routable GOOSE & Sampled Values  A Holistic Approach to Facilitate the IEC 61850 Process - Part 1  A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2  Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning  Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC  Charting A Course to a Digital Substation  Automated Testing  IEC 61850 Substation Design & Testing  SCADA Data Gateway Tips & Techniques  IEC 61850 Common Vision  Virtual Isolation & Testing with IEC 61850  Virtual Isolation  Troubleshooting & Conformance Testing with DNP3  Securing the Grid       Introducing SDG v5.1    Easy Button for IEC 61850 Security    IEC 61850 Tools and Techniques    Strategies for Upgrading to IEC 61850 Edition 2.1    DER Solutions    UCA International User Group (UCAIug) Supporting Users    What is a Data Gateway in the SCADA world?    A Holistic Approach to Industrial Cybersecurity    Debugging GOOSE Streamline Troubleshooting & System Validation    Simplifying Secure Routable GOOSE & Sampled Values    A Holistic Approach to Facilitate the IEC 61850 Process - Part 1    A Holistic Approach to Facilitate the IEC 61850 Engineering Process Webinar Series Part 2    Reduce Travel & Increase Productivity with SCADA Simulation Tools for Substation Maintenance & Commissioning    Automate SCADA Testing of your Gateway, Data Concentrator, or RTAC    Charting A Course to a Digital Substation    Automated Testing    IEC 61850 Substation Design & Testing    SCADA Data Gateway Tips & Techniques    IEC 61850 Common Vision    Virtual Isolation & Testing with IEC 61850    Virtual Isolation    Troubleshooting & Conformance Testing with DNP3    Securing the Grid                                                        Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: https://www.trianglemicroworks.com/SerialMonitorCablePage
                                               Cable Features          LEDS give a quick visual indication of signal status.    Use of two serial ports allows reliable capture of data in both directions.    The cable assembly includes a USB to dual RS232 serial port adapter.                                                              Quick Links  Contact Us:    Home  Products  Videos  Training  Support  Reference  About Us     Triangle MicroWorks, Inc. Sales: +1 919.870.5101 Support: +1 919.781.1931 Fax: +1 919.870.6692  sales@TriangleMicroWorks.com  support@TriangleMicroWorks.com         Copyright © 2013-2024 Triangle MicroWorks, Inc. All Rights Reserved                            The requested content cannot be loaded. Please try again later.
URL: http://www.trianglemicroworks.com/help/6tsp
      Getting Started  Intro to 61850 Test Suite Pro Connect to TMW Example Servers Connect to DTM Substation Demo Connect to DTM Virtual Isolation Demo Connect to or Simulate your IEDs Workspaces Tools (on the left)  Displays (on the right)  Use Cases  Configuration  Frequently Asked Questions     Skip To Main Content    Account  Settings  Logout             Getting Started » Tools (on the left) » Displays (on the right) » Use Cases » Configuration » Frequently Asked Questions  Account  Settings  Logout          Filter:       All Files        Submit Search       Account  Settings  Logout             Getting Started  Intro to 61850 Test Suite Pro Connect to TMW Example Servers Connect to DTM Substation Demo Connect to DTM Virtual Isolation Demo Connect to or Simulate your IEDs Workspaces Tools (on the left)  Displays (on the right)  Use Cases  Configuration  Frequently Asked Questions             You are here:  Getting Started > Intro to 61850 Test Suite Pro    Support Home            Intro to 61850 Test Suite Pro     Test Suite Pro "How To" Videos      There are 3 approaches to getting started with Test Suite Pro:         Connect to TMW Example Servers   Connect to DTM IEC 61850 Demos  Connect to or Simulate your IEDs       Load a predefined workspace to monitor and troubleshoot the simple example servers shipped with TSP.   Load more complex TSP workspaces with Signal Flow, Logic Analyzer and Test sequences designed to work with simulations defined in DTM workspaces with the same name.  Substation Demo   Virtual Isolation Demo    Load your SCD file or Discover IEDs by IP Address.                 Tools are on the left       Displays are on the right                    You are here:  Getting Started > Intro to 61850 Test Suite Pro                      Sales: +1 919.870.5101   sales@trianglemicroworks.com   Learn more about 61850 Test Suite Pro      Support: +1 919.781.1931   support@trianglemicroworks.com   Submit a Support Request Online     Triangle MicroWorks, Inc.          © 2021 Triangle MicroWorks, Inc. All rights reserved. All rights reserved.          


************TSP Manual**************
 Triangle MicroWorks, Inc. 61850 Test Suite Pro User Guide Version 4.6.0 March 2021 Copyright and Proprietary Information Property of Triangle MicroWorks, Inc. This Documentation and its associated distribution media contain proprietary information of Triangle MicroWorks, Inc. and may not be copied or distributed in any form without the written permission of Triangle MicroWorks, Inc. 2021 Triangle MicroWorks, Inc. All rights reserved. License Agreement and Warranty READ THIS BEFORE INSTALLING THE SOFTWARE AND BEFORE ACCEPTING THE LICENSE AGREEMENT THIS SOFTWARE LICENSE AGREEMENT IS BEING PROVIDED TO YOU AS PART OF A TRIAL FOR REVIEW BEFORE PURCHASE. THE FOLLOWING LICENSE AGREEMENT IS A CLICKTHROUGH LICENSE AGREEMENT THAT IS SHOWN DURING THE INSTALLATION OF THE SOFTWARE. YOU MUST AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT IN ORDER TO INSTALL THE SOFTWARE. THEREFORE, IF YOU DO NOT AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT, DO NOT INSTALL THE SOFTWARE. TRIANGLE MICROWORKS, INC. License Agreement and Warranty THIS IS A LEGAL AGREEMENT BETWEEN YOU (YOU) AND TRIANGLE MICROWORKS, INC. (TRIANGLE MICROWORKS). TRIANGLE MICROWORKS IS WILLING TO LICENSE THE TRIANGLE MICROWORKS SOFTWARE AND RELATED DOCUMENTATION PURCHASED BY YOU AND KNOWN AS WINDOWS CLIENT TESTER (THE SOFTWARE) TO YOU ONLY UPON THE CONDITION THAT YOU ACCEPT ALL OF THE TERMS CONTAINED IN THIS LICENSE AGREEMENT (THE "AGREEMENT"). PLEASE READ THE TERMS AND CONDITIONS CAREFULLY BEFORE CLICKING ON THE I ACCEPT THE LICENSE AGREEMENT BUTTON, AS CLICKING ON THIS BUTTON WILL INDICATE YOUR ASSENT TO THEM. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, THEN CLICK ON THE CANCEL BUTTON. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, TRIANGLE MICROWORKS IS UNWILLING TO LICENSE THE SOFTWARE TO YOU. Subject to the following terms and conditions, Triangle MicroWorks grants to you a nonexclusive, nontransferable limited license to install and use the Software solely for your internal business purposes and subject to the limitations in this Agreement. There are three mechanisms to license the Software. The Seat License is a PC based software license that is installed on a single com- puter and is solely for the use of the Software by you on a single computer at any one time. The Hardware Dongle License is a license that is stored on a dedicated USB hardware device that can be shared between computers by physically moving the Hard- ware Dongle License. The Hardware Dongle License is solely for the use of the Software by you on a single computer at any one time. The Network License is a license that is stored on a computer that can be shared across a local area network. The Network License is solely for the use of the Software by you on a single computer at any one time. The Network License may be installed on a corporate network and shared locally. The Network License cannot be shared outside of your company. You may physically trans- fer or copy the Software on to another computer; provided, however that each license instance may not be used on more than one computer or device at any given time. Subject to the foregoing limitation, you are entitled to copy the Software into any machine-readable or printed form for backup purposes in support of the use of the Software on a single computer by you. Any copy of the Software permitted by the preceding provision shall include a copyright notice of Triangle MicroWorks. You may use the Software solely for the applicable term (i.e., period of time) as set forth below. If you have licensed the Software for a free trial period (a Trial License), the term of the Software license will be for 21 days. You understand and acknowledge that after the 21-day period and unless otherwise extended by Triangle MicroWorks, in its sole dis- cretion, the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software. At the conclusion of the Trial License, you may pur- chase a Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have licensed the Software for a fixed period of time and not on a trial basis (a Term License), the term of the Software license will be the period of time agreed to by you and Triangle MicroWorks. At the end of that period of time, your use of the Soft- ware will terminate, and the Software will automatically deactivate and cease to operate. You agree that you are solely respons- ible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software except as explicitly set forth herein. At the conclusion of the Term License, you may purchase an additional Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have paid a one-time fee for a perpetual license, then your term of the Software License is perpetual (a Perpetual License) and subject to termination only as set forth herein. You may not transfer the Software to another entity or person. You shall not modify, translate, reverse assemble or reverse com- pile in whole or in part the Software. You shall not rent, sell, time-share, lease, sublicense, transfer, copy, disclose, display or oth- erwise make available the Software or copies thereof to others, except as provided herein. Any breach of the above terms and conditions shall immediately terminate your license under this Agreement. You hereby certify and agree that the Software will not be shipped, transferred, or re-exported, directly or indirectly, into any country prohibited by the United States Export Admin- istration Act and the regulations thereunder, and the Software will not be used for any purpose prohibited by the same. Neither you nor any of your subsidiaries, as applicable, or to your knowledge any of their directors, officers, or employees, is a person or entity that is directly or indirectly owned or controlled by any person, entity or country currently included on the List of Specially Designated Nationals and Blocked Persons or the Foreign Sanctions Evaders List maintained by the U.S. Treasury Departments OFAC, or is directly or indirectly owned or controlled by any person or entity that is located, organized, or resident in a country or territory that is, or whose government currently is, the target of countrywide sanctions imposed by any U.S. government Sanctions Authority. Neither you nor any of your subsidiaries, as applicable, or to your knowledge any of their directors, officers, or employees is a person or entity that is currently the subject of any Sanctions investigation, or is directly or indirectly owned or controlled by any person, entity or country that is currently the subject of a Sanctions investigation. You may not erase, delete or modify any copyright, trademark or other proprietary notices included on the Software. You agree that if you infringe Triangle MicroWorks intellectual property rights or exceed the scope of permitted use of this license, Triangle MicroWorks will be irreparably injured and may obtain a court order to enjoin you from further use of the Software. This license is not a sale. Title, ownership rights, and intellectual property rights in and to the Software shall remain in Triangle MicroWorks. You agree to abide by the copyright laws and all other applicable laws of the United States and international treat- ies. You acknowledge that the Software in source code form remains a confidential trade secret of Triangle MicroWorks. Failure to comply with the above restrictions will result in automatic termination of this Agreement and will make available to Triangle MicroWorks other legal remedies. Notwithstanding anything herein to the contrary, you acknowledge and agree that Triangle MicroWorks may terminate your license to the Software if you have failed to comply with any of your obligations under this Agreement. You shall not be entitled to a refund in the event of termination by Triangle MicroWorks pursuant the foregoing sentence. By purchasing a Term License or a Perpetual License to the Software you are automatically subscribed in the Software Main- tenance & Enhancement Plan (the Plan) for a period of one (1) year following the date of purchase (the Purchase Date) of the license, or if shorter, the term of your Term Licensee (such initial period referred to as the Initial Maintenance Period). Upon expiration of the Initial Maintenance Period, the Plan may be renewed on an annual basis subject to payment by you of the applic- able then-current annual fee. This Plan may provide a cost effective method for upgrading to the most recent release version of the Software. The Plan also provides technical support through email and Triangle MicroWorks website. The Initial Maintenance Period for Perpetual Licenses is free. The annual fee for the Plan for Perpetual Licenses after the first year is 25% of the Perpetual License purchase price; provided, however that Triangle MicroWorks, in its sole discretion, may adjust the annual fee at any time. The annual fee for the Plan for Term Licenses depends on the specific terms of the Term License. Although Triangle MicroWorks has no obligation to provide upgrades to the Software under the Plan, Triangle MicroWorks may from time to time provide upgrades. Any upgrades to the Software shall be deemed to become part of the Software and shall be subject to the terms and conditions of this Agreement. All Plan annual fees are nonrefundable. For Term Licenses and Perpetual Licenses only, Triangle MicroWorks warrants that the Software shall perform substantially as described in its documentation for a period of sixty (60) days from the Purchase Date. In the event of a breach of the foregoing war- ranty, the sole obligation of Triangle MicroWorks and your sole remedy is that Triangle MicroWorks will provide you with a refund of the most recent license fee paid for the Software. Triangle MicroWorks, as a condition to providing such refund, may require that you sign an affidavit certifying that you have deleted or destroyed all copies of the Software in your possession. Triangle MicroWorks makes no warranties with respect to Trial Licenses. Except as set forth in this Section 8, THE SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTIES OF ANY TYPE, AND TRIANGLE MICROWORKS HEREBY EXPRESSLY DISCLAIMS ALL WARRANTIES, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY, TITLE, NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL TRIANGLE MICROWORKS BE LIABLE FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL OR PUNITIVE DAMAGES WHATSOEVER, INCLUDING ANY CLAIMS FOR LOST PROFITS, BUSINESS INTERRUPTION, LOST INFORMATION OR OTHER DAMAGES ARISING OUT OF THE USE OF INABILITY TO USE THE SOFTWARE. IN NO EVENT SHALL TRIANGLE MICROWORKS HAVE ANY LIABILITY FOR ANY DATA STORED OR PROCESSED WITH THIS SOFTWARE, INCLUDING THE COST OF RECOVERING ANY LOST DATA. IN ALL EVENTS, TRIANGLE MICROWORKS SOLE OBLIGATION OR LIABILITY UNDER THIS AGREEMENT IN THE AGGREGATE IS A REFUND OF THE MOST RECENT LICENSE FEE PAID FOR THE SOFTWARE. THE PARTIES AGREE THAT THESE LIMITATIONS ARE AN ESSENTIAL PART OF THE AGREEMENT AND THAT TRIANGLE MICROWORKS WOULD NOT ENTER INTO THE AGREEMENT WITHOUT THESE LIMITATIONS. THIS LIMITATION OF LIABILITY CLAUSE WILL SURVIVE FAILURE OF ITS ESSENTIAL PURPOSE. You may terminate the license granted under this Agreement at any time by destroying or deleting all copies of the Software in your possession or control. In the event of termination of this Agreement for any reason, all the sections of this Agreement will sur- vive except for Sections 1 and 2. This Agreement shall be governed by and construed in accordance with the laws of the State of North Carolina, without regard to the choice of law provisions thereof. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to this Agreement. Any dispute or claim arising out of, or in connection with, this Agreement shall be finally settled by bind- ing arbitration in Raleigh, North Carolina, in accordance with N.C. Gen. Stat. 1-569.1 et seq. (the Uniform Arbitration Act) and the then-current rules and procedures of the American Arbitration Association by one (1) arbitrator appointed by the American Arbitration Association. The arbitrator shall apply the law of the State of North Carolina, without reference to rules of conflict of law or statutory rules of arbitration, to the merits of any dispute or claim. Judgment on the award rendered by the arbitrator may be entered in any court of competent jurisdiction. The parties agree that, any provision of applicable law notwithstanding, they will not request, and the arbitrator shall have no authority to award, punitive or exemplary damages against any party. In the event that any arbitration, action or proceeding is brought in connection with this Agreement, the prevailing party shall be entitled to recover its costs and reasonable attorneys' fees. In the event that the above arbitration provision is held invalid or unen- forceable, any dispute with respect to this Agreement shall be brought and heard either in the North Carolina state courts located in Raleigh, North Carolina or the federal district court, located in Raleigh, North Carolina. In such event, the parties to this Agree- ment each consent to the in personam jurisdiction and venue of such courts. The parties agree that service of process upon them in any such action may be made if delivered in person, by courier service, by fax, or by first class mail, and shall be deemed effect- ively given upon receipt. This Agreement sets forth the entire agreement between you and Triangle MicroWorks pertaining to the licensing of the Software and supersedes in its entirety any and all written or oral agreements previously existing between the parties with respect to such subject matter. If any provision of this Agreement is held invalid or unenforceable, such provision shall be revised to the extent necessary to cure the invalidity or unenforceability, and the remainder of this Agreement shall continue in full force and effect. In the event of any conflict between any provision of this Agreement and any applicable law, the provision or provisions of this Agree- ment affected shall be modified to remove such conflict and permit compliance with such law and as so modified this Agreement shall continue in full force and effect. This Agreement may not be assigned without the consent of Triangle MicroWorks. Triangle MicroWorks, Inc. 2840 Plaza Place, Suite 205, Raleigh, North Carolina 27612 USA (919) 870-5101 1.0 Getting Started9 1.1 Intro to 61850 Test Suite Pro9 1.2 Connect to TMW Example Servers11 1.3 TSP with DTM 61850 substation Demo21 1.4 Connect to or Simulate your IEDs35 1.5 Workspaces41 2.0 Tools (on the left)44 2.1 Overview of Tools45 2.2 System Status46 2.3 Compare SCL Files to IED Models48 2.4 GOOSE Tracker50 2.5 LGOS63 2.6 Mod/Beh/Sim75 2.7 Verify SCL Files90 2.8 Data Miner92 2.9 View IED Data Model98 2.10 What are Logs?99 2.11 How to Test Log Services99 2.12 What are File Services?102 2.13 How to Test File Services102 2.14 GOOSE Publisher104 2.15 SAV Tracker - Sniff Sampled Values on the Network112 2.16 IED Simulator113 3.0 Displays (on the right)126 3.1 Visualize Interactions Between Logical Nodes126 3.2 Signal Flow126 3.3 View Events in Logic Analyzer143 3.4 Custom Display153 3.5 View SCL Files173 3.6 View Reported Events175 3.7 Test Sequencer176 3.8 View a List of Object Data231 3.9 Server Data Simulation231 3.10 View GOOSE Events239 4.0 Use Cases241 4.1 Testing IEDs241 4.2 Testing Substations262 4.3 Testing GOOSE291 4.4 Testing Reports296 4.5 Testing SCL Files299 5.0 Configuration307 5.1 Application Options307 5.2 System Requirements312 5.3 Pre-Filtering Data Models with Data Miner313 6.0 Frequently Asked Questions315 Getting Started Introduction to 61850 Test Suite Pro Intro to 61850 Test Suite Pro Test Suite Pro "How To" Videos There are 3 approaches to getting started with Test Suite Pro: Connect to TMW Example ServersConnect to DTM Sub- station DemoConnect to or Sim- ulate your IEDsLoad a predefined workspace to monitor and troubleshoot the simple example servers shipped with TSP. Load a predefined workspace with Sig- nal Flow and Logic Analyzer displays to monitor and troubleshoot a more com- plex substation simulated in DTM. Load your SCD file or Dis- cover IEDs by IP Address. Tools are on the left Displays are on the right Connect to TMW Example Servers 1) Start the Example Servers 2) Using the Example Servers Operation of the example servers is explained in the following Help dialog box that is accessed from the File menu on the Example Serv- ers: 3) Open the Example Workspace in TSP 4) Verify the correct GOOSE adapter was automatically chosen by the Example servers Open GOOSE Tracker to show the status of each GOOSE stream published by the Example Servers. Verify the correct adapter is shown and that the status of each GOOSE stream is Valid as shown in the Screenshot below: If the GOOSE Streams have a status of Stream Error, try setting the GOOSE Adapter for Example Servers manually Setting the GOOSE Adapter for Example Servers manually The GOOSE adapter for the example servers is automatically selected when the servers start based on which loopback address is being used for the MMS client/server connection. If this is not correct, the GOOSE adapter may be set manually as follows: In TSP, hold the CTRL button and click on "Start Example Servers" Select one of the network adapters on the local PC 5)ImportantTools/Displaystotry: CustomDisplay-showstheSingleLinerepresentationoftheExampleServerapplication.Changesomeofthevoltages,currents, andbreaker/switchpositionsontheExampleServerandwatchthisdisplayupdate. 2021TriangleMicroWorks,Inc.Allrightsreserved. ViewReports-DatachangeeventsareautomaticallycreatedbytheExampleServerswhichwillcausereportstobegenerated.Thelevelofthecurrentsandvoltagescanbeadjustedwiththesliderbars. 2021TriangleMicroWorks,Inc.Allrightsreserved. ViewGOOSETripEvent-Ifthecurrentslidergoesoverthe"Trip"level,thenaGOOSEeventwillbegeneratedwithPIOC.Op.- general=true.ThiscanbeseenintheGOOSEEventViewer. GroupDisplay-DataplacedinaGroupDisplaycanbegroupedbyvariouspropertiessuchasIEDNameorLogicalNodePrefix. SettheGroupingtoLogicalNodeClassandwatchtheInterlocking(CILO)dataobjectschangevaluesasyouopenandclosethebreakerontheExampleServers. 2021TriangleMicroWorks,Inc.Allrightsreserved. AdvancedClient-CanbeusedtobrowsetheIEC61850DataModel: 2021TriangleMicroWorks,Inc.Allrightsreserved. LogicAnalyzer-CanbeusedtocharttherealtimevaluesofdataattributesfoundinIEDs: 2021TriangleMicroWorks,Inc.Allrightsreserved. Mod/Beh/Sim-SetLogicalNodesorentireLogicalDevicesinTestand/orBlockedmode. 2021TriangleMicroWorks,Inc.Allrightsreserved. TSP with DTM 61850 substation Demo 1 The DTM IEC 61850 Substation Demo The DTM IEC 61850 Substation Demo is a simulation of the high voltage part of a substation with two lines and two transformer bays con- necting to a single busbar. In each bay, we have a protection device (LPU/TFPU), a bay control unit (BCU), a switch control unit (SCU) and a merging unit (MU). The bay control unit implements the control of the switches and the measurements; the switch control unit implements the interface to the switches and the merging unit the interface to the current and voltage transformers. The protection devices implement protection functions including breaker failure protection. The line protection additionally implements reclosing and synchro check function. Information exchange between the IEDs is using GOOSE. 2 Open the Substation Demo Workspace in TSP Go to File, Select the workspace DTM IEC 61850 Substation Demo Version 1.5.0 Select Edit and verify the GOOSE adaptor is configured correctly. Then save the changes and load the workspace. Now, the workspace will load, TSP will connect to the servers simulated in DTM and perform some initial tests. After a while, in the Tool selection area on the left, all the checkboxes should be green and indicate that everything is correct. In the custom display to the right, you should see the status of the single line all switches are closed. 3 Use Custom Display to Control Switches In the custom display, select Line 1. The zoom level can be adjusted by pressing the control key and rolling the mouse wheel. By clicking on Open, you can open the circuit breaker. You will notice that the operation counter (OpCnt) will increase. After opening, you will as well notice an Event for OpOpn. OpOpn is the signal that is sent with a GOOSE message from the CSWI (in the BCU) to the XCBR (in the SCU) to open the breaker. If you right click on the tile with the DO OpOpn, you can see the event history each entry corresponds to a GOOSE message with a new state and you can reset the event history. 4 Use test sequencer to test all the switches Instead of manually testing all the switches, you can use test sequencer to automate that task. Go to the display Test Sequencer. There, two tests have been prepared one to test all the switches of line 1, the other to test all the switches of the bay TX1. On the top right, you can see the test plan and select the tests you want to run. On the left part, the tests are configured; the graphic shows an over- view. Note you can zoom in and out the graphic with the wheel of your mouse. Push on the green play button to start the test. First the test for Line 1 will be executed, then the test for bay TX1. You can observe the progress of the test in the display. At the end, the success is indicated with green color. 5 Analyze Protection and Control Schemes with Signal Flow and Logic Analyzer Under Display, go to SignalFlow. What you see there is an overview on the interactions for the control of Line 1. To test and record the scenario with a fault and a successful reclose, start recording in TSP by clicking the start button in the display for sig- nal flow; then go to DTM and push the first little red button in the list on the right. This will simulate a differential fault on line 1 detected by the differential element PDIF1. The data object PDIF1.Op will be activated, which will result in a trip generated by PTRC. The trip will be sent with a GOOSE message to the LN XCBR in the bay controller which will open the breaker. At the same time, the trip was internally sent as an initiate signal to the reclosing and the breaker failure functions. After a while, the reclosing function will reclose the breaker through the LN CSWI (note the assumption here is, that the reclosing needs to be done with synchrocheck). If release from synchrocheck (LN RSYN) is true, the CSWI will reclose the breaker through the LN XCBR. By default, the fault is assumed to not be permanent, so the reclosing will be successful. You can activate the checkbox next to the PDIF1 button in DTM, which will make the fault permanent. Following a permanent fault which makes the reclose to fail, you can reset the sim- ulation pushing the reset simulation button in DTM. Once you stop the recording in TSP Signal flow display, all of the events recorded will be shown on the timeline below the drawing. You can step through the recording and see the signals that have been sent for each event. To learn more about the Signal Flow Display, please click here. You can as well visualize the same interaction in the logic analyzer display. For the demonstration, select the pre-recorded chart set Reclose Successful new DTM. You can hide the signal selection. Then you select again Record New Chart Set from the pull-down list and click on the red circle to start recording. Initiate again a fault in DTM, stop recording, and analyze the signal timing. To learn more about the logic analyzer display, please click here. 6 Testing a Design with Errors So far, the DTM workspace we used to test was a design without errors. To demonstrate how to find errors in the design, run the script in DTM called InsertAllErrors.js to introduce errors in the design being simulated and TSP will be used to find these errors. This will run three individual scripts that create the following three errors: BreakSimulation.js: This will break the simulation for the switch QC1 in the bay TX1 (wrong connection to LN XSWI open input) ChangeModel.js: This will change the value of a configuration attribute in IED D1Line2BCU1 DisableGOOSE.js: For IED D1TX2BCU1, the capability to send and receive GOOSE messages will be disabled Note that you also can introduce just one of those errors by running the corresponding script. 7 Checking the system status in TSP The System Status group on the Tools menu shows the errors generated by running the script in DTM to introduce some errors. We now have problems with both the GOOSE tracker and LGOS tools that should either be resolved or acknowledged. 8 Verifying Messages on the Wire with GOOSE Tracker In GOOSE tracker, we see that the message from D1TX2BCU1MEAS is missing on the wire. Checking with Advanced Client tool, we see that the related GOOSE control block is disabled. Trying to enable fails, so apparently there is an issue with the GOOSE configuration in that IED. Looking into the LGOS tool, we see that D1TX2SCU1 is supposed to receive the GOOSE message from D1TX2BCU1, but the message is not seen on the wire by TSP and it is as well not received by D1TX2SCU1. We as well see, that D1TX2BCU1 is supposed to receive a mes- sage from D1TX2SCU1; that message is seen on the wire, but D1TX2BCU1 does not indicate reception. This looks like there is a problem with the configuration of the GOOSE messages in D1TX2BCU1. 9 Errors shown in Test Sequencer Running the script BreakingSimulation.js in DTM introduced an error in the control of the switch QC1 in the bay TX1. If we run again the test sequence for the bay TX1, this will show as an error. 10 Differences in the Model Running the script ChangeModel.js in DTM introduced a change of a configuration parameter in one IED. We can find that difference using the Compare tool of TSP. The easiest way to find differences is, to run Compare All: We then see that one error was found: Selecting an IED, the IED with the error is flagged in the list: When selecting that IED, the error is shown. It can as well be located by opening the model in the left part of the window below. 1.4ConnecttoorSimulateyourIEDs Thereare3optionsforconnectingtoIEDs: 1)FindIEDsusingSCLfiles-thisoptionloadstheIEDdatamodelfromanSCLFileSCLFile 2)Discovery-thisoptionletsyoutypeinthespecificIPAddressofanIEDforconnection 3)SearchIEDsbyIPAddressorIPRange-thisoptionusesdiscoverytoreadthedatamodelfromtherealIED FindIEDsusingSCLFiles IEDscanbefoundusingconfigurationinfofromanSCLFile: 1)Select"SCLFiles"intheSearchCriteria. 2)Click"Browse"toselecttheSCLfiles(.icd,.cid.,.scd,etc.).MultiplefilescanbeselectedbypressingCTRL. 2021TriangleMicroWorks,Inc.Allrightsreserved. 3) Click "Open" and Test Suite Pro will parse the SCL file and send an initial connection request to each IED found in the SCL file. *UpdatingtheSCLFile TSPdoesnotmodifySCLfiles.AreferencetotheSCLfileissavedintheworkspace-nottheactualfile.TheSCLfileusedinthework- spacecanbeupdatedbymodifyingtheSCLfileintheoriginalwindowsdirectoryfromwhereitwasloaded. Forexample,youcouldedittheSCLfileintheoriginallocationoryoucouldsaveanewcopyontopoftheoriginal.Theworkspacewillusethenewversionofthefilewhenitisreloaded. SearchIEDsbyIPAddress HowtofindIEDsbyIPAddress: IEDscanbefoundonthenetworkacrossarangeofIPaddresses.ThetoolwillfindanyIEC61850serverslisteningonport102andusedis- coverytoreadthedatamodelfromeachIED. 1)Click"IPAddressorIPRange"intheSearchCriteriabox 2)EntertheIPaddresstosearch.Tosearcharangeofaddresses,thenclick"andcontinuesearchingthrough"andentertheend- ingaddress. 3)ClicktheFindDevicesbuttontobeginthesearch. Note:IftheClearsearchresultsfirstcheckboxischecked,thenanyIEDsinthesearchareawillberemovedbeforenewlyfoundIEDsareadded. EachIEDthatisfoundintheIPaddressrangewillbeindicatedwithanIEDicon.Ifaconnectionisinitiated,theiconwillshowthegreencheckmark.IftheIPaddressdoesnotrespondwithacorrectlyformatted61850message,theIEDiconwillhaveaquestionmarkonit. BelowarescanresultswherethetoolinitiatedaconnectionwiththreeIEDs: 2021TriangleMicroWorks,Inc.Allrightsreserved. IEDConnectionTroubleshooting WhatifthetoolcannotconnecttoanIED? IfIEDsinthesearchresultsdonothaveagreencheckmark,readtheIEDConnectionConfigurationtopicformoreinformationaboutset- tingIEDconfigurationparameters. NowAddtheIEDstotheWorkspaceContents AftersearchingfororloadingIEDs,thenextstepistoaddIEDstotheworkspace. 1)SelecttheIEDstoincludeintheworkspace(multi-selectIEDsbyholdingdownCtrlorShift) 2)DraganddroptheselectedIEDsintotheWorkspaceContentsareaontherightOR 3)Right-clickontheselectedIEDsforoptionstoaddallIEDsinthesearchgroup(oneSCLfile)orallIEDsintheentiresearcharea(multipleSCLfiles) 2021TriangleMicroWorks,Inc.Allrightsreserved. SimulateanIED HowtosimulateyourIEDsinTSP AfterenteringyourworkspacewiththeIEDsselectedintheSCLfile: 1.GototheIEDSimulatortool(seescreenshotbelow): 2.ClickontheIEDdropdownmenu 3.SelecttheIEDyouwanttosimulate 4.Clickstartontheribbonbar 2021TriangleMicroWorks,Inc.Allrightsreserved. Workspaces Workspace Overview Workspaces are used to organize which IEDs will be tested, monitored, or analyzed. Multiple workspaces can be created and saved in 61850 Test Suite Pro. For example, saved workspace may be configured to test IEDs at a particular substation or to run a particular test. Workspaces can be opened, copied, edited, or deleted from the workspace list. Connecting to IEDs 61850 Test Suite Pro can connect to IEDs that have been added to a workspace. IEDs can be added to the workspace after performing a search for IEDs on the network. There are two options to search for IEDs: 1) Find IEDs using SCL Files - this option uses the data model from the SCL File. 61850 Test Suite Pro will search one or more SCL files and display the IEDs found in the file(s). 2) Find IEDs using an IP addresses - this option uses discovery to read the data model from the IED. 61850 Test Suite Pro searches IP address ranges for IEDs on the network. 61850 Test Suite Pro will attempt to initiate a connection to each IED IP Address. The initiate connect is very minimal and determines if an IEC 61850 server exists at each IP address. The user can then select which IEDs from the located set of IEDs are to be included in the workspace. Learn more about Simulating IEDs here. Start A New Workspace A new workspace can be started by going to the workspace tool and clicking "New Workspace" on the ribbon bar or click on the File menu to access the backstage area then click on the New Workspace option. Open or Edit A Workspace Open an existing workspace by: 1) Select the Workspace tool 2) Select "Open Workspace" on the ribbon bar This will display the list of existing workspaces. Hover the mouse over a workspace row to display options to load, edit, delete, or copy. Save Workspace There are several options when saving the workspace: 1) Give the workspace a unique name and an optional description 2) Optional: enable either the IED Simulator, GOOSE Publisher, or both for the workspace (by default these are disabled to reduce time to load workspace) 3) Optional: edit the connection settings by right-clicking an IED and selecting "Edit Properties for this IED" - see the topic IED Connection Configuration for more details 4) Select one of the save buttons at the top Note: the IED Simulator and GOOSE Publisher are disabled by default for a new workspace. This reduces the amount of memory used and allows the workspace to load faster. Also, the IED Simulator and GOOSE Publisher tools will not be shown in the list of active tools unless they are enabled for the workspace. Tools (on the left) Overview of Tools System Status Monitors the status and connection to the server by periodically attempting to make a connection when there is not one readily available. - learn more Compare Model As soon as the connection to the server is available, the SCL model is compared to that of the discovered server model to find if there are any mismatches. This can also be run manually. - learn more GOOSE Tracker GOOSE Tracker is started when the workspace is loaded and reports any errors or warnings found in GOOSE Streams - learn more LGOS - LGOS information is periodically collected from the server and reports any issues. LGOS data collection by read is also automatically started on workspace load. - learn more Mod/Beh/Sim - Mod/Beh/Sim information from the server is collected report any issues on workspace load. - learn more SCL Verify Verifies SCL files by checking for inconsistencies in the file, comparing the file to the standard, and attempting to resolve the data model from the file. - learn more Data Miner Helps find data from multiple IEDs across a substation with powerful filtering and grouping. - learn more Advanced Client The advanced client allows detailed communications testing of one IED at a time - learn more Log Services Tests the Log Services functionality of one IED at a time - learn more File Services Tests the Files Services functionality of one IED at a time - learn more GOOSE Publisher Publishes individual GOOSE control blocks in order to test and troubleshoot IEDs that subscribe to GOOSE - learn more SAV Tracker Sniffs the network for Sampled Values messages and compares the result to the SCL file to find potential issues and checks the validity of the messages - learn more IED Simulator Simulates an IEC 61850 server based on any SCL file or a discovered model from an IED with added functionality to aid in test- ing client implementations. - learn more System Status System Status monitors the status and connection to the server by periodically attempting to make a connection when there is not one readily available. Notifications alert the user to immediate issues in the system. The fol- lowing tools participate in the alerts and error reporting: System Status Monitors the status and connection to the server by periodically attempting to make a con- nection when there is not one readily available. - learn more Compare Model As soon as the connection to the server is available, the SCL model is compared to that of the discovered server model to find if there are any mismatches. This can also be run manually. - learn more GOOSE Tracker GOOSE Tracker is started when the workspace is loaded and reports any errors or warnings found in GOOSE Streams - learn more LGOS - LGOS information is periodically collected from the server and reports any issues. LGOS data collection by read is also automatically started on workspace load. - learn more Mod/Beh/Sim - Mod/Beh/Sim information from the server is collected report any issues on workspace load. - learn more Testing SCL Files Compare SCL Files to IED Models The Compare Model tool compares the IEC 61850 object model from an SCL file to the object model from an IED. This is done one IED at a time. Start the Tool 1) Select the Compare Model tool 2) Select which IED to compare The tool then starts the comparison. First, it discovers the object model from the IED and then it compares the IED to the local object model from the SCL file in the current workspace. Compare Results The left hand side has the combined object model shown as a tree. Green indicates no differences were found between the SCL file and the IED Orange indicates that one or more children were found to be different Red indicates the item with the difference The right hand pane shows the details of the difference found. Selecting any level on the left will show all the chil- drens differences on the right. The general algorithm compares all item available from discovery. This includes structure, names, types and sizes, members of datasets, etc. For value comparison Control Block values and CF, EX, SG,SP, and DC functional constraints are compared for val- ues matching. GOOSE Tracker Introduction The GOOSE Tracker monitors all the GOOSE streams defined in the SCL file(s) for the workspace and any addi- tional GOOSE streams found on the network. The GOOSE Tracker can be used for the following purposes: Find all the GOOSE Streams on a given network Identify GOOSE Streams that are missing in the SCL file(s) or missing on the network Find the health of the GOOSE Stream using various parameters like Timeouts, Missed or out-of-order packets, etc. Find duplicate and simulated GOOSE streams on the network Verify GOOSE Dataset definition on the server Each GOOSE stream is shown as a row in the table and the columns indicate the corresponding properties, status, warning conditions, and error conditions. Starting the GOOSE tracker First select the network adapter, then click Start Tracker. The GOOSE tracker loads all the available GOOSE control blocks from the devices loaded in the workspace. The screenshot below shows all the GOOSE control blocks in the TMW Sample Workspace. Status Column The Status column is used to summarize all the Error and Warning conditions that can occur in the other columns. Tooltips on the Header cells of a column describe the conditions for when a cell will be considered in an Error/Warn- ing state and a tooltip on the row explains why the GOOSE stream is in Error or Warning state. The content of a cell is color coded according to its Status: Red: Error Orange: Warning Green: Normal Black: Not Checked The Factory Filter Preset called By Status groups the GOOSE Streams by the status column value to quickly identify potential communication problems between devices. Custom Filter Presets may be created to restore a selected set of columns that are filtered, sorted, and grouped to identify specific issues or validate GOOSE com- munication is working properly. Columns that may cause the GOOSE Stream Status to be set to an Error condition Valid Stream If Valid Stream is marked as False, the Status of the stream is marked as Error. A GOOSE stream is valid only if: The stream is available in the network A retransmission message for the GOOSE stream is received within the Time Allowed to Live period in the previous message. SCL/Wire Match? The values in the SCL/Wire Match? column indicate if the corresponding GOOSE stream, as defined in the SCL file(s), matches a stream found on the network (Wire): Match Indicates that a match was found on the network Missing on Wire Indicates that the GOOSE stream defined in the SCL is missing on the network (wire) Missing in SCL Indicates that the GOOSE stream was found on the network but is miss- ing in the SCL If the value of this field is not Match, then the status of the stream is marked as Error. Is Duplicate Duplicate GOOSE streams on the network can prevent the GOOSE Subscribers from receiving any data. The GOOSE Tracker makes it easy to identify Duplicate GOOSE Streams and distinguish them from Sim- ulated GOOSE streams originating from a Testing Device, such as the Test Suite Pro GOOSE Publisher Tool: Duplicate: If a GOOSE stream has the same GOOSE ID, App ID, Destination Address and Reserverd1Simulate (Simulate Bit value) Simulated: If a GOOSE stream has the same GOOSE ID, App ID, Destination Address but different Reserverd1Simulate (Simulate Bit value) In the screenshot above, the first two highlighted items are duplicates of each other since they have the same GOOSE ID, APP ID, Destination address, and SimulateReserved1 (Simulate Bit value). The third highlighted line item is very similar to the first two, except that it has the Simulate bit set and hence it is treated as a unique stream. The GOOSE Stream Status is set to Error if it is a duplicate of an existing stream and Warning if Simulate Reserved 1 (Simulate) is set to True. Dataset Definition Checked The GOOSE Tracker provides an option to verify the dataset of the GOOSE control block as defined in the SCL file against the one received on the network. This can be done as follows: Ribbon Bar Button: The Additional Checks button indicates the checks that can be performed on all the matched GOOSE streams. Context Menu: The context menu option verifies the dataset on a single GOOSE stream at a time. If the dataset definition verification fails, the GOOSE Stream Status is set to Error. Number of State Drops (Num State Drops) If the number of state drops recorded on the stream is greater than 0, the GOOSE Stream Status is set to Error. Number of State Anomalies This is the count of the retransmission messages received on a GOOSE Stream with the State Number less than the current value. If this count is greater than 0 then the GOOSE Stream Status is set to Error. Number of Retransmission Anomalies (NumRetransAnamolies) Count of Goose messages received with a lower Sequence number than the current value. The GOOSE Stream Status is set to Error if this count is greater than 0. Dataset The GOOSE Stream Status is set to Error if the dataset of the incoming GOOSE message is not the same as the one defined in SCL. Number of Dataset Members The GOOSE Stream Status is set to Error if the number of dataset members of the incoming GOOSE mes- sage is not the same as the one defined in SCL. Config Revision (ConfRev) The GOOSE Stream Status is set to Error if the number of ConfRev of the incoming GOOSE message is not the same as the one defined in SCL. Needs Commissioning (NDSCom) The GOOSE Stream Status is set to Error if the value of NdsCom is set to true. Columns that may cause the GOOSE Stream Status to be set to a Warning condition Simulate Reserved1 The GOOSE Stream Status is set to Warning if Simulate Reserved 1 (Simulate) is set to True. Number of Retransmission Drops (NumRetransDrops) Number of Retransmission drops is the total number of retransmission messages missed. The GOOSE Stream Status is set to Warning if the number of retransmission drops is greater than 0. Additional Columns There are many additional columns showing properties and configuration data for each of the GOOSE Streams displayed. They may be accessed by the Factory Filter Preset Config on Wire or via the Column Chooser Tool as shown in the screenshot below. Tooltips on the column headers provide help on the column data. Context Menu Clear All: Resets the GOOSE streams to their initial state showing only the ones that are defined in the devices in the workspace loaded. All the discovered streams are removed. The metrics calculated so far are also reset. Subscribe: Subscribe to the GOOSE control block. Verify DataSet: Validates the dataset defined in the SCL file against that of the incoming GOOSE message. LGOS Introduction The LGOS Logical Node as defined in IEC 61850-7-4 2010 provides the status of the subscription to a GOOSE stream. The LGOS Tool in TSP assists by monitoring all the LGOS nodes in the current workspace. Each row corresponds to one GOOSE Stream in the publishing IED Each column in the Subscriber section represents a subscribing IED The intersecting cell represents the Subscribing IEDs LGOS node status for the subscription to the cor- responding GOOSE stream The Legend displays all the possible subscription statuses LGOS node details IEC 61850 Description of LGOS Below is the description of the LGOS node from IEC 61850 7-4: Data Object NameDescriptionStStatus of the GOOSE Subscription SimStStatus representing if Simulated GOOSE messages are received and acceptedLastStNumLast state number receivedConfRevNumExpected configuration revision numberNdsComSubscription needs commissioningGoCBRefReference to the subscribed GOOSE control block LGOS Statuses in Detail TSP takes advantage of the already existing GOOSE tools to combine the LGOS details with available GOOSE mes- sages on the wire to provide detailed information of possible errors in the subscription. View the list below of the possible statuses in the LGOS Tool: Tool Tip info When any cell is highlighted the details of the corresponding LGOS node are displayed as a tool tip as shown below: Updating the Display When a workspace is loaded, the display is updated by reading each of the LGOS Logical Nodes where the con- nection to the corresponding IED was successful. After the workspace is loaded, the display can be updated in the following ways: Manual Update Automatic Update Additional Visualizations in the display The following indicators are available to display specific states of the publishing and subscribing IEDs: Availability of a simulated GOOSE stream LPHD.Sim of a subscribing IED is set Context Menu Options LGOS Cell Open in Advanced Client Navigates to the LGOS Node in the Advanced Client Subscribing IED Clear/Set LPHD.Sim Sets or clears the LPHD.Sim of the subscribing IED Show in Mod/Beh/Sim Tool Navigates to the corresponding IED in the Mod/Beh/Sim Tool Publishing GOOSE Stream Update Update the LGOS node details of all the subscribing IEDs for the current GOOSE Stream Show in Advanced Client Navigate to the GOOSE Stream in the Advanced Client Show in GOOSE Tracker Navigate to the GOOSE Stream in the GOOSE Tracker Show in GOOSE Publisher Navigate to the GOOSE Stream in the GOOSE Publisher Filtering the LGOS Grid The LGOS grid layout can be filtered by selecting the corresponding status in the Legend The filter can be cleared by clicking on the selected item on the legend again Multiple statuses can be selected to filter the grid accordingly Mod/Beh/Sim Introduction IEC 61850 provides several features to facilitate system testing. There are two separate mechanisms that often interact to facilitate testing in place and in particular live testing; Simulated process bus messages and Beh (beha- vior) of the Logical Node. According to IEC 61850-7-1, An IED can be configured to respond to Simulated GOOSE or SAV messages on the network. When the device is configured with LPHD.Sim.stVal set to true, it will process any simulated streams it receives rather than the respective streams from the publisher(s) it is configured to subscribe to. These simulated messages are published by a test tool for this purpose. For any streams that are not simulated, the subscriber con- tinues to receive the normal streams. According to IEC 61850-7-4, the LN behavior can be configured to respond in a Test mode and operate in a Blocked mode: Test mode means that all data produced by the LN will have a quality flag that indicates it is test data and will then respond to control services and input data that have test flags indicated. Blocked means that while the LN will indicate via communications that it is functioning according to the standard, no physical outputs will be affected. Behavior is calculated by the various Mod (mode) Data Objects in the device hierarchy containing the LN of interest. Mod of LLNO affects the Beh of all LNs in that LD, and GrRef points to another LD which this LD inherits from. The Mod/Beh/Sim tool is used to view and manage: Mod/Beh of a Logical Device / Logical Node LPHD.Sim of an IED The image below shows a list of all IEDs in the workspace. All Logical Devices and Logical Nodes in each IED are enu- merated in a tree structure. Manage LPHD.Sim LPHD.Sim determines whether the Logical Device / Logical Node receives an original or simulated signal. Below are the options to manage the LPHD.Sim of an IED: Context Menu on IED Row Manage Multiple IEDs Control Parameters to Manage LPHD.Sim To modify the parameters of the operate command issued to change the LPHD.Sim of an IED, hold CTRL while clicking on the Set/Clear LPHD.Sim context menu option. This will bring up a dialog window as shown below where the parameters for the control operation can be modified: Manage Mod The value of Mod determines the type of data that is accepted by the Logical Node as an input. Context Menu on a Logical Device / Logical Node Manage Mod on multiple Logical Devices / Logical Nodes Updating the Display Update all the IEDs Update one IED at a time Other context menu options Show in Advanced Client: Beh Navigate to the Beh Data Object of the selected Logical Device / Logical Node Mod Navigate to the Mod Data Object of the selected Logical Device / Logical Node LPHD.Sim Navigate to the LPHD.Sim Data Object of the selected IED Expand Tree Expands all the Logical Devices and Logical Nodes in the display: Expand Siblings Expands all the siblings of the current selected node: Collapse Tree Collapse all the nodes of the tree: Collapse Siblings Collapse all the siblings of the current node: Verify SCL Files SCL Verify will verify one file at a time either from the current workspace or by selecting a file. The severity of each test can be configured by running the SCL Verify Configuration tool. Run the Verifier 1) Select the SCL Verify tool 2) Choose which SCL file to check. This can be a file from the current workspace or you can browse to select a file from disk. 3) Select Run Viewing Results The results of the SCL Verify are shown on the left. The SCL File will appear in the SCL Viewer on the right. Clicking on line items after running the Verifier will navigate to the line where the error was found. Setting the Edition for Modeling Checks The edition (1 or 2) that SCL Verify will use when comparing the object model in the file to the object model defined by the standard is set at the top of the screen: There are two options: Use version/revision attribute on SCL Determines the version by inspecting the SCL attribute at top level. Use originalSclVersion/originalSclRevision attributes on each IED Determines each individual IED by using the originalSclVersion/originalSclRevision attributes on each IED element. The edition of the SCL File is determined by looking at the version and revision in the <SCL> tag at the top of the SCL File: 2007:B4 is considered edition 2.1 2007:A or 2007:B are considered edition 2 2003: ? (where ? is the revision and can be any value) or blank for those fields is considered edition 1 Data Miner Using the Data Miner The Data Miner is a powerful tool to find the data you need across multiple IEDs. Each row in the Data Miner is a data object in an IED Each column can be filtered, grouped, or sorted by value Any data objects can be viewed in a Custom Display, in a table view called the Data Monitor, or in the Advanced Client tool. Note: The Data Miner shows data objects across all of the IEDs in the workspace. Data objects are complex objects that contain multiple data attributes. For example, the data object Pos in the example above may contain stVal (status value), q (quality), and t (time). Values for data attributes can be viewed in the Cus- tom Display, Data Monitor, or Advanced Client. Finding Data Objects Three techniques can be used to find data objects in the Data Miner: Pre-filtering with the Data Source Filtering data by columns Grouping data by columns Viewing Data Object Values Data values can be viewed in three different ways: Custom Display Data Monitor Advanced Client Pre-Filtering with Data Source The Data Source is a global pre-filter that is applied to the SCL Files when creating a workspace. This pre-filter lim- its the amount of data to a manageable size. This is necessary because some IEC 61850 SCL Files can contain mil- lions of data attributes, some of which are not necessary for the testing you are doing. Filtering Data by Columns Any of the columns in the table view in the Data Miner can be filtered or grouped to find IED data objects. Column filters that are active are highlighted in red. Example 1: Data can be filtered by Logical Node Class (CSWI, PTRC, MMXU, etc.) as seen below. Example 2: Data can be filtered to only show data contained in GOOSE messages. The same type of filter can be setup to show data available from Reports. Grouping Data by Columns Data objects can be grouped by any column. For example, all the data objects can be grouped by IED Name or Logical Node Prefix to group the data according to substation bays. Example: Data can be grouped by IED Name. Select Which Data Columns to View Several different columns can be displayed for data objects in the Data Miner: Column Selection - columns can be added or removed from the table view. Viewing Data Values Data values for data attributes can be viewed in the Custom Display or the Data Monitor: Custom Display - for a graphical view of data, drag data objects to the Custom Display. For many data objects, this will create a graphical view of the data. Data Monitor - for a table view of data, drag data objects to the Data Monitor. This will create a table view of the data attributes in the data objects. Advanced Client - for a detailed view of the IED data model, right click on the data object and select "Open in Advanced Client". This opens the data object in the full IED data model. This view is the same as the original Hammer data model view in previous version of the 61850 Test Suite. Pre-Filtering Data Models with Data Miner The Data Miner runs the Data Source pre-filter on the entire data model for each IED in the current workspace. This pre-filtering allows the tool to show the most important data for the job being performed. Changing the Data Source Pre-Filter The settings for the Data Source pre-filter can be configured to include the data objects that you need for testing. The default settings include most of the essential process data based on Common Data Classes (CDC's) like WYE, DPC, SPC, ACT, and ACD. To change the Data Source pre-filter: Go to the Data Miner tool Select the "Data Source" tab at the top of the ribbon bar You can select a new Data Source configuration from the dropdown Or click "Configure Data Sources" to change the settings or create a new configuration Configure a New Data Source The Data Source configuration window lists all of the possible IEC 61850 data objects. Use the column filters to find data objects Select the data objects that you want to be included in the Data Miner by clicking the boxes on the far left Use "Save As" to save as a new Data Source configuration Close the window The new data source can be selected in the Data Miner and this will apply a new pre-filter to the entire workspace (all IED data models) Note: the Data Source configurations are saved locally on the PC. They are not part of a workspace, so that they are available to be used from any workspace. View IED Data Model Advanced Client The entire data model for an IED can be viewed in the Advanced Client tool. From the Data Miner You can go from the Data Miner to the Advanced Client by right clicking on a data object. Note: This will open the Advanced Client and take you to where the data object is located in the data model. What are Logs? Logs provide a mechanism for an IEC 61850 server in an IED to maintain Sequence of Events information. They are based on datasets and store information exactly as in reporting, but the transmission is completely different. The client must query the log to get the events. Multiple clients can query the same log, and multiple Log Control Blocks can store events in the log. How to Test Log Services The Log Services tool can test IED logs by querying the log from in different ways. First, select the Log Services tool from the tool selection on the left. Logs are tested one IED at a time by choosing an IED Connection in the upper left hand corner. Upon choosing an IED a connection will be established if one has not already been made. After selecting an IED Connection choose a log in the device to query: In this case BayController1CTRL/LLN0.Q has bee selected. Once selected a Reset Log is performed which goes and retrieves all available entries in the log. The left side shows the header information for each entry in the log. Select- ing an entry shows the data contained in the log entry on the right. To monitor the log for changes setup an Auto Refresh of the log by setting a time interval then pressing the Start Refreshing button. Or a manual query can be done by EntryID or by time. The log can be queried from an EntryID. The combo box will be filled in with the available entryIDs. Choose one and press Query. The Server should return all EntryIDs after the one is chosen. The other way to query the log is between two points in time. There are two parameters; After time and Start Time to End Time, all with available times. Once After time is chosen or Start time to End time are chosen, the Query button will be enabled. The Server should return all times subsequent to After time or between Start time to End time depending upon which parameter used. What are File Services? IEC 61850 provides basic file transfer services upload, download, delete, and directory. The directory structure is simplified as no assumptions about the underlying platform or filesystem are assumed. How to Test File Services There are 4 basic File Services offered by IEC 61850: Directory Listing Ability for a Client to retrieve a directory listing of files on a Server. File Delete Ability for a Client to delete a file residing on the Server File Upload Ability for a Client to upload (or copy) a file to a Server. File Download Ability for a Client to download (or retrieve) a file from a Server. Client File Services Client File Services are exercised one connection at a time. First, go to the File Services tool. Choose a connection by picking one of the available IEDs in your workspace in the upper left hand corner. Once a selection is made a connection will be established automatically. BayController1 is selected below: Once a connection is made a File Services Directory on the current Server directory shown in the edit box is per- formed. If root is shown then a top level File Services Directory is shown. At any point the Refresh button may be pressed which will cause a File Services Directory to be performed on the directory shown in the edit box. Note: Directories are generally shown ending with a \ however this is server dependent. Clicking on a directory in the files list will navigate to that directory and clicking .. will navigate up one level. Download Service Once a file is selected in the server file listing the download buttons will become active. Pressing this button will bring up a file dialog so you can save the file to the local disk. Delete Service Once a file is selected in the server file listing the delete buttons will become active. Press- ing this button will permanently delete the file from the server. Upload Service To activate the upload service choose a local file by clicking the button under the Local File text box: Once a file is chosen the Destination file name will default to the same file name and the Upload Client File to Server button will be enabled. Simply press the button to start the upload of the file. IED Simulator File Services Test Suite Pro Server has a complete implementation of all File Services by default. Specific configuration of File Services is available via the editing/creating of workspaces. On each specific IED a File Services Root directory can be configured along with flags indicating if the particular File Services should be available when initiated by a Cli- ent. These are configured under the How to Simulate tab of the IED Connection Properties dialog. Then under Advanced Settings->FileServices. All File Services are enabled by default. The File Services Root Directory defaults to: C:\Users\Public\Documents\Triangle Microworks\61850 Test Suite Pro\TestSuitePro\FileTransfer GOOSE Publisher Introduction The GOOSE Publisher is a tool specifically designed to simulate GOOSE streams without needing to simulate the whole device. It is particularly useful for modifying the headers to test specific negative cases. Like all the other tools, the GOOSE Publisher is available on the Left side of the application. The GOOSE Publisher shows all the available GOOSE streams from the devices loaded in the workspace. The screenshot above shows all the GOOSE streams in the TMW Sample Workspace. The Simulate (Reserved1Sim- ulate) bit is set by default on all the streams in the GOOSE Publisher. Publishing a GOOSE Stream A main advantage provided by the GOOSE Publisher when testing GOOSE streams is its ability to change the GOOSE header and the underlying data values before publishing. The following screenshot explains the various options available in the tool to help publish and test GOOSE. As shown above, select the network adapter on which to publish the GOOSE streams. You can then either start all the displayed GOOSE streams by clicking on the Start All button on the ribbon bar or start individual GOOSE streams by using the Start button in individual rows. The following screenshot shows the state of the tool while publishing the GOOSE streams. There are visible, clear distinctions between streams that are being published and those that are not. A green background means that the stream is currently being published. If a stream is selected, it will be highlighted in blue, regardless of the streams publish status. PublishingaGOOSEStreamwithIEDSimulator TopublishalltheGOOSEcontrolblocksforanIED: 1)FollowthestepsforsimulatinganIED-thesimulatedIEDwillpublishallthevalidGOOSEcontrolblocksintheIEDdatamodel 2)Selecteither"SendSimulatedGOOSEMessages"or"SendNormalGOOSEMessages"(simulatedGOOSEmes- sagesaretypicallysentbytestequipment) 3)FindDataSetintheIEDdatamodel 4)ChangedatavaluesintheGOOSEDataSetbyeither: ManuallychangedatavaluesinthemodelviewoftheIEDsimulator ORAutomaticallychangedatavaluesintheServerDataSimulation 2021TriangleMicroWorks,Inc.Allrightsreserved. Modifying a Published Stream The GOOSE Publisher tool lets the user modify the header configuration and the underlying data member values even while the stream is running. As soon as one of the GOOSE header fields or the data member values are modified, the Re-publish button (as shown in the screenshot above) will show up in line with the GOOSE stream being modified. The user will also have indications on which fields have been modified. If the user chooses to republish the stream by clicking on the respective button, the new values will take effect. Creating a Duplicate GOOSE Stream GOOSE Publisher allows creation of duplicate streams based on the selected stream. This would be used to test how subscriber IEDs respond to normal and simulated GOOSE messages in different modes (on, blocked, test, test- blocked). To copy a GOOSE stream: Select the GOOSE stream that needs to be duplicated Click on the Duplicate button in the Ribbon Bar This will create a new stream with a name prefixed by Copy of (Note: The Simulate bit is turned on by default) Change Tracking To help keep track of changes to fields in a GOOSE stream, the tool provides a way to review the changes to fields. A field marked with a red flag in the top left indicates a change. Hovering the cursor over a marked field allows you to view changes. The following changes are tracked on fields: Is the current value different from the original value (based on the configuration)? In the screenshot below, the State Number (St Num) has been changed from 0 to 10. The cell containing the state number is marked red indicating the change in value. The tooltip on the cell shows that the Original Value of the field was 0. Is the published value different from the current value? In the screen shot below, the State Number (StNum) changed from 0 to 10 and then to 20. The State Number was changed from 0 to 10 The Stream was published with state number 10 The State Number was then changed again from 10 to 20 Other Options Context Menu (right click) Open in Advanced Client: Navigates to the corresponding data member in the advanced client SAV Tracker - Sniff Sampled Values on the Network The SAV Tracker can be used to sniff Sampled Values streams on the network: 1) Go to SAV Tracker tool 2) Select the Ethernet Adapter on you PC that will be used 3) Start the tool The tool will then list all the sampled values streams that can be seen on that Ethernet adapter. IED Simulator Simulate an IED Simulate an IED from an SCL File Note: Important: make sure that the simulated IED and the real IED are not on the same network to avoid conflicts between the real IED and the simulated IED. This could cause conflicting GOOSE messages to be published by the simulated IED and real IED or potentially cause other conflicts for IEC 61850 clients that are connected to the real IED. To simulate an IED from an SCL file: Create a new workspace using the SCL file that defines the IED data model (make sure to enable the IED Simulator when creating the workspace) Go to the IED Simulator tool (see screen shot below): Click on the IED drop down menu Select the IED you want to simulate Click start on the ribbon bar Tip: Any IED in a 61850 Test Suite Pro workspace can be simulated. IEDs can be loaded from an SCL File or by reading the data model from an IED using discovery. Simulate a Real IED's Discovered Data Model To simulate an IED based on the configured data model of a real IED: Create a new Workspace and discover the IED data model using the IP address of the IED Important: after you discover the IED data model, you will need to either disconnect 61850 Test Suite Pro or the real IED from the network to avoid conflicts between the real IED and the simulated IED Go to the IED Simulator tool (see screen shot below): Click on the IED drop down menu Select the IED you want to simulate Click start on the ribbon bar Note: It is important to make sure the simulated IED and real IED are not on the same network. This could cause conflicting GOOSE messages to be published by the simulated IED and real IED or potentially cause other conflicts for IEC 61850 clients that are connected to the real IED. Tip: Any IED in a 61850 Test Suite Pro workspace can be simulated. IEDs can be loaded from an SCL File or by reading the data model from an IED using discovery. Simulate Data Changes Introduction TSP has the capability of simulating data changes on a simulated server. This is available as a display as shown in the image below: Create Auto Data Changes Attributes can be added by dragging them over from the IED Simulator tool. Data Changes can be simulated only on attributes with the functional constraint MX | ST | SP. Attributes from different servers can be added just like any of the other displays. Attributes can also be added by using the Add Points dialog as shown below: Then sorted by the options of Functional Constraint: Or Dataset: Configuring Auto Data Changes Simulation settings are described in the screen shot below: Max: The max value that will be assigned to the data attribute Min: The minimum value that will be assigned to the data attribute Step: The value that the attribute will be incremented or decremented by while simulating data changes Interval(ms): Minimum time in milliseconds between consecutive data changes Saving your setup The points being simulated are saved within the workspace so that the simulation can be quickly restored later. Auto Data Change Playback Start Data Changes: To Start simulating data changes, click on the Play button: Stop Data Changes: To stop simulations, click the Stop button: Manually Change Data Values You can also change individual data attributes manually in the data model: Start the simulated IED in the IED Simulator tool Find the data you want to change in the data model Click in the value column, change the value, and press enter Example: In the example above, a report will be generated by the IED when the value is changed if the data attribute T1MMXU1.PhV.phsA.cVal.mag.f is included in a dataset for a report control block Displays (on the right) Visualize Interactions Between Logical Nodes Signal Flow - visualize how the devices communicate with each other via GOOSE messages or Reports - learn more Logic Analyzer - view real time signal values using an eight-channel logic analyzer - learn more Custom Display - create custom displays to show data from IEDs - learn more SCL Viewer - view SCL files loaded in the SCL Verify tool - learn more Report Viewer - view reports that are received from IEDs - learn more Test Sequencer - build repeatable command and testing sequences - learn more Data Monitor - create a custom table of data from IEDs - learn more Group Display - quickly group data from IEDs in a custom display Server Data Simulation - create automated data changes in simulated IEDs - learn more GOOSE Viewer - view GOOSE events that are received from IEDs (GOOSE messages that contain new events from data or quality changes, not GOOSE re-transmission messages) - learn more Signal Flow Introduction The Signal Flow Display is a visual representation of how a change in a data attribute triggers changes in other logical nodes in the same device (via Reports or GOOSE) or other devices (via GOOSE). When a new workspace is created, it contains an empty Signal Flow Dia- gram. Nodes are added to the Signal Flow by dragging them from the Data Miner . Building a Signal Flow Diagram As the data will be retrieved either via GOOSE or Reports, the nodes being dragged will have to be enabled for either GOOSE or Reports. Nodes that can only be polled cannot be added to the Signal Flow Diagram. The availability of a node through GOOSE or Reports can be established by examining the Used in GOOSE Control(s) and Used in Report Control(s) columns of the Data Miner. If a node is available through both GOOSE and Report, the source is determined by the GOOSE and Report buttons in the Data Retrieval Method section of the Data Miner Ribbon Bar. If both the GOOSE and Report buttons are selected, priority is given to GOOSE. When a Data Attribute is dropped onto the Signal Flow Diagram, TSP will automatically issue the necessary subscription to the server if it is not already enabled. If a Data Attribute containing external references from other Logical Nodes is added to the Signal Flow, those Logical Nodes are also added to the diagram. The following screenshot explains the various options provided to ease the process of creating a Signal Flow Diagram: The Data Retrieval Method section determines the method that the Signal Flow should use to retrieve the data for the Data Attrib- ute being dragged. The precedence of the methods is in the order of appearance (i.e. GOOSE/Report) in case the node has both GOOSE and Report enabled. If neither GOOSE nor Report is enabled on a node, it cannot be used in the Signal Flow. The Data Miner has a Factory Preset (GOOSE Publishers) to show only Data Objects that are available in a GOOSE stream so they may be easily added to a Signal Flow diagram. The Data Miner has a Factory Preset (Reported) to show only Data Objects that are available in Report so they may be easily added to a Signal Flow diagram. These signals can only connect Logical Nodes in the same device since servers do not normally com- municate via Reports. The Data Miner column Input to LNs identifies the Logical Nodes with an External Reference to each Data Object that is available in a GOOSE stream. Besides setting up the links between the nodes that refer to each other via external references, the Signal Flow also lets the user draw a link from an output to any node of choice. The Signal Flow automatically lays out the logical nodes in the order of the flow of signals between nodes by default. One can choose to reset the arrangement of nodes by choosing the context menu option Auto Layout. Capturing Data in a Signal Flow Diagram When a node is dragged onto the Signal Flow Diagram, it subscribes to receive events either through GOOSE or Reports. This is based on the data retrieval method that is selected as described in the previous section. From this point forward, the diagram actively displays the current value and quality of the attributes (provided there is an active connection). The diagram also displays the connection status of the client to the server. The diagram above displays a Signal Flow Diagram and its various components. Different visualizations and color codes are used to dif- ferentiate between the connection status, data retrieval methods and the quality of the signals. As shown, multiple data objects cor- responding to one logical node can be dragged over and they are automatically grouped by Logical Node -> Logical Device -> IED. The node that is being dragged over will be the source and will be placed on the right side of the Logical Node. If the dragged node has external references, the dependent nodes are automatically added, and links are drawn between the source (dragged node) and the des- tination (External References). The source will therefore behave as an Output port and the destination will behave as an Input port for a logical node. Besides being able to display the current condition of the signals, the display also lets the user record the events as they occur. On press- ing the Record button, the display will start to collect all value changes and plot them on a timeline with millisecond accuracy. The fol- lowing screenshot provides details of the recording: Transitions added on the Logical Nodes and the Links between them help visualize a signal flowing from source to destination. The events are tagged on the timeline with a number that indicates the sequence in which they occurred. Reviewing Results The display provides tools for reviewing and replaying events that occurred during a recorded session. The following diagram explains the various parameters of the recording and the visualization details of the diagram: Color Codes in Detail The signals are coded by color & pattern based on how they are captured (GOOSE/Report), the state of the signal (Con- nected/Disconnected) and the quality of the value received. Events on the timeline are color coded by how they are captured (GOOSE/Re- port) and the quality of the value received. The following diagram shows how the display looks when replaying events captured during a recorded session and explains the meaning of the port colors and line colors/patterns: FilteringEvents Sometimes,theremightbetoomanyeventsonthetimelinetoeasilyinterprettheinformation.TheSignalFlowDiagramprovidestwowaysinwhichtheeventsonthetimelinecanbefiltered: a.ByLogicalNode:Whenoneormorenodesareselected,thetimelineisfilteredtoonlyshoweventswhereanyoftheselectednodesareeitherasourceoradestination: 2021TriangleMicroWorks,Inc.Allrightsreserved. b.ByLink:Whenoneormorelinksareselected,thetimelineisfilteredtoonlyshoweventsoriginatingfromthesourceofanyoftheselectedlinks: 2021TriangleMicroWorks,Inc.Allrightsreserved. Tool Tips Tool Tips provide information about the condition of each signal source as well as information on the individual event that was captured. Tool Tip on a Signal Source: A tool tip with information about the signal source is displayed when the cursor is positioned on the value for a link or the link itself. The tool tip contains a detailed list of all the changes that have occurred on that signal from the time recording was started to the playback position shown on the timeline. This provides a history of value changes for the signal including quality and connection information in one place. Tool Tip on an event in the timeline: When the mouse is positioned over an Event on the timeline, a tooltip shows information about the event such as the timestamp, value, and quality. Multiple Event Tool Tip: When there are multiple events on the timeline in a short distance, the tool tip shows the events in a chart. Creating snapshots The Current Capture or a portion of it may be saved for later reference as a Snapshot. A Snapshot does not have to be displayed on the same diagram it was captured on. Selecting a previously saved Snapshot from the dropdown will show events for Data Objects that are part of the currently selected Diagram on the timeline. In a similar manor, selecting a different Diagram from the dropdown will show events from the currently selected Snapshot on the timeline provided the signals captured in the snapshot are part of the diagram being displayed. The following is a step by step process to capture a snapshot from the Current Capture or another previously saved snapshot: Select the Current Capture or a previously saved snapshot from the Snapshot Menu. Choose a Start Time for the snapshot by entering an appropriate offset in the Start Offset box (found at the left side of the timeline). Choose an End Time for the snapshot by entering an appropriate offset in the End Offset box (found at the right side of the timeline). 4.Createthesnapshot. Therearemultiplewaystocreateasnapshotofasignalflowrecording: a.Creatingasnapshotfromthemenuoption: b.FromthecontextmenuofthePlayBackControlsArea: 2021TriangleMicroWorks,Inc.Allrightsreserved. View and Manage Snapshots Once a snapshot is created, it is saved as part of the workspace. This can be viewed by selecting from the combo box for snapshots on the top-left of the Signal Flow display: A few things to note in regards to the above image: Current Capture: This selection has a specific behavior based on whether any data was recorded in the diagram: If there is no recorded data, the Current Capture represents the current state of signals in the device. Any changes in the cor- responding values and signals are then updated and displayed automatically in the diagram. If data was recorded, the diagram retains and displays the recorded data. It no longer represents the current state of the device. Snapshot: Snapshots are user created and show events recorded in the past. The nodes and links show the values at the playback pos- ition and do not reflect the current values in the devices. Snapshot Management: The buttons to delete and edit snapshots are inline in the snapshot dropdown menu. Please note that the currently selected snapshot cannot be modified. Menu Options The Signal Flow has a menu with options to manage the display: New: Creates a new Signal Flow Diagram Duplicate: Copies the current diagram (not the data) and creates a duplicate Lock/Unlock: Locks/Unlocks modifications to the current diagram Clear: Clears either the data or the signals in the diagram Rename: Renames the current diagram Save: Saves the current diagram Delete: Deletes the current diagram. Send Signals: Sends signals in the current diagram to other displays Import: Imports a previously exported Signal Diagram Export: Exports a Signal Diagram to a CSV file Create Snapshot: Creates a snapshot of the events in the selected bounds Other Options Show All Events: This action is available as a context menu in the Playback controls section. It will show all the events captured as part of the recording by adjusting the bounds of the timeline. Zoom In: This action is also available as a context menu in the Playback controls section. It will spread out the events currently in range as much as possible by adjusting the bounds so that they do not overlap in each other. View Events in Logic Analyzer Using the Logic Analyzer The Logic Analyzer can be used to chart the real time values of up to eight data attributes found in IEDs in the current workspace. Charted values can be saved as part of the workspace for later viewing and can be exported as data files. Data Sources for the Logic Analyzer To view a data attribute in the Logic Analyzer, the attribute value must be retrieved through GOOSE or a report. If the value is retrieved as part of a report, the timestamp of the data change must be included in the report. The Logic Analyzer charts attribute values against time based on timestamps received in reported data from IEDs and GOOSE headers from publishers. The time plotted in the chart is not the local PC time at which the signal was received by the application. They are the timestamps from the signals received from the IED (GOOSE/Report). Adding Data Objects and Data Attributes to the Logic Analyzer 1) Select the Data Miner tool on the left side and select data objects 2) Select the Custom Display on the right side 3) Drag data objects from the Data Miner onto the Logic Analyzer. The data objects will be deconstructed into data attributes which will be added to the Signal Path list. 4) The first eight attributes added to the Signal Path list will automatically have a chart created. 5) The value and timestamp (and quality if available) for the data attribute is retrieved using GOOSE or reports based on how the Data Retrieval Method is set. Note that adding polled data attributes will be reported as an error. This Figure shows the POS data object for a CSWI being dragged onto the Logic Analyzer. The object can be dropped onto the chart area or on the Signal Path list. After the object is dropped, its corresponding stVal attribute is added to the Signal Path list and a chart is created for the value. Test Suite Pro will open a connection to the IED and subscribe to the appropriate control blocks to prepare to acquire data for the attribute. A large number of signal paths can be added to the Logic Analyzer, however only eight charts can be displayed at any time. The check boxes in the column labeled Graph? determine which paths are charted. When data acquisition is started, data is collected for all attributes in the Signal Path list, not just the ones that are currently charted. Because of this, charts for any of the signals can be reviewed at any time by clicking the Graph? checkbox to view the data even after data collection is stopped. Acquiring Data To begin collecting data for the attributes in the Signal Path list, click on the red Start Recording button in the Logic Analyzer control bar. When acquisition begins, the control bar will turn green and the Start Time and Time Elapsed values become active. The figure above shows the Logic Analyzer collecting data for three data attributes. In this example, the top two attributes experienced a state change about 30 seconds after the data collection began. Cursors The orange, vertical line running across the three charts is a cursor. The cursor can be moved with the mouse and can be turned off with the orange hand icon next to the Start Recording button. A second cursor can be turned on by clicking on the second cursor icon. When both cursors are turned on, the time delta between the cursor positions is displayed in the control bar below the elapsed time value. Interacting with the Charts While data is being collected, by default the new values will be displayed on the right side of the charts. This is called Autoscrolling. This behavior can be stopped by unchecking the Autoscoll checkbox in the control bar or by moving the chart with mouse which will disable the Autoscroll function. Autoscrolling can be started again by clicking the Autoscroll checkbox. The number of seconds of data shown in the window is controlled by the Data Window slider. By default, 30 seconds of data is shown. The Data Window slider only works while Auto- scroll is enabled. When Autoscroll is disabled, the zoom function controls the amount of data shown in the window. The charts can be zoomed by using the scroll wheel on a mouse or by pressing the + and - keys. Additional functions can be found on the context menu for the charts. Right clicking on a chart will show the context menu. The Find Next/Previous Event menu items will reposition the charts so that the next/previous change in the attributes value is centered in the display. The Move Up/Down menu items will reorder the charts on the screen. Archiving and Recalling Chart Sets The current set of charts displayed in the Logic Analyzer as well as the data collected for all of the attributes in the Signal Path list can be saved as a named chart set. To archive this set of data, select the Archive Chart Set from the Logic Analyzer menu. Then provide a unique name for the chart set and click the check mark to accept it. The new name of the chart set will be displayed at the top of the Logic Analyzer display in a drop down box. This box is used to select the archived chart set to view. When viewing an archived chart set, most of the tools are available to manipulate the chart such as zooming and panning however the Start Recording and Autoscroll functions are disabled. To prepare to start collecting new data, select the item Record New Chart Set from the drop down list. This will reset the Logic Analyzer and allow the Start Recording button to work. The Signal Path list is prepopulated with the attributes that were present in the last Archived Chart Set. If these attributes are not correct, delete individual attributes by selecting them then right clicking and selecting Delete Selected Signals or delete all of the attributes by clicking the Clear All button at the top of the Signal Path list. Exporting Data The values plotted on the Logic Analyzer can be exported to a file using a comma separated value (CSV) format. The Export functions are found on the Logic Analyzer menu and are only accessible when data has been collected and the Logic Analyzer is not currently collecting data. There are two options for exporting data. The data for all attributes in the Signal Path list can be exported or the data for just the attributes currently displayed in charts can be exported. In either case, Test Suite Pro will request the user to identify a file name and directory location for the export file. By default, the file will be given a file extension of .csv if no file extension is provided as part of the file name. The first line of the exported file contains column headers which describe the meaning of the values in each of the following rows. Understanding the Charts The Logic Analyzer charts are designed to show the value of attributes in an object model as they change over time. The values are those received by the IEC 61850 client running within Test Suite Pro. The client in Test Suite Pro subscribes to reports and GOOSE as need to receive the attributes requested by the user. Charts are organized into charts sets that contain an unlimited number of attribute paths for which up to eight charts can be plotted at a time. When an attribute is charted, the values collected for the attribute are displayed from the time the Start Recording button was clicked to the time the Stop Recording button was clicked. The X axis on each chart represents the number of milliseconds that passed from the time that the user clicked the Start Recording but- ton until the timestamp included with the received data value. This means that the PC on which Test Suite Pro is running should be time synchronized with the network on which the IEDs are running. Since the data is plotted using timestamp values, all relative meas- urements between events will be accurate even if the PC is not synchronized; however, an unsynchronized PC could result in values being missed if the local PC clock is well ahead of the network clock since the charts will not plot values that have timestamps that occurred before the time that the user clicked the Start Recording button. Each chart clearly shows the full path of the data attribute that is being plotted. At the end of the path, the source of the data is listed in parenthesis as Report or GOOSE. If the data is coming from Reports, then the timestamps are read from the dataset in the report. Attribute values can only be charted in the Logic Analyzer if the dataset includes the timestamp and the quality of the signal. If the data is coming from GOOSE, then the timestamp will be read from the dataset if it is available. If the timestamp is not in the dataset, then the timestamp from the header of the GOOSE message will be used. A number of line styles are used to plot values on the charts. A solid blue line indicates that a value has been received with a good qual- ity indicator. A solid yellow line indicates that a value has been received with a quality indicator other than good. The cursor will display the quality values for a yellow line. A dotted line indicates that a value has been received in the past and no new information has been received; therefore, the value is assumed to be saved as the last received value. When a new Report or GOOSE value is received, the dotted line is replaced with a solid line indicating the value is now confirmed. Continuity of data is maintained in GOOSE messages with a state change number which is part of the GOOSE header. The continuity of data in buffered reports is indicated by a buffer overflow flag which is in the report header. If Test Suite Pro receives GOOSE data where the state change number is not properly incremented or receives report data with the buffer overflow flag set, the new data values are shown on the graph but a gap in the line (no line drawn) will appear where the missing data occurred. Custom Display Introduction The Custom Display can be used to visualize and monitor the data in a Substation. Each Tile represents a Data Object. Customized Tiles have been designed for each type of Data Object. These Data Objects can be stacked on top of each other to create displays much like a Single Line diagram as shown below: A predefined list of connectors and elements normally used in an electrical scheme is available in the palette on the right. Data in the Custom Display is acquired via either GOOSE/Report/Polling. Designing a Custom Display Add a New Display Add Tiles to the Display Tiles can be added to the Custom Display by dragging Data Objects from the Data Miner as shown below: The Data Retrieval section determines the method that the Tile in the Custom Display should use to retrieve the data for the Data Object being dragged. The precedence of the methods is in the order of appearance (i.e. GOOSE/Report/Polling) in case multiple meth- ods are selected. Add Symbols from the Palette to the Display Symbols used in building a Single Line Diagram are available in the palette on the right of the Custom Display. These symbols can be dragged on to the display as show below: Types of Tiles General Tile Switch (Closed) (DPC - Double point) Switch(Open) SwitchasBreaker SwitchasArrow 2021TriangleMicroWorks,Inc.Allrightsreserved. SwitchasTextOnly SwitchasBusbar+ 2021TriangleMicroWorks,Inc.Allrightsreserved. Switch as Busbar T Breaker (On) (DPC - Double point) Breaker (OFF) Single Point Status (SPS, SPC) Trip Counter (ACT, ACD) Tile Context Menu Show Event History: Displays all the events received so far. Reset Event History Clears all the events captured so far. Reset Trip Counter Resets the trips counted. Other Tile Indicators Enumerated Status (ENS, ENC, INC, INS, VSS, DPS) Measurand (WYE) Quality Indicators on the Tile Data Retrieval from the Server By GOOSE *Simulated GOOSE By Report (MMS) By Polling the Server (MMS) Changing the Data Retrieval Method The data retrieval method of the Tile can be changed by using the context menu as shown below: The available methods are shown in the sub-menu of the Data Acquisition context menu option. The user has the option to select the method needed. Offline Tiles Interacting with Tiles Select a Tile: Left click the Tile. Move a Tile: Select a Tile. Then click and hold a Tile while dragging it to a different location. Pan the Display: Left click anywhere on the canvas and drag in a desired direction. Delete a Tile: Select a Tile. Then use the 'Delete' key or right-click and delete from the context menu. Copy a Tile: Using the Keyboard: Select a Tile. Use key combination CTRL + C to copy. On the canvas, hover the cursor where you want the Tile to be placed. Use key combination CTRL + V to paste the Tile. Using the Mouse: Right click on a Tile to open the context menu. Use the 'Copy' option in the context menu to copy the Tile. Right click anywhere in the canvas where you need the Tile to be copied to. In the context menu select the option 'Paste' to paste the Tile. Zoom In/Out: Using the Keyboard: Click anywhere on the display to bring it into focus (if it is not already focused). Use the key combination CTRL and +/ - to zoom in or out. Using the Mouse: Click anywhere on the display to bring it into focus (if it is not already focused). Mouse wheel up or down will zoom in or out the display. Tool Tip Details Each Tile represents a Data Object. The tool tip on each Tile provides more information about the server and the hierarchical elements in the model from which the data is being captured. Below are the details displayed in the tool tip: IED: Name of the IED LD: Name of the Logical Device Prefix: Name of the Logical Node Prefix LNode: The path of the Logical Node in the Substation section of the SCL file Beh: Beh of the Logical Node Method: The current type of subscription to capture data from the server Value: The value on the important attributes in the Data Object Time: The time stamp of the change in the value Quality: The quality of the value Control Block: The Goose/Report Control Block of the subscription to receive data for the Tile In the case of a complex Data Object that contains further Data Objects as children, the tool tip provides information for each Data Object as shown below: MenuOptions NewDisplay CreatesanewCustomDisplay. CopyDisplay Duplicatesthecurrentdisplay. 2021TriangleMicroWorks,Inc.Allrightsreserved. Clear Display Deletes all the Tiles in the current display. Rename Display Changes the name of the current display. Hide Grid Lines and Labels Hides the grid lines and the IEC 61850 specific labels in the Tiles of the Custom Display. Delete Display Deletes the current display. Send Signals Send signals to other displays. Status Values Send all status value attributes to the selected display GOOSE Signals Send all attributes whose values are acquired via GOOSE Selected Signals Send the attributes from the selected Tiles All Signals Send all the attributes in the current display Clear All Event History Clear all the events captured from all the Tiles. Reset All Trip Counters Reset all the trip counters from all the Tiles in the current display. View SCL Files You can view SCL Files in the SCL Viewer display Select the SCL File 1) Select the SCL Verify tool 2) Choose which SCL file to check. This can be a file from the current workspace or you can browse to select a file from disk. Viewing SCL File The loaded SCL File will appear in the SCL Viewer: View Reported Events Report messages can be viewed in the Report Viewer display. Any Report Control Block that has been enabled from the Advanced Client or the Data Miner will be displayed here. Each row in the left side represents one Report message. The columns contain the header information sent with each Report message. Choosing one of these messages will cause the data to be shown in the right pane. Data for multiple messages can be shown by checking multiple check boxes in the Report message pane as shown below: 3.7TestSequencer Introduction ThisintroductionisintendedtoprovideanoverallpictureofhowtheTestSequenceroperates.TheTestSequencerisatoolthatisusedtobuildrepeatable61850commandandtestingsequences.ATestiscomprisedofsequentialSteps.EachStepiscomposedofoneormorecommands.Testsareexecutedandresultsareprovidedtotheuser.TheTestSequencerprovidesinstantresultsaboutcommands, Steps,andtheoverallTest. Components TestSequencercontainsthefollowingcomponents: 1.TestBuilder-avisualdisplayofTestStepsandtheTestSequence. 2.TestExplorer-providesaviewofalltheTestsinaworkspaceandorganizesthemintoTestPlansandTestGroups. 3.Commands-repositorythatprovidespredefinedcommandsthatareindependentofdatapoints. 4.ExecutionLog-arunninglogofalltheoperationsthatarebeingexecutedandtheresponsesreceivedfromthe61850server. 5.StepDetails-providesasummaryofallthecommandsthataregoingtoberunaspartoftheTest. 2021TriangleMicroWorks,Inc.Allrightsreserved. Tests Creating a New Test A new Test can be created from the Test Builder menu, in the top left corner: Tests can also be created by using the context menu in Test Explorer. The context menu on a Test Plan gives the options to create another Test Plan, Test Group, or individual Test: The context menu on a Test Group has the option to create an individual Test: Building a Test There are two ways to build a Test: Create Steps by dragging and dropping data points from the Data Miner to either the Test Builder display or Step Details pane as shown in the image below. Send a snapshot from the Signal Flow Timeline to the Test Sequencer. Steps A Step is a logical group of commands in a Test. A Step is visually represented in the Test Builder by this symbol: Important Notes Steps are executed sequentially. Commands in a Step are executed sequentially. The result of a Step relies on the results of the commands it contains (if a command fails, the Step will fail). If a Step fails, all subsequent Steps are canceled and the Test fails. How to Create a Step As a point is dropped onto the Test Sequencer, it creates a Step. A Step configuration dialog will pop up as shown below: Within this dialog, a command is created for that specific data point. It is important to note that each Step can have one or more com- mands using the same or different data points. To create multiple commands, simply drag another data point onto that Step. Types of Steps The following are the different types of Steps that get created depending on the commands being used. Execution Step Created when data points are dragged from the Data Miner. The Step commands correspond to the points dragged. Execution Steps can perform the following commands (types): Write Select Operate Select and Operate Cancel Verify Data *Each of these commands are explained in detail in the Types of Commands section. Composite Step Created when a predefined command is dragged from the Commands pane. These commands are typically Connection or Control Block related. Setup Step A Setup Step is also a Composite Step but is created automatically when other Steps are created. It contains the prerequisites for the Steps created. The Setup Step is the first Step to be executed in a Test Sequence. It also provides the ability to Auto Cleanup the actions performed on devices after the Test is completed. Auto Cleanup is an undo operation of the commands that were performed as part of Setup. For example, a Connect command will then Disconnect once the Test is complete. Types of Commands A command type can vary depending on the type of Step that was created. This section will cover the command types in detail. Common Fields There are 5 common fields among commands: Order The order in which the commands in the step are executed. Path The path of the 61850 point. Expected Value The expected value or response from a server. Actual Value The actual value received from the server as a response to a request or by Read/GOOSE/Report. Delay The time in milliseconds to wait before executing the command. For the Verify command, an event will not be skipped because it occurred during the delay period. Setup Timeout Time to wait for the initial value for this Data Attribute during the Setup Step. *Required Fields on any command, required fields will be highlighted in orange. Execution Step commands Execution Steps have 6 command types: Write Select Operate Select and Operate Cancel Verify Write Command to write a value to the data point. Control Parameters There are 5 types of control parameters used across all Control Command Types (Select, Operate, Select and Operate, Cancel): CtlValue: Value to be sent to the control Check OrCat OrIdent IsTest TIP: Except for CtlValue, all parameters are default settings in Test Suite Pro. These settings are configured in the Options dialog (File > Options) as shown below: Select Selects the control point. To be able to create this command for a point, make sure the control model that is not read-only. Operate Operates the control point. Select and Operate Selects and then operates the control point. Cancel Cancels the control point. Verify This detailed command is used to validate and verify data. Expected Value The expected value. Validate Order of Events Enable to validate the order in which events are received. Operator The operator used for comparison of the Expected and Actual values. See below for operators and their individual functions: Equals[Actual == Expected] Not Equals[Actual != Expected] Greater ThanActual > ExpectedValid only on numeric valuesLess ThanActual < ExpectedValid only on numeric valuesContainsActual contains ExpectedValid only on numeric valuesIn BetweenMax > Actual > MinValid only on numeric valuesGreater Than or Equal ToActual >= ExpectedValid only on numeric valuesLess Than or Equal ToActual <= ExpectedValid only on numeric values Max If the operator selected is in between, this is the maximum accepted actual value. Min If the operator selected is in between, this is the minimum accepted actual value. Data Acquisition This parameter defines how the data is acquired from the server: READ Reads a data attribute from the server GOOSE Subscribes to GOOSE and monitors State Changes REPORT Subscribes to REPORTs and monitors State Changes Trigger Options to define what is considered a change to the record: State Change A value is recorded only if it is a state change. Initial values are ignored for test evaluation. Existing State The current state of the data attribute is used for test evaluation. Initial values are used in case there are no state changes. Control Block Set the control block used to enable/subscribe to get the Report/GOOSE. Event time (UTC) This is relative to how we are receiving the data as described below: If the Data Acquisition is READ, this is the time when the server responded with the data being requested for. If the Data Acquisition is GOOSE, this is the time on the header of the GOOSE message. If the timestamp is not available on the header of the GOOSE message, then this is the time stamp associated with the data attribute. If the Data Acquisition is REPORT, this is the time stamp associated with the data attribute. Predefined Commands Predefined Commands can be found in the Commands pane (click tab at the bottom of Test Explorer): Connect command Disconnect command Enable Report command Disable Report command Enable GOOSE command Disable GOOSE command Subscribe to GOOSE command Unsubscribe from Goose command User Action A Step that creates specific instructions (text only) to be carried out by the user before continuing with the test. For example, if manual intervention is required as part of the test, using this Step will stop the Test Sequence until the pop-up resulting from the Step is acknow- ledged. Step Details - view and edit Group and Filter Steps Steps and their commands are displayed in the Step Details pane as shown below: Use the drop-down menu at the top of the Step Details pane to change the view and grouping of Steps. In the image below, the Step Details pane has been filtered to only show commands of the selected Step by using the filter Selected Step. Edit options There are two ways to edit the commands on a step: Double-click the Step in the Test Builder pane. This will open the Step configuration dialogue with a list of all Step commands and their respective configurations. Right-click the Step or any of its commands in the Step Details pane and then select Edit from the context menu. You can also double-click a Step in this pane to edit the Step and commands. Context Menu Options Throughout Test Sequencer (and TSP for that matter), most objects have context menus. The following section provides more inform- ation about these menus and their options. Test builder Copy/Paste: Used to copy and paste a Step. A copied Step is added to the end of the Test. Move Forward: Used to move a Step forward in the order of execution. Move Backward: Used to move the Step backward in the order of execution. Edit: Used to view/edit Step configuration. Split: Used to split the commands in a Step into two Steps. Note that the created Step from the Split will appear immediately after the selected Step. Merge: Used to merge selected commands in two or more steps into a single step. Use CTRL + Select multiple Steps (must be of the same Type). Align: Used to align two or more Steps relative to each other. Delete: Used to delete a step. Step Details Pane Edit: Used to view/edit the Step configuration. Move Up: Used to move a command up one row in the execution order of Step. Move Down: Used to move a command down one row in the execution order Step. Delete Command: Used to delete a command. Move Step Up: Used to move the Step up in the order of execution. Move Step Down: Used to move the Step down in the order of execution. Step details window Set Expected to Current State: Used to set the expected value to the current value of the data attribute in the server Bulk Edit: Used to edit one or more fields in one or more commands 2021TriangleMicroWorks,Inc.Allrightsreserved. Move Up: Used to move the command up in the order of execution/validation. Move Down: Used to move the command down in the order of execution/validation. Copy: Used to copy a command. Paste: Used to paste a command . Delete: Used to delete a command. Test Execution To execute a Test, use one of the following methods in Test Explorer: Execute button - located in the Test Explorer pane as shown below. This can be used to run all of the tests in the workspace: Context Menu by right-clicking on selected Test(s) in the Test Explorer window, multiple Tests can be executed: TIP: Executed Tests run in the background, allowing users to continue working while they are in progress. The only exception to this is when a User Action command is part of the Test. Reviewing Results Test results can be viewed using the following methods: Test Builder When a Test is executed, the status of each Step is represented in the diagram as shown below: 2021TriangleMicroWorks,Inc.Allrightsreserved. Step Color Status Green Passed Red Failed/Fault Gold Cancelled Light Yellow In Progress Purple Scheduled Test Explorer The Test Explorer shows the overall status of the Test Plan and the contained Test Groups and Tests: Step results More details about the results of Test Step can be found in the Step dialog, by either double-clicking a Step or by double-clicking a com- mand in the Step Details Pane. Test results are displayed and illustrated on the Step dialog image below: Results can also be viewed in the Step Details pane: Tool tip on the Verify Command: All events and changes on a data point are shown via a tool tip, including the timestamps. The event selected to evaluate the command is highlighted. Exporting Results The results of a Test can be exported to Microsoft Excel using the context menu option in the Test Explorer pane as shown below: A Test Results dialog shows a summary of the results prior to the export: Execution Log As commands in a Test are executed, results are also logged in the Execution Log pane: For easy viewing, filters can be applied to the Execution Log: Selected Step this filter shows logs for the Step selected in the display: Selected Command this filter shows the logs for the selected command in the Step Details pane: Test Explorer The Test Explorer is a tree representation of the tests as shown in the image below: Clicking on a Test in the Test Explorer displays the Test details and Steps in Test Sequencer. Creating a Test Tests can be created from the context menu in Test Explorer. The context menu on a Test Plan gives the options to create a Test Plan, Test Group, or individual Test: The context menu on a Test Group has the option to create an individual Test: A Test can also be created from the Test Builder menu, in the top left corner: Organizing Tests Within Test Explorer, Tests can be organized by placing them into Test Groups and Test Plans. This section will discuss each. Test Plan A Test Plan contains a collection of Test and/or Test Groups used together (e.g. to test an area of the substation). The Test Plan is the highest element that contains Test Groups and individual Tests. A Test can be created under an existing Test Plan or a new one as shown below: Test Group When there multiple Tests in each Test Plan, a Test Group is a good way to better organize them. A Test Group provides easier nav- igation and structure. Like a Test Plan, a Test can be assigned to a Test Group while creating it: Megger Testing Introduction Commands can be issued to the SMRT class Megger devices as part of a test in the Test Sequencer. The Megger command is built into the Test Sequencer as shown below: Types of Megger Commands Add a command to a test by dragging it from the Add New Action pane. TSP offers two ways of issuing commands to set the necessary values in the Megger; Front Panel and Command. Front Panel The Front Panel view can be used to issue pre-configured commands to set the required values on the device. Sample Execution result: Command This mode can be used to send an individual command to the device. Sample Execution Result: Other Options Test device connection The device connection can be tested by using the below option: Scan devices on the network If not sure of the IP Address of the device, use the option to scan the devices on the network. Depending on how the network subnet is configured, this might take some time to find the device. View a List of Object Data The Data Monitor display is used to create a custom table of data from multiple IEDs. 1) Go to the Data Monitor display 2) Select data objects in the Data Miner 3) Drag the objects to the Data Monitor The tool will automatically retrieve data from the IEDs using the selected Data Retrieval Methods at the top. Server Data Simulation Introduction TSP has the capability of simulating data changes on a simulated server. This is available as a display as shown in the image below: Create Auto Data Changes Attributes can be added by dragging them over from the IED Simulator tool. Data Changes can be simulated only on attributes with the functional constraint MX | ST | SP. Attributes from different servers can be added just like any of the other displays. Attributes can also be added by using the Add Points dialog as shown below: Then sorted by the options of Functional Constraint: Or Dataset: Configuring Auto Data Changes Simulation settings are described in the screen shot below: Max: The max value that will be assigned to the data attribute Min: The minimum value that will be assigned to the data attribute Step: The value that the attribute will be incremented or decremented by while simulating data changes Interval(ms): Minimum time in milliseconds between consecutive data changes Saving your setup The points being simulated are saved within the workspace so that the simulation can be quickly restored later. Auto Data Change Playback Start Data Changes: To Start simulating data changes, click on the Play button: Stop Data Changes: To stop simulations, click the Stop button: Manually Change Data Values You can also change individual data attributes manually in the data model: Start the simulated IED in the IED Simulator tool Find the data you want to change in the data model Click in the value column, change the value, and press enter Example: In the example above, a report will be generated by the IED when the value is changed if the data attribute T1MMXU1.PhV.phsA.cVal.mag.f is included in a dataset for a report control block View GOOSE Events GOOSE messages can be viewed in the GOOSE Event Viewer display. Any GOOSE Control Block that has been subscribed to (either from the Advanced Client, Data Miner, or GOOSE Tracker) will be displayed in the GOOSE Event Viewer. Left Pane GOOSE event messages from multiple IEDs are shown in the pane on the left. You can filter or group messages by any column including Connection Name (IED) or GOOSE ID or any other parameter in the GOOSE header. Each row on the left side represents a GOOSE state change event message that has been received. The columns show the header inform- ation sent in the GOOSE message. Note: a GOOSE state change event is caused when either the data or quality changes for an object in the GOOSE dataset. Right Pane Choosing one of the GOOSE event messages (on the left side) will show the message data in the right pane. Data for multiple messages can be shown by selecting multiple check boxes on the left side as shown below. Note: only the first GOOSE state change event message is shown in the GOOSE Event Viewer. All re-transmission GOOSE mes- sages (after the state change event) are filtered out and are not shown in this view. 4.0UseCases 4.1TestingIEDsIEDConnections TestingIEDs IEDConnections ConnecttoIEDs TherearetwostepstoestablishaClient/ServerMMSconnectiontoanIED: 1)CreateorOpenaWorkspace CreateanewworkspacetofindIEDsbyusingSCLfilesorIPaddressesOR Openanexistingworkspacetoloadasavedconfiguration 2)ManuallyConnecttoIEDs 1.GototheAdvancedClienttool 2.SelecttheIEDfromthedropdownmenu 3.PresstheConnectbutton 2021TriangleMicroWorks,Inc.Allrightsreserved. Note:Iftheconnectiondoesnotwork,readtheIEDConnectionConfigurationtopicformoreinformationaboutIEDconfigurationparameters. 3)AutomaticIEDConnections ConnectionsareautomaticallycreatedasneededtoretrievedatafromIEDs.Belowaresomeoftheactionsthatmayautomaticallycre- ateaconnection: l DraggingaDataObjectfromtheDataMinertotheCustomDisplay l DraggingaDataObjectfromtheDataMinertotheGroupDisplay l DraggingaDataObjectfromtheDataMinertotheDataMonitor Example:whentheobjectisaddedtothecustomdisplay,aconnectiontotheIEDisestablishedbecausethedataretrievalmethodissettoReportandPolled(bothClient/Serverservices).However,iftheretrievalmethodwassettoGOOSEandthedatawasavailablefromaGOOSEcontrolblock,thenaconnectionwouldnotmade.Instead,thetoolwouldsubscribetotheGOOSEcontrolblockthatincludesthedataandwouldnotneedaClient/Serverconnection. 2021TriangleMicroWorks,Inc.Allrightsreserved. 4) Discover an IED Data Model To discover a data model from an IED: 1) Go to the creating a workspace topic 2) Find the section "Find IEDs by IP Address" (this will create a workspace by discovering data models from a set of IEDs) 3) Go to the Advanced Client to view the complete data model from the IED View IED Connection Status Connections are made to an IED when the user performs an operation that requires data from the IED or a connection can be made manu- ally by the user. Reasons that a connection to an IED will be made include (but may not be limited to): Dragging a Data Object from the Data Miner onto the Custom Display Dragging a Data Object from the Data Miner onto the Group Display Dragging a Data Object from the Data Miner onto the Data Monitor Manually connecting to IED from the Advanced Client Note: when a workspace is opened, 61850 Test Suite Pro only makes connections to IEDs that were previously connected when the workspace was saved. If the workspace is new, then no IEDs will be connected yet. Once a connection is established with an IED, the connection will be open until data is no longer needed from the IED or until the work- space is closed. The Workspace Tool provides a column that indicates if a connection is currently open to an IED. The Workspace Tool also shows the following status for each IED in the current workspace: The number of reports are enabled The number of GOOSE subscriptions The number of reports received The number of GOOSE events received The number of GOOSE timeouts Whether the model was acquired through discovery If the model was discovered, the date it was discovered View IED Data Model Advanced Client The entire data model for an IED can be viewed in the Advanced Client tool. From the Data Miner You can go from the Data Miner to the Advanced Client by right clicking on a data object. Note: This will open the Advanced Client and take you to where the data object is located in the data model. IED Connection Configuration Connecting to an IEDs server requires Test Suite Pro to internally create an IEC 61850 client. The client is created with a default set of communication parameters, however those parameters can be modified to match the particular requirements of the server. To access the clients communication parameters, edit the workspace and right click on an IEDs icon. Click the option Edit Properties for this IED on the context menu. IED Connection Properties The top of the window shows the IED name, source and connection name: IED Name - name of the IED from the IEC 61850 data model IED Source - location of file used to create the IEDs model. If the IED was located using an IP search (reading the model from the IED), then the model was derived using discovery techniques and the location of the file is maintained internally to the application (indicated with the ~ symbol as shown in the example below.) Connection Name - By default, this is the same as the IED Name. The Connection Name can be changed to a more meaningful name if the data model name is not helpful. Below this there are two tabs: How to Connect - The first tab holds parameters that control how the local client will connect to the server. How to Simulate - contains configuration parameters for simulating the IED. How To Connect Tab On the How To Connect tab, the connection parameters are divided into three areas. The first area has configuration settings for the IED's server The second area configures the local client (in 61850 Test Suite Pro) to communicate with the IED's server The third area provides information about security setting on the IEDs server The simple configuration shows just an IP address but there are more advanced parameters that are accessible by clicking the "Show Advanced" button. IED's Server Settings The advanced settings for the IEDs server shows available settings to control the stack. The listening port can also be changed from 102 although 102 is the mandatory MMS setting. Local Client Settings The advanced settings for the local client also allow for changing the stack settings but also include selections for GOOSE and Sampled Values adapters as well as settings for controlling the MMS configuration. The default selection for both the GOOSE and Sampled Value adapter is to use the same adapter as the MMS stack uses. The MMS stack uses whichever adapter is configured to support the Local IP Address. The drop down selection for the Local IP Address will only show IP addresses which are supported by the adapters found on the PC. If the Local IP Address is set to 0.0.0.0, then the adapter selected is based on the IP of the server. In that case, Test Suite Pro will attempt to find the network adapter that is able to communicate with the IP address of the server. Security Settings The third section of the dialog box describes the security settings on the server which can be none, password based, or certificate base. If the server uses a password, the only value that must be provided in order for the local client to connect is the weak password. If the server uses certificates, then the following parameters are made available. Options are presented for configuring the client to use a certificate authority*, for configuring MMS certificates, and for encrypting using TLS and TLS RSA security. *To use Triangle MicroWorks sample Certificate Authority, right-click the words "Certificate Authority" and select the option "Use TMW Sample Security": Testing DataSets Testing DataSets What are DataSets? A dataset is an ordered group of references to Data Objects and/or Data Attributes contained in an IEC 61850 data model. These data are organized for the application requirements and to optimize communications. The list of members and their order is known to client and server or publisher and subscriber so that only the name of the dataset and the list of values must be transmitted. How to Test DataSets DataSets can be tested from the Advanced Client. DataSets are used by Report Control Blocks, GOOSE Control Blocks, Log Control Blocks, and Sampled Value Control Blocks. DataSet Test Options Read Reading or polling of a DataSet may be done by selecting a DataSet or all DataSets and selecting Read/Start Polling. If no errors appear in the Error List or Output windows then the DataSet definition between the client and IED server Match. Verify DataSet To confirm that the IED servers definition of a DataSet is the same as the client's definition of the same DataSet. Select a DataSet and press Verify. This will do a discovery of the DataSet in the IED server and compare against the local definition of the DataSet in the client. Update Dataset This performs a discovery of the IED servers DataSets and synchs it with the local copy. This can be useful for updating dynamic DataSets created by other clients. Deleting Datasets Deleting can be done by selecting a DataSet and pressing delete. Note: Only dynamic DataSets can be deleted. The IED server should reject deleting any fixed DataSet (fixed DataSets are the ones defined in the SCL File). Creating Dynamic DataSets Select any Logical Node or DataSet container and choose Create DataSet to open the DataSet editor window. Choose which data object or data attribute members you would like in the DataSet. A preview of the DataSet is shown on the right as you select members. Selecting OK sends the create DataSet command to the IED server. The newly created dynamic DataSet is shown in the data model and can be read, verified, updated, or deleted. Note: Only persistent dynamic DataSets can be created (persistent dynamic DataSets can be seen by multiple clients). Creating non-persistent dynamic DataSets is not supported at this time. Testing Controls Testing Controls What are Controls? IEC 61850-7-2 provides specific state machines to allow changing of specific internal and external data attributes by a client. Five control models are provided with the choice of direct or select before operate and normal or enhanced security. Also, a status only model is allowed. Select before operate requires two commands to be sent before the operate a select allows to validate the control parameters and reserve the resource. Enhanced security requires additional information in the request to validate the sender, and provides and additional asynchronous response to verify the operation. Send Controls to an IED Testing control operation is done from the Advanced Client. All output/error information will be in the Output/Error List window. Select, Operate, and Cancel are all available from the right click context menu and the toolbar menu. There are two ways to activate Select, Operate, and Cancel: Option 1: right-click on the object in the data model Option 2: use the buttons on the ribbon bar. Option 1: Right-Click on Control Object 1) Go to the Advanced Client and find the control object (or go to Data Miner and find control object and then right-click -> "Open in Advanced Client") 2) Right-click a control object and choose 'Select' if the ctlModel parameter has 'sbo' (select before operate) in the value 3) Fill in the control parameters in the following dialog. This will be used for the Select, Operate, or Cancel operation. 4) To operate the control: right-click and select 'Operate' on the same control. For an 'sbo' type ctlModel, the same control parameters for both the Operate and Select must be used to successfully work the control. 5) To cancel the control: right-click and select 'Cancel' on the same control. Use the same control parameters for the Cancel as what was used for the Select. Note: The exact same parameters must be entered for Select/Operate/Control in order to successfully work the control. Other- wise, the control request will be rejected. Note: In the case of Select before Operate (without enhanced security), the Select command will do a read of the SBO node which is all that is required to put the control in the selected state. Tip: If Select is required for an 'sbo' type ctlModel make sure sboTimeout is a non zero value. sboTimeout controls how long the control will stay in the selected state waiting for the operate or cancel command. Option 2: Use Buttons in Ribbon Bar You can perform the same control requests with the Select, Operate, and Cancel buttons in the ribbon bar 1) Go to the Advanced Client and find the control object (or go to Data Miner and find control object and then right-click -> "Open in Advanced Client") 2) Press 'Select' button if the ctlModel parameter has 'sbo' (select before operate) in the value 3) Fill in the control parameters in the following dialog. This will be used for the Select, Operate, or Cancel operation. 4) To operate the control: right-click and select 'Operate' on the same control. For an 'sbo' type ctlModel, the same control parameters for both the Operate and Select must be used to successfully work the control. 5) To cancel the control: right-click and select 'Cancel' on the same control. Use the same control parameters for the Cancel as what was used for the Select. Note: The exact same parameters must be entered for Select/Operate/Control in order to successfully work the control. Other- wise, the control request will be rejected. Note: In the case of Select before Operate (without enhanced security), the Select command will do a read of the SBO node which is all that is required to put the control in the selected state. Tip: If Select is required for an 'sbo' type ctlModel make sure sboTimeout is a non zero value. sboTimeout controls how long the control will stay in the selected state waiting for the operate or cancel command. Testing Log Services What are Logs? Logs provide a mechanism for an IEC 61850 server in an IED to maintain Sequence of Events information. They are based on datasets and store information exactly as in reporting, but the transmission is completely different. The client must query the log to get the events. Multiple clients can query the same log, and multiple Log Control Blocks can store events in the log. How to Test Log Services The Log Services tool can test IED logs by querying the log from in different ways. First, select the Log Services tool from the tool selection on the left. Logs are tested one IED at a time by choosing an IED Connection in the upper left hand corner. Upon choosing an IED a connection will be established if one has not already been made. After selecting an IED Connection choose a log in the device to query: In this case BayController1CTRL/LLN0.Q has bee selected. Once selected a Reset Log is performed which goes and retrieves all available entries in the log. The left side shows the header information for each entry in the log. Selecting an entry shows the data contained in the log entry on the right. To monitor the log for changes setup an Auto Refresh of the log by setting a time interval then pressing the Start Refreshing button. Or a manual query can be done by EntryID or by time. The log can be queried from an EntryID. The combo box will be filled in with the available entryIDs. Choose one and press Query. The Server should return all EntryIDs after the one is chosen. The other way to query the log is between two points in time. There are two parameters; After time and Start Time to End Time, all with available times. Once After time is chosen or Start time to End time are chosen, the Query button will be enabled. The Server should return all times subsequent to After time or between Start time to End time depending upon which parameter used. Testing File Services What are File Services? IEC 61850 provides basic file transfer services upload, download, delete, and directory. The directory structure is simplified as no assump- tions about the underlying platform or filesystem are assumed. How to Test File Services There are 4 basic File Services offered by IEC 61850: Directory Listing Ability for a Client to retrieve a directory listing of files on a Server. File Delete Ability for a Client to delete a file residing on the Server File Upload Ability for a Client to upload (or copy) a file to a Server. File Download Ability for a Client to download (or retrieve) a file from a Server. Client File Services Client File Services are exercised one connection at a time. First, go to the File Services tool. Choose a connection by picking one of the available IEDs in your workspace in the upper left hand corner. Once a selection is made a connection will be established automatically. BayController1 is selected below: Once a connection is made a File Services Directory on the current Server directory shown in the edit box is performed. If root is shown then a top level File Services Directory is shown. At any point the Refresh button may be pressed which will cause a File Services Directory to be performed on the directory shown in the edit box. Note: Directories are generally shown ending with a \ however this is server dependent. Clicking on a directory in the files list will navigate to that directory and clicking .. will navigate up one level. Download Service Once a file is selected in the server file listing the download buttons will become active. Pressing this button will bring up a file dialog so you can save the file to the local disk. Delete Service Once a file is selected in the server file listing the delete buttons will become active. Pressing this button will per- manently delete the file from the server. Upload Service To activate the upload service choose a local file by clicking the button under the Local File text box: Once a file is chosen the Destination file name will default to the same file name and the Upload Client File to Server button will be enabled. Simply press the button to start the upload of the file. IED Simulator File Services Test Suite Pro Server has a complete implementation of all File Services by default. Specific configuration of File Services is available via the editing/creating of workspaces. On each specific IED a File Services Root directory can be configured along with flags indicating if the particular File Services should be available when initiated by a Client. These are configured under the How to Simulate tab of the IED Connection Properties dialog. Then under Advanced Settings->FileServices. All File Services are enabled by default. The File Services Root Directory defaults to: C:\Users\Public\Documents\Triangle Microworks\61850 Test Suite Pro\TestSuitePro\FileTransfer Testing Sampled Values Testing Sampled Values What are Sampled Values? Sampled Values is mechanism to publish sampled analog measurements from measurement devices like current transformers (CT) and voltage transformers (VT). Sampled Values messages are published by a device via Ethernet multicast such that they can be sub- scribed by any number of other devices. Unlike GOOSE, Sampled Values are not event driven but instead sample driven. The samples are measured at a specific sampling rate and use a specific dataset. Sampled Values publishers are typically referred to as merging units and subscribers would be some type of IED. Exampled: The 9-2 LE profile for Sampled Values has a sampling rate of 80 or 256 samples/cycle (the cycle could be based on a sys- tem frequency of 50 Hz or 60 Hz). The dataset for 9-2 LE includes measurements for 3 phase and neutral for both CTs and VTs. Sampled Values Tests The Sampled Values Tracker can be used to sniff sampled values streams on the network. SAV Tracker - Sniff Sampled Values on the Network The SAV Tracker can be used to sniff Sampled Values streams on the network: 1) Go to SAV Tracker tool 2) Select the Ethernet Adapter on you PC that will be used 3) Start the tool The tool will then list all the sampled values streams that can be seen on that Ethernet adapter. 4.2TestingSubstationsFindingDatainSubstation TestingSubstations FindingDatainaSubstation ConnectingtoMultipleIEDs 61850TestSuiteProcanconnecttomultipleIEDstofacilitatesubstationleveltesting. TheprocessisexactlythesameasconnectingtoasingleIED. 1)CreateorOpenaWorkspace CreatinganewworkspaceallowsyoutofindIEDsbyusinganSCLfileorusingIPaddressesOpeninganexistingworkspacewillloadasavedconfigurationtocommunicatewithIEDs 2)ConnecttoIEDsintheWorkspace ConnectionsareautomaticallycreatedasdataisneededfromIEDs. WhenyoucreateacustomdisplaywithdataobjectsfrommultipleIEDs,then61850TestSuiteProwillconnecttoeachofthoseIEDsasneededtoretrievethedataobjects. Example:dataobjectsfromthreeIEDshavebeenaddedtothecustomdisplay.ThetoolestablishesaconnectiontoallthreeIEDsandretrievesthedatafromreportsbecausethedataisavailablefromareportandthedataretrievalmethodissettoReported. 2021TriangleMicroWorks,Inc.Allrightsreserved. Using the Data Miner The Data Miner is a powerful tool to find the data you need across multiple IEDs. Each row in the Data Miner is a data object in an IED Each column can be filtered, grouped, or sorted by value Any data objects can be viewed in a Custom Display, in a table view called the Data Monitor, or in the Advanced Client tool. Note: The Data Miner shows data objects across all of the IEDs in the workspace. Data objects are complex objects that contain multiple data attributes. For example, the data object Pos in the example above may contain stVal (status value), q (quality), and t (time). Values for data attributes can be viewed in the Custom Display, Data Monitor, or Advanced Client. Finding Data Objects Three techniques can be used to find data objects in the Data Miner: Pre-filtering with the Data Source Filtering data by columns Grouping data by columns Viewing Data Object Values Data values can be viewed in three different ways: Custom Display Data Monitor Advanced Client Pre-Filtering with Data Source The Data Source is a global pre-filter that is applied to the SCL Files when creating a workspace. This pre-filter limits the amount of data to a manageable size. This is necessary because some IEC 61850 SCL Files can contain millions of data attributes, some of which are not necessary for the testing you are doing. Filtering Data by Columns Any of the columns in the table view in the Data Miner can be filtered or grouped to find IED data objects. Column filters that are active are highlighted in red. Example 1: Data can be filtered by Logical Node Class (CSWI, PTRC, MMXU, etc.) as seen below. Example 2: Data can be filtered to only show data contained in GOOSE messages. The same type of filter can be setup to show data available from Reports. Grouping Data by Columns Data objects can be grouped by any column. For example, all the data objects can be grouped by IED Name or Logical Node Prefix to group the data according to substation bays. Example: Data can be grouped by IED Name. Select Which Data Columns to View Several different columns can be displayed for data objects in the Data Miner: Column Selection - columns can be added or removed from the table view. Viewing Data Values Data values for data attributes can be viewed in the Custom Display or the Data Monitor: Custom Display - for a graphical view of data, drag data objects to the Custom Display. For many data objects, this will create a graphical view of the data. Data Monitor - for a table view of data, drag data objects to the Data Monitor. This will create a table view of the data attrib- utes in the data objects. Advanced Client - for a detailed view of the IED data model, right click on the data object and select "Open in Advanced Client". This opens the data object in the full IED data model. This view is the same as the original Hammer data model view in previous version of the 61850 Test Suite. Viewing Substation Data Data Retrieval from the Server By GOOSE *Simulated GOOSE By Report (MMS) By Polling the Server (MMS) Changing the Data Retrieval Method The data retrieval method of the Tile can be changed by using the context menu as shown below: The available methods are shown in the sub-menu of the Data Acquisition context menu option. The user has the option to select the method needed. Offline Tiles Designing a Custom Display Add a New Display Add Tiles to the Display Tiles can be added to the Custom Display by dragging Data Objects from the Data Miner as shown below: The Data Retrieval section determines the method that the Tile in the Custom Display should use to retrieve the data for the Data Object being dragged. The precedence of the methods is in the order of appearance (i.e. GOOSE/Report/Polling) in case multiple meth- ods are selected. Add Symbols from the Palette to the Display Symbols used in building a Single Line Diagram are available in the palette on the right of the Custom Display. These symbols can be dragged on to the display as show below: Types of Tiles General Tile Switch (Closed) (DPC - Double point) Switch(Open) SwitchasBreaker SwitchasArrow 2021TriangleMicroWorks,Inc.Allrightsreserved. SwitchasTextOnly SwitchasBusbar+ 2021TriangleMicroWorks,Inc.Allrightsreserved. Switch as Busbar T Breaker (On) (DPC - Double point) Breaker (OFF) Single Point Status (SPS, SPC) Trip Counter (ACT, ACD) Tile Context Menu Show Event History: Displays all the events received so far. Reset Event History Clears all the events captured so far. Reset Trip Counter Resets the trips counted. Other Tile Indicators Enumerated Status (ENS, ENC, INC, INS, VSS, DPS) Measurand (WYE) Quality Indicators on the Tile View Events in Logic Analyzer Using the Logic Analyzer The Logic Analyzer can be used to chart the real time values of up to eight data attributes found in IEDs in the current workspace. Charted values can be saved as part of the workspace for later viewing and can be exported as data files. Data Sources for the Logic Analyzer To view a data attribute in the Logic Analyzer, the attribute value must be retrieved through GOOSE or a report. If the value is retrieved as part of a report, the timestamp of the data change must be included in the report. The Logic Analyzer charts attribute values against time based on timestamps received in reported data from IEDs and GOOSE headers from publishers. The time plotted in the chart is not the local PC time at which the signal was received by the application. They are the timestamps from the signals received from the IED (GOOSE/Report). Adding Data Objects and Data Attributes to the Logic Analyzer 1) Select the Data Miner tool on the left side and select data objects 2) Select the Custom Display on the right side 3) Drag data objects from the Data Miner onto the Logic Analyzer. The data objects will be deconstructed into data attributes which will be added to the Signal Path list. 4) The first eight attributes added to the Signal Path list will automatically have a chart created. 5) The value and timestamp (and quality if available) for the data attribute is retrieved using GOOSE or reports based on how the Data Retrieval Method is set. Note that adding polled data attributes will be reported as an error. This Figure shows the POS data object for a CSWI being dragged onto the Logic Analyzer. The object can be dropped onto the chart area or on the Signal Path list. After the object is dropped, its corresponding stVal attribute is added to the Signal Path list and a chart is created for the value. Test Suite Pro will open a connection to the IED and subscribe to the appropriate control blocks to prepare to acquire data for the attribute. A large number of signal paths can be added to the Logic Analyzer, however only eight charts can be displayed at any time. The check boxes in the column labeled Graph? determine which paths are charted. When data acquisition is started, data is collected for all attrib- utes in the Signal Path list, not just the ones that are currently charted. Because of this, charts for any of the signals can be reviewed at any time by clicking the Graph? checkbox to view the data even after data collection is stopped. Acquiring Data To begin collecting data for the attributes in the Signal Path list, click on the red Start Recording button in the Logic Analyzer control bar. When acquisition begins, the control bar will turn green and the Start Time and Time Elapsed values become active. The figure above shows the Logic Analyzer collecting data for three data attributes. In this example, the top two attributes experienced a state change about 30 seconds after the data collection began. Cursors The orange, vertical line running across the three charts is a cursor. The cursor can be moved with the mouse and can be turned off with the orange hand icon next to the Start Recording button. A second cursor can be turned on by clicking on the second cursor icon. When both cursors are turned on, the time delta between the cursor positions is displayed in the control bar below the elapsed time value. Interacting with the Charts While data is being collected, by default the new values will be displayed on the right side of the charts. This is called Autoscrolling. This behavior can be stopped by unchecking the Autoscoll checkbox in the control bar or by moving the chart with mouse which will disable the Autoscroll function. Autoscrolling can be started again by clicking the Autoscroll checkbox. The number of seconds of data shown in the window is controlled by the Data Window slider. By default, 30 seconds of data is shown. The Data Window slider only works while Auto- scroll is enabled. When Autoscroll is disabled, the zoom function controls the amount of data shown in the window. The charts can be zoomed by using the scroll wheel on a mouse or by pressing the + and - keys. Additional functions can be found on the context menu for the charts. Right clicking on a chart will show the context menu. The Find Next/Previous Event menu items will reposition the charts so that the next/previous change in the attributes value is centered in the display. The Move Up/Down menu items will reorder the charts on the screen. Archiving and Recalling Chart Sets The current set of charts displayed in the Logic Analyzer as well as the data collected for all of the attributes in the Signal Path list can be saved as a named chart set. To archive this set of data, select the Archive Chart Set from the Logic Analyzer menu. Then provide a unique name for the chart set and click the check mark to accept it. The new name of the chart set will be displayed at the top of the Logic Analyzer display in a drop down box. This box is used to select the archived chart set to view. When viewing an archived chart set, most of the tools are available to manipulate the chart such as zooming and panning however the Start Recording and Autoscroll functions are disabled. To prepare to start collecting new data, select the item Record New Chart Set from the drop down list. This will reset the Logic Analyzer and allow the Start Recording button to work. The Signal Path list is prepopulated with the attributes that were present in the last Archived Chart Set. If these attributes are not correct, delete individual attributes by selecting them then right clicking and selecting Delete Selected Signals or delete all of the attributes by clicking the Clear All button at the top of the Signal Path list. Exporting Data The values plotted on the Logic Analyzer can be exported to a file using a comma separated value (CSV) format. The Export functions are found on the Logic Analyzer menu and are only accessible when data has been collected and the Logic Analyzer is not currently collecting data. There are two options for exporting data. The data for all attributes in the Signal Path list can be exported or the data for just the attributes currently displayed in charts can be exported. In either case, Test Suite Pro will request the user to identify a file name and directory location for the export file. By default, the file will be given a file extension of .csv if no file extension is provided as part of the file name. The first line of the exported file contains column headers which describe the meaning of the values in each of the following rows. Understanding the Charts The Logic Analyzer charts are designed to show the value of attributes in an object model as they change over time. The values are those received by the IEC 61850 client running within Test Suite Pro. The client in Test Suite Pro subscribes to reports and GOOSE as need to receive the attributes requested by the user. Charts are organized into charts sets that contain an unlimited number of attribute paths for which up to eight charts can be plotted at a time. When an attribute is charted, the values collected for the attribute are displayed from the time the Start Recording button was clicked to the time the Stop Recording button was clicked. The X axis on each chart represents the number of milliseconds that passed from the time that the user clicked the Start Recording but- ton until the timestamp included with the received data value. This means that the PC on which Test Suite Pro is running should be time synchronized with the network on which the IEDs are running. Since the data is plotted using timestamp values, all relative meas- urements between events will be accurate even if the PC is not synchronized; however, an unsynchronized PC could result in values being missed if the local PC clock is well ahead of the network clock since the charts will not plot values that have timestamps that occurred before the time that the user clicked the Start Recording button. Each chart clearly shows the full path of the data attribute that is being plotted. At the end of the path, the source of the data is listed in parenthesis as Report or GOOSE. If the data is coming from Reports, then the timestamps are read from the dataset in the report. Attribute values can only be charted in the Logic Analyzer if the dataset includes the timestamp and the quality of the signal. If the data is coming from GOOSE, then the timestamp will be read from the dataset if it is available. If the timestamp is not in the dataset, then the timestamp from the header of the GOOSE message will be used. A number of line styles are used to plot values on the charts. A solid blue line indicates that a value has been received with a good qual- ity indicator. A solid yellow line indicates that a value has been received with a quality indicator other than good. The cursor will display the quality values for a yellow line. A dotted line indicates that a value has been received in the past and no new information has been received; therefore, the value is assumed to be saved as the last received value. When a new Report or GOOSE value is received, the dotted line is replaced with a solid line indicating the value is now confirmed. Continuity of data is maintained in GOOSE messages with a state change number which is part of the GOOSE header. The continuity of data in buffered reports is indicated by a buffer overflow flag which is in the report header. If Test Suite Pro receives GOOSE data where the state change number is not properly incremented or receives report data with the buffer overflow flag set, the new data values are shown on the graph but a gap in the line (no line drawn) will appear where the missing data occurred. View a List of Object Data The Data Monitor display is used to create a custom table of data from multiple IEDs. 1) Go to the Data Monitor display 2) Select data objects in the Data Miner 3)DragtheobjectstotheDataMonitorThetoolwillautomaticallyretrievedatafromtheIEDsusingtheselectedDataRetrievalMethodsatthetop. 4.3TestingGOOSE TestingGOOSE WhatisGOOSE? TheGenericObjectOrientedSubstationEvent(GOOSE)modelprovidesthepossibilityforafastandreliablesystem-widedistributionofinputandoutputvalues.GOOSEmessagesarepublishedbyadeviceviaEthernetmulticastsuchthattheycanbesubscribedbyanynumberofotherdevices. Thesemessagesarebasedondatasets.WhileIEC61850allowsanydatasettobeusedwithGOOSE,thepracticalusessuggestthatthedatasetsshouldcontainfairlysmallnumbersofstatusvaluesandtheirrelatedqualityinformation.Achangeinvalueofanydatasetmem- berisconsideredachangeofstate,thenewinformationispublishedimmediately.Themessageisthenretransmittedincaseoflostpacketsordevicescomingonlinewhichneedthecurrentstate.Inordertopreservebandwidth,thedelaybetweenretransmissionsgrowsovertimefromminTimeimmediatelyfollowingthestatechangetomaxTimeatthesteadystatecondition. SubscribingtoGOOSE TherearethreewaystosubscribetoGOOSE: l FromtheDataMiner l FromtheGOOSETracker l FromtheAdvancedClient SubscribetoGOOSEfromDataMiner: 1)FindthedataobjectsavailablefromGOOSEcontrolblocksbyusingDataMinerfilters 2021TriangleMicroWorks,Inc.Allrightsreserved. 2) Make sure GOOSE is selected on Data Retrieval Method 3) Drag the data object(s) you wish to subscribe and view to one of the following TSP Displays: Signal Flow Custom Display Data Monitor Logic Ana- lyzer Group Dis- play The tool will subscribe to the first GOOSE stream that is found which contains that data object You can confirm the data retrieval method by hovering the mouse over the display object Subscribe from GOOSE Tracker In the GOOSE Tracker, you can right click on any GOOSE stream to subscribe or unsubscribe. Subscribe from Advanced Client: In the Advanced Client, GOOSE Control Blocks can be found under the logical node LLN0 in any logical device To subscribe to a GOOSE control block: 1) Select the GOOSE control block name 2) Optional: read the GOOSE control block to ensure you have the current IED configuration (either click read button or right click -> Read) 3) Subscribe to the selected GOOSE control block (either click subscribe button or right click -> Subscribe) Once a GOOSE control block has been subscribed, you can view GOOSE events (data change, quality change) in the GOOSE Event Viewer. If GOOSE events are not being received, try these GOOSE debug instructions. Note: Enable and Disable on the GOOSE control block will turn on/off the publishing of the GOOSE message from the server in the IED. Some IEDs do not allow this. Find Data Available from GOOSE Publishers You can use filters in the Data Miner to find data that is available from GOOSE publishers. Go to the Data Miner tool Click on the GOOSE filter. This filters all the data objects in the substation to show data that is available from GOOSE publishers. It also groups the data by IED name. You can further filter the GOOSE data by other columns like LN Class, Common Data Class, or LN Prefix. After you find the data you need, then you can subscribe to the GOOSE stream to receive events Creating New Filters You can create your own filters in the Data Miner: Set your filter and group settings for the columns Select "Save as" Name your filter and click save Note: the filters in the Data Miner are available from any workspace because they are saved globally (not in individual work- spaces). View GOOSE Events GOOSE messages can be viewed in the GOOSE Event Viewer display. Any GOOSE Control Block that has been subscribed to (either from the Advanced Client, Data Miner, or GOOSE Tracker) will be displayed in the GOOSE Event Viewer. Left Pane GOOSE event messages from multiple IEDs are shown in the pane on the left. You can filter or group messages by any column including Connection Name (IED) or GOOSE ID or any other parameter in the GOOSE header. Each row on the left side represents a GOOSE state change event message that has been received. The columns show the header inform- ation sent in the GOOSE message. Note: a GOOSE state change event is caused when either the data or quality changes for an object in the GOOSE dataset. Right Pane Choosing one of the GOOSE event messages (on the left side) will show the message data in the right pane. Data for multiple messages can be shown by selecting multiple check boxes on the left side as shown below. Note: only the first GOOSE state change event message is shown in the GOOSE Event Viewer. All re-transmission GOOSE mes- sages (after the state change event) are filtered out and are not shown in this view. Debug GOOSE If GOOSE subscriptions are not working, try these steps to debug: Sniff GOOSE with the GOOSE Tracker to ensure that the GOOSE stream is found on the network Ensure that the correct Ethernet adapter on the computer running 61850 Test Suite Pro is connected to the correct part of the net- work based on VLAN filtering or any other network configurations Ensure the IED that is publishing GOOSE is turned on and connected to the network If you are using an SCL file to configure the workspace: compare the IED model to the SCL file Testing Reports Testing Reports What are Reports? Information reports in IEC 61850 allow a server to provide information to a client based on configured conditions. The information sent in a particular report is based on a dataset. The conditions upon which the data is set are configured in a report control block. The General Integrity trigger allows a client to request a complete snapshot of the dataset, but otherwise the server sends reports per configuration, either on a periodic timer (Integrity report) or based on a change in the value or quality of the dataset members. Enable and Disable Reports In the Advanced Client, reports can be enabled in several ways: The ribbon bar at the top will show report enable/disable buttons A right click on a report control block will show a context menu Report Commands Read This will cause the client to read the current configuration of the Report Control Block from the IED server. Enable For buffered Report Control Blocks, this will perform a resynch between the client and IED server. Before the report is enabled, the client will write the EntryID value for the last report that the client received from the IED server. This will cause the IED server to send all reports after the last received EntryID (if the buffer has not wrapped). If the buffer has wrapped, the IED server will send an error. Note: EntryID is an optional field for Report Control Blocks. Therefore, the resynch feature is not available in cases where it is not supported. Enabled (Raw) This sets RptEna to true and writes the value to the IED server. Note: RptEna cannot be edited directly in the value column so that the GUI can more easily track it. Purge and Enable For buffered reports, this first purges the report buffer on the IED and then enables the reportcontrol block. Purge For buffered reports, this purges the report buffer on the IED. Send GI This sends the GI (general interrogation) command to the IED server and a report is sent by the IED with all current values for dataset members. Find Data Available in Reports Use the filters in the Data Miner to find data that is available through reports from IEDs. Go to the Data Miner tool Click on the "Reported" filter. This filters all the data objects in the substation to show data that is available from report control blocks. It also groups the data by IED name. You can further filter the reported data by other columns like LN Class, Common Data Class, or LN Prefix. After you find the data you need, then you can enable the reports to receive updates Creating New Filters You can create your own filters in the Data Miner: Set your filter and group settings for the columns Select "Save as" Name your filter and click save Note: the filters in the Data Miner are available from any workspace because they are saved globally (not saved in individual work- spaces). View Reported Events Report messages can be viewed in the Report Viewer display. Any Report Control Block that has been enabled from the Advanced Client or the Data Miner will be displayed here. Each row in the left side represents one Report message. The columns contain the header information sent with each Report message. Choosing one of these messages will cause the data to be shown in the right pane. Data for multiple messages can be shown by checking multiple check boxes in the Report message pane as shown below: Testing SCL Files Testing SCL Files Compare SCL Files to IED Models The Compare Model tool compares the IEC 61850 object model from an SCL file to the object model from an IED. This is done one IED at a time. Start the Tool 1) Select the Compare Model tool 2) Select which IED to compare The tool then starts the comparison. First, it discovers the object model from the IED and then it compares the IED to the local object model from the SCL file in the current workspace. Compare Results The left hand side has the combined object model shown as a tree. Green indicates no differences were found between the SCL file and the IED Orange indicates that one or more children were found to be different Red indicates the item with the difference The right hand pane shows the details of the difference found. Selecting any level on the left will show all the childrens differences on the right. The general algorithm compares all item available from discovery. This includes structure, names, types and sizes, members of datasets, etc. For value comparison Control Block values and CF, EX, SG,SP, and DC functional constraints are compared for values matching. Verify SCL Files SCL Verify will verify one file at a time either from the current workspace or by selecting a file. The severity of each test can be configured by running the SCL Verify Configuration tool. Run the Verifier 1) Select the SCL Verify tool 2) Choose which SCL file to check. This can be a file from the current workspace or you can browse to select a file from disk. 3) Select Run Viewing Results The results of the SCL Verify are shown on the left. The SCL File will appear in the SCL Viewer on the right. Clicking on line items after run- ning the Verifier will navigate to the line where the error was found. Setting the Edition for Modeling Checks The edition (1 or 2) that SCL Verify will use when comparing the object model in the file to the object model defined by the standard is set at the top of the screen: There are two options: Use version/revision attribute on SCL Determines the version by inspecting the SCL attribute at top level. Use originalSclVersion/originalSclRevision attributes on each IED Determines each individual IED by using the ori- ginalSclVersion/originalSclRevision attributes on each IED element. The edition of the SCL File is determined by looking at the version and revision in the <SCL> tag at the top of the SCL File: 2007:B4 is considered edition 2.1 2007:A or 2007:B are considered edition 2 2003: ? (where ? is the revision and can be any value) or blank for those fields is considered edition 1 View SCL Files You can view SCL Files in the SCL Viewer display Select the SCL File 1) Select the SCL Verify tool 2) Choose which SCL file to check. This can be a file from the current workspace or you can browse to select a file from disk. Viewing SCL File The loaded SCL File will appear in the SCL Viewer: Configure the SCL Verify Tool SCL Verify Configuration allows you to customize the severity of an error, the file extensions that this error applies to and the edition of the specification this error will be applied to. Changing the Configuration File To change the Configuration file: Go to the SCL Verify tool Select the "Choose a Configuration Source" tab at the top of the ribbon bar You can select a new Configuration Source from the dropdown Or click "SCL Verify Configuration" to change the settings or create a new configuration Edit or Create a Configuration The configuration settings allow you change: Change the severity each test is assigned by changing the column "Severity" Set which tests should be skipped based on the SCL file extension by changing the column "File Extensions to Mask Out" Decide which editions this test will apply to by changing the column "Apply to Edition and Above" Note: Configurations are saved locally on the PC. They are not part of a workspace, so that they are available to be used from any workspace or any Triangle Tool that runs SCL Verify Configuration Application Options 61850 Test Suite Pro has global configuration settings that are used across workspaces. To open the Application Options dialog box, click on the File menu then select Options from the left side of the display. The Application Options dialog box will be displayed with several tabs: Workspace Tab The Workspace tab controls whether the layout of a workspace is automatically saved when it is closed. When the layout is saved, items such as windows sizes, table column sizes, table filters and grouping, and tool visibility are restored when the workspace is opened the next time. This tab also controls whether the last open workspace is immediately opened when Test Suite Pro is started up. Display Settings Tab The Display Settings tab is used to select colors displayed when visualizing data attributes that are single and double point controls. These color selections are used on the Custom Display tab and the Group Display tab. The setting for maximum number of GOOSE mes- sages is applied to the GOOSE Event Viewer tab and the setting for maximum number of Reports is applied to the Reports tab. Client Configuration Tab The Client Configuration tab settings are used by the client connection to each IED. Server Configuration Tab The Server Configuration tab controls the port on which a simulated server will listen for commands. Tracing Tab The Tracing tab is used to control the types of messages sent to the Output window. The messages can also be sent to a log file by check- ing the check box at the top of the tab. The log file is written to the directory C:\Users\Public\Documents\Triangle MicroWorks\61850 Test Suite Pro. This file is not overwritten at startup or when a workspace is opened. New information is appended to the end of the file. System Requirements Operating System 61850 Test Suite Pro supports the following versions of WindowsTM: Windows Embedded Standard 7 Windows 7 (32-bit and 64-bit) Windows Server 2008 (32-bit and 64-bit) Windows 8.1 (32-bit and 64-bit) Windows Server 2012 (64-bit) Windows 10 (32-bit and 64-bit) Windows Server 2016 (64-bit) Windows Server 2019 (64-bit) Note: 61850 Test Suite Pro supports virtual machines (VMs) Minimum system requirements: 1.5 GHz processor; recommended: 2.4 GHz multi-core processor 2GB RAM; recommended: 4GB RAM Windows 7 or higher (not Embedded Compact) Microsoft .NET 4.6.2 Framework Note: the minimum requirements are for small IEC 61850 SCL files. Larger SCL files with large data models will require more resources to process. Pre-Filtering Data Models with Data Miner The Data Miner runs the Data Source pre-filter on the entire data model for each IED in the current workspace. This pre-filtering allows the tool to show the most important data for the job being performed. Changing the Data Source Pre-Filter The settings for the Data Source pre-filter can be configured to include the data objects that you need for testing. The default settings include most of the essential process data based on Common Data Classes (CDC's) like WYE, DPC, SPC, ACT, and ACD. To change the Data Source pre-filter: Go to the Data Miner tool Select the "Data Source" tab at the top of the ribbon bar You can select a new Data Source configuration from the dropdown Or click "Configure Data Sources" to change the settings or create a new configuration Configure a New Data Source The Data Source configuration window lists all of the possible IEC 61850 data objects. Use the column filters to find data objects Select the data objects that you want to be included in the Data Miner by clicking the boxes on the far left Use "Save As" to save as a new Data Source configuration Close the window The new data source can be selected in the Data Miner and this will apply a new pre-filter to the entire workspace (all IED data models) Note: the Data Source configurations are saved locally on the PC. They are not part of a workspace, so that they are available to be used from any workspace. 6.0FrequentlyAskedQuestions MyevaluationcopyofTSPstatesthereisnolicense.Why? AreyourunningTSPonaVirtualMachine?Ifso,thetriallicensewillnotactivate.Pleasecontactsales@trianglemicroworks.comforantemporarylicense.IfyouarenotusingaVMorarestillhavingissueswithyourlicense,pleasecontactsupport@trianglemicroworks.com WhattypesofSCLfilesareSupported? SCLFilesareXMLbasedtextfilesusingtheIEC61850SystemConfigurationLanguage(SCL)format.61850TestSuiteProsupportsSCD, CID,ICD,oranyotherfilecontainingvalidSCLwithIEDdatamodels. WhereisAnvil? ThesamecapabilitiesoftheoriginalAnviltoolcannowbefoundintheIEDSimulatortool: l SimulatinganyIEDserverfromanSCLfileorbydiscoveringarealIEDdatamodel l TestingIEC61850services(reports,GOOSE,controls,etc.) l SimulatingdatachangesintheIEDdatamodel 2021TriangleMicroWorks,Inc.Allrightsreserved. WhereisHammer? ThesamecapabilitiesoftheoriginalHammertoolcannowbefoundintheAdvancedClienttool: l ViewingtheentiredatamodelforanIED l TestingIEC61850services(reports,GOOSE,controls,etc.) LearnmoreabouttestingIEDsasatestClient 2021TriangleMicroWorks,Inc.Allrightsreserved. SometestingfeaturesfromHammerhavebeensplitintoothertoolstomakethemeasiertouse: l GOOSETracker l SCLVerify l CompareModel l LogServices l FileServices WhereisSCLForge? SCLForgeisaseparateapplicationthatwaspartoftheoriginal61850TestSuite(V3.9andearlier). AnewapplicationcalledSCLNavigatorwillreplaceSCLForgeinOctober2017. SCLFileCapabilitiesin61850TestSuitePro ThereareSCLFiletestingcapabilitiesbuiltinto61850TestSuitePro: l ViewingSCLFileswiththeSCLViewerdisplay l CheckingSCLFileswiththeSCLVerifytool l VerifyingIEDconfigurationwithCompareModeltool IdonotseealloftheavailableToolsandDisplays,why? 2021TriangleMicroWorks,Inc.Allrightsreserved. 1.AllofthetoolsmaynotbeenabledviatheToolBox: 2.YouareusingTestSuiteLite.Pleasecontactsales@trianglemicroworks.comifyouwouldliketoupgradetoTestSuitePro. WhenIdragpointstotheServerDataSimulator,itstatestheyarenotsupported. UnlikeotherDisplays,theServerDataSimulatorcanonlyhavepointsaddedfromtheIEDSimulatortool. 2021TriangleMicroWorks,Inc.Allrightsreserved. 

************Iron Manual**************
 TRIANGLE MICROWORKS, INC. Iron Product Documentation Version 4. 5 J uly , 20 2 3 Property of Triangle MicroWorks, Inc. This Documentation and its associated distribution media contain proprietary information of Triangle MicroWorks, Inc. and may not be copied or distributed in any form without the written permission of Triangle MicroWorks, Inc. Copies of the source code may be made only for backup purposes. 2000 - 2023 Triangle MicroWorks, Inc. All rights reserved. READ THIS BEFORE OPENING THE MEDIA PACKAGE THIS SOFTWARE LICENSE AGREEMENT IS BEING PROVIDED TO YOU BEFORE YOU OPEN THIS MEDIA PACKAGE. ONCE YOU OPEN THE MEDIA PACKAGE, YOU HAVE AGREED TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT. THEREFORE, IF YOU DO NOT WISH TO AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT, YOU MUST RETURN THE UNOPENED MEDIA PACKAGE THEREOF, WHEREUPON YOUR MONEY WILL BE REFUNDED. TRIANGLE MICROWORKS, INC. License Agreement and Warranty 1) LICENSE. Triangle MicroWorks, Inc. hereby grants to the person (Customer) who is purchasing the enclosed software media to use the same (the contents of which, together with all modifications and accompanying documentation are referred to herein as the Software), a limited, non-exclusive license to use the software on the terms and conditions set forth. This license is solely for the use of the Software by the Customer on a single computer. The Customer may physically transfer the Software to another computer by having the media read into such computer, provided that the Software may not be used on more than one computer at any given time. Subject to the foregoing limitation, the Customer is entitled to copy the Software into any machine-readable or printed form for backup purposes in support of the use of the Software on a single computer by the Customer. The Customer may likewise merge the Software or incorporate the same into another program provided that such program will, for so long as the Software is included therein, be subject to all of the terms and conditions of this license. Any copy of the Software permitted by the preceding provision shall include a copyright notice of Triangle MicroWorks, Inc. 2) TERM OF LICENSE. The license will continue until the Customer physically destroys all copies of the Software and returns the original program disk and documentation to Triangle MicroWorks, Inc. 3) MAINTENANCE & ENHANCEMENT PLAN. Customer is subscribed in the Maintenance & Enhancement Plan for a period of one (1) year following the purchase of the Windows Client Tester. The Plan may be renewed each year on the anniversary of the purchase date unless cancelled by either party. This optional Maintenance & Enhancement Plan provides a cost effective method for upgrading to the most recent release version of the Windows Client Tester. It also provides technical support by telephone, fax, or email. The first year of the Maintenance & Enhancement Plan is free. Renewal price after the first year is 25% of the purchase price; however, future years may be purchased in advance at 20% of the purchase price. 4) COPYRIGHT. The Software is copyrighted, and all rights therein are reserved by Triangle MicroWorks, Inc. 5) LIMITED WARRANTY. The Customer assumes all responsibility for the selection of the Software as appropriate to achieve the results intended by the Customer. Triangle MicroWorks, Inc. warrants that the enclosed media medium upon which the Software is recorded shall be free from defects in material and workmanship under normal use and conditions, and that the Software shall perform substantially as described in its documentation for a period of sixty (60) days from purchase. EXCEPT FOR THE FOREGOING LIMITED WARRANTY. THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. 5) LIMITATION OF REMEDIES. In the event of the breach of the limited warranty set forth above, the Customer shall be entitled to return the media to the place of purchase and, at the election of the vendor, to have the purchase price refunded or to receive media containing the Software which conforms with the above limited warranty. In no event will Triangle MicroWorks, Inc. be liable for any damages arising out of the Customers use of the Software, including, by way of illustration, lost profits or any incidental or consequential damages. 6) ACKNOWLEDGMENTS. The Customer acknowledges that the Customer has read this agreement, understands it and agrees to be bound by its terms and conditions, which agreement will be acknowledged in writing upon the request of Triangle MicroWorks, Inc. 7) GENERAL PROVISIONS. The Customer shall have no right to sublicense any of the rights of this Agreement, for any reason. In the event of the breach by the Customer of this Agreement, the Customer shall be liable for all damages to Triangle MicroWorks, Inc., and this Agreement shall be terminated. This Agreement can only be modified in writing between the Customer and Triangle MicroWorks, Inc. If any provision of this Agreement shall be deemed to be invalid, illegal or unenforceable, the validity, legality and enforceability of the remaining portions of this Agreement shall not be affected or impaired thereby. The laws of the State of North Carolina shall govern this Agreement. Triangle MicroWorks, Inc. 2840 Plaza Place, Suite 205, Raleigh, North Carolina 27612 USA (919) 870-5101 Contents 1. Introduction ................................................................................................................. 3 1.1 Installation........................................................................................................... 3 1.2 Licensing ............................................................................................................. 3 2. User Interface .............................................................................................................. 3 2.1 Menu Bar ............................................................................................................ 5 2.1.1 File Menu ........................................................................................................ 5 2.1.2 Options ............................................................................................................ 6 2.1.3 Tools ............................................................................................................... 6 2.1.4 View ................................................................................................................ 6 2.1.5 Windows ......................................................................................................... 7 2.1.6 Help Menu ...................................................................................................... 7 2.2 Toolbar ................................................................................................................ 7 2.2.1 Open, Save Workspace (, ) ..................................................................... 7 2.2.2 Print () ......................................................................................................... 7 2.2.3 Find () ......................................................................................................... 7 2.2.4 Configure () ................................................................................................. 8 2.2.5 Show/Hide Properties Pane () ..................................................................... 8 2.2.6 Show/Hide Output Pane () .......................................................................... 8 2.2.7 Creating a New ICCP Application .................................................................. 8 2.2.8 (Optional Items) .............................................................................................. 9 2.3 Window Managements ....................................................................................... 9 2.4 Configuration .................................................................................................... 11 2.5 Workspace Pane ................................................................................................ 12 2.6 Properties Pane.................................................................................................. 12 2.7 Output Pane () ............................................................................................... 12 2.8 Report Pane ....................................................................................................... 12 3. IEC 60870-6 (ICCP/TASE2) Test Server ................................................................. 13 3.1 Menu Bar .......................................................................................................... 14 3.1.1 View .............................................................................................................. 14 3.2 Toolbar .............................................................................................................. 14 3.2.1 Show/Hide Simulation Pane () ................................................................. 14 3.2.2 Start the Active ICCP Server () ................................................................ 14 3.2.3 Stop the Active ICCPServer () .................................................................. 15 3.3 ICCP Server Configuration ............................................................................... 15 3.4 ICCP/TASE2 Server Model .............................................................................. 18 3.5 Starting/Stopping a ICCP/TASE2 Server ......................................................... 20 3.6 Simulation Pane ................................................................................................ 21 3.7 Writing Values .................................................................................................. 22 4. IEC 60870-6 (ICCP/TASE2) Test Client ................................................................. 24 4.1 Menu Bar .......................................................................................................... 24 4.1.1 Context Information ...................................................................................... 24 4.2 Client Configuration ......................................................................................... 24 4.3 Configuring Client Model ................................................................................. 27 4.4 Connecting to an ICCP/TASE2 Server ............................................................. 28 4.4.1 Connecting to a Server .................................................................................. 28 4.4.2 Read .............................................................................................................. 28 4.4.3 Write ............................................................................................................. 28 4.4.4 Creating Data Sets......................................................................................... 29 4.4.5 Deleting Data Sets......................................................................................... 31 4.4.6 Enable/Disable DSTransferSets .................................................................... 31 4.4.7 Polling ........................................................................................................... 33 1. I ntroduction Congratulations on your purchase the Iron. The Iron is a Microsoft Windows based application that supports the IEC 60870-6 ICCP specification. Note that ICCP is sometimes called TASE2 or TASE.2. 1.1 Installation The Triangle MicroWorks, Inc. Iron is delivered as a single installation file named Iron vm.nn.nnnn.exe where m is the major version and nn is the minor version and nnnn is the build number of the Iron. To install Iron simply run this file and follow the directions. After the Iron is installed you can reinstall or remove it by rerunning the installation application. This can also be accessed from the Windows Add/Remove Programs option in the Windows Control Panel. In addition to installing the required files on your system the installation application performs the following steps. 1) Creates entries in the Start->Programs menu to access the features of the Iron. 2) Creates a desktop icon to start the Iron. Once the Iron is installed it can be run by double clicking the Desktop Icon, using the appropriate entry from the Start->Programs->Triangle MicroWorks-> Iron folder on the Start Menu, or by executing the Iron.exe file found in the installation/bin directory. This executable starts the Iron. 1.2 Licensing Iron uses the Sentinel LDK licensing system from SafeNet, Inc. This is an advanced and full featured system that allows you to manage your license keys for Triangle MicroWorks products. The system provides the capability to transfer licenses to other machines with needing to contact Triangle MicroWorks. If you have purchased a network license, you can detach license from the network pool and use the license away from the network. All of the capabilities of the system are described in the document called Guide to Triangle MicroWorks Licensing. This document fully describes how to convert your demo version of this product into a fully licensed version. 2. U ser Interface This section will discuss the use of the interface in detail. The figure below shows the user interface provided with Iron. Figure 1 Iron User Interface Iron consists of a Menu Bar, Tool Bars and many Panes. It also makes extensive use of context menus accessed by a right mouse button clicks on specific items. The menu bar provides access to various control and configuration information. The Panes can be grouped into three different types. The first are the Workspace Panes. The Workspace Panes consist of ICCP Servers and ICCP Clients. Workspace Panes can only be docked in the main application area and cannot be made to float outside the application. Workspace Panes represent an object model and are either loaded from a file, built in the user interface in the case of a Server, or in the case of a Client built from browsing the Servers object model (i.e. discovery). The second type of pane is Focus Specific Pane. The content of a Focus Specific Panes changes to reflect the current focus of Workspace Panes. The currently active Workspace Pane sets the content of the Focus Specific Pane(s). The Properties Pane is currently the only Focus Specific Panes. Focus Specific Panes can be docked anywhere within the application and can float outside the application. The third type of pane is the General Pane. A General Pane can be docked anywhere within the application and can float outside the application. General Panes are specific to the Workspace Panes and generally apply to a specific Workspace Pane. Each General Pane is described in the specific Workspace Pane section. The Tree on the left shows the current ICCP applications. It cannot be closed. 2.1 Menu Bar The menu bar is broken up into the following menus and commands. 2.1.1 File Menu 2.1.1.1 Open, Close, Save, and Save As Workspace A workspace is the entirety of the current Iron application. These menu items are self- explanatory and have to do with saving and opening workspaces. Note that the current workspace is automatically saved upon exiting the application. 2.1.1.2 Page Setup This option displays the printer page setup dialog. 2.1.1.3 Print Preview This option displays generates a preview of the print job. 2.1.1.4 Print This option prints the document. Note: The document prints and scales to what is shown on the screen. For example, to make the print larger make the width narrower and vice versa. 2.1.1.5 Exit This option will exit Iron (equivalent to pressing the X at the top right corner). 2.1.2 Options 2.1.2.1 Find This option displays the Find Dialog. The Find Dialog searches for matching text in the name field. Press F3 to find the next matching entry. The search is case insensitive. 2.1.2.2 Configure This option displays the Configuration Dialog. 2.1.2.3 Save Configuration as Default This option will save the current configuration as the default configuration when Iron is displayed. 2.1.3 Tools Tools menu items are added to the Main Menu Bar when a Workspace Pane is opened. The Tools menu item is context specific based on the Workspace Pane that has focus. 2.1.4 View The View menu controls the visibility of the panes inside the Iron. Properties, Output, and Data Monitor Panes can all be docked in various modes, floated outside of the main application, or hidden from view. To hide a pane hit the x button in the upper right corner of the Pane or from the View menu click on the pane name. Similarly, to show a pane that his currently hidden go to the View menu and click on the pane name. 2.1.4.1 Properties This option allows you to hide or show the Properties pane. 2.1.4.2 Output This option allows you to hide or show the Output pane. 2.1.4.3 (Optional Items) Various other item will appear here based on the active Workspace Pane. These are explained in ICCP Client and ICCP Server sections. 2.1.4.4 Set Default Layout This option restores the panes back to their original layout when the application first started. 2.1.5 Windows Provides a list of currently open Workspace Panes. 2.1.6 Help Menu 2.1.6.1 Iron Manual This option opens the Iron Manual.doc (this document). 2.1.6.2 License This option displays the license installation dialog box described in section X2.2X. 2.1.6.3 About This option displays a dialog box that contains the Iron version, release date, and other pertinent information. 2.1.6.4 www.TriangleMicroWorks.com A Web link to Triangle MicroWorks web site. 2.2 Toolbar 2.2.1 Open, Save Workspace (, ) The same as the File menu options. 2.2.2 Print () This option prints the document. Note: The document prints and scales to what is shown on the screen. For example, to make the print larger make the width narrower and vice versa. 2.2.3 Find () This option displays the Find Dialog. The Find Dialog searches for matching text in the name field. Press F3 to find the next matching entry. The search is case insensitive. 2.2.4 Configure () This option displays the Configuration Dialog. 2.2.5 Show/Hide Properties Pane () This option allows you to hide or show the Properties pane. 2.2.6 Show/Hide Output Pane () This option allows you to hide or show the Output pane. 2.2.7 Creating a New ICCP Application This drop down is used to create new client, server, and bidirectional applications. When an item is selected from this menu, a configuration dialog will appear with a server and/or a client depending on what option is chosen: Each item in this dialog will be explained in detail below. 2.2.8 (Optional Items) Various other toolbar buttons will appear here based on the active Workspace Pane. These are explained in relevant sections. 2.3 Window Managements Different look and feels can be achieved by rearranging the panes to suit ones particular needs. For example this configuration shows a ICCP Server and a ICCP Client side by side. Note: Most panes can be opened again from the View menu. This section will give you a basic understanding of how to dock and undock the panes. To dock or undock a pane simply drag the tab of the pane you wish to move to another area. A docking arrow bar will appear: This helps give you an indication of where the plane will be docked if you dropped it at the current mouse position. The middle box indicates that the currently dragged pane will become a member of the tab window pane. The top, left, right, and bottom boxes indicated where the window will be docked respectively. To dock the dragged item to the right for example you would move the mouse to the right arrow and release. 2.4 Configuration This dialog contains both global application settings and specific settings for both ICCP/TASE2 Clients and Servers. Application contains the global application settings. Application: Settings o Display Refresh Rate Many windows in the user interface refresh to allow users to see data changes. Refresh Rate defines how many milliseconds between each update. o Enable Full 7 Layer Stack Addressing Enables full 7 layer stack addressing. Note: This must be enabled in order to start multiple servers on the same IP address. See ICCP spec for more details. o Maximum Number of Information Reports Defines the maximum number of Information Reports to keep for each enabled DSTransferSet o Poll Time Defines the default poll time when setting up polled data items. o Save On Close if the user wants Iron to automatically save workspace and all model changes when closing this should be set to true. Otherwise the application will only be saved when the save is manually performed. Note: For more information on the individual settings high light the property and read the help displayed in the grey box. 2.5 Workspace Pane The Main pane consists of one of two different types of documents either an IEC 61850 Server or a ICCP Server. 2.6 Properties Pane The Properties Pane shows the properties of the currently selected item of the active Workspace Pane. 2.7 Output Pane () The Output Pane is used to display errors, warnings, and relevant information. 2.8 Report Pane The Report Pane is used to display DS Transfer Set Information Reports. To view this pane right click on a client tab and select View Reports. 3. I EC 60870 - 6 ( ICCP/ TASE2 ) Test Server The Test Server provides a conformant ICCP Server. ICCP does not define a standard format to store and retrieve the object model like IEC 61850 Substation Configuration Language (SCL). Iron uses an XML representation of the object model as a persist mechanism. The ICCP Servers object model can be constructed from scratch or through the Edit Model menu which allows for reading in a .CSV (or .XML) file. The .CSV format is as follows: <Domain>/<PointName>,Tase2Type <Domain>/<ControlName>,<IsSBO>,<CheckbackId> <Domain>/<Bilateral Table ID>,Bilateral_Table_ID Where <Domain> can be empty, which implies VMD visibility. <IsSBO> is True or False and if True, a checkback id needs to be specified. Here is an example defining both VMD points and points in a domain ICC1: VMDDiscrete1,Discrete VMDDiscreteQ1,DiscreteQ VMDDiscreteQTimeTag1,DiscreteQTimeTag VMDDiscreteExtended1,DiscreteExtended VMDReal1,Real VMDRealQ1,RealQ VMDRealQTimeTag1,RealQTimeTag VMDRealExtended1,RealExtended VMDState1,State VMDStateQ1,StateQ VMDStateQTimeTag1,StateQTimeTag VMDStateExtended1,StateExtended ICC1/Discrete2,Discrete ICC1/DiscreteQ2,DiscreteQ ICC1/DiscreteQTimeTag2,DiscreteQTimeTag ICC1/DiscreteExtended2,DiscreteExtended ICC1/Real2,Real ICC1/RealQ2,RealQ ICC1/RealQTimeTag2,RealQTimeTag ICC1/RealExtended2,RealExtended ICC1/State2,State ICC1/StateQ2,StateQ ICC1/StateQTimeTag2,StateQTimeTag ICC1/StateExtended2,StateExtended ICC1/NonSBOControl,RealSetPoint,False ICC1/SBOControl,DiscreteSetPoint,True,1 ICC1/SBOControlCommand,ControlCommand,True,1 ICC1/BTID1,Bilateral_Table_ID This example also shows all supported types. There is a sample .CSV and .XML model file located in the Public Documents folder named TMWSampleTase2.csv and TMWSampleTase2.xml. The model is persisted in the XML file specified in the configuration window for the property XML Model File. When Iron runs it will use this file to automatically load the model. If the XML Model File path is changed, Iron will both load in any existing model and begin to persist any additional model changes to the new file. 3.1 Menu Bar 3.1.1 View The following items are added to the View menu when a ICCP Server is the active Workspace Pane. 3.1.1.1 Simulation This option shows/hides the Simulation Pane. See Simulation for more information. 3.2 Toolbar When ICCP is the active Workspace Pane the following icons are added to the toolbar. 3.2.1 Show/Hide Simulation Pane () This option displays the Simulation Pane. See Simulation for more information. 3.2.2 Start the Active ICCP Server () This option starts the ICCP Server. 3.2.3 Stop the Active ICCPServer () This option stops the ICCP Server. Note: You can only change the object model of a stopped server. 3.3 ICCP Server Configuration ICCP Server configuration is found under the Options->Configure menu option and on the Toolbar. The ICCP Settings item contains the settings pertaining to ICCP Servers. ICCP Servers contain the following fields: Basic: o ICCP/TASE2 Version Specifies what version to run the ICCP Server in. o Name Name assigned to the IE o XML Model File Path to an XML file to load and persist the model from to. This is normally defaulted to the name of the server. This path must be specified. It can be set to any valid XML file in the correct ICCP format. If the file is left empty, the model will need to be defined (using the Edit Model menu item) before the server can be started. Server Specific: o AE Invoke ID ASCE AE invoke ID. Value from 0 to 65536 o AE Qualifier ASCE AE qualifier. Value from 0 to 65536 o AP Invoke ID ASCE AP invoke ID. Value from 0 to 65536 o Application ID ACSE AP title of the Server o Presentation Selector Presentation selector of the Server o Session Selector Session selector of the Server o Transport Selector Transport Selector of the Server Trace Raw Messages Specific: o Trace Raw TCP Flag to control tracing raw TCP. RFC 1006: Local IP Address Specifies the IP address for the Server to Listen on. 0.0.0.0 means use the OS default IP in case of multiple IPs/NICs. Listening Port Number Specifies the Port the Server will listen on. Defaults to 102. Security: Authentication Mechanism: o None No authentication o Password - Password or Weak authentication in use o Certificate Certificate or Strong authentication in use Security Parameters: o Password Weak- Password Password in use o Certificate Central Authority Certificate Authority File File containing Certificate Authority Certificates Certificate Chaining Depth Depth of certificate chaining verification Path to Certificate Authority Path to Certificate Authority Certificates (instead of file) MMS Certificate file for ASCE private key File containing the certificate for the ACSE private key Client Certificate Path Path to list of possible Client-side Certificates Common Name Common name to expect on incoming MMS certs (empty string disables) Pass Phrase for decrypting private key Pass Phrase for decrypting the private key ACSE authentication Private Key file for signing ACSE File containing the private key for signing ACSE authentication TLS Common Name Common name to expect on incoming TLS certs (empty string disables) DH File Name File containing DH parameters for TLS cipher suites Max PDUs before forcing cipher renegotiation Max PDUs before forcing cipher renegotiation Max renegotiate wait time Max time to wait to respond to renegotiation request Pass Phrase DSA Private Key Pass Phrase for decrypting the private key for DSA TLS ciphers Pass Phrase RSA Private Key - Pass Phrase for decrypting the private key for RSA TLS ciphers Renegotiation (Seconds) Max time in seconds before forcing cipher renegotiation. RSA Private Certificate ID File - File containing the certificate for key for RSA TLS ciphers RSA Private Key File File containing the private key for RSA TLS ciphers 3.4 ICCP/TASE2 Server Model A ICCP Server Model can be created and modified using either a manual single point addition/deletion menu or using the Edit Model tab popup. Note that the server must be stopped before its model can be changed. Here a Domain is being added to the object model. Once a domain is added more object types become available. This is self-explanatory. If an option is not available at a particular level its because ICCP does not support it. To read in a .CSV file (format described above), right click on the server tab and select Configure Model And the Model Configuration dialog will appear: Select the file format first (.CSV or .XML) and click Browse to find and select a valid model file and then click Load to parse and load the model. The Clear Model button will delete all items in the model. The Export Model will export the model to an XML file. This dialog can also be used to view the current model. 3.5 Starting/Stopping a ICCP/TASE2 Server ICCP servers are automatically started if they have a defined model, else they are not started. If a new server is created and the xml file exists, it will be automatically started. If Iron is closed it will automatically create this xml file (or save to it) and thus when started again, the server will be started with a basic model. To Start/Stop a server right click on the server tab and select the desired action: 3.5.1.1 Setting a Value Data in the server can be changed in three ways; by a client, by simulation, or manually. To change data manually right click Data item in the server and choose Write or Set Value. An appropriate dialog will appear for the desired action. 3.6 Simulation Pane The Simulation Pane allows a Server to randomly change data which causes data change events which in turn activates Reports if DSTransferSets are enabled. To Start/Stop the simulation, press the Start button. Once the simulation is running the buttons text will say Stop. The data change interval is controlled by the Change Data Every (Milliseconds) field. If an item has a check mark then its data is being changed. Un-checking an item turns off the data change. The pane on the right shows all Data objects of the selected item in the right pane including all children. The fields of the right panel are as follows: Name Shows the name of the Data Attribute Value Current value Type Type of the value Step Upon each data change this is the value that is added to the current value. Current value is stepped until simulation max is reached it is then seeded with a random value and restarted. Simulation Min Minimum value Simulation Max Maximum value Value, Step, Simulation Min and Simulation Max can be changed by double clicking on the value. 3.7 Writing Values Values in the server can be manually written to using the Set Value functionality. Right click on a point and select Set Value to see the Write Dialog Select the desired data to change, type in or select the new value and then select the Write button. 4. IEC 60870 - 6 ( ICCP/TASE2 ) Test Client The ICCP Client is a generic client that can connect to any ICCP Server. The Test Client is able to perform typical client operations including services such as Discovery and Reporting. 4.1 Menu Bar 4.1.1 Context Information The user can right click on the client tab or on a client in the tree for client relevant functionality. 4.2 Client Configuration Client configuration is found under the Options->Configure menu or by right clicking on the client in the tree or its tab and selecting Configure ICCP. The configuration window below will popup: Basic: o Auto Reconnect Will cause the client to automatically retry connecting to its server whenever it loses its connection or is unable to make a connection except when manually disconnected. Defaults to False. o Auto Reconnect Tries The number of times to try to reconnect when Auto Reconnect is set to true. The value of 0 means try forever. o Auto Reconnect Wait Time The number of milliseconds to wait to try to reconnect when auto reconnect is true. o Connect on Start Indicates if the client should attempt to connect to the IP Address when starting. If false, the user must manually select connect. o IP Address IP Address of the Server to connect to o Name Name of the Server (connection) o Port Port the server is listening on. Default is 102 o XML Model File Name of the XML file for the model. This will normally default to the name of the client. This can be set to any valid XML file that is in the correct ICCP model format. Or it can be left empty if discovery is desired. Client Specific: These are used by the server to identify this client. o AE Invoke ID ASCE AE invoke ID. Value from 0 to 65536 o AE Qualifier ASCE AE qualifier. Value from 0 to 65536 o AP Invoke ID ASCE AP invoke ID. Value from 0 to 65536 o Application ID ACSE AP title of the Server o Presentation Selector Presentation selector of the Server o Session Selector Session selector of the Server o Transport Selector Transport Selector of the Server Server Specific: These are used by the server to identify a particular server o AE Invoke ID ASCE AE invoke ID. Value from 0 to 65536 o AE Qualifier ASCE AE qualifier. Value from 0 to 65536 o AP Invoke ID ASCE AP invoke ID. Value from 0 to 65536 o Application ID ACSE AP title of the Server o Presentation Selector Presentation selector of the Server o Session Selector Session selector of the Server o Transport Selector Transport Selector of the Server Trace Raw Messages o Trace Raw TCP: Enables/Disables a raw trace of TCP messages. Produces a file that TMW can use to help diagnose issues. Note: Only used when the use of a sniffer such as Wireshark is unavailable. RFC 1006: Local IP Address The IP address that the client will use to connect to a server. 0.0.0.0 means the OS default. Security: Identical to server settings (see above) To add a new Client Select Create ICCP Client or Select Create ICCP Client/Server (Bidirectional) to create a bidirectional test application. 4.3 Configuring Client Model The client model can be configured in the identical way that a server model is configured described above. It can be set via a .csv or .xml file. Or individual domains and points and datasets can be added: Or via the Configure Model menu option shown below. As with the server, except for dynamic datasets, the client must be stopped before modifying the model is allowed. 4.4 Connecting to an ICCP/TASE2 Server 4.4.1 Connecting to a Server To connect to a server press the right arrow button on the right on the Client tab and select Connect Keep Model or Connect Discover. Discovering the servers model is not recommended and can lead to invalid types in the client because the protocol does not give enough type information to determine certain types. 4.4.2 Read To read a particular item in the model Right click on the item and select Read. Status messages are written to the Output Pane. 4.4.3 Write The client is technically not supposed to write values to points unless the point is a control type. However, since Iron is a test tool, writes are allowed on non-control points. Iron supports all control types and they are specified in the description of the .CSV file above. When the user right clicks on a control point the control point context menu appears: The user can Read, Select, Operate, or Select And Operate the control point. Select will only appear if SBO is enabled for the control, else only the Operate menu will appear. If SBO is enabled, the SBO point will also appear with the _SBO appended to the name. The SBO point can be selected to view properties and right clicked on to Read, which has the same effect as selecting the control itself. The value of the SBO point is the checkback name, which is a long value and must match the server. 4.4.4 Creating Data Sets Data Sets can be created by right clicking on the Data Sets entry in the tree and selecting Add Data Set. Choose Add Dataset to show the following window: Simply select the domain you wish to make members of the DataSet. The Include with Report are special items that can be added to the Report returned from the server. The items on the left are all of the items in the model. Select the Create/Modify DataSet button to create a dynamic data set on the client and server that can now be associated with a DS Transfer Sets. Note the client must be connected before adding a data set is allowed. 4.4.5 Modifying Data Sets Similar to creating datasets, right client on an existing dataset and select Modify DataSet : And then modify dataset as desired, removing or adding members, or change Include with Report options, or change the name, and select Create/Modify Dataset : Iron will then attempt to dynamically make the modifications. If any failures occur, the errors will show up in the protocol analyzer logs. 4.4.6 Deleting Data Sets Data Sets that have you have created can be deleted. This will be indicated by appending deletable to any Data Set that you are allowed to delete. Simply right click on the Data Set and select Delete DataSet. 4.4.7 Enable/Disable DSTransferSets Using DSTransferSets in ICCP is a multi-step process. The ICCP Client shows all discovered DSTransferSets. In order to use one it must be reserved by the Client. To reserve a DSTransferSet right click on the Active DS Transfer Sets item in the tree and choose Get Next Available. If successful you will see the following change in the tree. Notice the DS Transfer Sets item is now empty and the Active DS Transfer Sets now has one item. This DSTransferSet, DSTrans1, can now be used by the client. At this point you are able to write to it in order to change its settings. One thing you might want to do is to assign a Dataset to report. To set or change the Data Set in the DSTransferSet. Right click on the DSTransferSet entry and choose Add Data Set Reference->< Data Set Name>. A list of available Data Sets is displayed. To remove the Data Set in the Control Block. Right click on the Control Block and choose Delete Data Set Reference. Finally to enable the DSTransferSet right click on it and choose Enable DSTransferSet. To Disable a DSTransferSet right click on the DSTransferSet and Select Disable DSTransferSet. Doing so will disable the DSTransferSet as well as deactivate the DSTransferSet. After disabling it the tree will look like the following: To enable it the process must be started again. Once a TransferSet is enabled its reports can be viewed in the Report Pane described above. 4.4.8 Polling Any Data Object or its children can be read on a polled basis. Right click the item and select Start Polling. Select the time between polls. Select Start Polling Enter the Polling period and press OK. To Stop polling right click on the item being polled and select Stop Polling. 

************Test Harness Manual**********
TRIANGLE MICROWORKS, INC. CommunicationProtocol TestHarness ProductDocumentation Version3.31 May1,2023 Communication Protocol Test Harness User Manual Property of Triangle MicroWorks, Inc. This Documentation and its associated distribution media contain proprietary information of Triangle MicroWorks, Inc. and may not be copied or distributed in any form without the written permission of Triangle MicroWorks, Inc. Copies of the source code may be made only for backup purposes. 2000 - 2023 Triangle MicroWorks, Inc. All rights reserved. iii Communication Protocol Test Harness User Manual READ THIS BEFOREINSTALLING THE SOFTWARE AND BEFORE ACCEPTING THE LICENSEAGREEMENT THIS SOFTWARE LICENSE AGREEMENT IS BEING PROVIDED TO YOU AS PART OF A TRIAL FOR REVIEW BEFORE PURCHASE. THE FOLLOWING LICENSE AGREEMENT IS A CLICKTHROUGH LICENSE AGREEMENT THAT IS SHOWN DURING THE INSTALLATION OF THE SOFTWARE. YOU MUST AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT IN ORDER TO INSTALL THE SOFTWARE. THEREFORE, IF YOU DO NOT AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT, DO NOT INSTALL THE SOFTWARE. TRIANGLEMICROWORKS,INC.LicenseAgreementandWarranty THIS IS A LEGAL AGREEMENT BETWEEN YOU (YOU) AND TRIANGLE MICROWORKS, INC. (TRIANGLE MICROWORKS). TRIANGLE MICROWORKS IS WILLING TO LICENSE THE TRIANGLE MICROWORKS SOFTWARE AND RELATED DOCUMENTATION PURCHASED BY YOU AND KNOWN AS COMMUNICATIONS PROTOCOL TEST HARNESS (THE SOFTWARE) TO YOU ONLY UPON THE CONDITION THAT YOU ACCEPT ALL OF THE TERMS CONTAINED IN THIS LICENSE AGREEMENT (THE "AGREEMENT"). PLEASE READ THE TERMS AND CONDITIONS CAREFULLY BEFORE CLICKING ON THE I AGREE BUTTON, AS CLICKING ON THIS BUTTON WILL INDICATE YOUR ASSENT TO THEM. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, THEN CLICK ON THE I DO NOT AGREE BUTTON. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, TRIANGLE MICROWORKS IS UNWILLING TO LICENSE THE SOFTWARE TO YOU. 1. Subject to the following terms and conditions, Triangle MicroWorks grants to you a nonexclusive, nontransferable limited license to install and use the Software solely for your internal business purposes and subject to the limitations in this Agreement. There are three mechanisms to license the Software. The Seat License is a PC based software license that is installed on a single computer and is solely for the use of the Software by you on a single computer at any one time. The Hardware Dongle License is a license that is stored on a dedicated USB hardware device that can be shared between computers by physically moving the Hardware Dongle License. The Hardware Dongle License is solely for the use of the Software by you on a single computer at any one time. The Network License is a license that is stored on a computer that can be shared across a local area network. The Network License is solely for the use of the Software by you on a single computer at any one time. The Network License may be installed on a corporate network and shared locally. The Network License cannot be shared outside of your company. You may physically transfer or copy the Software on to another computer; provided, however that each license instance may not be used on more than one computer or device at any given time. Subject to the foregoing limitation, you are entitled to copy the Software into any machine-readable or printed form for backup purposes in support of the use of the Software on a single computer by you. Any copy of the Software permitted by the preceding provision shall include a copyright notice of Triangle MicroWorks. 2. You may use the Software solely for the applicable term (i.e., period of time) as set forth below. If you have licensed the Software for a free trial period (a Trial License), the term of the Software license will be for 21 days. You understand and acknowledge that after the 21-day period and unless otherwise extended by Triangle MicroWorks, in its sole discretion, the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software. At the conclusion of the Trial License, you may purchase a Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have licensed the Software for a fixed period of time and not on a trial basis (a Term License), the term of the Software license will be the period of time agreed to by you and Triangle MicroWorks. At the end of that period of time, your use of the Software will terminate, and the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software except as explicitly set forth herein. At the conclusion of the Term License, you may purchase an additional Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have paid a one-time fee for a perpetual license, then your term of the Software License is perpetual (a Perpetual License) and subject to termination only as set forth herein. v Communication Protocol Test Harness User Manual 3. You may not transfer the Software to another entity or person. You shall not modify, translate, reverse assemble or reverse compile in whole or in part the Software. You shall not rent, sell, time-share, lease, sublicense, transfer, copy, disclose, display or otherwise make available the Software or copies thereof to others, except as provided herein. Any breach of the above terms and conditions shall immediately terminate your license under this Agreement. You hereby certify and agree that the Software will not be shipped, transferred, or re-exported, directly or indirectly, into any country prohibited by the United States Export Administration Act and the regulations thereunder, and the Software will not be used for any purpose prohibited by the same. 4. You may not erase, delete or modify any copyright, trademark or other proprietary notices included on the Software. You agree that if you infringe Triangle MicroWorks intellectual property rights or exceed the scope of permitted use of this license, Triangle MicroWorks will be irreparably injured and may obtain a court order to enjoin you from further use of the Software. 5. This license is not a sale. Title, ownership rights, and intellectual property rights in and to the Software shall remain in Triangle MicroWorks. You agree to abide by the copyright laws and all other applicable laws of the United States and international treaties. You acknowledge that the Software in source code form remains a confidential trade secret of Triangle MicroWorks. Failure to comply with the above restrictions will result in automatic termination of this Agreement and will make available to Triangle MicroWorks other legal remedies. 6. Notwithstanding anything herein to the contrary, you acknowledge and agree that Triangle MicroWorks may terminate your license to the Software if you have failed to comply with any of your obligations under this Agreement. You shall not be entitled to a refund in the event of termination by Triangle MicroWorks pursuant the foregoing sentence. 7. By purchasing a Term License or a Perpetual License to the Software you are automatically subscribed in the Software Maintenance & Enhancement Plan (the Plan) for a period of one (1) year following the date of purchase (the Purchase Date) of the license, or if shorter, the term of your Term Licensee (such initial period referred to as the Initial Maintenance Period). Upon expiration of the Initial Maintenance Period, the Plan may be renewed on an annual basis subject to payment by you of the applicable then-current annual fee. This Plan may provide a cost effective method for upgrading to the most recent release version of the Software. The Plan also provides technical support by telephone, fax, or email. The Initial Maintenance Period for Perpetual Licenses is free. The annual fee for the Plan for Perpetual Licenses after the first year is 25% of the Perpetual License purchase price; provided, however that Triangle MicroWorks, in its sole discretion, may adjust the annual fee at any time. The annual fee for the Plan for Term Licenses depends on the specific terms of the Term License. Although Triangle MicroWorks has no obligation to provide upgrades to the Software under the Plan, Triangle MicroWorks may from time to time provide upgrades. Any upgrades to the Software shall be deemed to become part of the Software and shall be subject to the terms and conditions of this Agreement. All Plan annual fees are nonrefundable. 8. For Term Licenses and Perpetual Licenses only, Triangle MicroWorks warrants that the Software shall perform substantially as described in its documentation for a period of sixty (60) days from the Purchase Date. In the event of a breach of the foregoing warranty, the sole obligation of Triangle MicroWorks and your sole remedy is that Triangle MicroWorks will provide you with a refund of the most recent license fee paid for the Software. Triangle MicroWorks, as a condition to providing such refund, may require that you sign an affidavit certifying that you have deleted or destroyed all copies of the Software in your possession. Triangle MicroWorks makes no warranties with respect to Trial Licenses. Except as set forth in this Section 8, THE SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTIES OF ANY TYPE, AND TRIANGLE MICROWORKS HEREBY EXPRESSLY DISCLAIMS ALL WARRANTIES, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY, TITLE, NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE. 9. TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL TRIANGLE MICROWORKS BE LIABLE FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL OR PUNITIVE DAMAGES WHATSOEVER, INCLUDING ANY CLAIMS FOR LOST PROFITS, BUSINESS INTERRUPTION, LOST INFORMATION OR OTHER DAMAGES ARISING OUT OF THE USE OF INABILITY TO USE THE SOFTWARE. IN NO EVENT SHALL TRIANGLE MICROWORKS HAVE ANY LIABILITY FOR ANY DATA STORED OR PROCESSED WITH THIS SOFTWARE, INCLUDING THE COST OF RECOVERING ANY LOST DATA. IN ALL EVENTS, TRIANGLE MICROWORKS SOLE OBLIGATION OR LIABILITY UNDER THIS AGREEMENT IN THE AGGREGATE IS A REFUND OF THE MOST RECENT LICENSE FEE PAID FOR THE SOFTWARE. THE PARTIES AGREE THAT THESE LIMITATIONS ARE AN ESSENTIAL PART OF THE AGREEMENT AND THAT TRIANGLE MICROWORKS WOULD NOT ENTER INTO THE AGREEMENT WTIHOUT THESE LIMITATIONS. THIS LIMITATION OF LIABILITY CLAUSE WILL SURVIVE FAILURE OF ITS ESSENTIAL PURPOSE. Page vi Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 10. You may terminate the license granted under this Agreement at any time by destroying or deleting all copies of the Software in your possession or control. In the event of termination of this Agreement for any reason, all the sections of this Agreement will survive except for Sections 1 and 2. 11. This Agreement shall be governed by and construed in accordance with the laws of the State of North Carolina, without regard to the choice of law provisions thereof. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to this Agreement. Any dispute or claim arising out of, or in connection with, this Agreement shall be finally settled by binding arbitration in Raleigh, North Carolina, in accordance with N.C. Gen. Stat. 1-569.1 et seq. (the Uniform Arbitration Act) and the then-current rules and procedures of the American Arbitration Association by one (1) arbitrator appointed by the American Arbitration Association. The arbitrator shall apply the law of the State of North Carolina, without reference to rules of conflict of law or statutory rules of arbitration, to the merits of any dispute or claim. Judgment on the award rendered by the arbitrator may be entered in any court of competent jurisdiction. The parties agree that, any provision of applicable law notwithstanding, they will not request, and the arbitrator shall have no authority to award, punitive or exemplary damages against any party. In the event that any arbitration, action or proceeding is brought in connection with this Agreement, the prevailing party shall be entitled to recover its costs and reasonable attorneys' fees. In the event that the above arbitration provision is held invalid or unenforceable, any dispute with respect to this Agreement shall be brought and heard either in the North Carolina state courts located in Raleigh, North Carolina or the federal district court, located in Raleigh, North Carolina. In such event, the parties to this Agreement each consent to the in personam jurisdiction and venue of such courts. The parties agree that service of process upon them in any such action may be made if delivered in person, by courier service, by telegram, by telefacsimile or by first class mail, and shall be deemed effectively given upon receipt. 12. This Agreement sets forth the entire agreement between you and Triangle MicroWorks pertaining to the licensing of the Software and supersedes in its entirety any and all written or oral agreements previously existing between the parties with respect to such subject matter. If any provision of this Agreement is held invalid or unenforceable, such provision shall be revised to the extent necessary to cure the invalidity or unenforceability, and the remainder of this Agreement shall continue in full force and effect. In the event of any conflict between any provision of this Agreement and any applicable law, the provision or provisions of this Agreement affected shall be modified to remove such conflict and permit compliance with such law and as so modified this Agreement shall continue in full force and effect. This Agreement may not be assigned without the consent of Triangle MicroWorks. Triangle MicroWorks, Inc. 2840Plaza Place, Suite 205, Raleigh, NorthCarolina 27612 USA (919)870-5101 vii Communication Protocol Test Harness User Manual ix Communication Protocol Test Harness User Manual 1. Overview................................................................................................................... 15 1.1 Background Information ................................................................................... 17 1.1.1 IEC 60870-5 Documents............................................................................... 17 1.1.2 DNP3 Documents ......................................................................................... 17 1.2 Organization of This Manual............................................................................ 17 1.3 Product Architecture ......................................................................................... 19 1.3.1 Tcl/Tk scripting ............................................................................................. 19 1.3.2 Python ........................................................................................................... 20 1.3.3 .NET Plug-ins ............................................................................................... 20 1.4 Modes of Operation .......................................................................................... 21 1.4.1 Default End User Application ....................................................................... 21 1.4.2 Custom Applications..................................................................................... 22 2. Installation and Setup................................................................................................ 22 2.1 System Requirements........................................................................................ 22 2.2 Installation......................................................................................................... 22 2.3 Licensing ........................................................................................................... 23 2.4 Configuration .................................................................................................... 23 2.4.1 Command line options.................................................................................. 24 3. Basic Functionality ................................................................................................... 25 3.1 Using an Existing Workspace........................................................................... 25 3.2 Opening a Connection to a Device ................................................................... 26 3.3 Configuring a Connection to a Device.............................................................. 27 3.3.1 Channel Tab.................................................................................................. 28 3.3.2 Session Tab................................................................................................... 29 3.3.3 Sector Tab ..................................................................................................... 30 3.3.4 Next Step Tab ............................................................................................... 30 3.4 Command Window........................................................................................... 32 3.5 Simulator Window............................................................................................ 34 3.6 Using Workspaces ............................................................................................ 35 3.7 Monitoring Connections ................................................................................... 36 3.7.1 Modbus Monitor Channel............................................................................. 39 3.8 Other Test Harness Features ............................................................................. 41 3.8.1 Conformance Test Module ........................................................................... 41 3.8.2 Validating Points and Controlling External Instrumentation........................ 42 4. User Interface............................................................................................................ 45 4.1 Menu Bar .......................................................................................................... 46 4.1.1 File Menu...................................................................................................... 47 4.1.2 Open Menu.................................................................................................... 48 4.1.3 Edit Menu...................................................................................................... 49 4.1.4 Window Menu .............................................................................................. 51 4.1.5 Plugins Menu ................................................................................................ 51 4.1.6 Help Menu .................................................................................................... 52 4.2 Protocol Analyzer Display ................................................................................ 53 4.2.1 Controlling the protocol display format........................................................ 54 4.2.2 Searching the protocol display ...................................................................... 55 xi Communication Protocol Test Harness User Manual 4.2.3 Log to Text File ............................................................................................ 55 4.3 Command Shell................................................................................................. 56 4.4 Data Window .................................................................................................... 57 5. Extending Functionality ............................................................................................ 59 5.1 Extending with .NET Plug-Ins.......................................................................... 59 5.1.1 Introduction to .NET Plug-Ins ...................................................................... 59 5.1.2 Testing Plug-in Framework .......................................................................... 60 5.1.3 User Plug-in Framework............................................................................... 62 5.1.4 Debugging .NET Plug-ins............................................................................. 64 5.1.5 Examples....................................................................................................... 66 5.2 Extending with Tcl/Tk ...................................................................................... 69 5.2.1 Introduction to Tcl/Tk ................................................................................... 69 5.2.2 Asynchronous Commands ............................................................................ 74 5.2.3 Test Script Generation and Execution .......................................................... 75 5.2.4 Duplicating Configurations........................................................................... 75 5.2.5 Generic Test Harness Commands................................................................. 75 5.3 Extending with Python...................................................................................... 80 5.3.1 Introduction to Python .................................................................................. 80 5.3.2 Tutorial.......................................................................................................... 80 5.3.3 Python Command Shell ................................................................................ 80 5.4 Examples........................................................................................................... 82 5.4.1 IEC 60870-5-101/104 Examples................................................................... 82 5.4.2 IEC 60870-5-102 Examples.......................................................................... 83 5.4.3 IEC 60870-5-103 Examples.......................................................................... 84 5.4.4 DNP3 Examples............................................................................................ 84 6. Advanced Topics ...................................................................................................... 86 6.1 TCP/IP Connections.......................................................................................... 86 6.1.1 Configuring Master (client) channels ........................................................... 86 6.1.2 Configuring Outstation (Controlled Station or Server) channels ................. 87 6.2 IP Networking Support for DNP3..................................................................... 87 6.2.1 Configuring DNP3 channels......................................................................... 88 6.2.2 DNP Dual End Point Plugin ......................................................................... 90 6.3 Secure Authentication ....................................................................................... 92 6.3.1 DNP3 Secure Authentication ........................................................................ 92 6.3.2 Simulating a DNP3 Secure Authentication Master ...................................... 93 6.3.3 Simulating a DNP3 Secure Authentication Outstation ................................. 93 6.3.4 DNP3 Secure Authentication Version 2 or Version 5 .................................. 94 6.3.5 IEC 60870-5-101 and 104 Secure Authentication ........................................ 95 6.3.6 Simulating a 101/104 Secure Authentication Master ................................... 96 6.3.7 Simulating a 101/104 Secure Authentication Controlled Station ................. 96 6.3.8 Key Types ..................................................................................................... 96 6.3.9 Sample Asymmetric Keys included with Test Harness................................ 98 6.3.10 Testing a Secure Authentication Device................................................. 100 6.3.11 OpenSSL for Cryptography Functionality .............................................. 100 6.4 Using DNP3 Binary Configuration Files........................................................ 100 6.5 Generating DNP3 Events................................................................................ 101 Page xii Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 6.5.1 Generating DNP Analog Events................................................................. 101 6.5.2 Generating all DNP Events......................................................................... 102 6.6 SSL or TLS over TCP/IP................................................................................ 104 6.7 IEC 60870-5-101/4 Measured Values: MMENA/B/C, PMENA/B/C, and PACNA explained. ..................................................................................................... 106 6.8 IEC 60870-5-104 Redundancy ....................................................................... 109 6.8.1 Configuring Redundant Controlled Station Channels ................................ 109 6.8.2 Configuring Redundant Master Channels................................................... 109 6.9 IEC 60870-5 Private orCustom ASDU Support............................................ 109 6.9.1 Master ......................................................................................................... 110 6.9.2 Controlled Station....................................................................................... 111 6.10 Periodic Outstation Data Changes using Plugins............................................ 112 6.11 Simulating Data changes with CSV files........................................................ 114 6.11.1 DNP CSV file format.............................................................................. 114 6.11.2 User Interface.......................................................................................... 116 6.12 File Transfer.................................................................................................... 117 6.12.1 DNP File Transfer................................................................................... 118 6.12.2 IEC 101 and 104 File Transfer................................................................ 118 7. Conformance Test Modules.................................................................................... 121 7.1 DNP3 Outstation Conformance Tests............................................................. 121 7.1.1 DNP3 Subset Levels ................................................................................... 122 7.1.2 Setting Test Conditions............................................................................... 123 7.2 MESA-DER Conformance Tests.................................................................... 124 7.2.1 Output ......................................................................................................... 129 7.2.2 Saving Results............................................................................................. 130 7.2.3 Test Sections............................................................................................... 130 7.2.4 Test Results File.......................................................................................... 131 7.2.5 Testing your Outstation DER EUT. ............................................................ 134 8. Troubleshooting ...................................................................................................... 135 8.1 No License Found........................................................................................... 135 8.2 DLL Not Found............................................................................................... 135 8.3 Help Format error ........................................................................................... 135 8.4 No communication with a remote device seems to be occurring ................... 135 8.4.1 Physical layer error: Error opening channel............................................ 135 8.4.2 Configuration .............................................................................................. 136 8.4.3 Other issues................................................................................................. 136 8.5 IEC 60870-5-104 Sessions Connect, but then disconnect immediately ......... 137 8.6 Using an RS-232/RS-485 Converter with the Test Harness........................... 137 8.7 Issuing Control Commands in IEC 60870-5 Protocols................................... 137 8.8 Simultaneously Simulating Master and Outstation Devices........................... 137 xiii Communication Protocol Test Harness User Manual 1. Overview Congratulations on your purchase of a Triangle MicroWorks Communication Protocol Test Harness. This application is a Microsoft Windows based application that supports the development and testing of several different communication protocols. The Test Harness is designed to support the testing of a communication system, ranging from an individual device to an entire communication network. Some sample applications are shown in Figure 1 below. Triangle MicroWorks, Inc. Page 15 Communication Protocol Test Harness User Manual Figure 1: Example Test Harness application configurations. This Test Harness supports a full set of protocol specific commands which provide access to the functionality of each of the supported protocols. In addition, the protocol Test Harness supports advanced scripting and other features that make it ideal for testing communications systems during development, production, and in the field. The Test Harness scripting language(s) and Plug-ins can be used to generate test scripts or to develop custom test applications with custom user interfaces and functionality. Page 16 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 1.1 BackgroundInformation The following documents define the protocols supported by the Test Harness and are useful in determining how to test your network. 1.1.1 IEC60870-5Documents IEC 60870-5-101 = Companion standard for basic telecontrol tasks IEC 60870-5-101 A2 = Addendum 2 for IEC 60870-5-101 IEC 60870-5-102 = Companion standard for the transmission of integrated totals in electric power systems IEC 60870-5-103 = Companion standard for the informative interface of protection equipment IEC 60870-5-104 = Companion standard for IEC 60870-5-101 over TCP/IP IEC 60870-5-5 = Basic Application Functions IEC 60870-5-2 = Link Transmission Procedures IEC 60870-5-4 = Definition and Coding of Application Information Elements IEC 60870-5-3 = General Structure of Application Data IEC 60870-5-1 = Transmission Frame Formats 1.1.2 DNP3Documents DNP3 Basic 4 Document Set DNP3 Subset Definitions Technical Bulletins released by the DNP3 User Group Technical Committee DNP3 Intelligent Electronic Device (IED) Certification Procedure, Subset Level 1, Version 1.00, 18-Jan-99 DNP3 Intelligent Electronic Device (IED) Certification Procedure, Subset Level 2, Version 1.00, 18-Jan-99 1.2 Organization ofThis Manual This section provides a brief overview of this manual and will help determine which portions of the manual are pertinent to a particular users needs. Triangle MicroWorks, Inc. Page 17 Communication Protocol Test Harness User Manual Chapter 1 provides a high level overview of the Protocol Test Harness including product architecture and other information that will help in determining how the Test Harness fits into a specific environment. This chapter will also provide information which will help understand the details presented in later chapters. This chapter also provides an overview of the Test Harness, including the product architecture and a discussion of some of the most common ways in which the product is typically used. The information in this chapter will be useful in determining how to use the Test Harness most effectively in your situation. Chapter 2 describes how to install, license, and configure the Test Harness. Chapter 3 describes the user interface provided with the Test Harness. This user interface is the typical end users view of the Test Harness and will provide access to all of the Test Harness functionality. Chapter 4 provides an introduction to Tcl/Tk scripting which is one way to begin writing test scripts and ultimately design custom test applications. This chapter also describes details pertinent to Triangle MicroWorks, Inc. extensions to the Tcl/Tk environment. Chapter 5 describes generic commands available in all Triangle MicroWorks, Inc. Test Harnesses. Chapter 6 provides various troubleshooting tips and guidelines. The appendices provide protocol specific details including available commands, examples, simulations, and test scripts where available. Page 18 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 1.3 ProductArchitecture Triangle MicroWorks, Inc. Test Harness supports several different protocols in a single application. Which protocols are available is based on the current product license installed on the target computer. The currently supported protocols and their associated abbreviations are listed in the table below. Protocol Abbreviation IEC 60870-5-101 Master m101 IEC 60870-5-101 Controlled Station s101 IEC 60870-5-102 Master m102 IEC 60870-5-102 Controlled Station s102 IEC 60870-5-103 Master m103 IEC 60870-5-103 Controlled Protection Equipment s103 IEC 60870-5-104 Master m104 IEC 60870-5-104 Controlled Station s104 DNP3 Master mdnp DNP3 Outstation sdnp Modbus Master mmb Modbus Server smb Table 1 Currently Available Protocol Test Harness Products The Triangle MicroWorks Test Harness is based on the Microsoft .NET Framework and the TMW .NET Protocol Components. The .NET Protocol Components provide an Object oriented high level of abstraction to the TMW Source code libraries. The Test Harness accesses the underlying protocols through this abstraction layer. The Test Harness comes with a predefined user interface. This user interface supports most of the common actions a user would perform. The Test Harness also provides several mechanisms to support user extension: 1.3.1 Tcl/Tkscripting The Test Harness can utilize the Tcl/Tk scripting language. Tcl/Tk is a popular and mature scripting language that is designed with a flexible architecture so that it can be Triangle MicroWorks, Inc. Page 19 Communication Protocol Test Harness User Manual incorporated into a wide variety of applications. Tcl/Tk is easy to use, yet powerful enough to support the development of advanced test scripts and/or full custom test applications. Triangle MicroWorks has extended the basic set of Tcl/Tk commands to include commands required to fully test each of the supported protocols. The extended command set includes a small set of protocol independent commands and a set of protocol specific commands which provide an interface to the target protocol functionality. Built into the default user interface is an embedded Tcl interpreter that supports the execution of test scripts written in the Tcl language. Test scripts can range from very simple scripts that execute a series of commands to advanced scripts that validate status, check data values, and monitor other protocol details. The Tcl language includes a full set of conditional, looping, and other constructs required to build powerful test scripts. The generation of test scripts will be described later in this document In addition to the development of test scripts you can use the power of Tk (the Graphical User Interface extension to Tcl) to build full custom test applications. These custom applications are specific to each situation and can be customized to meet the exact needs of the target system. This may be useful for regression testing, production testing, and/or field-testing a device. 1.3.2 Python The Test Harness can utilize the Python language. Python is a popular and easy to learn Dynamic Object-oriented programming language. The Test Harness integrates the IronPython version of Python. As a result of the high level of integration between the .NET Framework and IronPython the Test Harness provides access to the full features of the TMW .NET Protocol Components as well as the Microsoft .NET Framework. Just like with Tcl/Tk the end user can create anything from simple scripts to full featured applications using the Python extensions. 1.3.3 .NETPlug-ins Since the Test Harness is based on the TMW .NET Protocol Components and is built on top of the .NET Framework, the Test Harness can utilize this architecture to support user defined plug-ins. These plug-ins must conform to a predefined interface in order to integrate into the Test Harness. Plug-ins can be used to extend the Test Harness with additional commands, simulators and conformance/regression test applications. Two interfaces are provided 1) a User plug-in and 2) a conformance/regression test plug-in. The end-user can use common commercial and free .NET development tools to create these plug-ins. Examples are provided for command extensions, simulators and regression tests. Plug-ins can be developed in any supported .NET programming language. Page 20 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 1.4 ModesofOperation As described above, there are two main modes of operation for this product. The first mode utilizes the predefined user. This user interface is quite powerful; supports embedded scripting; and should be adequate for most testing needs. This mode of operation includes the support of custom test scripts written in Tcl, Python, and .NET using the standard features of the Test Harness. A second mode of operation includes generating custom test applications by customizing or extending the scripts provided by Triangle MicroWorks, or generating completely custom scripts from scratch. Each of these modes will be discussed briefly in this section and in more detail in the rest of this document. 1.4.1 DefaultEndUserApplication The Test Harness is delivered with a predefined user interface. An example of the predefined user interface is shown below. This interface is the default interface displayed when the Test Harness application is invoked and should be adequate for the vast majority of your testing needs. This interface supports the execution of interactive commands typed into the interface as well as the execution of test scripts written in any extension mechanism. Figure 2 Test Harness User Interface Triangle MicroWorks, Inc. Page 21 Communication Protocol Test Harness User Manual The default user interface consists of a menu bar, a protocol analyzer display with a control panel and device filter, and a command input window. The command input window includes an embedded Tcl/Tk shell, a Python shell, and a DOS shell that can be used to enter commands from the keyboard or from a predefined test script. Each of these features will be discussed in detail below. 1.4.2 CustomApplications If the default user interface and application is not sufficient the user can develop custom applications using the extension mechanisms described above. For example, if you were developing a device that required the execution of four distinct tests for validation you might want to develop a simple interface with four buttons and a status display to be used as a factory acceptance test tool. Again, this will be described in more detail later in this document. 2. InstallationandSetup This chapter describes in detail how to install and configure the Protocol Test Harness. 2.1 System Requirements The Triangle MicroWorks Test Harness is a Windows based application and is supported under the following operating systems (32-bit and 64-bit) Windows 7 Windows 7 Embedded Standard Windows Server 2008 Windows 8 Windows 8 Embedded Standard Windows Server 2012 Windows 10 Windows 10 Embedded Standard Windows Server 2016 The minimum system requirements for the host computer are: Windows 7 Microsoft .NET 4.6 Framework 2.2 Installation The Triangle MicroWorks, Inc. Protocol Test Harness is delivered as a single installation file named thvmnn.exe where m is the major version and nn is the minor version of the Test Harness. To install a Protocol Test Harness simply execute this file and follow the directions. Page 22 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual After the Test Harness is installed you can reinstall or remove it by rerunning the installation application. This can also be accessed from the Windows Add/Remove Programs option in the Windows Control Panel. When the Test Harness is first run on a single computer it will install a demo license which supports all of the supported protocols for 21 days. To continue using the Test Harness beyond this period you must purchase and install a permanent license for the desired protocols. Contact sales@tmw-usa.com. In addition to installing the required files on your system the installation application performs the following steps. 1) Creates entries in the Start->Programs menu to access the features of the Test Harness. 2) Creates a desktop icon to start the Test Harness. Once the Test Harness is installed it can be run by double clicking the Desktop Icon, using the appropriate entry from the Start->Programs->Triangle MicroWorks->Protocol Test Harness folder on the Start Menu, or by executing the tmwtest.exe file found in the installation/bin directory. This executable starts the Test Harness. Alternatively you can run the tmwtest.exe executable with various command line options to launch the Test Harness and execute a custom test application. Enter tmwtest -? at the windows command prompt to get help on the supported command line options. Each of these approaches will be discussed in the appropriate sections below. 2.3 Licensing Test Harness uses the Sentinel LDK licensing system from SafeNet, Inc. This is an advanced and full featured system that allows you to manage your license keys for Triangle MicroWorks products. The system provides the capability to transfer licenses to other machines with needing to contact Triangle MicroWorks. If you have purchased a network license, you can detach license from the network pool and use the license away from the network. All of the capabilities of the system are described in the document called Guide to Triangle MicroWorks Licensing. This document fully describes how to convert your demo version of this product into a fully licensed version. 2.4 Configuration In general, establishing communications amounts to opening one or more communication channels, then opening one or more sessions on each channel, and finally one or more sectors for each session (if required by the protocol). Each protocol has specific open/close channel, open/close session, and open/close sector commands. These commands are described in the command line help. The diagram below illustrates the relationship of channels, sessions, and sectors. Triangle MicroWorks, Inc. Page 23 Communication Protocol Test Harness User Manual 2.4.1 Commandlineoptions By default, the Test Harness executes the startup script tmwtest.tcl which calls tmwstart.tcl when the Test Harness is started. By specifying command line arguments to tmwtest.exe this behavior can be changed. Use tmwtest -? To list the available command line options. For example, the following command: tmwtest.exe tcl SDNPSim will load and execute the DNP3 generic simulator. Page 24 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 3. BasicFunctionality 3.1 Using an ExistingWorkspace The Test Harness ships with several example workspaces. Many of these example workspaces create both Master and Server (Outstation Controlled Station) sessions. This allows communications to occur immediately without requiring a connection to an external device. These workspaces provide a good demonstration of the capability of the Test Harness and are an excellent tool for learning about the protocols. When the application is started, the Test Harness main window (Figure 4) and the Open Workspace dialog box (Figure 3) are displayed. The Open Workspace dialog box is used to select a Test Harness Workspace (*.thws). The Test Harness ships with several demonstration workspaces. These workspaces are useful for reviewing configurations and capabilities of the Test Harness. Select one of the predefined workspaces; or select Cancel and continue to the next section to create a customized workspace. Figure 3. Test Harness Open Workspace dialog Triangle MicroWorks, Inc. Page 25 Communication Protocol Test Harness User Manual Figure 4. Test Harness Main Window 3.2 Opening aConnection to aDevice To open a connection to a device, click the Open Connection button on the toolbar of the Test Harness main window. The Open Connection dialog box will appear. Select the desired protocol and indicate whether the Test Harness should act as a Master (Controlling Station), Server (Outstation or Controlled Station), or Monitor for this connection. In Monitor mode, data is displayed from a Master to Server connection in the protocol analyzer window. For example, to configure a DNP3 Master device, select DNP3 in the left column and Master in the right column, as shown in Figure 5. Next, click the Open button to configure the connection for the selected device. Page 26 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Figure 5. Open Connection Dialog 3.3 Configuring aConnection to aDevice The Open dialog box (Figure 6) contains tabs for Channel, Session, and Next Step. For IEC 60870-5 protocols, a Sector tab is also displayed. Click on each tab to configure the connection and click the Open button. Triangle MicroWorks, Inc. Page 27 Communication Protocol Test Harness User Manual Figure 6. Example Channel Tab for Open Dialog Box 3.3.1 ChannelTab The Channel tab (Figure 6) supports configuring a new channel. The Channel Name field specifies the name that will be displayed in the Protocol Analyzer display, the Data Window, and the channel scroll box. The Connection Type section specifies the desired type of connection (e.g., serial or TCP/IP). The TCP/IP Parameters or SerialPort section is automatically updated to show parameters specific for the selected connection type. If TCP/IP is selected you should specify the host name or address of the remote device. If the Test Harness is acting as the client (Master) it will attempt to connect to the specified address and Port number. If this is a Server (Outstation or Controlled Station) it will listen on this Port number and is willing to accept connections from the IP address or list of IP addresses specified. You can also set the Local IP address to use in messages if you have multiple Network Interface Cards (NICs) or IP Addresses. The default value of 0.0.0.0 allows the TCP stack to choose the source address to use. For more details see the Advanced Topics section on TCP/IP Connections. Page 28 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual To configure additional parameters for the channel, click the Advanced Settings button. However, the default values for these settings will normally establish communications, so it may not be necessary to change the Advanced Settings values. Figure 7. Session Tab for DNP3 Master Session 3.3.2 SessionTab The Session tab (Figure 7) displays the available configuration options for the session parameters. The Session Name field specifies a name for the session. This name will be used in the Data Window. Triangle MicroWorks, Inc. Page 29 Communication Protocol Test Harness User Manual The remaining fields vary for each protocol. For information on a particular field, hold the cursor over the field for a few seconds to view the tool tip. 3.3.3 SectorTab The Sector tab only appears for IEC 60870-5 protocols. This tab configures IEC 60870-5 Sectors (ASDU Addresses). For information on a particular field, hold the cursor over the field for a few seconds to view the tool tip. \ Figure 8. Next StepTabfor DNP3Master Session 3.3.4 NextStepTab The Next Step tab (Figure 8) defines further actions, such as opening a Command Window and/or a Data Window. To open the Data Window when a connection is established, check the Create new Data Window option box on the Next Step tab. Please note, the Data Window can also be Page 30 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual opened by clicking the button on the toolbar or by selecting OpenData Window from the menu of the Test Harness main window. To open the Command Window when a connection is established, check the Create new Command Window option box on the Next Step tab. Please note, the Command Window can also be opened by clicking the button on the toolbar or by selecting OpenCommand WindowProtocol from the menu of the Test Harness main window. To open the Statistics Window when a connection is established, check the Create new Statistics Window option box on the Next Step tab. Please note, the Statistics Window can also be opened by selecting OpenStatistics Window from the menu of the Test Harness main window. After reviewing each tab, press the Open button to open a connection to the device. Depending on the options selected on the Next Step tab, the following windows may be displayed. Data Window The Data Window (Figure 9) displays data points and values. When simulating an Outstation (Controlled Station or Server), these points and values represent the Outstations database. When simulating a Master, the Data Window displays all points and values that have been reported by the Outstation. An example of the Data Window is shown in Figure 9. The order of the columns can be changed by dragging the column header to the desired location. Customized descriptions for each point are entered by double clicking in the Description field. Channels, sessions, sectors, and Outstation session data points are all modified by right clicking the channel, session, sector, or data point and selecting the desired operation. To change the value of one or more Outstation Data points in the Data Window, select the desired row(s) and right click to access the options available for the selected data type. Triangle MicroWorks, Inc. Page 31 Communication Protocol Test Harness User Manual Figure 9. Example Data Window 3.4 CommandWindow The Command Window (Figure 10) provides a convenient means for issuing Tcl/Tk or Python commands. The Command Window is available for DNP3, Modbus, IEC 608705- 101, and IEC 60870-5-104 Master connections. Page 32 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Figure 10. Example DNP3 Commands Window To execute a particular command, click on the button to the left of the command description. To customize a command, double click on the command description to display the Configuration Window. The Configuration Window also allows configuration of repeat intervals for commands. If a repeat interval value is configured, a progress Triangle MicroWorks, Inc. Page 33 Communication Protocol Test Harness User Manual bar underneath the command description is displayed to provide a visual indication of when the command will be executed next. A new command window may be opened for each remote device or one command window may be used to access multiple devices by using the session menu to switch between target devices. Tool Tips describe the purpose of each command, as well as the meaning of all configuration options in the Configuration Window. 3.5 Simulator Window When the Test Harness is simulating a Outstation (Controlled Station or Server0, a simulator may be attached to the session or sector. This allows the data values to be automatically changed on a timed interval or through a functional front panel image of a real or virtual device. An example of a Outstation simulator is shown in Figure 11. New simulators can be added to the Test Harness by duplicating one of the examples provided (see Triangle MicroWorks on the Start menu) and registering the server in tmwstart.tcl (search for SDNPSim) or placing the simulator in the Plug-ins folder. Plugins are discussed in the section Extending with .NET Plug-Ins. If you would like a simulator of your device included in the Test Harness, please send a copy of the simulator source code file(s) to: support@TriangleMicroWorks.com. Page 34 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Figure 11. Example Outstation Simulator This example Simulator Window contains several buttons that can be used during the simulation. When the cursor is placed over an active button, the cursor icon will change to a hand symbol indicating the button is functional. Setting the Load Current to over 400 amps will cause a trip condition. To reclose the breaker, lower the current below 400 amps and click on the CLOSE button in the image of the relay. 3.6 UsingWorkspaces To save the current Test Harness configuration, click the button on the toolbar or select FileSave Workspace from the menu of the Test Harness main window. The workspace includes the communications parameters, Command Window, and Data Window settings. To restore a previously saved workspace, click the button on the toolbar or select FileOpen Workspace from the menu of the Test Harness main window. Saved workspaces can also be selected from the Select Test Harness Workspace window that opens during startup. To configure the Test Harness to automatically load a favorite workspace each time the Test Harness is started, edit the tmwstart.tcl file located in the Test Harness executable directory. Open the file in a text editor, then scroll to the bottom of the file and follow the instructions for inserting the name of the workspace. Triangle MicroWorks, Inc. Page 35 Communication Protocol Test Harness User Manual Several example workspaces are provided with the Test Harness application. These workspaces demonstrate a number of the core Test Harness features including: Master/Outstation communications for various protocols, device functional testing, and loading device configurations from a spreadsheet (CSV file). 3.7 MonitoringConnections The Communication Protocol Test Harness can also be used to monitor existing connections. This feature supports both serial and TCP/IP connections for all of the protocols provided by the Protocol Test Harness. Monitoring TCP/IP connections requires installation of wpcap.dll on the PC the Test Harness is installed on. Previously this was supported by and downloaded from WinPcap.org which can still be used. It is now recommended to instead install Npcap, the Nmap Projects packet capture library for Microsoft Windows. Npcap may be downloaded from (www.npcap.com) and installed with the Install Npcap in WinPcap API-compatible Mode checkbox checked. If you attempt to use the TCP/IP monitor feature of the Test Harness and the wpcap.dll cannot be found on the PC, a dialog box will notify you and assist with the installation. To open a Monitor connection, click the Open Connection button on the toolbar of the Test Harness main window. The Open Connection dialog box appears (Figure 12). Next, configure the channel in the Open DNP3 Monitor dialog box (Figure 13). Figure 12. Opening a Monitor Connection Page 36 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Figure 13. Open Monitor Channel The Test Harness can decode manually entered text strings. This is useful for decoding messages that were captured from another source. To do this, open a Monitor Channel as described above, selecting "Raw Input Bytes" as the connection type. Raw input bytes are entered via the tmwmonitordata command in the main window. NOTE: It is also possible to load an entire Wireshark log file to the Test Harness for decoding using the File Menu Item Open Wireshark Log described in that section of this manual. For example, to decode a string of Raw Input Bytes enter: TCL> tmwmonitordata data "05 64 0a 44 03 00 03 00 1d 29 c0 f0 82 90 00 43 a2" The data stream will be decoded and displayed in the protocol analyzer log as shown in Figure 14. Triangle MicroWorks, Inc. Page 37 Communication Protocol Test Harness User Manual Figure 14. Raw Input Bytes Sample Please note, to monitor both directions of a full-duplex connection (such as RS-232), two monitor channels must be opened (one for each direction). Special cabling (Figure 15) is also required. Figure 15. Full Duplex Monitor Cable Wiring Diagram A Serial Monitor cable kit is available from Triangle MicroWorks, Inc. This kit includes the cable, a dual-port USB-to-Serial adapter, and an LED monitor to allow quick visual Page 38 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual monitoring of RS-232 signals. The RS-232 Serial Monitor Cable kit is available for $250.00 USD. Please see http://www.trianglemicroworks.com/SerialMonitorCablePage.htm for more information. 3.7.1 ModbusMonitorChannel The Protocol Test Harness can be used to monitor a connection between a Modbus Master and a Modbus Server. To open a Monitor connection, click on the Open Connection button on the toolbar of the Test Harness main window. An Open Connection dialog box will appear, as shown in Figure 16. Select Modbus and Monitor and then click on the Open button. Figure 16. Opening a Modbus Monitor Connection An Open Modbus Monitor Channel dialog box is displayed (Figure 17) which allows you to configure the Test Harness for monitoring. Two channels are required to fully monitor Modbus connections. One should be connected to the Master device and the other connected to the Server device. The channel names can either be specified or they will be automatically generated for you. Triangle MicroWorks, Inc. Page 39 Communication Protocol Test Harness User Manual Figure 17. Open Modbus Monitor Channel dialog box For the Modbus protocol, the data at the link layer is different depending on the Connection Type. This may be specified as ASCII, RTU or TCP/IP. (Currently, monitoring of Modbus-Plus is not supported. If this is required please contact sales@TriangleMicroWorks.com). If ASCII or RTU are chosen, the two Serial ports should be specified. You may also change the Baud rate if needed. Clicking on the Advanced Settings button allows you to modify the data and stop bits as well as parity if necessary. If TCP/IP is selected, you must specify the host name or IP address of the Master and Server devices. The port number of the Server device (default is 502) should also be specified. If the raw bytes have already been collected by some other means, they can be manually input into the Communication Test Harness by selecting Raw Input Bytes and using the Page 40 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual tmwmonitordata command. As previously stated, the correct Connection Type must still be chosen since the data at the link layer is different for all the different types. Clicking on Open causes two channels to open which can then be used to monitor the Modbus protocol data. Master and Server sessions will be opened automatically when the data is received on those channels. The filter check boxes in the Test Harness can be used to determine what data will be displayed. After the channels and sessions are open, some of the Modbus monitor parameters, such as name, can be modified if necessary. 3.8 Other Test Harness Features 3.8.1 ConformanceTestModule Triangle MicroWorks also offers Conformance Test Modules. These modules perform the conformance test procedures published by the technical committees of each protocol. The Conformance Test modules are very useful as a periodic regression test during continued product development or in preparation for third party testing. The Conformance Test Modules are licensed separately. However, a subset is always enabled for evaluation. To run a Conformance Test Module, click on the button on the Test Harness main window toolbar. An example of the Conformance Test module is shown in Figure 18. Figure 18. Conformance Test Module For information on licensing the Conformance Test Modules, contact Triangle MicroWorks at sales@TriangleMicroWorks.com. Triangle MicroWorks, Inc. Page 41 Communication Protocol Test Harness User Manual 3.8.2 ValidatingPointsandControllingExternalInstrumentation The Communication Protocol Test Harness can control external instrumentation such as the OMICRON CMC156, CMC256, or CMD356 voltage and current generators to establish test conditions and confirm external results. To interface to a CMD256 or CMC356, use the following procedure: 1. Install the OMICRON Test Universe software. This software should ship with the OMICRON unit. 2. Start Test Universe 3. On the right had side under Setup, click on Test Set Association 4. The Test Set Association and Configuration window will open. It should show the OMICRON unit and indicate that it is not associated. If the unit is not listed, verify that your PC and the OMICRON unit are on the same network, and that the OMICRON unit is plugged in, turned on, and connected to the network via one fo the Ethernet ports on the back of the unit. If you are on a VM, you mane need to set the Network Device to Bridged instead of Shared. 5. When the unit is discplayed, click on it to select it, and then lick the Associate button at the top of the window: 6. You will get a prompt to press the Associate button and click OK: The associate button is a small black button on the back of the unit just above the Ethernet ports. Once you follow these instructions, the unit should be associated to your PC: Page 42 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 7. After associating the unit, start the Test Harness. After completing your steps, the OMICRON unit should be ready to use with the Test Harness, as described below. Triangle MicroWorks, Inc. Page 43 Communication Protocol Test Harness User Manual Using the Command Window shown above, a sequence of commands can be configured to run automatically. Commands may establish a condition in the device under test through a communication protocol command, a dialog box instructing the user to change the test conditions, or a command to control external equipment (notice the Omicron icon). After this, data is read from the device under test through the communication protocol. The Validate points command can then be configured to ensure a selected group of points have reached expected values. Page 44 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual The Command Window sets up 5 different test conditions (2 via DNP3 trip/close commands, 2 via the CMC156, and 1 via a user prompt) and validates selected points in the data base after each test condition. Double click the OMICRON command description to configure the OMICRON. To run the commands, use the RunRun All Commands command. The test conditions must initially be set up by right clicking on the point in the Point Validation Configuration window and setting the value. The RunRun All Commands & Update Expected Values command can be used to set the subsequent test conditions for all Validate Points Commands. Test Harness can automatically run all commands in this window whenever the workspace is loaded. To configure this option, select OptionsRun All Commands When Load Workspace. 4. UserInterface As mentioned above the Test Harness comes with a complete user interface that should be adequate for most applications. This section will discuss the use of this interface in detail. The figure 19 below shows the user interface provided with the Test Harness. Triangle MicroWorks, Inc. Page 45 Communication Protocol Test Harness User Manual Figure 19 Test Harness User Interface Tool bar Command Shell Filter control panel Device Filter Protocol analyzer display Menu bar The user interface has several components, a menu bar, a toolbar, a protocol analyzer display with a filter control panel and device filter, and a command input window. The menu bar provides access to various control and configuration information. The protocol analyzer display shows the message traffic into and out of the Test Harness. The command input window that can be used to enter commands interactively or to run test scripts. The portion of the window occupied by the protocol analyzer window and the interpreter window can be adjusted by moving the separator between the two windows up or down. 4.1 MenuBar As mentioned above, the menu bar provides access to the bulk of the configuration and control parameters for the Test Harness. It should be noted that all of the functionality provided by the menu bar is also available from the Tcl command interpreter through the various commands defined in the chapters below. The user interface is provided as a convenience, so you do not have to remember, and type, all of the individual commands. The menu bar is broken up into the following menus and commands. Page 46 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 4.1.1 FileMenu 4.1.1.1 Open Workspace ( ) This option allows you to open a previously saved workspace. Note that several example workspaces ship with the Communication Protocol Test Harness. ) 4.1.1.2 Save Workspace ( This option allows you to save the current workspace. The workspace includes the Channel, Session, and Sector configurations, and any changes you have made to the Command Window or Data window. 4.1.1.3 Save Workspace As This option allows you to save any changes to the current workspace in a new directory or under a new name. The original workspace remains unchanged, and the current workspace is renamed. 4.1.1.4 Close Workspace This option closes all open channels, sessions, and sectors, and closes the current workspace. 4.1.1.5 Open Protocol Analyzer Log This option allows you to open a saved protocol analyzer buffer from an XML file. This option will display a new window that contains the protocol analyzer control with the protocol data displayed. 4.1.1.6 Open Wireshark Log This option allows you to open a saved Wireshark log file and import the raw bytes into the protocol analyzer window. A GUI will be displayed that allows you to specify the protocol, Open the Monitor Channels with Raw Input Bytes selected, specify the Wireshark File Name and import the file into the Test Harness. There is also a Help button that displays the following information about how to save the file from Wireshark. Use Wireshark to capture communications between two devices. Right click on one frame of the conversation and select Follow TCP Stream. Select Entire Conversation and Show data as Hex Dump. Then Save as any file (ie WiresharkOutput.txt). This file can then be loaded to the Test Harness as Wireshark Input File Name. 4.1.1.7 Save Protocol Analyzer Log 4.1.1.7.1 As Text This option allows you to save the current protocol analyzer buffer to a text file. This option saves the entire protocol analyzer buffer to the specified file. You can also choose to Enable Log to Text File from the Test Harness Options GUI. This will write the Triangle MicroWorks, Inc. Page 47 Communication Protocol Test Harness User Manual protocol analyzer file as text to a Windows file. For more information search for Log to Text File in the Protocol Analyzer Display section below. 4.1.1.7.2 As XML This option allows you to save the current protocol analyzer buffer to a XML file for later viewing with the Open Protocol Analyzer Log menu item. This option saves the entire protocol analyzer buffer to the specified file. 4.1.1.8 Exit This option will exit the Test Harness application. 4.1.2 OpenMenu 4.1.2.1 Conformance Test Each of the supported testing tools are listed under this menu item. Select an item to open a conformance testing GUI for the desired test. The testing GUI can then be used to configure and execute the selected test. 4.1.2.2 Master Session Each of the supported protocols are listed under this menu item to open a session of the selected type. The Open session dialog box will be displayed to configure the new Master session. 4.1.2.3 Server Session Each of the supported protocols are listed under this menu item to open a session of the selected type. The Open session dialog box will be displayed to configure the new Outstation (Controlled Station or Server) session. 4.1.2.4 Monitor Channel Each of the supported protocols are listed under this menu item to open a channel of the selected type. The Open channel dialog box will be displayed to configure the new monitor channel. 4.1.2.5 Data Window Use this item to open a new Data Window. 4.1.2.6 Command Window Each of the supported protocols are listed under this menu item to open a command window for the selected protocol. The Command window can then be used to configure and execute selected commands on that protocol. 4.1.2.7 Statistics Window Use this item to open a new Statistics Window. Page 48 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 4.1.3 EditMenu 4.1.3.1 Test Harness Options This option displays a dialog box that controls parameters common for the Triangle MicroWorks Test Harness. The following parameters are supported: Main Loop Period How often the Test Harness polls for new data on any input channel and/or data to send. Log Buffer Size Sets the number of protocol data object items to be stored in the log buffer. Enable Log to Text File If checked will automatically write the protocol log to a text file. Ask to save Workspace at exit If checked asks if you wish to save the workspace when exiting the Test Harness. Ask to load Workspace at startup If checked asks if you wish to load a workspace at Test Harness startup. Load the following Workspace at startup without asking If checked and a filename of a workspace is specified, it will be loaded when the Test Harness is started. Triangle MicroWorks, Inc. Page 49 Communication Protocol Test Harness User Manual 4.1.3.2 Protocol Format This option allows you to change the indentation, font, color of various layers of the protocol output using the following dialog box. 4.1.3.3 Re-Filter Protocol Analyzer This option will cause the new Protocol Format settings to be applied to the currently displayed data in the Protocol Analyzer. 4.1.3.4 Find This option will bring up a dialog box to search the protocol data for a specific text string. Page 50 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 4.1.3.5 Plug-ins This will bring up a dialog box with all the user plug-in GUIs displayed. Same functionality as Plugins Menu item. 4.1.4 WindowMenu This menu allows you to show, hide, or close the Test Harness Windows that are separate from the main window. 4.1.5 PluginsMenu This will bring up a dialog box with all the user plug-in GUIs displayed. Triangle MicroWorks, Inc. Page 51 Communication Protocol Test Harness User Manual 4.1.6 HelpMenu 4.1.6.1 License This option displays the license installation dialog box described in the Licensing Section. 4.1.6.2 License Quote Request This option provides a convenient method for requesting quotes for an application license or for additional protocol licenses from Triangle MicroWorks. 4.1.6.3 Test Harness Help This option opens the Communication Protocol Test Harness User Manual (this document). 4.1.6.4 Quick Start Guide This option opens the Communication Protocol Test Harness Quick Start Guide. 4.1.6.5 Tcl/Tk Help This option launches the Tcl/Tk Reference Manual. 4.1.6.6 Whats New This option downloads and displays the Communication Protocol Test Harness Whats New document from the Triangle MicroWorks, Inc. web site. This document describes changes included in all versions of the Test Harness. Since this option will always download the latest version of the Whats New document, it is useful for determining if you should download and install any available updates. 4.1.6.7 TMW Web Site This option launches a web browser and points it to the Triangle MicroWorks, Inc. web site. This option is useful for obtaining the latest information about Triangle MicroWorks, Inc. products. 4.1.6.8 About This option displays a dialog box that contains the Test Harness version, release date, and other pertinent information. Page 52 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 4.2 ProtocolAnalyzer Display The protocol analyzer display shows text output of the actual protocol exchange between the Test Harness and the device(s) under test. The protocol analyzer display includes two control panels that can be used to filter the current display. The device filter and the protocol layers filter. The text display area supports scrolling vertically and/or horizontally. It displays the last N lines where N is determined by the Log Buffer Size parameter. The protocol analyzer display is shown below. The device filter is used to specify one or more channels or sessions to display. The default will display all channels or sessions. Errors Controls the display of all errors. Time Stamp Controls the display of time stamps with each protocol analyzer line. MMI Controls display of MMI level messages. User, Application, Transport, Data Link, and Physical Controls the display of messages from this layer of the protocol stack. Database o Header Controls the display of object header information. o Data Display data Static Controls the display Static Data/Header Information Triangle MicroWorks, Inc. Page 53 Communication Protocol Test Harness User Manual Event Controls the display Event Data/Header Information Cyclic Controls the display Cyclic Data/Header Information Security Currently only applies to DNP, controls display of Security Data/Header Information Clear Display Clears the protocol analyzer display and the protocol log buffer. Pause Pauses or Restarts all input to the protocol analyzer display. 4.2.1 Controllingtheprotocoldisplayformat The protocol display can be customized by selecting the Edit/Protocol Format menu item and using the following dialog box: Page 54 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual As an example, when selecting one of the protocol layers such as you can adjust the color, font or indentation level of that layer. The result of this change will be as follows: 4.2.2 Searchingtheprotocoldisplay Right clicking inside the Protocol display or using the Edit/Find menu item will bring up a Find dialog box. The Find dialog box can be used to search for specific text in the protocol buffer. 4.2.3 LogtoTextFile You can also Enable Log to Text File functionality from the Test Harness Options GUI found on the Edit Menu. This will continuously write the same protocol log text to a Windows file. This can be configured using the Configure button. Triangle MicroWorks, Inc. Page 55 Communication Protocol Test Harness User Manual 4.3 CommandShell The Command Shell is a command shell that can interpret Tcl, Python and DOS commands. It can be set into one of these modes by typing tclshell or pythonshell. In the example below (Figure 20) the shell is set to Tcl mode and any Tcl command, including the Triangle MicroWorks extensions can be typed into the command window. An example of a Tcl command session is shown below: Page 56 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Figure 20. Example Tcl Command Session The command interpreter supports a command history. The current command history can be reviewed by typing history. The up and down arrow keys can be used to move backwards and forwards through the command history. The history command is described in more detail below. For a description of the standard Tcl commands and operations and how to generate and run Tcl/Tk test scripts see Extending with Tcl/Tk. For more information on the Test Harness extensions to the standard Tcl/Tk commands, see Generic Test Harness Commands. Type helpshell to get help on the command shell. TCL>helpshell TestHarness Shell Help (not scriptable): helpshell -shows this help history -shows command history cls -clears the command prompt text tclshell -use the Tcl/Tk shell pythonshell -use the Python shell TCL> 4.4 Data Window Single or Multiple Data Windows can be used to view and modify the points in a particular session or sector database. Selecting a Channel can be used to show all the points in all of the Sessions/Sectors on that Channel. Selecting individual Sessions/Sectors or individual Data Types can show just those points. Individual rows in the Data Window will display Point Number, Value, State, Time Stamp, Description and Protocol Specific information for each point. Triangle MicroWorks, Inc. Page 57 Communication Protocol Test Harness User Manual Selecting an individual point will show and allow Modification in the Lower Left Property Grid for the current Properties of that specific point. This includes the information displayed in the row as well as other configuration information for that point. The actual data value can typically only be modified on the Outstation/Controlled Device. The Master device is typically just a representation of what has been received from the Outstation/Controlled Device. Page 58 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual SDNP Control Points Binary Output Statuses and Analog Output Statuses have an AssocPoint Property which can be used to associate a Binary Input and Analog Input respectively. When a control from the Master changes the value of the Output Point, the Associated Input Point will also change. This may result in a DNP Change Event being generated depending on configuration. S101 and S104 Monitor Points similarly have a ControlIOA Property. (For 101 and 104 there are no Control Points in the database.) This ControlIOA is used to associate a Control Point with this Monitor Point. When a control from the Master changes the value of the Control Point, the Associated Monitor Data Point will also change. Similarly, this may result in a Data Change Event being generated and sent Spontaneously to the Master. 5. ExtendingFunctionality This section describes how to extend the Test Harness by developing custom scripts or plug-ins. It is divided in to 3 major sections: Plug-ins, Tcl/Tk, and Python. 5.1 Extending with.NETPlug-Ins 5.1.1 Introductionto.NETPlug-Ins The .NET plug-in architecture allows a user to add significant functionality to the Test Harness using any of the .NET languages. Each plug-in becomes an integrated piece of Triangle MicroWorks, Inc. Page 59 Communication Protocol Test Harness User Manual functionality of the Test Harness. The Test Harness defines two distinct types of plugins. The first is the Testing plug-in. It is used to create a standalone set of tests such as a conformance test or a regression test. The second is the User plug-in that is used to add more general functionality. The plug-in framework defines how the Test Harness and the plug-in communicate with each other. This communication is defined using Interfaces. The Test plug-in defines the ITHTestPlugin interface and the User plug-in defines the ITHUserPlugin interface. These allow the Test Harness to talk to the plug-in in a predefined manner. The plug-in also needs to talk to the Test Harness for example, to send it status messages. This is defined by the ITHTestPluginHost and ITHUserPluginHost interfaces which the Test Harness implements. At startup the Test Harness searches for plug-ins in the <TestHarness>\Plugins directory. It loads all plug-ins of both types that it finds. The plug-in does not get unloaded until the user exits the Test Harness. Test plug-ins are available from the Open->Conformance Tests menu item. User plugins are available from the Plug-in Manager found in Edit->Plugins menu item. User plug-ins are also available as commands from the Command Windows and the Tcl/Tk Command Shell. 5.1.2 TestingPlug-inFramework Testing plug-ins are loaded into an existing windows form and thus dont have a GUI of their own. The IEC-102 Regression plug-in example is shown below. Page 60 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Each test is displayed on the left under its corresponding category. Test results generated by the plug-in are output on the left. If a test succeeds its back-ground color is green. If a test fails, its back-ground color is red. Press Reset State to clear the back-ground color. A typical user scenario: A user chooses which tests to perform by checking the corresponding test name in the window on the right. The user presses the Connect button. The Connect method is called on the plug-in which makes a connection between the Master and Outstation or Controlled Station. After the connection is made the Execute button becomes enabled and the user can press this to start the test. The Test Harness calls RunTest for each selected test. The plug-in sends status messages to the Test Harness which it displays in the panel to the right. After the selected tests are complete the Test Harness calls the Disconnect method. The Connect button becomes enabled and the Execute button becomes disabled. The File menu contains: Edit Configuration Calls plug-in to allow user to specify configuration settings Save Configuration Calls plug-in to allow user to save configuration settings Load Configuration Calls plug-in to allow user to load configuration settings Triangle MicroWorks, Inc. Page 61 Communication Protocol Test Harness User Manual The Options menu contains: Verbose Calls plug-in to set Verbose flag Verify Data Calls plug-in to set Verify Data flag 5.1.2.1 TestingPlug-in Interface Test plug-ins implement the ITHTestPlugin interface described in the Plug-Ins Help file accessed from the start menu. 5.1.3 UserPlug-inFramework User plug-ins add commands to the Test Harness. These commands can be very simple such as sending a single message to a Outstation or they can be more complicated such as opening a window that simulates a device. (see the ABB Simulator example.) Commands created by User plug-ins can be accessed in three different ways. 1. Plug-in Manager 2. Command Window 3. Command Shell 5.1.3.1 Plug-in Manager All User plug-ins are available from the plug-in Manager window. The User plug-ins are listed on the left. Each plug-in shows its own GUI in the panel to the right. This is a windows user control implemented by the plug-in. In this example the Start ABB Simulator button brings up a new Windows Form implemented by the plug-in. Depending on the plug-in functionality this could be configuration, help, or contain buttons for specific functionality. Page 62 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 5.1.3.2 Plug-ins in the Command Window A plug-ins command may or may not reside inside a command window. To see how to assign a plug-in to the appropriate command window see CommandWindowTargetEnum in the ITHUserPlugin interface. Below is an example of the m102badchecksum command that is implemented inside of the IEC102InvalidMsgs plug-in. Note the PluginCommandString and the PluginOptionsString fields in the lower left corner. These are taken directly from the plug-in. Note: There are two styles of command windows. The Test Harness 2 style and the Test Harness 3 style. The Test Harness 3 style shown above will automatically load commands from plug-ins. The Test Harness 2 style does not automatically load the plugins. In order to put a plug-in command in a Test Harness 2 style dialog use the Custom Tcl Command step and type the command name. If the sector/session is required by the command the %target% variable can be used instead. For example, m102badchecksum %target% will evaluate to m102badchecksum sector 1 assuming your Master sector is 1. Triangle MicroWorks, Inc. Page 63 Communication Protocol Test Harness User Manual 5.1.3.3 Plug-ins in the Command Shell All plug-in commands are available as Tcl commands from the Tcl command shell. Simply type the name of the command along with any arguments needed. For example, m102badchecksum sector 1 will send a bad check sum to the Controlled Station device. Plug-ins are also available from the Python shell as follows: pythonshell sys.path.Add("<TestHarness Folder>\\plugins") clr.AddReference("IEC102InvalidMsgs.dll") import IEC102InvalidMsgs master = IEC102InvalidMsgs.InvalidCheckSumMaster() sctr = TMWCommonSCL.TMWSector.LookupSector(1) master.StartTest(sctr) 5.1.3.3.1 User Plug-in Interface User plug-ins implement the ITHUserPlugin interface described in the Plug-Ins Help file accessed from the start menu. 5.1.4 Debugging.NETPlug-ins .NET plug-ins can be developed in any IDE that supports .NET development. All examples included with the Test Harness were developed using Visual Studio. There is also a free IDE called sharp develop for C# development. It can be found at: http://www.icsharpcode.net/OpenSource/SD/. This is a full featured IDE including a debugger. To use the debugger in either IDE environment some configuration is needed since the plug-in is a DLL that is loaded by the Test Harness. 5.1.4.1 Visual Studio .NET 2017 Right click on the plug-in project and choose properties. Go to the Debug tab and click the Start external program radio button. Point this at <TestHarness>\bin\tmwtest.exe. When the debugger is started the Test Harness will start and the plug-in will be loaded along with its debugging symbols which will allow the plug-in to be properly debugged. Page 64 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 5.1.4.2 Sharp develop With Sharp Develop Right click on the project and select Properties. Select the Start External program radio button on the Debug tab and set its value to the location of the Test Harness tmwtest.exe file. When the Test Harness is run and the plug-in used you will be able to debug the plug-in. Triangle MicroWorks, Inc. Page 65 Communication Protocol Test Harness User Manual 5.1.5 Examples IEC102Regressions Is a Test Plug-in that demonstrates the use of the ITHTestPlugin interface as a regression testing framework. ABBSim Is a User Plug-in showing a ABB Simulator using DNP. SELSim Is a User Plug-in showing a SEL Simulator using DNP. SELSimVB Is a User Plug-in written in VB .NET showing a SEL Simulator using DNP. DNPDualEndPoint Is a User Plug-in that can open either a DNP3 Master or DNP3 Outstation configured for Dual End Point Functionality as well as create a GUI that allows you to Connect/Disconnect, Enable Unsolicited Responses, Send Read Requests and Generate Events to verify proper Dual End Point Functioning of a Device. SDNP_DER Is a User Plugin that opens an SDNP Session populated with a MESA-DER database, associated point, Curve and Schedule functionality from a DNP3 point of view. M14FileTransfer Is a User Plug-in that provides a GUI for exercising 60870-5 101 and 104 File Transfer functionality. *InvalidMsgs Are User Plug-ins showing how to send invalid messages. Page 66 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual IEC101MspChange, DNPBinInChange etc. Are User Plug-ins which modify Controlled Station database point values. These can be called periodically to provide database changes and spontaneous events. These simple plug-ins serve a good starting examples for command based plug-ins. ExampleGUIPlugin Is a User Plug-in showing how to use a Windows Form that provides access to an SDNP Session. This can serve as a good starting example for creating your own Forms based plug-in to provide a separate display and input window. These examples are provided in the <installdir>\Plugins\User\ directory. If you have installed Test Harness in the Program Files directory, you may have limited access to this location. Development work should be done in a directory to which you have full access. Copy the example code elsewhere to use the examples as a starting point for your own plug-ins. 5.1.5.1 ABB PCD Simulator A C# user plug-in is provided which implements the core functionality of an ABB PCD. This plug-in source code can be found in <installdir>\Plugins\User\ABBSim\. To load and start this simulator type the following command at the Tcl command prompt: TCL> ::ABBSim::ABBSim By default the simulator will open a TCP/IP channel using the TCP loopback address (127.0.0.1) and configuration parameters compatible with the opendnpmaster command described in the DNP3 examples above. Hence after starting the simulator typing the following command at the Tcl command prompt will open a Master session in the Test Harness which will communicate with the simulator in loopback mode. TCL> opendnpmaster The simulation will simulate a breaker trip when the simulated load (controlled by the slider bar in the lower write) goes over 400. In addition a manual trip/close can be performed by pressing the open/close button on the front panel display. 5.1.5.2 SEL PCD Simulator We have also provided 2 versions of an SEL Simulator one written in C# and a second written in VB.NET. To start the C# version enter: TCL> ::SELSim::SELSim To start the VB.NET version enter: TCL> SelSimVB open Triangle MicroWorks, Inc. Page 67 Communication Protocol Test Harness User Manual By default the simulator will open a TCP/IP channel using the TCP loopback address (127.0.0.1) and configuration parameters compatible with the opendnpmaster command described in the DNP3 examples above. Hence after starting the simulator typing the following command at the Tcl command prompt will open a Master session in the Test Harness which will communicate with the simulator in loopback mode. TCL> opendnpmaster 5.1.5.3 Example GUI Plugin We have provided this simple plugin to serve as an example to create your own Windows Forms based plugin. This example provides a simple ctlMain.cs implementing a UserControl to display help in the Test Harness Plugins window and to provide a button to open an SDNP Session and separate Windows Forms interface window. The file exampleGUI.cs implements a Windows Form and ITHUserPlugin allowing use of the full GUI functionality of the Windows Forms class library. This example Form contains Binary Input and Output checkboxes and Point Indexes. It demonstrates how to open or attach to an existing SDNP Session as well as allow changes to Binary Input values and to display Binary Output values for specified Point Indexes. To create your own Form based plug-in you should copy the ExampleGUIPlugin directory to a new directory (ie MyPlugin) and modify those files to differentiate them from the ExampleGUIPlugin. You should change the following: Project filename ExampleGUIPlugin.csproj (ie to MyPlugin.csproj) Page 68 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual namespace ExampleGUIPluginNamespace (ie to MyPluginNamespace) class ExampleGUIPluginClass (ie to MyPluginClass) _CommandName exampleGUICommandName (ie to myPluginCommand) PluginName "SDNP Example GUI Plugin" (ie to My Plugin) You should then build this new plugin which will create a MyPlugin.dll in the Plugins directory. This plugin will then be loaded when the Test Harness is restarted. You can verify this using the Test Harness Plugins window. Once verified you can then edit your new exampleGUI.cs to modify the Forms GUI and other functionality of this plugin. 5.2 Extending withTcl/Tk 5.2.1 IntroductiontoTcl/Tk This section starts by presenting an overview of the Tcl/Tk programming environment. Next it describes features and techniques specific to the Triangle MicroWorks, Inc. Test Harnesses. Finally it will put all this together to develop some custom test scripts and a full custom test application. The Tcl/Tk scripting language is widely used in a number of areas and there are numerous resources available to help get started and aid development. In this section we will introduce you to the basics of Tcl/Tk and point you to references that will help you become an expert (if you want). In general you will need only a very basic understanding to develop most test scripts and even some custom applications. The external references should only be required if you want to develop very complex test applications. 5.2.1.1 Getting Started Entering commands in Tcl is easy. You simply type the command followed by any arguments used by the particular command as follows: TCL> command arg1 arg2 arg3 The following example is a simple command that prints the string Hello World on the console. TCL> puts Hello World The puts command puts a string to a file where the default file is stdout or the standard output device (the console). The puts command requires one argument, a string to output. In this case that string is Hello World. Note that the double quotes are necessary since the string has a space in it. Otherwise it would be interpreted as two arguments. In Tcl, commands must start and end on the same line. The exception to this is the use of the \ continuation character as follows: TCL> puts \ TCL> Hello World Triangle MicroWorks, Inc. Page 69 Communication Protocol Test Harness User Manual 5.2.1.2 Variables Most scripts will require some form of variable (i.e. a value that changes over the life of the script). Variables in Tcl are set as follows: TCL> set a 1 The above commands sets the variable a to a value of 1. Tcl is not a strongly typed language, hence I can set a variable to any type of data. For instance, all of the following commands are valid. TCL> set a Hello World TCL> set a 1.23456 TCL> set a January 1, 2001 To use a variable you de-reference it by putting a $ before it as in the following examples. TCL> set a 1 TCL> set b $a The above sequence sets the variable a to 1 and then sets the variable b to the value of a (which is currently 1). 5.2.1.3 Equations The Tcl interpreter does not evaluate math expressions directly, hence you must use a command for it. The command to evaluate math expressions is expr and the following examples show how it is used: TCL> expr 5 * 10 TCL> expr 10 / 5 TCL> expr 10 - 5 5.2.1.4 Command Substitution The power of Tcl begins to emerge when we discuss command substitution. Command substitution refers to the ability to nest commands inside other commands. This is done using square brackets []. Examples of command substitution are: TCL> puts [expr 10 / 5] TCL> puts [expr $a + $b] TCL> set a [expr $a + 10] In each of these examples the characters inside the square brackets is evaluated as an independent Tcl command and the result used for the corresponding argument in the original command line. 5.2.1.5 Program Control Finally, no language would be complete without the ability to alter or control the program flow. Specifically this refers to the ability to add conditional operations and/or provide a looping mechanism. The following operators are available to control program flow in Tcl. Page 70 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 5.2.1.5.1 If Then Else TCL> if condition body1 else body2 An example would be: TCL> if {$a < 10} { TCL> puts a is greater than 10 TCL> } else { TCL> puts a is not greater than 10 TCL> } 5.2.1.5.2 Switch TCL> switch flags value pattern1 body1 pattern2 body2 An example would be: TCL> switch exact -- $value { TCL> foo {doFoo} TCL> bar {doBar} TCL> default {do something else} TCL> } 5.2.1.5.3 While TCL> while booleanExpression body An example would be: TCL> while {$a < 10} { TCL> incr $a 1 TCL> } 5.2.1.5.4 For TCL> for initial test finale body An example would be: TCL> for {set a 0} {$a < 10} {incr a 1} { TCL> do something TCL> } 5.2.1.5.5 Break and Continue The break and continue commands can be used to terminate a loop and or continue with the next iteration of a loop. 5.2.1.6 Common Tcl Commands This section will briefly list some of the common Tcl commands that might prove useful in developing test scripts. For a more complete explanation of these commands, and all the remaining Tcl/Tk commands see the references below or the Tcl/Tk online help provided with the Test Harness installation. Triangle MicroWorks, Inc. Page 71 Communication Protocol Test Harness User Manual 5.2.1.6.1 incr The incr command increments a variable by the specified amount. An example of its usage is: TCL> incr a 4 This command increments the value of a by 4 and is a more efficient way of typing: TCL> set a [expr $a + 4] 5.2.1.6.2 file The file command controls access to files. The following are some of the most commonly used options. See the references below for more information. file copy ?-force? source destination Copies file source to destination file delete ?-force? filename Deletes the file filename. file dirname filename Returns the parent directory of file filename. file exists filename Returns 1 if the file filename exists. 5.2.1.6.3 history The history command is used to manage the command history. The following are some of the most commonly used options. See the references below for more information. history or history info Displays the current command history buffer. history clear Clear the command history buffer. history keep count Set the number of commands to keep in the history buffer (defaults to 20). history redo id Reissue the requested command. 5.2.1.6.4 string The string command manipulates strings. The following are some of the most commonly used options. See the references below for more information. string bytelength str Returns the number of bytes used to store a string. string compare ?-nocase? ?length len? str1 str2 Compares strings, returns 0 of equal, -1 if str1 sorts before str2 and 1 if str1 sorts after Page 72 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual str1. string equal ?-nocase? str1 str2 Compares strings and returns 1 if they are the same. string first str1 str2 Returns the index of the first occurrence of str1 in str2. string index str charindex Returns the character in str at position charindex. string last str1 str2 Returns the index of the last occurrence of str1 in str2 string length str Returns the number of characters in str string range str start end Returns the range of characters in str from start to end 5.2.1.6.5 clock The clock command performs one of several operations that may obtain or manipulate strings or values that represent some notion of time. Note that the time returned by the clock command is in number seconds since an epoch. For the Windows operating systems the epoch is December 31, 1969 midnight GMT. clock scan dateString ?-base clockVal? ?gmt boolean? Return the time specified in the string in seconds. This command can parse and convert virtually any standard date and/or time string, which can include standard time zone mnemonics. If only a time is specified, the current date is assumed. If the string does not contain a time zone mnemonic, the local time zone is assumed, unless the -gmt argument is true, in which case the clock value is calculated assuming that the specified time is relative to Greenwich Mean Time. -gmt, if specified, affects only the computed time value; it does not impact the interpretation of -base. If the -base flag is specified, the next argument should contain an integer clock value. Only the date in this value is used, not the time. This is useful for determining the time on a specific day or doing other date-relative conversions. Triangle MicroWorks, Inc. Page 73 Communication Protocol Test Harness User Manual clock seconds Return the current date and time as a system-dependent integer value. The unit of the value is seconds, allowing it to be used for relative time calculations. The value is usually defined as total elapsed time from an epoch. You shouldn't assume the value of the epoch. 5.2.1.7 References As mentioned above, Tcl/Tk is widely used and there are numerous references available for further exploration. The http://www.scriptics.com web site provides information on all aspects of Tcl/Tk, current and future development, educational resources, etc. In addition numerous text books have been published on Tcl/Tk. The references below are only a few of the more frequently used ones. Title Author Publisher ISBN Number Tcl and the Tk Toolkit John Ousterhout Addison-Wesley ISBN0-201-63337-X Practical Programming in Tcl And Tk Brent B. Welch Prentice Hall ISBN:0-13-022028-0 Tcl/Tk in A Nutshell Paul Raines, Jeff Tranter, Andy Oram OReilly ISBN:1-56592-433-9 5.2.2 AsynchronousCommands In general when a Tcl command is invoked the associated processing is done synchronously (before Tcl attempts to read another command) and the result returned in some form. When sending a receiving messages from a remote device however the processing must be done asynchronously (i.e. the Test Harness must not block waiting for responses to requests). Thus, several of the Test Harness commands really only initiate a particular operation before returning to the interpreter. The actual processing of the request is done asynchronously. This fact must be taken into account when developing scripts that use these commands. Specifically, if you want to check the status of a particular operation before moving to the next step of a test you must wait for that operation to complete. The following sections describe techniques for synchronizing test scripts when required. 5.2.2.1 Use of vwait to synchronize scripts TCL> m101csc ioa 100 value on statvariable cscStatus TCL> vwait cscStatus Page 74 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 5.2.3 TestScriptGenerationandExecution As mentioned above, all of the Tcl commands can be typed directly into the command interpreter or stored in an ASCII text file and run using the Tcl source command. The format of this command is: TCL> source filename.tcl 5.2.4 DuplicatingConfigurations Tcl scripting can be used to duplicate a stations configuration. For example, if the Test Harness is configured as a Master, and it will communicate with 20 Outstation devices that differ only in their Session/Sector addresses, you may wish to configure once device and duplicate that configuration for the other 19 devices. Two options are available for duplicating a channel/session/sector: 1. Modify a Workspace To use this method, configure the first device, then save the workspace. Then open the workspace file in a text editor, locate the openChannel/openSession/openSector commands in the workspace. Copy these commands and modify them for use with the new device. 2. Write Tcl Scripts To use this method, write Tcl scripts to call the openChannel/openSession/openSector commands to configure each device. The tcl for command can be used to repeated call these functions. For example Tcl script that open a channel/session/sector, see the scripts in the ./Examples directory. 5.2.5 GenericTestHarnessCommands This section describes commands supported by the Triangle MicroWorks Test Harness. Test Harness commands can be divided into three types: generic commands, protocol specific commands, and comm port commands 5.2.5.1 help The help command displays all of the Test Harness commands, including the protocol specific commands, and a brief explanation of what they do. This command has no options. 5.2.5.2 Generic Commands The tmwhelp command displays all of the generic Test Harness commands, but not the protocol specific commands, and a brief explanation of what they do. This command has no options. Triangle MicroWorks, Inc. Page 75 Communication Protocol Test Harness User Manual 5.2.5.3 Protocol Specific Commands A list of protocol specific commands, along with a brief description of each command, can be obtained with the protocol specific help commands: mdnphelp mmbhelp m101help m102help m103help m104help sdnphelp smbhelp s101help s102help s103help s104help These commands have no options. Many protocol specific commands are followed by a set of name value pairs where the name specifies the parameter and the value specifies the value for that parameter. An example would be m101cscna sector 1 ioa 100 value on where m101cscna is the command, sector is the first parameter whose value is 1, ioa is the second parameter with a value of 100, and value is the third parameter with a value of on. Some protocol specific commands follow the format of command option [parameter value] where command specifies the data type, option specifies the action to perform, and the parameters provide the required values. An example would be sdnpbinin add class 1 value off which would add a new binary input data point which responds to events in class 1 and an initial value of off. Note that for each command, most of the command parameters are optional and will use a reasonable default value; you only need to specify the parameters that need to be changed from the default. 5.2.5.4 Comm Port Commands The following commands provide information about comm. ports on the host computer. 5.2.5.4.1 getallcommports Returns a list of all serial communications ports on the host computer. 5.2.5.4.2 getavailablecommports Returns a list of all serial communications ports on the host computer not currently in use by the Test Harness or any other application. Page 76 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 5.2.5.5 Help for Specific Commands Typing ? as a single argument for any Triangle MicroWorks command extension will list the command usage for that command. For example: TCL>tmwlog ? usage: tmwlog [option] [parameter1 value1] ... supported options <parameters>: ? : displays help clear : clears the buffer pause : starts or stops log input (1, 0, true, false, on, or off) [false] insert : inserts the specified text into the log [] mark : adds a mark at the current position in the log [False] delete : deletes the specified mark [0] save : saves text from 'begin' to 'end' into 'filename' channel : specify which channel(s) to display (default is All) [0] session : specify which session(s) to display (default is All) [0] filter : filter what' 'state' -Enable/Disable messages in protocol analyzer silent : disables/enables input to log. Simular to pause but does not put data in protocol buffer (1, 0, true, false, on, or off) [false] getCurrentTime : sets specified Tcl array varName to current time TCL> 5.2.5.6 Tcl Callback commands Some commands include a parameter to specify callback function. This function will be called when the command completes. The following is an example usage of a callback function. This example uses the sdnptxcallback, which allows you to specify a Tcl command to be called before transmitting the message. The example shows Tcl code for registering a callback after application layer and after physical layer. It also shows modifying IIN bits and modifying link control: proc registerCallbacks {} { global fragment frame sdnptxcallback layer application cmdVariable afterApplTxCallback dataVariable fragment sdnptxcallback layer physical cmdVariable afterPhysTxCallback dataVariable frame } proc afterApplTxCallback {} { global fragment set control [tmwbinary $fragment extract 0] set fc [tmwbinary $fragment extract 1] Triangle MicroWorks, Inc. Page 77 Communication Protocol Test Harness User Manual set iin1 [tmwbinary $fragment extract 2] set iin2 [tmwbinary $fragment extract 3] puts "afterApplTxCallback called: puts control = $control, function code = $fc, iin1 = $iin1, iin2 = $iin2 # if sequence number 8 modify IIN bits if {$control == 0xc8} { set iin1 [expr $iin1 | 0x40] puts control c8 found set IIN bits, set $iin1 tmwbinary $fragment replace 2 $iin1 } # dont call sdnptxcallback from callback function # to deregister this callback function or else the replace # will fail } proc afterPhysTxCallback {} { global frame set start [tmwbinary $frame extract 0] set len [tmwbinary $frame extract 2] set control [tmwbinary $frame extract 3] puts afterPhysTxCallback called: puts start = $start, len = $len, control = $control return if {$control == 0x44} { puts control 0x44 found tmwbinary $frame replace 3 0x43 tmwbinary $frame crc 0 8 } # dont call sdnptxcallback from callback function to deregister this # callback function or else the replace will fail } 5.2.5.7 Request Ids Most commands return the request ID. The outstanding request Ids can be viewed using the tmwrequests command. The xxxCancel command can be used to cancel a command by specifying the request ID of the desired command (e.g., m101 cancel request xxx). 5.2.5.8 Command Status Many commands return the command status in a cmdStatus variable. This value can be used to determine whether a command succeeds. The values are slightly different between the IEC, DNP and Modbus protocols. For example: m101fscna ioa 0 cot request statVariable cmdStatus vwait cmdStatus puts $cmdStatus In this example, cmdStatus would contain: Page 78 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 0 if successful 1 if intermediate response (a response was received, but the command is not yet complete; this could be due to a multi-fragment response or due to a multiple-step operation such as Select/Operate) 2 if failure 3 if Timeout 4 if Canceled (either by the user or because a second duplicate command was queued, so the first was canceled) mdnpbincmd point 2 control loff statVariable cmdStatus vwait cmdStatus puts $cmdStatus In this example, cmdStatus would contain: 0 if successful 1 if intermediate response (a response was received, but the command is not yet complete; this could be due to a multi-fragment response or due to a multiple-step operation such as Select/Operate) 2 if failure 3 if mismatch (e.g., the Select or Execute did not echo the request) 4 if status code (i.e., the Select or Execute was echoed, but the status code was nonzero) 5 if IIN status (the IIN bits indicated the command failed) 6 if Timeout 7 if Canceled (either by the user or because a second duplicate command was queued, so the first was canceled) mmbreadcoils start 1000 quantity 4 statVariable cmdStatus vwait cmdStatus puts $cmdStatus In this example, cmdStatus would contain: 0 if successful 1 if failure 2 if Timeout Triangle MicroWorks, Inc. Page 79 Communication Protocol Test Harness User Manual 3 if Canceled (either by the user or because a second duplicate command was queued, so the first was canceled) 5.3 Extending withPython 5.3.1 IntroductiontoPython Python is a general purpose programming language that is often applied in scripting roles. It is commonly defined as an object-oriented scripting languagea definition that blends support for OOP with an overall orientation toward scripting roles. The Test Harness implementation of Python is based on Microsofts IronPython. IronPython was chosen due to its strong integration with the .NET framework and the TMW .NET Protocol Components. The installation of the Test Harness includes documentation for the TMW .NET Protocol Components which can be accessed from the Start menu or the Test Harness Help menu. See the References section below for links to various sites that provide documentation on Python and IronPython. Python can be used both as a scripting language and as a command line interpreter in the Test Harness. This section of the manual will describe both uses. When the Test Harness starts up it executes the th.py python script in the PythonScripts directory. Modify this file should you need certain initialization at Test Harness startup. 5.3.2 Tutorial We have included the standard IronPython Tutorial to get you started with Python with the Test Harness. It can be accessed by entering the following commands in the Command Window. pythonshell SetupPythonTutorial() After entering these commands the IronPython Tutorial will display in a web browser and the Test Harness will be ready to start the tutorial. Most users will want to follow tutorial sections 1,2,3,4 and 7. Sections 5 and 6 deal with embedding IronPython into your own application so does not apply. Sections 4 and 7 deal with debugging using the ipy.exe python console and do not directly apply to the Test Harness. Test Harness specific debugging is described below. When the tutorial tells you to Exit the IronPython Interactive console (Ctrl+Z or F6 followed by Enter) simply exit the Test Harness. 5.3.3 PythonCommandShell The Test Harness includes a Python command shell which can be accessed by typing pythonshell at the command prompt in the command window. Page 80 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual After entering the pythonshell command the prompt will change to PYTHON> indicating the python shell is active. If you execute a command that requires more input, a ... prompt will be displayed. When entering multi-line commands proper python indentation should be observed: in the example above return a+b+c was indented 2 spaces because it is part of the add3 function being defined. When the multi-line command is finished press return to finish the multi-line command. While in the python shell Python commands can be entered at the command prompt. If you enter the name of a valid python object followed by a period (i.e. System. ) a small window will pop up allowing you to select the next part of the command. Triangle MicroWorks, Inc. Page 81 Communication Protocol Test Harness User Manual Press the cursor up or down or use the mouse to select the option. Press escape to cancel the window. To return back to the Tcl/Tk shell enter tclshell. 5.4 Examples Generic example scripts for each protocol can be found in <installdir>\Examples\tmw101.tcl or <installdir>\Examples\tmwxxx.tcl. These scripts are automatically loaded by the default startup script (tmwstart.tcl). Each script defines procedures that will open or close a channel/session/sector, as well as simple procedures which execute most of the available commands. These examples can be used as is or as the basis for the development of custom scripts specific to an individual situation. In general, it is best to copy the installed examples to a local working directory before modifying them so that it is easy to restore the original if needed. Example simulators (written in Tcl/Tk) are included for some protocols. 5.4.1 IEC60870-5-101/104Examples This section discusses example scripts and utilities provided with the Test Harness that target the IEC 60870-5-101 and 104 protocols. These examples can be used as is or as the basis for the development of custom scripts specific to an individual situation. In general it is best to copy the installed examples to a local working directory before modifying them so that it is easy to restore the original if needed. Page 82 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 5.4.1.1 IEC 60870-5-101/104 Example Scripts Generic example scripts for each protocol can be found in <installdir>\Examples\tmw101.tcl or <installdir>\Examples\tmw104.tcl. These scripts are automatically loaded by the default startup script (tmwstart.tcl). Each script defines procedures that will open or close an IEC 60870-5-101 or 104 session respectively, as well as simple procedures which execute most of the available 101 and 104 commands. The following list describes the main procedures defined in tmw101.tcl. Equivalent procedures are defined in tmw104.tcl. Simply replace the 101 in the procedure name with 104. For a complete list of the defined procedures see the tmw101.tcl or tmw104.tcl script file. open101master Open a basic IEC 60870-5-101 Master session and sector on the TCP loopback address (127.0.0.1) close101master Close the IEC 60870-5-101 Master session and sector opened by open101master open101Controlled Open a generic IEC 60870-5-101 Controlled Station session and sector on the TCP loopback address (127.0.0.1) close101Controlled Close the IEC 60870-5-101 Controlled Station session and sector opened by open101Controlled open101 Calls open101master and open101Controlled to open a Master and Controlled Station session and sector connected to each other via the TCP loopback address. close101 Closes the IEC 60870-5-101 Master and Controlled Station session and sectors opened by open101 gi1 Issue a IEC 60870-5-101 general interrogation on the sector opened by open101master ci1 Issue a IEC 60870-5-101 counter interrogation on the sector opened by open101master 5.4.2 IEC60870-5-102Examples This section discusses example scripts provided with the IEC 60870-5-102 Protocol Test Harness. These scripts include example Tcl test scripts which can be run from within the default user interface. open102master Open a basic IEC 60870-5-102 Master session and sector on the TCP loopback address (127.0.0.1) close102master Close the IEC 60870-5-102 Master session and sector opened by open102master open102Controlled Open a generic IEC 60870-5-102 Controlled Station session and sector on the TCP loopback address (127.0.0.1) close102Controlled Close the IEC 60870-5-102 Controlled Station session and sector opened by open102Controlled Triangle MicroWorks, Inc. Page 83 Communication Protocol Test Harness User Manual open102 Calls open102master and open102Controlled to open a Master and Controlled Station session and sector connected to each other via the TCP loopback address. close102 Closes the IEC 60870-5-102 Master and Controlled Station session and sectors opened by open102 5.4.3 IEC60870-5-103Examples This section discusses example scripts and utilities provided with the Test Harness that target the IEC 60870-5-103 protocols. These examples can be used as is or as the basis for the development of custom scripts specific to an individual situation. In general it is best to copy the installed examples to a local working directory before modifying them so that it is easy to restore the original if needed. 5.4.3.1 IEC 60870-5-103 Example Scripts A generic example script can be found in <installdir>\Examples\tmw103.tcl. This script is automatically loaded by the default startup script (tmwstart.tcl). This script defines procedures that will open or close an IEC 60870-5-103 session, as well as simple procedures which execute most of the available 103 commands. The following list describes the main procedures defined in tmw103.tcl. For a complete list of the defined procedures see the tmw103.tcl script file. open103master Open a basic IEC 60870-5-103 Master session and sector on the TCP loopback address (127.0.0.1) close103master Close the IEC 60870-5-103 Master session and sector opened by open103master open103Controlled Open a generic IEC 60870-5-103 Controlled Station session and sector on the TCP loopback address (127.0.0.1) close103Controlled Close the IEC 60870-5-103 Controlled Station session and sector opened by open103Controlled open103 Calls open103master and open103Controlled to open a Master and Controlled Station session and sector connected to each other via the TCP loopback address. close103 Closes the IEC 60870-5-103 Master and Controlled Station session and sectors opened by open103 gi3 Issue a IEC 60870-5-103 general interrogation on the sector opened by open103master 5.4.4 DNP3Examples This section discusses example scripts and utilities provided with the Test Harness that target the DNP3 protocol. These examples can be used as is or as the basis for the development of custom scripts specific to an individual situation. In general it is best to Page 84 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual copy the installed examples to a local working directory before modifying them so that it is easy to restore the original if needed. 5.4.4.1 DNP3 Example Script A generic example script can be found in <installdir>\Examples\tmwdnp.tcl. This script is automatically loaded by the default startup script (tmwstart.tcl). This script defines procedures that will open or close a generic DNP3 session, as well as simple procedures which execute most of the available DNP3 commands. The following list describes the main procedures defined in tmwdnp.tcl. For a complete list of the defined procedures see the tmwdnp.tcl script file. opendnpmaster Open a basic DNP3 Master session on the TCP loopback address (127.0.0.1) closednpmaster Close the DNP3 Master session opened by opendnpmaster opendnpOutstation Open a generic DNP3 Outstation session on the TCP loopback address (127.0.0.1) closednpOutstation Close the DNP3 Outstation session opened by opendnpOutstation opendnp Calls opendnpmaster and opendnpOutstation to open a Master and Outstation session connected to each other via the TCP loopback address. closednp Closes the DNP3 Master and Outstation sessions opened by opendnp dnpintegrity Issue a DNP3 integrity poll on the session opened by opendnpmaster dnpevent Issue a DNP3 event data poll on the session opened by opendnpmaster 5.4.4.2 DNP3 Outstation Simulators One of the features of the Test Harness is the ability to create scripts that will simulate a wide variety of DNP3 devices. The Test Harness comes with several scripts which support the simulation of various DNP3 Outstation devices. 5.4.4.3 Generic DNP3 Outstation Simulator A Tcl script which implements a generic DNP3 Outstation simulation can be found in <installdir>\Simulators\Generic\sdnpsim.tcl. This simulation creates a small sample database and ties the input and output points directly to user interface controls. This simulation can be used to test a simple DNP3 Master or as a basis for implementing a custom simulation. All of the procedures for this simulator are implemented in the SDNPSim Tcl namespace as part of the SDNPSim Tcl package. This package is loaded by the default startup script by the following lines of Tcl code: # Load Generic DNP3 Outstation simulator if {![catch {package require SDNPSim} msg]} { namespace import ::SDNPSim::* } Triangle MicroWorks, Inc. Page 85 Communication Protocol Test Harness User Manual To load and start this simulator type the following command at the Tcl command prompt: TCL> SDNPSim By default the simulator will open a TCP/IP channel using the TCP loopback address (127.0.0.1) and configuration parameters compatible with the opendnpmaster command described in the DNP3 examples above. Hence after starting the simulator typing the following command at the Tcl command prompt will open a Master session in the Test Harness which will communicate with the simulator in loopback mode. TCL> opendnpmaster 6. AdvancedTopics 6.1 TCP/IPConnections If TCP/IP is selected you should modify the TCP/IP Parameters as needed. The Parameters are common to all protocols supported in the Test Harness. You can specify the Host name or address of the remote device, the Port number to connect to or to listen on, and the Local IP address to use as a source address in IP messages. 6.1.1 ConfiguringMaster(client)channels If you are configuring the Test Harness to act as a Master, it is normally configured as a TCP client. The Test Harness will attempt to connect to the IP address and Port number specified. If you specify a Host name instead of an IP Address, it will attempt to resolve that name to an IP Address before connecting. If you are having difficulty connecting you should turn on the Target Layer Protocol Analyzer filter to display useful logging information about the connection sequence. If the PC the Test Harness is running on has multiple Network Interface Cards (NICs) or IP Addresses you may also want to specify which IP Address to use as the source address sent in IP messages. Normally you should use the default value of 0.0.0.0 which allows the TCP stack to choose the source IP address to use. Choosing a source IP address does not guarantee that a particular NIC will be used when sending IP messages. This choice is made by the TCP stack based on the destination IP address and the IP Routing Table. You can display the IP Routing Table by going to a Windows Command Prompt window and entering route print. It is possible to add a Page 86 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual route to this table to cause the messages to be sent out a desired NIC by "add route destIPAddress gateway". Enter "route ?" to see more information about this command. 6.1.2 ConfiguringOutstation(ControlledStationorServer)channels If you are configuring the Test Harness to act as an Outstation (Controlled Station or Server), it is normally configured as a TCP server. The Test Harness will listen on the Port number specified. When a connection is received it will use the Host (name or IP Address) configured to determine if the source of this connection is allowed. A value of *.*.*.* indicates any source address is allowed. Specific addresses 192.168.1.44 or a comma or semicolon separated list of names or addresses may also be entered. 127.0.0.1 is the loopback address and is often used if a connection is made from a client running on the same PC. If multiple channels are listening on the same port number, IP addresses can be specified to cause the incoming connection to be accepted by the desired channel based on the Master IP Address. If one channel is listening for a specific IP address and another is willing to accept any IP Address, a connection from the specified address will be accepted by the channel waiting for that address if it is not currently connected. If no channel is configured for that specific address the connection will be accepted by a channel waiting for *.*.*.*. It is not a good configuration to list both *.*.*.* AND a specific IP address for a single channel, *.*.*.* is appropriate in that case. On the Advanced Settings window there is a parameter called DisconnectOnNewSyn. For IEC and Modbus channels this parameter defaults to true, for DNP3 channels this defaults to false. This parameter controls whether or not to disconnect an existing connection when a new TCP connection request arrives. This is useful when a connection has failed, but the server is not yet aware of it. The Master can reconnect and continue communicating. For IEC and Modbus, this defaults to true. For DNP3 this defaults to false, since the DNP Spec IP Networking says to reject new connections except if configured as a dual end point Outstation and the original connection was originated by Outstation, but a new connection request is from a Master. If multiple IEC Channels are configured to listen on the same port, this parameter should be set to false to prevent new connections from disconnecting existing connections instead of connecting to other channels. Local IP address is not used by listening channels. However, DNP3 Outstations that are configured as UDP Only will use this address in UDP datagrams. 6.2 IPNetworkingSupport for DNP3 The Triangle MicroWorks Test Harness provides full IP Networking support as described in DNP3 Specification IP Networking Draft H. This document describes the use of TCP, UDP and Dual End Point support between Master and Outstation (Controlled Station or Server) devices. Triangle MicroWorks, Inc. Page 87 Communication Protocol Test Harness User Manual 6.2.1 ConfiguringDNP3channels By default, the Test Harness will open a DNP3 Master channel using TCP and UDP, as a TCP Client, which will attempt to connect to an Outstation server device at loopback address 127.0.0.1. It will send broadcast requests using UDP datagrams from local port 20000 to destination port 20000 on the Outstation device. By default, the Test Harness will open a DNP3 Outstation channel as a client listening for incoming TCP connections on port 20000, and will receive UDP datagrams containing broadcast requests on port 20000 Note, if both the Master and Outstation are running in a Test Harness on the same PC, the LocalUDPPort and DestUDPPort of the Master and Outstation must not all be configured to the default of port 20000. If they both attempt to use port 20000, broadcast messages will not be received by the Outstation channel. Configuration of the DNP3 Master device may be done by selecting OpenMaster SessionDNP3 from the menu of the Test Harness main window.. The TCP/IP address and port number should be set to the address of the Outstation (Server) device. The Advanced Settings button should be selected to change other parameters, including those related to UDP. Figure 21 shows the configuration window using the default values. Help will be displayed for each of the parameters on the advanced configuration window. Mode: mode for TCP connection (client server UDPOnly DualEndPoint) DualEndPointIpPort: TCP port number to connect to if Dual End Point device (0-65535) LocalUDPPort: local port for sending and receiving UDP datagrams on (noUDP or 0-65535) DestUDPPort: destination UDP/IP port for sending broadcast requests if Master or responses if Outstation to (0-65535) ValidateUDPAddress: validate source address of UDP datagrams (true or false) Page 88 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Triangle MicroWorks, Inc. Page 89 Communication Protocol Test Harness User Manual Figure 21. Configure DNP3Channel Networking Default Parameters Example 1: To configure a Master device as a Dual End Point device, WinTCPmode should be changed from CLIENT to DUAL_ENDPOINT, and DualEndPointIpPort should be set to the port number on the server to connect to. In this case, TCP Parameter WinTCPipPort will be the local port number datagrams will be received on. Example 2: To configure a Master or Outstation as a UDP only device, NetworkType should be set to UDP_ONLY, WinTCPmode should be changed to UDP, and LocalUDPPort and DestUDPPort should be set to the desired values. 6.2.2 DNPDualEndPointPlugin A DNP Master or Outstation can be configured to both listen and initiate a TCP connection when it is needed. Either device would typically disconnect until there is more data to send by either end. A connection could then be initiated by the device that has data to send. To demonstrate Dual End Point functionality and to facilitate testing other devices the Test Harness includes a DNP Dual End Point Plugin to configure a Master and/or SDNP channel and session configured for Dual End Point functionality. This GUI allows you to configure the destination Host Name or IP Address as well as the Listen and Destination ports. If you are opening both a Master and Outstation from the test harness on the same PC the Listen and Destination ports must be switched to be Page 90 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual compatible typically by changing the Master configuration. This would allow the Master to connect to the default Listening port 20000 for the Outstation. You can send an Enable Unsolicited Responses request from the Master to the Outstation. Then you can have either the Master or Outstation disconnect. You will see the status go to Disconnected on both devices. Then when the Outstation generates an Event and tries to send an Unsolicited Response it will initiate a TCP connection to the Master, allowing it to then send the data. Typically after an idle period either the Outstation or Master might choose to disconnect. You can simulate that by choosing Disconnect on either device. While disconnected if the Master needs to send a Read request it will initiate a TCP connection to the Outstation and perform the Read request. Optionally you can use the Disconnect after being idle for x seconds check box. Triangle MicroWorks, Inc. Page 91 Communication Protocol Test Harness User Manual You can choose to start only the Master or the Outstation configured to connect to a device outside of the Test Harness that implements Dual End Point functionality. You can then test that functionality by disconnecting and requesting data to be sent causing a reconnection. 6.3 Secure Authentication Secure Authentication functionality based on IEC 62351 has been specified for use in the DNP3, IEC 60870-5-101 and 104 protocols. This functionality is available as an option in the TMW Protocol Test Harness as well as other TMW products. 6.3.1 DNP3SecureAuthentication In February 2007 the DNP3 Users group released the DNP3 Specification Volume 2, Supplement 1 Secure Authentication Version 1. Since then the Test Harness has provided support for this Secure Authentication specification. After further review and discussion by the DNP3 Technical Committee a new version, Version 2.0 was released in July of 2008. The Test Harness, beginning with version 3.04.00, implemented that version of the specification known as SAv2 DNP3 Secure Authentication Version 5 (SAv5) was released in November 2011. This version of Secure Authentication is contained in IEEE 1815-2012. This version adds the ability to remotely distribute User Update Keys over the DNP3 protocol under the direction of a trusted third party known as an Authority. Other changes to SAv5 make it incompatible with SAv2. In order to interoperate with a device that supports SAv2 that association must use SAv2. Version 3.14.0 of the Communication Protocol Test Harness continues to support SAv2 as well as an optional component implementing SAv5. If the optional SA component is licensed, support for either SAv2 or SAv5 can be selected per session. The Test Harness provides a good way to simulate devices that support DNP3 Secure Authentication, view Secure Authentication message exchanges or to test a device that implements Secure Authentication. There is a default workspace provided which can be used as a starting place for learning about Secure Authentication. "DemonstrationWorkspaceDNPMastertoTestHarness OutstationWithSecureAuthentication.thws".This workspace creates both a DNP3 Master and DNP3 Outstation in the Test Harness. The Master is configured to connect to the Outstation using TCP/IP. Both the Master and the Outstation are configured with Secure Authentication Enabled, and contain 3 Secure Authentication users with matching update keys. Using this workspace the protocol analyzer will display the messages being exchanged between the Master and the Outstation. Transmitted Secure Authentication messages are in green. Received Secure Authentication messages are in blue. Page 92 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual If you just want to see the data sent and received by the Master you can turn off the Outstation diagnostics, by expanding and unchecking the Device Filter item for sDNP channel and session. Of course, all of the above configuration can be modified, including adding or deleting users, configuring update keys, changing protocol diagnostic format, etc. 6.3.2 SimulatingaDNP3SecureAuthenticationMaster When a Master DNP3 session is opened, AuthenticationEnabled should be set to true. Setting the session parameter AuthExtraDiags (under Advanced Settings) to true will cause additional diagnostic information to be displayed in the protocol analyzer window. Other configuration options are available under the Advanced Settings option. Most of the Test Harness Tcl commands allow Aggressive Mode to be specified and to specify which Authentication User number is to be used for Aggressive Mode or in response to an authentication challenge by the Outstation. For example: mdnpbincmd authUserNumber 20 mdnpbincmd authAggressiveMode true authUserNumber 300 In the Tcl command window, the command mdnpauthentication can be used for some Secure Authentication testing. You can specify that the next response received is to be considered critical and should be challenged. You can force a key exchange sequence for a particular user number. You can also force a Secure Authentication Error Message to be sent when the next response is received on the Test Harness Master. Enter mdnpauthentication in the Test Harness Tcl command window for more details. 6.3.3 SimulatingaDNP3SecureAuthenticationOutstation When a Outstation DNP3 session is opened, AuthenticationEnabled should be set to true. Setting the session parameter AuthExtraDiags (under Advanced Settings) to true will cause additional diagnostic information to be displayed in the protocol analyzer window. Other configuration options are available under the Advanced Settings option. In the Tcl command window the command sdnpauthentication can be used for some Secure Authentication testing. You can cause responses to be sent with an Aggressive Mode object. You can force a Session Key to timeout. You can cause responses containing events to issue a preChallenge of the expected application confirm. You can also force a Secure Authentication Error Message to be sent when the next request is received on the Test Harness Outstation. Enter sdnpauthentication in the Test Harness Tcl command window for more details. Triangle MicroWorks, Inc. Page 93 Communication Protocol Test Harness User Manual 6.3.4 DNP3SecureAuthenticationVersion2orVersion5 DNP3 Secure Authentication Version 5 (SAv5) adds the ability for the Master to remotely add and modify Secure Authentication Users using Globally Unique User Names, a specified Role, and User Update Keys over the DNP3 protocol. This means that the User Numbers and User Update Keys do not have to be preconfigured on the Master and Outstation, but can be added or changed remotely. SAv5 is not compatible with SAv2. The Test Harness continues to provide SAv2 as well as providing a separately licensed component providing SAv5. If SAv5 is licensed, you can specify whether to use SAv5 or SAv2 when a session is opened. Set AuthOperateInV2Mode on the Session Advanced Settings window, or authOperateInV2Mode on the mdnpopensession abd sdnpopensession TCL commands to choose which version to use. AuthOperateInV2Mode is the Advanced Settings property if the Open Master or Outstation Session GUI is used. When using SAv5 the Master session in the Test Harness supports the following TCL commands. mdnpauthuserstatuschange To send a Secure Authentication User Status Change request to the Outstation, beginning the update key change sequence, adding, modifying, or deleting a user on the Outstation. mdnpauthusercertificate To send a Secure Authentication User Certificate request to the Outstation, beginning the Asymmetric update key change sequence, adding, modifying, or deleting a user on the Outstation. This is an alternative to sending the Authentication User Status Change using an extension of a standard X.509v3 certificate. mdnpauthuserupdatekey To send a Secure Authentication Update Key Change Request to update or verify an existing User Update Key. Enter mdnpauthuserstatuschange, mdnpauthusercertificate, or mdnpauthuserupdatekey in the TCL window to get more details about each command or look at some of the example commands and their descriptions shown below. By selecting the mdnp or sdnp session in the data window you can look at CryptoDB.AuthUsers in the property window to get more information about the current SA users. There are a number of other Secure Authentication Configuration and Status values in the property window. Alternatively, you can right click on the session to modify the session selecting the Configure User Numbers and Update Keys button to view and add/delete/modify SA users. The mdnpauthuser and sdnpauthuser commands also provide a TCL interface to get/add/delete SA users. Example 1 The following command will instruct the Test Harness Master to add a Globally Unique User Name johnsmith with the permissions assigned to an operator using the symmetric key change method 3 (AES-128/SHA-1/SHA-1) and to also generate and send Page 94 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual the User Update Key to the Outstation. This should remain valid for 30 days after the Outstation receives this command. TCL>mdnpauthuserstatuschange username johnsmith operation add method 3 role operator expires 30 sendUpdateKey true If this remote update sequence succeeds, the Outstation will return a User Number to be used on this association. This User Number can then be used for authentication in the way a preconfigured User Number and Update Key was used by SAv2. You can either edit the sdnp session or select it in the data window to display the Users. Example 2 The following command will instruct the Test Harness Master to delete the Gobally Unique User Name johnsmith from the Outstation. TCL>mdnpauthuserstatuschange username johnsmith method 4 operation delete If this remote delete succeeds, the Outstation will no longer have a User johnsmith configured, and the User Number that was previously being used will no longer work. The Test Harness DNP Master will still show Authentication User johnsmith but will indicate User Number 0 which is not used. If you want to delete the User from the Master session database you should either modify the session or use the TCL command. TCL>mdnpauthuser remove username johnsmith 6.3.5 IEC60870-5-101and104SecureAuthentication In 2013 IEC 60870-5-7 Security Extensions to IEC 60870-5-1-1 and IEC 60870-5-104 protocols (applying IEC 62351) was released. This Secure Authentication functionality has been added to the Test Harness and TMW 101 and 104 Source Code Libraries. The Test Harness provides a good way to simulate devices that support Secure Authentication, view Secure Authentication message exchanges or to test a device that implements Secure Authentication. There is a default workspace provided which can be used as a starting place for learning about Secure Authentication in 101 and 104. "DemonstrationWorkspaceIEC60870-5-101 MastertoTestHarnessControlledStationWithSecureAuthentication.thws"or"Demonstration WorkspaceIEC60870-5-104MastertoTestHarnessControlledStationWithSecure Authentication.thws"These workspaces create both a 101/4 Master and 101/4 Controlled Station in the Test Harness. The Master is configured to connect to the Controlled Station using TCP/IP. Both the Master and the Controlled Station are configured with Secure Authentication Enabled and contain two Secure Authentication users with matching update keys. Using these workspaces the protocol analyzer will display the messages being exchanged between the Master and the Controlled Station. Triangle MicroWorks, Inc. Page 95 Communication Protocol Test Harness User Manual 6.3.6 Simulatinga101/104SecureAuthenticationMaster When a Master 101/104 sector is opened, AuthenticationEnabled should be set to true. Setting the session parameter AuthExtraDiags (under Advanced Settings) to true will cause additional diagnostic information to be displayed in the protocol analyzer window. Other configuration options are available under the Advanced Settings option. For 101/104 SA, Aggressive Mode is the default mode of operation for critical requests and responses. The Master must be configured with a list of TypeIds that are considered critical by the Controlled Station. When a critical request is sent by the Master it will be encapsulated in an Aggressive Mode (authenticated) request. For example: m101(4)cscna ioa 2100 authUserNumber 20 In the Test Harness Tcl command window the command m101(4)authentication can be used for some Secure Authentication testing. You can force a key exchange sequence for a particular user number. You can also force a Secure Authentication Error Message to be sent when the next response is received on the Test Harness Master. Enter m101(4)authentication in the Test Harness Tcl command window for more details. 6.3.7 Simulatinga101/104SecureAuthenticationControlledStation When a Controlled Station 101/4 sector is opened, AuthenticationEnabled should be set to true. Setting the session parameter AuthExtraDiags (under Advanced Settings) to true will cause additional diagnostic information to be displayed in the protocol analyzer window. Other configuration options are available under the Advanced Settings option. In the Tcl command window the command s101/4authentication can be used for some Secure Authentication testing. You can force a Session Key to timeout. You can also force a Secure Authentication Error Message to be sent when the next request is received on the Test Harness Controlled Station. Enter s101/4authentication in the Test Harness Tcl command window for more details. Any request or response can be configured as a Critical Message. All critical messages will be sent using Aggressive Mode. Both the Master and the Controlled Station device must be configured with which requests and which responses are considered critical. Any critical messages that are not sent using Aggressive Mode will be discarded. 6.3.8 KeyTypes Depending on whether the optional remote User Update Key Change and Asymmetric Key Change methods are supported a variety of keys are required for DNP and 101/4 Secure Authentication. Here is a description of each of these keys and where they are generated, configured and used. Page 96 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Note that Asymmetric (Public/Private) Keys because of their size and complexity are read from Windows files. Symmetric Keys which can be represented as a short sequence of hex octets are configured directly in the Test Harness. TMWAuthUser.UpdateKey User Update Key, 1 per User. Used to encrypt/decrypt the Session Keys when sending them from the Master to the Outstation. These Keys can be preconfigured per User on both the Master and Outstation or dynamically added and changed and sent over the DNP or 101/4 protocols to the Outstation under the direction of the Authority if Remote Key Update is supported. TMWAuthUser.AsymPubKey Name of file containing the User Public Key, 1 per user, when an Asymmetric Key Change Method is used. Specification says user should generate the public/private key pair and provide the Public Key to the Master. This key pair could be generated by the Authority and given to the User, or even generated by the Master on behalf of the User and given to the Authority. The Authority must certify this key and therefore both the Master and Authority must have this key. TMWAuthUser.AsymPrvKey -Name of file containing the User Private Key, 1 per user, when an Asymmetric Key Change Method is used. Specification says user should generate the public/private key pair and provide the public and private key to the Master. This key pair could be generated by the Authority and given to the User, or even generated by the Master on behalf of the User. Only the Master will use the User Private Key to sign User Update Key data to be sent to the Outstation. The Outstation will use the User Public Key that the Master has sent to the Outstation over DNP in a g120v10 object or over 101/4 in a TypeId 90 request. AuthOSAsymPubKey Name of file containing the Outstation Public Key, 1 per Outstation, when an Asymmetric Key Change Method is used. The Outstation Public/Private key pair should be generated on the Outstation and the Public Key securely configured on the Master. AuthOSAsymPrvKey Name of file containing the Outstation Private Key, 1 per Outstation, when an Asymmetric Key Change Method is used. The Outstation Public/Private key pair should be generated on the Outstation. The Private Key will only be known by the Outstation. AuthoritySymCertKey Authority Certification Key, 1 per Outstation, when a Symmetric Key Change Method is used. The secret Certification Key will be generated on the Authority and must be securely configured on the Outstation. Both the Master acting as an Authority and the Outstation must know this key. AuthorityAsymPubKey Name of file containing the Authority Public Key, 1 per Authority, when an Asymmetric Key Change Method is used. The Public/Private key pair should be generated on the Authority and the Public Key must be securely configured on the Outstation. AuthorityAsymPrvKey Name of file containing the Authority Private Key, 1 per Authority, when an Asymmetric Key Change Method is used. The Public/Private key pair Triangle MicroWorks, Inc. Page 97 Communication Protocol Test Harness User Manual should be generated on the Authority and the Private Key will only be known by the Master acting as an Authority. 6.3.9 SampleAsymmetricKeysincludedwithTestHarness To facilitate testing DNP SAv5 or IEC 60870-5 Remote Key Change functionality some sample asymmetric private and public key files are included with the Test Harness. These are now installed by default in a public directory such as C:\Users\Public\Documents\Triangle MicroWorks\TMWCertificates\SA_AsymmetricKeys THESE SAMPLE KEYS SHOULD ONLY BE USED FOR TESTING! The Key Change Method specifies the required type (RSA or DSA) and size of the asymmetric keys in bits. For each method. Asymmetric Key Change Method 67 requires 1024, 68 and 70 require 2048, and 69 and 71 require 3072. If an incorrect type or size key is specified for a particular method the Test Harness will indicate this and will fail to update the user. DNP3 TB2016-002 has added 5 new key change methods that use RSA keys for signing instead of DSA keys. Support for these was added in version 3.20.000 of the Test Harness. Included are sample Authority Private and Public Key pairs and Certificates for when the Test Harness is simulating the Authority. Configured in the Master and Outstation (Controlled Station) DNP3 sessions or 101/4 sectors are parameters AuthCryptoDb: AuthorityAsymPrvKey on Master, and AuthCryptoDb: AuthorityAsymPubKey (can contain Public Key or Certificate) on Outstation. TMWTestAuthorityDsa1024PrvKey.pem, PubKey.pem, Cert.pem TMWTestAuthorityDsa2048PrvKey.pem, PubKey.pem, Cert.pem TMWTestAuthorityDsa3072PrvKey.pem, PubKey.pem, Cert.pem TMWTestAuthorityRsa1024PrvKey.pem, PubKey.pem, Cert.pem TMWTestAuthorityRsa2048PrvKey.pem, PubKey.pem, Cert.pem TMWTestAuthorityRsa3072PrvKey.pem, PubKey.pem, Cert.pem Sample Outstation Private and Public Key pairs. Configured in the MDNP and SDNP sessions and M101/4 and S101/4 sectors. AuthCryptoDb: AuthOSAsymPubKey on Master, AuthCryptoDb: AuthOSAsymPrvKey on Outstation. TMWTestOSRsa1024PrvKey.pem, TMWTestOSRsa1024PubKey.pem TMWTestOSRsa2048PrvKey.pem, TMWTestOSRsa2048PubKey.pem TMWTestOSRsa3072PrvKey.pem, TMWTestOSRsa3072PubKey.pem Sample User Private and Public Key pairs. Used in the mdnpauthuserstatuschange or mdnpauthusercertificate (m101/4authuserstatuschange (future0) command parameter UserPrvKey, UserPubKey and UserCert when using an asymmetric method. Page 98 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual TMWTestUserDsa1024PrvKey.pem, PubKey.pem, Cert.pem TMWTestUserDsa2048PrvKey.pem, PubKey.pem, Cert.pem TMWTestUserDsa3072PrvKey.pem, PubKey.pem, Cert.pem TMWTestUserRsa1024PrvKey.pem, PubKey.pem, Cert.pem TMWTestUserRsa2048PrvKey.pem, PubKey.pem, Cert.pem TMWTestUserRsa3072PrvKey.pem, PubKey.pem, Cert.pem The included sample keys were generated using the OpenSSL commands found in C:\Users\Public\Documents\Triangle MicroWorks\TMWCertificates\SA_AsymmetricKeys\MakeCerts.bat. You should generate your own keys for testing and actual deployment. # Here is an example to Generate DSA 2048 private keys: openssl dsaparam -out ca_dsaparam.pem 2048 openssl req -newkey dsa:ca_dsaparam.pem -sha256 -keyout TMWTestAuthorityDsa2048PrvKey.pem -out ca_dsa2048.csr -config ca.cnf -passout pass:triangle openssl x509 -req -in ca_dsa2048.csr -sha256 -extfile ca.cnf -extensions certificate_extensions signkey TMWTestAuthorityDsa2048PrvKey.pem -days 9999 -out TMWTestAuthorityDsa2048Cert.pem -passin pass:triangle openssl dsa -in TMWTestAuthorityDsa2048PrvKey.pem -out TMWTestAuthorityDsa2048PubKey.pem -pubout -outform PEM -passin pass:triangle # Here is an example to Generate User DSA 2048 keys and X.509 Signed Certificates: openssl dsaparam -out dsa_param.pem 2048 openssl req -newkey dsa:dsa_param.pem -sha256 -keyout TMWTestUserDsa2048PrvKey.pem out dsa2048.csr -days 9999 -config user.cnf -subj "/CN=Common/C=US/ST=North Carolina/L=Raleigh/O=Triangle MicroWorks, Inc./" -passout pass:triangle openssl ca -in dsa2048.csr -out TMWTestUserDsa2048Cert.pem -cert TMWTestAuthorityDsa2048Cert.pem -keyfile TMWTestAuthorityDsa2048PrvKey.pem -config ca.cnf -passin pass:triangle -outdir certs -batch -noemailDN openssl dsa -in TMWTestUserDsa2048PrvKey.pem -out TMWTestUserDsa2048PubKey.pem pubout -outform PEM -passin pass:triangle # Here is an example to Generate RSA 2048 private keys for Outstation: openssl genrsa -out TMWTestOSRsa2048PrvKey.pem 2048 openssl rsa -in TMWTestOSRsa2048PrvKey.pem -out TMWTestOSRsa2048PubKey.pem pubout -outform PEM # Here is an example to Generate the Authority RSA 2048 keys and the User RSA keys and Certificates and sign them for for DNP3 TB2016-002: openssl genrsa -out TMWTestAuthorityRsa2048PrvKey.pem 2048 -config ca.cnf -passout pass:triangle openssl rsa -in TMWTestAuthorityRsa2048PrvKey.pem -out TMWTestAuthorityRsa2048PubKey.pem -pubout -outform PEM openssl x509 -req -in ca_dsa2048.csr -extfile ca.cnf -extensions certificate_extensions -signkey TMWTestAuthorityRsa2048PrvKey.pem -days 9999 -out TMWTestAuthorityRsa2048Cert.pem passin pass:triangle openssl genrsa -out TMWTestUserRsa2048PrvKey.pem 2048 Triangle MicroWorks, Inc. Page 99 Communication Protocol Test Harness User Manual openssl rsa -in TMWTestUserRsa2048PrvKey.pem -out TMWTestUserRsa2048PubKey.pem pubout -outform PEM openssl ca -in dsa2048.csr -out TMWTestUserRsa2048Cert.pem -cert TMWTestAuthorityRsa2048Cert.pem -keyfile TMWTestAuthorityRsa2048PrvKey.pem -config ca.cnf -passin pass:triangle -outdir certs -batch -noemailDN 6.3.10 TestingaSecureAuthenticationDevice Open a DNP3 channel/session or 101/4 channel/session/sector to communicate with your external Master or Outstation (Controlled Station) device that implements Secure Authentication. To get started enable Secure Authentication and make sure to configure the same user numbers and update keys in the Test Harness and your device. For 101//4 both the Master and Controlled Station devices must be configured as to which requests and responses are considered critical. This critical message configuration must match for the Master and Controlled Station. The protocol analyzer window can be used to look at the Secure Authentication message sequences. Communication with the Test Harness can be used to verify that the key generation, wrap/unwrap, encryption/decryption and hashing functions are properly implemented on the new device. Using the mdnpauthentication, sdnpauthentication, m101/4authentication or s101/4authentication commands described above will let you create a number of test situations to determine if your device responds appropriately. 6.3.11 OpenSSLforCryptographyFunctionality To support Secure Authentication the Test Harness requires access to advanced cryptography routines for encryption/decryption, signing and other functionality. To support Secure Authentication, the Test Harness makes use of a readily available Open Source toolkit, OpenSSL, to provide that functionality. See http://openssl.org/ for more information about that project. The OpenSSL dlls that provide this cryptographic functionality are included with the Test Harness installation. The version of OpenSSL that is shipped with this Test Harness may not be the latest version of OpenSSL and therefore may not have the most recent updates and security patches. Please check to see that you are using the most up to date approved version of OpenSSL. (DNP3 SAv2 which requires less cryptography functionality does not require OpenSSL). 6.4 UsingDNP3BinaryConfiguration Files When creating a Master or Outstation, Test Harness provides the option of configuring the device using a DNP3 Binary Configuration file. These file types can be created by Triangle MicroWorks DNP3 Forge application. DNP3 Forge is a Window application that is used to create and modify XML Device Profile documents. DNP3 Forge creates a Binary Configuration file in much the same way that it creates an XML Device Profile file. The user fills in values for applicable sections of the device profile and can then Page 100 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual export those values as a Binary Configuration file. Only the current values from subsections with values are included in the Binary Configuration file. When opening a DNP3 Master or Outstation, the Open dialog includes a text box and a Browse button to enter a path to a Binary Configuration file (See Session Tab Section). Once the path to the file is identified, the Apply Values button must be clicked to cause the values in the file to be applied to the session, channel, and target layer. The values can be verified by opening the Advanced Settings properties by clicking the Advanced Settings button at the bottom of the Open Dialog box. Each time a Binary Configuration file is applied to the device, the values contained in the file are written to their appropriate settings in the device. The values are not cleared before the operation occurs, so if the wrong file is applied, the Open dialog should be closed and a new device should be opened so that values from the incorrect file do not remain in the configuration. When creating an Outstation, the Binary Configuration file must have been created with the device function set to Outstation in the device profile. Trying to apply a Binary Configuration file with the device function set to Master will result in an error. When creating a Master, a Binary Configuration file with the device function either Outstation or Master can be opened. If the device function is set to Master, the Master will be created with settings found in the Binary Configuration file. If the device function is set to Outstation, the Master will be created using only the settings from the Binary Configuration file necessary to configure the Master to communicate with the Outstation described in the file. A convenient use case for this function is to apply a Binary Configuration file for a Master to the Master device, then apply a second Binary Configuration file for an Outstation to configure the Master to talk to that Outstation. A Binary Configuration file contains most of the values described in Section 1 of the DNP3 Device Profile although not all of those values are used by the masters and outstations in the Test Harness. DNP3 Forge provides a description for each subsection of the device profile that includes how the value is used in the DNP3 protocol stack. By examining these descriptions, you can determine if the values will be set in the device. For example, some values are described as not used in the SCL or set at compile time so the value will not be able to be imported from a Binary Configuration file even if the value is set in DNP3 Forge. 6.5 GeneratingDNP3Events 6.5.1 GeneratingDNPAnalogEvents DNP events are typically generated in the Outstation when something significant happens. This is done by default in the test harness when points change flags or value. There are some DNP events, such as binary and analog command events, that are less obvious how to create using the test harness. Concerning Analog Output points and events: Object Group 40 is an analog output status Triangle MicroWorks, Inc. Page 101 Communication Protocol Test Harness User Manual Object Group 41 is an analog output command Object Group 42 is an analog output event Object Group 43 is an analog output command event Object 42 analog output events are generated when changes to the analog output occur such as to the exception flags and changes of the output value when the underlying point returns a value. This event shall not be generated to simply report that a command was received. Object Group 43 event objects should be used for this purpose Object 43 analog output command events are optional. A command event may be generated by an Outstation when it receives a control command for an output point from an internal or external source. An example of this includes a command request received through a DNP3 analog output block, a command request from a different protocol, or on a control change command from an executing application. Object 43 analog output command events would not normally be sent back to the Master that sent the analog output command, since the Outstation already is sending back the response to the command itself. However, the test harness can be configured to generate Object 43 analog output command events. They would NOT be the result of receiving an analog output command from the Master for that point because the Master just sent the OS the output command and that is NOT the purpose of the event. To generate either the object 42 or object 43 events, the analog output point would need to be assigned to an event class. This could be done by sending a assign class command from the Master or by setting the event class in the data window of the TH SDNP session, or by using a TCL command to set the event class. The following would generate a command event object 43 for point 1 with a status value of 2 (NO SELECT) belonging to event class 1 TCL> sdnpanlgout set point 1 cmdstatus 2 class 1 Object 42 events can be generating by assigning the analog output point to an event class and then changing the actual value or flags of the analog output point either in the data window or in the TCL Command window. 6.5.2 GeneratingallDNPEvents This is a summary of how to generate all DNP events for an SDNP Session in the Test Harness. For most events this can be done from the Data Window or the TCL Command Window. Obj2 Binary Input Events Data Window - (1) Binary Inputs - Toggle value or set flags TCL> sdnpbinin set value or flags Page 102 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Obj4 Double-bit Binary Input Events Data Window - (3) Double Inputs - Change value or set flags TCL> sdnpdblin set value or flags Obj11 Binary Output Events Data Window - (10) Binary Output Statuses - Set Class Data Window - (10) Binary Output Statuses - Toggle Value or Set Flags TCL> sdnpbinout set class TCL> sdnpbinout set value or flags Obj13 Binary Output Command Event Data Window - (10) Binary Output Statuses - Set CmdEventClass TCL> sdnpbinout set cmdEventClass TCL> sdnpbinout set cmdStatus Obj22 Counter Output Events Data Window - (20) Running Counter -Increment Counter or Set Flags TCL> sdnpcntr set value Obj23 Frozen Counter Output Events Data Window - (20) Running Counter -Freeze TCL> sdnpcntr freeze Obj32 Analog Input Events Data Window - (30) Analog Input Events - Change Value or Set Flags Obj33 Frozen Analog Input Events Data Window - (31) Frozen Analog Inputs Set Class Data Window - (30) Analog Input -Freeze Obj42 Analog Output Events Data Window - (40) Analog Output Statuses - Set Class Data Window - (40) Change Value or Set Flags TCL> sdnanlgout set class TCL> sdnanlgout set value or flags Obj43 Analog Output Command Events Data Window - (40) Analog Output Statuses - Set CmdEventClass TCL> sdnanlgout set cmdEventClass TCL> sdnanlgout set cmdStatus Obj88 Data Set Events Data Window (87) Data Set Present Values - Edit Data Set Value Element TCL>sdnpdataset set elem and value Obj111 Octet String Events Triangle MicroWorks, Inc. Page 103 Communication Protocol Test Harness User Manual Data Window - (110) String Data - Change String Value TCL>sdnpstring set value Obj113 Virtual Terminal Events Data Window - (112) Virtual Terminal Output - Change String Value TCL>sdnpvterm set value Extended String Events Data Window - (110) String Data - Change String Value Data Window - (114) Extended String Data - Change String Value TCL>sdnpstring set value TCL>sdnpextstring set value or flags Obj120 Security Events TCL>sdnpauthentication sendErrorEvent & errorEventCode & errorEventAssocId & errorEventSequence Obj122 Security Statistic Events Data Window - (121) Authentication Security Statistics - Increment 6.6 SSL or TLSoverTCP/IP The Test Harness allows you to use Secure Socket Layer (SSL) or Transport Layer Security (TLS) over TCP connections for all of the supported protocols. TLS is an IETF standard based on Version 3 of SSL. Using these allows for verification of the server and or client credentials as well as encryption of the data transferred. The TLS negotiation and encryption/decryption code is not implemented or contained in the Test Harness. Instead the Test Harness makes use of a readily available Open Source cryptography toolkit, OpenSSL, to provide that functionality. See http://openssl.org/ for more information about that project. The OpenSSL dlls that provide this functionality are included with the Test Harness installation. The version of OpenSSL that is shipped with this Test Harness may not be the latest version of OpenSSL and therefore may not have the most recent updates and security patches. Please check to see that you are using the most up to date approved version of OpenSSL. When a TCP channel is opened in the Test Harness select the Advanced Setting button in the TCP/IP Communications Configuration Settings the following parameters can be configured. TlsEnabled - True Enables TLS on this channel. TlsCommonName - TLS Common Name to expect on incoming TLS certficates (empty string disables). CaFile - TLS Certificate Authority Certificates File. For verifying incoming certificates. Page 104 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual CaDirectory - TLS Certificate Authority Certificates Directory (instead of CaFile). This directory must be in hash format with file names of the form hash.0 where hash is the value of the hashed certificate subject name. You can use openssl.exe x509 -subject hash -in CAfilename.pem to calculate the hash value and then rename the file hash.0 for example fbb0c43.0. For more information see OpenSSL documentation on the CApath directory. CaCrlFile - TLS Certificate Authority Certificate Revocation List file. CaVerifyDepth - Maximum Depth of certificate chaining verification. DhFileName -File containing Diffie-Hellman parameters for TLS cipher suites. TlsRenegotiationCount - TLS Renegotiation Count. Number of PDUs before forcing cipher renegotiation TlsRenegotiationSeconds - TLS Renegotiation Seconds. Time before forcing cipher renegotiation. TlsRsaCertificateFile - TLS RSA Certificate File. Contains the certificate including the public key for RSA TLS ciphers TlsRsaPrivateKeyFile -TLS RSA Private Key File. Contains the private key for RSA TLS ciphers TlsRsaPrivateKeyPassPhrase - TLS RSA Private Key File Pass Phrase for decrypting private key if TlsRsaPrivateKeyFile is encrypted. TlsDsaCertificateFile TLS DSA Certificate File. Contains the certificate including the public key for DSA TLS ciphers TlsDsaPrivateKeyFile TLS DSA Private Key File. Contains the private key for DSA TLS ciphers TlsDsaPrivateKeyPassPhrase TLS DSA Private Key File Pass Phrase for decrypting private key if TlsDsaPrivateKeyFile is encrypted. Default versions of the files specified in the above configuration are delivered in a common place depending on your version of Windows. The typical place for Windows 7 systems for example is C:\Users\Public\Documents\Triangle MicroWorks\TMWCertificates When you open a channel with TLS enabled, you should look at the diagnostics messages in the protocol log from the target layer to determine if things are working properly. You Triangle MicroWorks, Inc. Page 105 Communication Protocol Test Harness User Manual should see messages similar to the following for a DNP Outstation indicating that TLS is configured. ### NET DNP OS -*.*.*.*:20000 -TCP open ### NET DNP OS -*.*.*.*:20000 -TLS, open ### NET DNP OS -*.*.*.*:20000 -TLS, Initialized SSL Library ### NET DNP OS -*.*.*.*:20000 -TLS, Created context ### NET DNP OS -*.*.*.*:20000 -TLS, Read CA list C:\Users\Public\Documents\Triangle MicroWorks\TMWCertificates\ca_public\tmw_sample_ca_rsa_public_certificate.pem ### NET DNP OS -*.*.*.*:20000 -TLS, Opened DH file ### NET DNP OS -*.*.*.*:20000 -TLS, Set DH parameters ### NET DNP OS -*.*.*.*:20000 -TCP listen for a connection ### NET DNP OS -*.*.*.*:20000 -TCP listen, no existing Listener found, creating one ### NET DNP OS -*.*.*.*:20000 -TCP Listen, add this channel to the listener ### NET DNP OS -*.*.*.*:20000 -TCP Listen, successfully listening Sometime later when a connection from the Master/TCP Client arrives you should see messages similar to the following. ### NET DNP OS -*.*.*.*:20000 -TLS, Certificate ### NET DNP OS -*.*.*.*:20000 -TLS, Subject=/CN=TLS/ST=North Carolina/C=US/O=Triangle MicroWorks, Inc. ### NET DNP OS -*.*.*.*:20000 -TLS, Issuer=/C=US/ST=North Carolina/L=Raleigh/O=Triangle MicroWorks, Inc./CN=Root CA ### NET DNP OS -*.*.*.*:20000 -TCP LISTENER: connection accepted If you see the following messages, verify the names and location of the files configured. ### .NET DNP OS -127.0.0.1:20000 -TLS, Can't read certificate from file ### .NET DNP OS -127.0.0.1:20000 -TLS, Can't read DSA or RSA key files If the TLS connection is made between two hosts it is possible to view the data being exchanged using WireShark or a similar TCP analyzer program. Using port 443 allows WireShark to provide a more detailed TLS interpretation. 6.7 IEC 60870-5-101/4MeasuredValues: MMENA/B/C, PMENA/B/C, andPACNA explained. In 60870-5-101 and 60870-5-104 there are three types of measured values; normalized, scaled and short floating point. The following description applies to both the 101 and 104 protocols. MMENA measured value, normalized value. MMENB measured value, scaled value MMENC measured value, short floating point value PMENA parameter of measured values, normalized value PMENB parameter of measured values, scaled value PMENC parameter of measured values, short floating PACNA parameter activation for all measured values Page 106 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual While the data types are different for the three types of measured values and parameters, the behavior is the same. Wherever MMENA or PMENA is used in the following description you can substitute MMENB/PMENB or MMENC/PMENC. For each MMENA created on a Controlled Station there are also 4 PMENAs and 1 PACNA created, each with unique IOAs different than the IOA for the MMENA. The PMENAs contain smoothing, threshold, low limit, and high limit values which can be used to determine when the Controlled Station should generate an event, causing the Controlled Station to send a spontaneous MMENA message to the Master. These PMENA values on the Controlled Station can be modified by sending a PMENA command from the Master. The Controlled Station will send the PMENA value to the Master in response to the write request from the Master, or can be configured to be sent in response to a station interrogation CICNA. In the Test Harness the 4 MMENA points now default to be members of the GI global group and will be sent in response to a general interrogation. They can also be configured out of that global group if desired. While a Controlled Station may or may not use these PMENA values to implement a variety of deadbanding algorithms, the Test Harness does not actually use these PMENA values to determine when a spontaneous event should be sent. To avoid confusion whenever a measured value is changed on the Test Harness Controlled Station sector it sends a spontaneous event to the Master. The PACNA allows the Master to enable or disable cyclic reporting of the associated measured value. While the Master can send a PACNA command, the protocol definition does not allow a PACNA to be part an interrogation response, sent as background or cyclic data, read with a CRDNA, and will not be sent spontaneously to the Master. If the Master sends a PACNA command and gets a response indicating success it can then know whether that measured value would be included in cyclic data. The Test Harness Data Window will show MMENA and PMENA types and their current values and configuration settings. The PACNA type does not show up in the data window since it is only used to enable/disable CYCLIC data for a particular MMENA, which can be viewed or modified by accessing the MMENA GroupString or Group properties. There is nothing in the defined protocol messages that allows the Controlled Station to indicate to the Master which PMENA and PACNA IOAs are associated with a MMENA. In the Test Harness Data Window, the Controlled Station MMENA contains properties that indicate the IOAs of the 4 associated PMENAs and the associated PACNA. Triangle MicroWorks, Inc. Page 107 Communication Protocol Test Harness User Manual A 101 Master in the Test Harness can send commands to the Controlled Station to change the measured values, parameter values and to activate/deactivate cyclic data for a particular measurand. As always, the control point for the measurand IOA is offset 2000 from the monitor point. Here are some examples: TCL>m101csena ioa 2500 value 100 TCL>m101pmena ioa 900 value 100 type threshold TCL>m101pmena ioa 901 value 10 type smoothing TCL>m101pmena ioa 902 value 0 type low TCL>m101pmena ioa 903 value 10 type high TCL>m101pacna ioa 1200 activate false NOTE: for the m101pmena command the type needs to match the type for that IOA or the command will fail. The 60870-5-101 specification defines MMENA/PMENA normalized values as F16[1..16]<1..+1 215>. Values should be converted to normalized values before being sent and converted back after being received. The largest possible value will be sent as 0x7fff, the median value as 0 and the largest negative value as 0x8000. The Test Harness treats MMENA/PMENA normalized values simply as 16 bit hex values and does not try to convert to and from normalized values. Page 108 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 6.8 IEC60870-5-104Redundancy The Triangle MicroWorks Test Harness supports redundancy as specified in IEC 608705- 104 Edition 2 (previously this was described in the Norwegian IEC 60870-5-104 User Conventions). A redundancy group can support 2 or more redundant connections. Only one of the connections will be active at a time. The Master is responsible for monitoring all of the redundant connections, determining which connection should be active and switching over to a redundant connection if necessary. By sending STARTDT and STOPDT on the connections the Master informs the Controlled Station which connection will be used for Information Frames. 6.8.1 ConfiguringRedundantControlledStationChannels To create a 104 Controlled Station that supports redundancy on the Test Harness, select Open, Server Session, IEC60870-5-104 as usual. On the Channel tab there is a check box Enable Redundancy that should be selected. Then Open will open a redundancy group with a single connection (channel). To add redundant connections to the redundancy group go to a Data Window. Under the Test Harness hierarchy you will see a redundancy group called rdcy by default. Under that you will see a single channel called S104 by default, and a single session s104 containing a single sector s104. Right click on the redundancy group and select Add Redundant Channel. Configure this channel including Advanced Settings as desired before selecting the Open button. To open two Controlled Station channels listening on the same port and IP Address it is necessary to set the Advanced Setting, DisconnectOnNewSyn property to False. Otherwise each time a new connect request arrives for the redundant connection it will disconnect the first connection. If you are using different network cards, or different IP ports this configuration setting is not required. Additional redundant channels can be opened by right clicking on the redundancy group. 6.8.2 ConfiguringRedundantMasterChannels To create a 104 Master that supports redundancy you follow almost exactly the same procedure as for the Controlled Station. Of course you will select Open, Master Session instead of Server Session. DisconnectOnNewSyn does not apply to TCP Clients(masters). The configuration must be compatible with the Controlled Station configuration as always. Use the corresponding IP addresses and port numbers as well as any other nonstandard configuration. Ordinarily there will only be a single session containing a single sector on the redundant channels. Commands with are sent from the Master will be sent on the active connection (channel) and responses will be received on that same connection. If there is a failure on one of the redundant connections, the Master will detect this and switch the active connection to an available redundant connection. There is also a Test Harness command available in the TCL Command window m104manualswitchover which can be used to force the Master to switch the active connection 6.9 IEC 60870-5Private or CustomASDUSupport Triangle MicroWorks, Inc. Page 109 Communication Protocol Test Harness User Manual The Test Harness implement standard ASDUs documented in the appropriate protocol specification documents. The 60870-5 protocols allow for private or custom ASDUs using other private range type ids. It is possible to send and process private or custom ASDUs in the Test Harness. Sample user plugins are provided that will send custom ASDU requests, see Extending with .NET Plug-Ins. This functionality may be accessed from the TCL command window or in the Command Windows. While these functions allow you to specify exactly what custom request to send, it is also possible for you to modify these plug-ins to provide the specific functionality you require. For example, while the sample plug-ins will send a custom request, they do not process Custom ASDU responses. Similar Custom ASDU processing functionality can be implemented on the Controlled Station side also. 6.9.1 Master Enter m101customASDUcmd ? in the TCL command window, or choose Edit/Plugins on the Test Harness main window and look for IEC 101 Send Custom ASDU. Similar plugins exist for 102, 103, and 104. This can be copied into a command window or executed from the TCL command line. TCL>m101customASDUcmdsector1typeId44vsq1cot5data1,2,3,4,77,88,fe,ff Or by modifying the PluginOptionString in the command window when the m101customASDUcmd command is selected Sector 1 typeId 55 vsq 1 cot 10 data 10,20,30,40,f0,ff It is also possible to register to receive responses that use non standard Type IDs, masterSesn101.ProcessCustomASDUEvent += new M14Session.ProcessCustomASDUDelegate(processCustomASDUEvent); masterSesn101.OpenSession(); // Process Custom ASDUs that the library does not implement // If this method is registered, it will be called for every // application layer response. // This is very rarely required. bool processCustomASDUEvent(M14Session m14Session, M14Sector m14Sector, byte typeId, byte[] rcvData) { if (typeId == 99) { // Process the response. int x = rcvData[2]; // The custom ASDU command that was requested is complete, // remove it from the lbirary queue. if (customASDURequest != null) { customASDURequest.CustomAsduRemove(); Page 110 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual customASDURequest = null; } // Tell the library we have processed this response. return true; } // Tell the library that we have not processed this response. return false; } 6.9.2 ControlledStation It is also possible to write a Controlled Station plug-in to process and respond to Custom ASDU requests from a Master. contrldSesn101.ProcessCustomASDUReqEvent += new S101Session.ProcessCustomASDUReqDelegate(processCustomASDUReqEvent); contrldSesn101.BuildCustomASDURespEvent += new S101Session.BuildCustomASDURespDelegate(buildCustomASDURespEvent); contrldSesn101.OpenSession(); // The following are used for Custom ASDU processing and responses // This is very rarely required. bool responseToSend; bool processCustomASDUReqEvent(S14Session s14Session, S14Sector s14Sector, byte typeId, byte[] rcvData) { if (typeId == 99) { // Add processing of rcvData here. responseToSend = true; return true; } return false; } bool buildCustomASDURespEvent(S14Session s14Session, S14Sector s14Sector, bool buildResponse) { if (responseToSend) { if(buildResponse) { Boolean dataUnitIdInData = false; Byte typeId = 99; Byte vsq = 1; Byte cot = 5; Byte[] dataArray = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 }; s14Sector.SendCustomASDUResponse(dataUnitIdInData, typeId, vsq, cot, dataArray); Triangle MicroWorks, Inc. Page 111 Communication Protocol Test Harness User Manual responseToSend = false; } return true; } return false; } 6.10Periodic Outstation DataChangesusingPlugins The Test Harness provides default User Plug-ins which can be used to cause changes to Outstation (Controlled Station or Server) database points, generating spontaneous events where appropriate. See IEC101MspChange.cs, IEC103DpiChange.cs, IEC104MdpChange.cs, MBCoilChange.cs, DNPBinInChange.cs. and other similarly named files. These plug-ins can be called from either the TCL Command Shell, or the Test Harness Command Windows. Since commands can be made repeatable, this provides an easy way to periodically change values for a list of points in the Outstation (Controlled Station or Server) database. These plug-ins can also be modified if the default behavior does not meet your needs. Here are some example calls of the built-in plug-ins: s101MspChange sector 1 indexes 1,4,5,6 s101MdpChange sector 0 ioas 100 s101MmenaChange ioas 500,501,505 s103DpiChange indexes 1 s103DpiRelChange sector 2 indexes 0,2,4,6 s104MspChange ioas 100,104 s104MstChange indexes 0,1,2,3 sdnpBinInChange session 0 points 0,1,2,3,4,5 sdnpAnlgInChange points 1,5 smbCoilChange points 0,5,10 smbDInputChange session 3 points 1 sdnpBinInChange session 0 points 0,1,2,3,4,5 sdnpAnlgInChange points 1,5 smbCoilChange points 0,5,10 smbDInputChange session 3 points 1 Page 112 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual A good way to run these commands is to open a Test Harness 3 style command window either by selecting the icon before opening a channel, session or sector, or by opening a 102 or 103 command window and then changing the Target to the desired session or sector. In the Command Window setting Target to the desired sector (101 in this example) and set View to show only the commands that apply to a 101 Controlled Station will show the commands you might want Configure. You can then double click or drag a desired Available Command into the Configured Commands pane. If you select the command in the Configured Commands pane you can edit the properties for that plug-in command. For the s101MspChange example shown sector 1 ioas 100,101 was chosen. This will toggle the value of the Single Points with IOAs 100 and 101 on sector 1. If the sector is not specified it will use the sector designated by Target. Do not use when specifying PlugInOptionString. You could also set PlugInOptionString to indexes 0,1,2,3,4 to toggle the values for the first 5 Single Points instead of specifying the IOAs. You can select Execute to change the values 1 time or set Period to the number of milliseconds desired and check the Repeat box for the desired command to cause it to change periodically. You can also use a Test Harness 2 style command window by copying the string s101MspChange sector 1 ioas 100,101 without the quotes into the Tcl/Tk Script field of a Custom Tcl Command. Select the Once button or set the Repeat Interval and select the Start Repeat button to toggle the values periodically. Triangle MicroWorks, Inc. Page 113 Communication Protocol Test Harness User Manual 6.11SimulatingData changes withCSVfiles The Test Harness provides the ability to drive data changes from a CSV (comma- separated values) file. The file is used to populate the point/tag data base for a Outstation or Master component. When using the file on a Outstation device the values in the file are applied to input points for instance DNP Analog Inputs or Binary Inputs. As the values are applied the tags specified in the CSV file will change value and for instance in DNP events can be reported by the Outstation device. On the Master side as values are applied from the CSV file command outputs are issued to a connected Outstation device. Access to the CSV file functionality is from the data window through a context menu on the session. 6.11.1 DNPCSVfileformat This section will describe the format of a DNP specific CSV simulation file. The file is essentially a set of columns and rows. The rows are used to specify individual points for the data base and the columns describe the attributes of these points in the rows. The columns are split into 2 sections Point configuration and simulated data. Here is an illustration of the file structure: The yellow highlighted columns are data base configuration columns. The green highlighted columns are data base simulation value columns. The blue highlighted section is data base simulation values. The purple highlighted section is data base configuration values. For SDNP For MDNP 6.11.1.1 Database configuration header column names The CSV file can be used to define new points for the device database. The following columns are required. Page 114 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Column Name Value DESCRIPTION tag description DNP3Obj point data type DNP3Addr The point number DNP3EventVar The variation for events issued by this point DNP3StaticVar The variation when this point is polled statically or when control is sent from Master DNP3Class The event class this point is reported in 6.11.1.2 Database simulation columns The simulation data columns in the CSV file specify the desired value of the points for a select column name (i.e. SIM1, SIM2, etc). The header value for a simulation column must be unique as it will be used as a key to apply the column data to the points. 6.11.1.3 Point values When applying point values to a Outstation a simple numerical value in the cell will suffice. When apply point values to a Master the outputs of the Master will be set. Any inputs on the Master are not affected. Specifying output values may require more detail than just a simple value. This functionality is supported as follows by specifying a point output specification string for the column value. For DNP Binary outputs: String Format: CTRL:FC:FBP:AUTO Where: CTRL can be LOFF, LON, POFF$time, PON$time, CLOSE, TRIP, 0, 1, on, off Where 0 and off are LOFF 1 and on are LON FC can be DIRECT, SELECT, OPERATE, DIRECT_OP_NOACK, or blank Where if FC is blank DIRECT_OP_NOACK is used FBP is either 'FBP_YES' or 'FBP_NO' AUTO is 'NO_AUTO_OP' or 'AUTO_OP'-send operate after select For DNP Analog outputs: Triangle MicroWorks, Inc. Page 115 Communication Protocol Test Harness User Manual String Format: VALUE:FC:FBP:AUTO VALUE is a number of double type FC can be DIRECT, SELECT, OPERATE, or blank where if FC is blank DIRECT is used and the ':' token is not required FBP is either 'FBP_YES' or 'FBP_NO' AUTO is 'NO_AUTO_OP' or 'AUTO_OP'-send operate after select 6.11.2 UserInterface The CSV file functionality is accessed in the Test Harness by using the Data Window as shown below. The highlighted options in the context menu are used to manipulate the CSV file and the session data base. The following options are available: Generate DNP3 Simulation File Load DNP3 Simulation File Apply DNP3 Simulation File Column Add DNP3 Simulation file Column These options are described in this section below. Page 116 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 6.11.2.1 Generate DNP3 Simulation File This option will create a new CSV file with the current content of the data base. A simulation value column will be added with the name NewCol. The user will be asked if they would like to make the new file active by loading it. 6.11.2.2 Load DNP3 Simulation File This option will bring up a file browser allowing the user to select the simulation file. 6.11.2.3 Apply DNP3 Simulation File Column This option will bring up a list of columns available in the simulation file allowing the user to select one of the columns and apply it to the data base. This will cause outputs or events to be sent as appropriate. 6.11.2.4 Add DNP3 Simulation file Column This option prompts for a new column name and adds that column to the simulation file using the current data point values in the data base. 6.12 File Transfer Both DNP3 and IEC 60870-5-101 and 104 provide file transfer functionality within the protocol. The file transfer mechanisms specified between DNP3 and 101/104 are very Triangle MicroWorks, Inc. Page 117 Communication Protocol Test Harness User Manual different. DNP file transfer follows a familiar Open File, Read Data, Write Data, Close File paradigm. The 101 and 104 protocols specify a very different procedure that will be described below. The Test Harness provides the functionality of each of these protocols while actually reading and writing the data contained in Windows files. 6.12.1 DNPFileTransfer DNP file transfer works in a familiar manner. The Master can send Open File, Close File, Read, Write, Delete File commands to the Outstation to perform file transfer in both directions. There is also File Authentication containing a User Name and clear text Password which is no longer recommended because it is not secure. Responses which are delayed can be sent as events when they are ready. 6.12.2 IEC101and104FileTransfer File transfer for IEC 101 and 104 follows a different paradigm. Refer to IEC 60870-5101Ed 2 for more detail. Each file like other data points is identified by a unique combination of Common Address of ASDU (sector address) and IOA. Each file also has a Name of File which is a 16 bit integer actually indicating whether it is a transparent file or contains disturbance data, sequences of events, sequences of recorded analog value, or indicates reserved or private file types. Files consist of 1 or more sections of up to 64000 octets each with each section transmitted as individual segments with a maximum length of 240 octets each. To read a file the M14 device does not send an Open File request, but rather sends Select File, Call File, Call Section x, Ack Section x, Ack File requests. The S14 device in response to those requests sends File Ready, Section Ready, Segment, Segment, , Last Segment, Last Section responses. The actual data is transferred in the Segment responses which are sent one after the other until the data from the entire section has been sent. To write a file an equivalent but mirrored sequence of requests and responses are exchanged with segments of data being sent in the control direction. The M14 device sends File Ready, Section Ready, Segment, Segment, , Last Segment, , Last Section Requests. The S14 device in response to those requests sends Call File, Call Section, Ack Section, , Ack File responses. There are 8 different TypeId messages defined for File Transfer consisting of FFRNA, FSRNA, FSCNA, FLSNA, FAFNA, FSGNA, FDRTA, and FSCNB. 6.12.2.1 M101 M104 Test Harness Commands The Test Harness provides a set of commands that allow you to test File Transfer. All of the commands are available for both m101 and m104. For simplicity the ones listed here are for m104. The first is a high-level command that provides put/write, get/read, read remote directory, delete remote file, remove local file, and manage local Windows file directory. TCL>m104file ? Page 118 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual usage: m104file [parameter1] [value1] ... supported parameters are: ? : displays help put : put <sector> <ioa> <fileName> -send a file including all sections(from separate Windows files) to the remote device get : get <sector> <ioa> <fileName> -send a request to get a file including all sections from the remote device readdir : readdir <sector> -read the directory of files from the remote device delete : delete <sector> <ioa> <fileName>-send a request to a delete file from the remote device remove : remove <sector> <ioa> <fileName> <description> -remove a file including all sections(in separate Windows files) from the master file directory directory : directory <sector> <dirName> -get or set directory where Windows files are stored by this sector : See Also: m14FileTransfer <sector> to open M101/M104 File Transfer GUI Plugin The following can also be used to display the m14 database in the protocol analyzer window including the local files in the Windows directory as well as the directory of files that has been read from the s14 device TCL>m104showdata There are also low-level commands corresponding to the low level File Transfer Request TypeIds. A sequence of these individual TypeId Requests are used by the high level m104file put, get, readdir, and delete commands. TCL>m104ffrna TCL>m104srna TCL>m104fscna TCL>m104flsna TCL>m104fafna TCL>m104fsgna TCL>m104fscnb There is also a Test Harness Plugin that provides a File Transfer GUI that runs on an open M101 or M104 sector which can be started from the Edit/Plugins Manager Window or from the TCL Command Window. TCL> m14FileTransfer sector x (where x is the sector Id for the m101/m104 sector) Triangle MicroWorks, Inc. Page 119 Communication Protocol Test Harness User Manual 6.12.2.2 S101 S04 Test Harness Commands S101 and S104 devices and therefore in the Test Harness have little control over File Transfer. Files and sections can be added and removed though the following interface, or more likely through any Windows file editor. TCL>s104file usage: s104file [parameter1] [value1] ... supported parameters are: ? : displays help add : add <sector> <ioa> <fileName> <sdata|bdata> -add a new file section(in separate Windows file) to Controlled file directory remove : remove <sector> <ioa> <fileName> -delete an entire file including all sections(in separate Windows files) from Controlled file directory directory : directory <sector> <dirName> -get or set directory where Windows files are stored by this sector The following can also be used to display the s14 database in the protocol analyzer window including the local files in the Windows directory. TCL>s104showdata 6.12.2.3 Test Harness Files in Windows For the Test Harness to access 101/104 File data identified by an IOA and Name of File with 1 or more sectors in Windows Files certain conventions must be followed. Note this convention is not required of a remote Non Test Harness device. The Test Harness looks in the specified directory for Windows files with the following naming convention (spaces or lack of them in the name is required as shown). File system file names must be formatted as follows: "File IOA1800 1 1" where IOA=1800, FileName=1, SectionName=1 "File IOA1800 1 2" where IOA=1800, FileName=1, SectionName=2 Page 120 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual "File IOA1900 4 1" where IOA=1900, FileName=4, SectionName=1 File system directory names (only exist on the s14 device) must be formatted like "File IOA1804 1810" where IOA=1804, FileName=1810, No SectionName "File IOA1805 1820" where IOA=1805, FileName=1820, No SectionName In 101/104 the Name of File in for a Directory is used to provide a mapping to the IOAs of the files in that directory. For the Test Harness the FileName indicates what range of file IOAs are contained in that directory. For example directory File IOA1804 1810 can only contain files of the range IOA 1810-1819. File IOA1805 1820 will contain files with an IOA 1820 or higher with no limit because there are no other directories identified. By default, the two directories the Test Harness defaults to is: C:/Users/Public/Documents/Triangle MicroWorks/Protocol Test Harness/FileTransfer/m14files/ C:/Users/Public/Documents/Triangle MicroWorks/Protocol Test Harness/FileTransfer/s14files/ If you are using multiple sectors, you may want to specify separate directories for the sectors. This can be set by m104file directory dirName c:/temp or as the FileDirectory property in the Data Window or on the Modify Sector Advanced Settings window. 7. ConformanceTestModules 7.1 DNP3OutstationConformance Tests The Test Harness comes with scripts that implement a small subset of the DNP3 IED Conformance tests. A more complete set of DNP3 Conformance test scripts are available from Triangle MicroWorks, Inc. in either binary or source form. For more information contact Triangle MicroWorks at: Triangle MicroWorks, Inc. 2840 Plaza Place, Suite 205 Raleigh, North Carolina, 20716 USA phone: (919)870-5101 web: www.trianglemicroworks.com. All of the procedures for the SDNP conformance tests are implemented in the SDNPTest Tcl namespace as part of the SDNPTest Tcl package. This package is loaded by the default startup script (tmwstart.tcl) using the following lines of Tcl code: # Load SDNP Conformance Test Scripts if they were installed if {![catch {package require SDNPTest} msg]} { namespace import ::SDNPTest::* } The conformance tests can be run interactively with a user interface or in batch mode. The typical scenario would be to run the scripts in interactive mode until the desired configuration is determined. Then the exact same tests can be rerun in batch mode to confirm that the device still passes all required tests. To run the conformance tests interactively execute the following command at the Tcl prompt: Triangle MicroWorks, Inc. Page 121 Communication Protocol Test Harness User Manual TCL> SDNPTest Or more typically Select the Open/Conformance Test/DNP menu item The following dialog box will be displayed: To run the conformance tests in batch mode execute the following command at the Tcl prompt entering a configuration file saved from a previous interactive run: TCL> SDNPRun configfile The results will be written to a file called sdnptest.log and if there are errors a protocol log will be written to dnp.err. By default, these files will be written to a common writable directory such as C:\Users\Public\Documents\Triangle MicroWorks\Protocol Test Harness\logfiles. This can be viewed or changed by editing the configuration and modifying the Log Directory Name if desired. Setting this to empty will cause the files to be written to the old location, the Test Harness install directory. 7.1.1 DNP3SubsetLevels Version 3.0 of DNP3 IED Certification Procedure Subset Levels 1, 2, and 3 changed the way testing is performed. In Certification Procedures before Version 2.8 there were separate Level 1 and Level 2 Certification Procedure documents. These tests have been modified, additional tests have been written and they have combined into a single document providing a single set of tests that can be for level 1, 2 and 3 devices. To test a Page 122 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual device for Level x compliance you should select the Test Level x button on the GUI. You may want to test a particular device as compliant to more than a single level by selecting the appropriate Level button and repeating the tests. Some of the tests verify different behavior for different Level compliance as specified in the Test Procedures. For Level 1 compliance certain object groups, for example frozen counters, are not allowed to be sent in a response to a class poll, but are legal for a Level 2 or 3 device response. By selecting the Select Conformance button only the tests specified in the current supported Certification Procedure document will be selected for execution. By selecting Select All some additional non-standard but useful tests will also be selected. You can choose to select or un-select individual tests using the GUI. It is necessary to configure the Conformance Test tool to exactly match the capabilities of the Device Under Test (DUT) as specified by the Device Profile for that device. The tests will verify the capabilities and points as configured. If the configuration does not match the DUT tests will be improperly performed and/or result in failure indications. These configuration values should be verified and set by selecting File/Edit Configuration. After setting the configuration properly it may be saved File/Save Configuration to a file to allow it to be reloaded File/Load Configuration when retesting this DUT. 7.1.2 SettingTestConditions The DNP3 Outstation Conformance Test Procedures require setting binary, analog and counter input point values to specific values to establish test conditions. In order to establish these conditions, the Conformance Test modules will try the following methods (in the order listed) until one of them returns a successful status: 1. Omicron - Basic control of the Omicron through the Configuration dialog of the Conformance Test Module: The User cannot modify these internal functions, but can control them using the Test Module Configuration Window. If an Omicron is not being used or these examples do not provide sufficient control, the User defined functions described next may be modified to provide desired control. 2. User defined functions - UserSetBinIn, UserSetAnalogIn and UserSetCntr Examples of these functions that contain code to control an Omicron can be found in: .\Examples\Conformance Test Utilities\Omicron.tcl The user may modify these functions to do whatever is necessary to control the input points on the Device Under Test. The provided example controls an Omicron, but the user can modify these to provide further control of an Omicron, control another test device, or send custom commands to the DUT using whatever interface is available for that device. 3. Virtual Terminal Functions - UserVTermSetBinIn UserVTermSetAnlgIn and UserVTermSetCntr Examples of these functions can be found in: Triangle MicroWorks, Inc. Page 123 Communication Protocol Test Harness User Manual .\Examples\Conformance Test Utilities\Virtual Terminal.tcl The examples send command strings compatible with a DNP3 Outstation Simulation running in a Triangle MicroWorks Protocol Test Harness The user may modify these to send commands formatted for their DUT if it supports Virtual Terminal. 4. Connection is set to "Simulated Outstation in Test Harness" - test values will be set automatically This provides a good demonstration of the test procedure requiring no manual input from the user. It can also be used to generate an example protocol analyzer log providing a baseline for comparison with a log from a failed test. 5. Dialog box -a pop-up window indicates the points to be changed and the values they should be set to If none of the above methods indicated they were able to set the points, the tester will be prompted with a pop-up dialog box to set the inputs as directed. 7.2 MESA-DERConformance Tests The Test Harness implements the MESA-DER Phase One Test Procedures Communication Testing and Certification According to the MESA-DER Specification and DNP3 Application Note AN2018-001 as published by the MESA Standards Alliance. These tests use a MESA-DER Protocol Information Conformance Statement (PICS) Spreadsheet to verify an Equipment Under Test (EUT). The EUT being tested provides a PICS indicating the DNP3 Binary and Analog Inputs and Outputs with information about each point that the EUT supports. This PICS includes standard Points with specified DNP Point Indexes, as well as optional Historian Device Points and Device Specific Points. On the Test Harness Main Window select the Blue Ribbon Icon Drop-down menu, select MESA-DER Conformance Tests. This item will only be displayed if you have that feature licensed for your Test Harness. Page 124 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual This will bring up the following Test Framework GUI File Menu The File Drop-down menu allows for Test Configuration to be Modified, Saved and Loaded. It also allows the Test Results and Test Execution Output to be saved to Windows files after the test has been run. Triangle MicroWorks, Inc. Page 125 Communication Protocol Test Harness User Manual Edit Configuration The Edit Configuration option allows you to choose the connection configuration for an Outstation DER EUT (Equipment Under Test). By default, this will be configured to connect using the Loopback IP Address to a Simulated Outstation DER EUT automatically created in the same Test Harness. This will use an EUT PICS that matches the points and functionality provided in the Simulated EUT. If you are testing an actual separate EUT (select Connection Type Serial or TCP/IP) you need to provide a PICS Excel file that matches your actual device functionality. This file is by default loaded from C:\Users\Public\Documents\Triangle MicroWorks\Protocol Test Harness\DERConformance\EUT.xlsx. You should either replace that file with your own PICS or modify the configuration on the Settings Page to load your EUT PICS file from a different location. Page 126 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Select, Connect, Execute Choose specific or All tests to be Selected for Execution from the left-hand pane. Triangle MicroWorks, Inc. Page 127 Communication Protocol Test Harness User Manual The Connect button is used to connect to your actual separate or Simulated EUT. The Execute Button will execute the selected tests. Page 128 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 7.2.1 Output An output log will be displayed in the lower right-hand Execution Output pane showing the individual test functions that are performed along with Successes and Failures marked in Red. A Summary of the Error messages will also be displayed in the upper right-hand Errors pane. After the tests are executed, the successful tests will be marked as Green, the failed tests will be marked as Red. Execute again and only newly selected or failed tests will be run. The Reset State button resets all tests to the not having been run state and resets the current Results information that can later be written to a file. Triangle MicroWorks, Inc. Page 129 Communication Protocol Test Harness User Manual 7.2.2 SavingResults Select the File drop-down menu to Save the Test Results and Test Execution Output to selected Windows files. It can be useful to select individual tests for running or re-running when attempting to resolve an issue. However, for the most complete and accurate Results file, ALL the tests should be Executed sequentially, as specific configuration and functionality which is checked in earlier tests may not be re-verified in later tests. Capturing all the results in sequence gives a better indication of conformance. 7.2.3 TestSections The tests have been organized into 5 separate sections. Section 1 Verifies the EUT PICS data is consistent with the MESA-DER PICS provided with the Test Harness. Verifies the data in the relevant columns of the EUT PICS have not been modified where not allowed. Tests include verifying DNP Point Index, Maximum, Minimum, Multiplier, IEC61850 Unique String, Assoc Point, Mandatory, Purpose, 1547-2018, and EUT Maximum and Minimum columns. Tests will indicate an error if any of these values do not match the MESA-DER Golden PICS. EUT Max and Min if specified must be within the range found in the MESA-DER PICS. Verifies that Vendor Specific Points do not use DNP Point Indexes that are used by Static and Historian Points. Section 2 Verifies the EUT PICS indicates EUT Support x for the points that are considered Mandatory. o Mandatory Column: Yes, For Storage, For Function o 1547-2018 Column: Yes BI31-51 will be read to determine what Functions are supported by the device. Whether or notFor Storage is mandatory will be determined by Configuration on the Key Page. Page 130 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Tests in this section do NOT read the device in this test to determine that the points indicated as Supported in the PICS actually exist on the device. Section 3 Sends Read Requests to the input and output points listed as Supported in the EUT PICS to verify they are accessible on the EUT. Sends Control Requests to the output points listed as Supported and reads back the associated input point if there is one verifying that the written and read values match. o For Binary Outputs, the test sends Latch On and Latch Off. o For Analog Outputs, the test sends Min and Max specified in the EUT PICS, as well as two other values within that range. o Section 4 Writes 10 Curves using data from a spreadsheet provided by the Test Harness. Limits the number of X-Y Points to the EUT Maximum. Reads the same 10 Curves back and verifies the values match what was written. Section 5 Writes 10 Schedules using data from a spreadsheet provided by the Test Harness. Limits the number of X-Y Points to the EUT Maximum. Reads the same 10 Schedules back and verifies the values match what was written. 7.2.4 TestResultsFile If all tests are selected and run against the Default Simulated Outstation DER EUT with NO Errors the MESA-DER Conformance Results file will resemble the following: ResultsofrunningMESA-DERConformance Tests 1.1BinaryInputPICSVerification PASSED 1.2BinaryOutputPICSVerification PASSED 1.3AnalogInputPICSVerification PASSED 1.4AnalogOutputPICSVerification PASSED 2.1BinaryInputVerificationofMandatoryPoints PASSED 2.2BinaryOutputVerificationofMandatoryPoints PASSED 2.3AnalogInputVerificationofMandatoryPoints PASSED 2.4AnalogOutputVerificationofMandatoryPoints PASSED 3.1BinaryInputPointReadVerification PASSED 3.1.1BinaryInputSchedule PointReadVerification PASSED 3.1.2BinaryInputHistorianPointVerification PASSED 3.1.3BinaryInputVendorSpecificPointReadVerification PASSED 3.2BinaryOutputReadVerification PASSED 3.2.1BinaryOutputSchedule PointReadVerification PASSED 3.2.2BinaryOutputVendorSpecificPointReadVerification PASSED 3.2.3BinaryOutputControlVerification PASSED 3.2.4BinaryOutputSchedule ControlVerification PASSED 3.3AnalogInputPointReadVerification PASSED 3.3.1AnalogInputCurve PointReadVerification PASSED 3.3.2AnalogInputMeterPointReadVerification PASSED Triangle MicroWorks, Inc. Page 131 Communication Protocol Test Harness User Manual 3.3.3AnalogInputSchedule PointReadVerification PASSED 3.3.4AnalogInputHistorianPointVerification PASSED 3.3.5AnalogInputVendorSpecificPointReadVerification PASSED 3.4AnalogOutputReadVerification PASSED 3.4.1AnalogOutputCurve PointReadVerification PASSED 3.4.2AnalogOutputMeterPointReadVerification PASSED 3.4.3AnalogOutputSchedule PointReadVerification PASSED 3.4.4AnalogOutputHistorianPointVerification PASSED 3.4.5AnalogOutputVendorSpecificPointVerification PASSED 3.4.6AnalogOutputControlVerification PASSED 3.4.7AnalogOutputMeterControlVerification PASSED 3.4.8AnalogOutputHistorianPointControlVerification PASSED 3.4.9AnalogOutputVendorSpecificControlVerification PASSED 4.1Curve Verification PASSED 5.1Schedule Verification PASSED NumberoftestsSelected35 NumberoftestsPassed 35 NumberoftestsFailed 0 Errorsdetectedduringtesting: NOTE:Alltestsshouldbe runforthiserrorlisttobe complete Comparisonofthe EUTPICStothe MESA-DERPICS 0Errorswere detectedforComparisonofthe EUTPICStotheMESA-DERPICS Verificationofthe MandatoryPoints 0Errorswere detectedforVerificationofthe MandatoryPoints VerificationoftheMandatory"ForFunction"Points VoltRide-ThroughisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired FreqRide-ThroughisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired DynReactCurrSuppisSupportedaccordingtoEUT 16pointswere foundforthisfunctionandverified DynVolt-WattisSupportedaccordingtoEUT 10pointswere foundforthisfunctionandverified Freq-WattisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired ActPowerLimitisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired SetActPowerisSupportedaccordingtoEUT Page 132 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual Nopointswere foundforthisfunction,therefore NOverificationwasrequired CoordCharge-DischgisSupportedaccordingtoEUT Nopointswere foundforthisfunction,thereforeNOverificationwasrequired ActPowerFollow#1isSupportedaccordingtoEUT 8pointswere foundforthisfunctionandverified ActPowerFollow#2isSupportedaccordingtoEUT 8pointswere foundforthisfunctionandverified ActPowerFollow#3isSupportedaccordingtoEUT 8pointswere foundforthisfunctionandverified AGCisSupportedaccordingtoEUT 11pointswere foundforthisfunctionandverified ActPowerSmoothisSupportedaccordingtoEUT 10pointswere foundforthisfunctionandverified Volt-WattisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired Freq-WattCurve isSupportedaccordingtoEUT 13pointswere foundforthisfunctionandverified ConstVarsisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired FixedPFisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired Volt-VarisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired Watt-VarisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired PFCorrectisSupportedaccordingtoEUT Nopointswere foundforthisfunction,therefore NOverificationwasrequired Price isSupportedaccordingtoEUT 4pointswerefoundforthisfunctionandverified 0Errorswere detectedforVerificationofthe Mandatory"ForFunction"Points Verificationofthe Mandatory"ForStorage"Points 0Errorswere detectedforVerificationofthe Mandatory"ForStorage"Points Triangle MicroWorks, Inc. Page 133 Communication Protocol Test Harness User Manual Verificationofthe Required1547-2018Points 0ErrorsweredetectedforVerificationofthe Required1547-2018Points Verificationofthe Non-MandatorySupportedPointsontheEUT 0Errorswere detectedforVerificationofthe Non-MandatorySupportedPointsonthe EUT VerificationofCurvesonthe EUT 0Errorswere detectedforVerificationofCurvesonthe EUT VerificationofSchedulesonthe EUT 0Errorswere detectedforVerificationofSchedulesonthe EUT 7.2.5 TestingyourOutstationDEREUT. File Edit Configuration o Connection information for your EUT Settings o Select your EUT PICS file, o Or replace the example EUT.xlsx provided with the Test Harness Save Configuration (for later reloading) Then Select, Connect and Execute to test your EUT. Save Test Results Save Test Execution Output Page 134 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 8. Troubleshooting This section will describe various problems that could cause things not to work and how to correct them. 8.1 NoLicense Found If the Test Harness displays the line No license found. Please contact Triangle MicroWorks, Inc. to obtain a valid license. but a license should be installed, use the Request Activation Key button to send licensing information to Triangle MicroWorks, Inc. Fill out the form, and in the notes section, note that you are sending this request because your Test Harness is showing a No license found error, but you believe it should be licensed. 8.2 DLLNotFound If "Error Starting Program" dialog box, with the message "A required .DLL file, Tcl84.DLL, was not found." Is displayed, then the path to the Tcl installation has not been installed correctly. Modify your path to include the Tcl binary and library directories. By default, these directories are: C:\Program Files/Triangle MicroWorks\Protocol Test Harness\bin C:\Program Files/Triangle MicroWorks\Protocol Test Harness\lib. 8.3 HelpFormat error If you receive the following error when attempting to open the Tcl Help and Tcl/Tk Help, you should install WinHlp32.exe from the Microsoft Website. The Help for this program was created in Windows Help format, which depends on a feature that isn't included in this version of Windows. 8.4 No communication with aremotedevice seems to be occurring 8.4.1 Physicallayererror:Erroropeningchannel The message "physical layer error: Error opening channel" indicates that the device is unable to open the specified channel and port. The cause of the message depends on whether the channel is a serial or TCP/IP channel. . Turning on the Target Layer Protocol Analyzer filter will display useful information about the connection attempts. 8.4.1.1 TCP/IP Connection With TCP/IP, you will normally get this message until the corresponding device also opens the port. It is not unusual to see several of these messages while the other device starts up and opens the port. Triangle MicroWorks, Inc. Page 135 Communication Protocol Test Harness User Manual If you are still getting this message after your corresponding device attempts to open the port, then check the configuration of both the Outstation and Master device. The typical settings that can cause this problem are: Host: the Master (client) must specify the TCP/IP address of the Outstation (Server) device. The Outstation (Controlled Station or Server) must either specify the TCP/IP address of the client or use an address of "*.*.*.*". (The *.*.*.* address instructs the server to accept a connection from any client.). The Server can also use a list of semicolon or comma separated list of hosts or addresses it is willing to accept connections from. port: both the Master and Outstation must use the same port; for DNP3, you would normally use port 20000, for IEC 2404, and for modbus 502. mode: typically, the Outstation (Controlled Station) should be a Server and the Master should be a Client. 8.4.1.2 Serial Connection With serial connections, this message usually indicates that the port cannot be opened. If the specified port is installed in the computer, then this is error is usually caused by another program using the serial port, preventing it from being opened. PDA hotsync programs are frequently the culprit. 8.4.2 Configuration Verify the configuration of both the Protocol Test Harness and your remote device: For a serial connection, verify the baud rate, parity, and number of stop bits are configured the same for both devices. For a TCP connection, verify the TCP and port address, as well as the client/server settings. For either type of connection, verify that the link address and common address of ASDU (sector address) match the addresses set in the remote device. Check the return status of the <scl>openchannel (for example, sdnpopenchannel) command to verify that the port opens. 8.4.3 Otherissues Verify that the desired protocol is licensed. Verify the physical connection. Turn on the Physical Layer in the Protocol Analyzer window and verify that bytes are sent and received and that the Physical layer error: Error opening channel message is not being issued. Page 136 Triangle MicroWorks, Inc. Communication Protocol Test Harness User Manual 8.5 IEC 60870-5-104SessionsConnect,but then disconnect immediately Verify the settings of the following parameters: T0 - Time-out of connection establishment - default value = 30 s T1 - Time-out of send or test APDUs - default value = 15 s T2 - Time-out for acknowledges in case of no data messages t2 < t1 - default value = 10 s T3 - Time-out for sending test frames in case of a long idle state - default value = 20 s k - Maximum difference receive sequence number to send state variable - default value = 12 APDUs w - Latest acknowledge after receiving w I format - default value = 8 APDUs These parameters are also described in more detail in clause 5.1 of IEC 60870-5-104. 8.6 Using anRS-232/RS-485Converter with theTest Harness The Communication Protocol Test Harness is compatible with any RS-232/RS-485 converter using Send Data (SD) control. If you experience communication problems when using the Communication Protocol Test Harness with an RS-232/RS-485 converter, make sure that your converter is configured for Send Data control. Triangle MicroWorks has tested the Test Harness using the 485SD9TB Port Powered RS-232 to RS-485 Converter from B&B Electronics: http://www.bbelec. com/product.asp?sku=485SD9TB&dept_id=171 8.7 IssuingControlCommands in IEC60870-5Protocols In IEC 60870-5-101 and -104, Monitor Single Points (Type T1) are read-only. Similarly, Single Point Commands (Type T45) are write-only. In the default Controlled Station Test Harness configuration, command points are addressed by adding 2000 to the corresponding monitor point. Thus, to write to the point monitored by Single Point with IOA 100, write to the Single Point Command Point with IOA 2100. The offset for control points can be changed via the s101db offset or s104db offset commands. 8.8 SimultaneouslySimulatingMaster andOutstation Devices The Test Harness can simultaneously simulate both Master and Outstation (Controlled Station or Server) devices on a single computer. Both devices can be run from within a single instance of the Test Harness, or run two (or more) instances of the Test Harness can be used (e.g., running the Master on one instance and the Outstation on the other). However, when configured for TCP/IP on a single computer, the TCP/IP data never leaves the PC. This is true whether you are using the TCP/IP loopback address Triangle MicroWorks, Inc. Page 137 Communication Protocol Test Harness User Manual (127.0.0.1) or two Ethernet cards in the PC -- the TCP/IP routing is smart enough to realize that the message is being sent to the same PC, so it routes it internally instead of to the physical port. Thus, if you want to monitor the TCP/IP traffic, you need two PCs, with two Test Harness licenses (one for each PC). Page 138 Triangle MicroWorks, Inc. 

***********SDG Implementers Guide**********
 SCADA Data Gateway Implementation Guide Software Release Version 5.2 Manual Revised: June 2023 Property of Triangle MicroWorks, Inc. This Documentation and its associated distribution media contain proprietary information of Triangle MicroWorks, Inc. and may not be copied or distributed in any form without the written permission of Triangle MicroWorks, Inc. Copies of the source code may only be made for backup purposes. READ THIS BEFORE INSTALLING THE SOFTWARE AND BEFORE ACCEPTING THE LICENSE AGREEMENT THIS SOFTWARE LICENSE AGREEMENT IS BEING PROVIDED TO YOU AS PART OF A TRIAL FOR REVIEW BEFORE PURCHASE. THE FOLLOWING LICENSE AGREEMENT IS A CLICKTHROUGH LICENSE AGREEMENT THAT IS SHOWN DURING THE INSTALLATION OF THE SOFTWARE. YOU MUST AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT IN ORDER TO INSTALL THE SOFTWARE. THEREFORE, IF YOU DO NOT AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT, DO NOT INSTALL THE SOFTWARE. TRIANGLE MICROWORKS, INC. License Agreement and Warranty THIS IS A LEGAL AGREEMENT BETWEEN YOU (YOU) AND TRIANGLE MICROWORKS, INC. (TRIANGLE MICROWORKS). TRIANGLE MICROWORKS IS WILLING TO LICENSE THE TRIANGLE MICROWORKS SOFTWARE AND RELATED DOCUMENTATION PURCHASED BY YOU AND KNOWN AS SCADA DATA GATEWAY (THE SOFTWARE) TO YOU ONLY UPON THE CONDITION THAT YOU ACCEPT ALL OF THE TERMS CONTAINED IN THIS LICENSE AGREEMENT (THE "AGREEMENT"). PLEASE READ THE TERMS AND CONDITIONS CAREFULLY BEFORE CLICKING ON THE I AGREE BUTTON, AS CLICKING ON THIS BUTTON WILL INDICATE YOUR ASSENT TO THEM. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, THEN CLICK ON THE I DO NOT AGREE BUTTON. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, TRIANGLE MICROWORKS IS UNWILLING TO LICENSE THE SOFTWARE TO YOU. 1. Subject to the following terms and conditions, Triangle MicroWorks grants to you a nonexclusive, nontransferable limited license to install and use the Software solely for your internal business purposes and subject to the limitations in this Agreement. There are three mechanisms to license the Software. The Seat License is a PC based software license that is installed on a single computer and is solely for the use of the Software by you on a single computer at any one time. The Hardware Dongle License is a license that is stored on a dedicated USB hardware device that can be shared between computers by physically moving the Hardware Dongle License. The Hardware Dongle License is solely for the use of the Software by you on a single computer at any one time. The Network License is a license that is stored on a computer that can be shared across a local area network. The Network License is solely for the use of the Software by you on a single computer at any one time. The Network License may be installed on a corporate network and shared locally. The Network License cannot be shared outside of your company. You may physically transfer or copy the Software on to another computer; provided, however that each license instance may not be used on more than one computer or device at any given time. Subject to the foregoing limitation, you are entitled to copy the Software into any machine-readable or printed form for backup purposes in support of the use of the Software on a single computer by you. Any copy of the Software permitted by the preceding provision shall include a copyright notice of Triangle MicroWorks. 2. You may use the Software solely for the applicable term (i.e., period of time) as set forth below. If you have licensed the Software for a free trial period (a Trial License), the term of the Software license will be for 21 days. You understand and acknowledge that after the 21-day period and unless otherwise extended by Triangle MicroWorks, in its sole discretion, the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software. At the conclusion of the Trial License, you may purchase a Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have licensed the Software for a fixed period of time and not on a trial basis (a Term License), the term of the Software license will be the period of time agreed to by you and Triangle MicroWorks. At the end of that period of time, your use of the Software will terminate, and the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software except as explicitly set forth herein. At the conclusion of the Term License, you may purchase an additional Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have paid a one-time fee for a perpetual license, then your term of the Software License is perpetual (a Perpetual License) and subject to termination only as set forth herein. 3. You may not transfer the Software to another entity or person. You shall not modify, translate, reverse assemble or reverse compile in whole or in part the Software. You shall not rent, sell, time-share, lease, sublicense, transfer, copy, disclose, display or otherwise make available the Software or copies thereof to others, except as provided herein. Any breach of the above terms and conditions shall immediately terminate your license under this Agreement. You hereby certify and agree that the Software will not be shipped, transferred, or re-exported, directly or indirectly, into any country prohibited by the United States Export Administration Act and the regulations thereunder, and the Software will not be used for any purpose prohibited by the same. 4. You may not erase, delete or modify any copyright, trademark or other proprietary notices included on the Software. You agree that if you infringe Triangle MicroWorks intellectual property rights or exceed the scope of permitted use of this license, Triangle MicroWorks will be irreparably injured and may obtain a court order to enjoin you from further use of the Software. 5. This license is not a sale. Title, ownership rights, and intellectual property rights in and to the Software shall remain in Triangle MicroWorks. You agree to abide by the copyright laws and all other applicable laws of the United States and international treaties. You acknowledge that the Software in source code form remains a confidential trade secret of Triangle MicroWorks. Failure to comply with the above restrictions will result in automatic termination of this Agreement and will make available to Triangle MicroWorks other legal remedies. 6. Notwithstanding anything herein to the contrary, you acknowledge and agree that Triangle MicroWorks may terminate your license to the Software if you have failed to comply with any of your obligations under this Agreement. You shall not be entitled to a refund in the event of termination by Triangle MicroWorks pursuant the foregoing sentence. 7. By purchasing a Term License or a Perpetual License to the Software you are automatically subscribed in the Software Maintenance & Enhancement Plan (the Plan) for a period of one (1) year following the date of purchase (the Purchase Date) of the license, or if shorter, the term of your Term Licensee (such initial period referred to as the Initial Maintenance Period). Upon expiration of the Initial Maintenance Period, the Plan may be renewed on an annual basis subject to payment by you of the applicable then-current annual fee. This Plan may provide a cost effective method for upgrading to the most recent release version of the Software. The Plan also provides technical support by telephone, fax, or email. The Initial Maintenance Period for Perpetual Licenses is free. The annual fee for the Plan for Perpetual Licenses after the first year is 25% of the Perpetual License purchase price; provided, however that Triangle MicroWorks, in its sole discretion, may adjust the annual fee at any time. The annual fee for the Plan for Term Licenses depends on the specific terms of the Term License. Although Triangle MicroWorks has no obligation to provide upgrades to the Software under the Plan, Triangle MicroWorks may from time to time provide upgrades. Any upgrades to the Software shall be deemed to become part of the Software and shall be subject to the terms and conditions of this Agreement. All Plan annual fees are nonrefundable. 8. For Term Licenses and Perpetual Licenses only, Triangle MicroWorks warrants that the Software shall perform substantially as described in its documentation for a period of sixty (60) days from the Purchase Date. In the event of a breach of the foregoing warranty, the sole obligation of Triangle MicroWorks and your sole remedy is that Triangle MicroWorks will provide you with a refund of the most recent license fee paid for the Software. Triangle MicroWorks, as a condition to providing such refund, may require that you sign an affidavit certifying that you have deleted or destroyed all copies of the Software in your possession. Triangle MicroWorks makes no warranties with respect to Trial Licenses. Except as set forth in this Section 8, THE SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTIES OF ANY TYPE, AND TRIANGLE MICROWORKS HEREBY EXPRESSLY DISCLAIMS ALL WARRANTIES, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY, TITLE, NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE. 9. TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL TRIANGLE MICROWORKS BE LIABLE FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL OR PUNITIVE DAMAGES WHATSOEVER, INCLUDING ANY CLAIMS FOR LOST PROFITS, BUSINESS INTERRUPTION, LOST INFORMATION OR OTHER DAMAGES ARISING OUT OF THE USE OF INABILITY TO USE THE SOFTWARE. IN NO EVENT SHALL TRIANGLE MICROWORKS HAVE ANY LIABILITY FOR ANY DATA STORED OR PROCESSED WITH THIS SOFTWARE, INCLUDING THE COST OF RECOVERING ANY LOST DATA. IN ALL EVENTS, TRIANGLE MICROWORKS SOLE OBLIGATION OR LIABILITY UNDER THIS AGREEMENT IN THE AGGREGATE IS A REFUND OF THE MOST RECENT LICENSE FEE PAID FOR THE SOFTWARE. THE PARTIES AGREE THAT THESE LIMITATIONS ARE AN ESSENTIAL PART OF THE AGREEMENT AND THAT TRIANGLE MICROWORKS WOULD NOT ENTER INTO THE AGREEMENT WTIHOUT THESE LIMITATIONS. THIS LIMITATION OF LIABILITY CLAUSE WILL SURVIVE FAILURE OF ITS ESSENTIAL PURPOSE. 10. You may terminate the license granted under this Agreement at any time by destroying or deleting all copies of the Software in your possession or control. In the event of termination of this Agreement for any reason, all the sections of this Agreement will survive except for Sections 1 and 2. 11. This Agreement shall be governed by and construed in accordance with the laws of the State of North Carolina, without regard to the choice of law provisions thereof. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to this Agreement. Any dispute or claim arising out of, or in connection with, this Agreement shall be finally settled by binding arbitration in Raleigh, North Carolina, in accordance with N.C. Gen. Stat. 1-569.1 et seq. (the Uniform Arbitration Act) and the then-current rules and procedures of the American Arbitration Association by one (1) arbitrator appointed by the American Arbitration Association. The arbitrator shall apply the law of the State of North Carolina, without reference to rules of conflict of law or statutory rules of arbitration, to the merits of any dispute or claim. Judgment on the award rendered by the arbitrator may be entered in any court of competent jurisdiction. The parties agree that, any provision of applicable law notwithstanding, they will not request, and the arbitrator shall have no authority to award, punitive or exemplary damages against any party. In the event that any arbitration, action or proceeding is brought in connection with this Agreement, the prevailing party shall be entitled to recover its costs and reasonable attorneys' fees. In the event that the above arbitration provision is held invalid or unenforceable, any dispute with respect to this Agreement shall be brought and heard either in the North Carolina state courts located in Raleigh, North Carolina or the federal district court, located in Raleigh, North Carolina. In such event, the parties to this Agreement each consent to the in personam jurisdiction and venue of such courts. The parties agree that service of process upon them in any such action may be made if delivered in person, by courier service, by telegram, by telefacsimile or by first class mail, and shall be deemed effectively given upon receipt. 12. This Agreement sets forth the entire agreement between you and Triangle MicroWorks pertaining to the licensing of the Software and supersedes in its entirety any and all written or oral agreements previously existing between the parties with respect to such subject matter. If any provision of this Agreement is held invalid or unenforceable, such provision shall be revised to the extent necessary to cure the invalidity or unenforceability, and the remainder of this Agreement shall continue in full force and effect. In the event of any conflict between any provision of this Agreement and any applicable law, the provision or provisions of this Agreement affected shall be modified to remove such conflict and permit compliance with such law and as so modified this Agreement shall continue in full force and effect. This Agreement may not be assigned without the consent of Triangle MicroWorks. Triangle MicroWorks, Inc. 2840 Plaza Place, Suite 205, Raleigh, North Carolina 27612 USA (919) 870-5101 How to Obtain Additional Assistance Technical Support: http://www.trianglemicroworks.com/SDGSupportRequest.aspx SDG Quick Start Video: http://www.trianglemicroworks.com/Videos/SDG Quick Start Video/player.aspx Additional References: SDG Quick Start Guide.pdf SDG 61850 Quick Start Guide.pdf SDG Whats New (located on the Triangle MicroWorks website) Office: Triangle MicroWorks, Inc 2840 Plaza Place Suite 205 Raleigh, North Carolina 27612 Phone: 919.870.5101 Fax: 919.870.6692 Email: sales@trianglemicroworks.com Website: http://www.trianglemicroworks.com Implementers Guide Contents 1.Overview ....................................................................................................................................................... 15 1.1Purpose of this Manual ......................................................................................................................... 15 1.2Introduction to SDG ............................................................................................................................. 15 1.3SDG Components ................................................................................................................................. 17 1.3.1 ................................................................................... 17 Master/Client Components of the SDG 1.3.2 .................................................................................... 17 Slave/Server Components of the SDG 1.3.3 ........................................................................................................ 18 Master / Slave Data Flow 1.3.3.1Sequence from remote Slave devices to Master Component data collection ............. 18 1.3.3.2Command Sequence from Remote Master Devices ................................................... 18 1.4Data Point Addressing .......................................................................................................................... 19 1.4.1 ....................................................................................................... 20 Communication Channels 1.4.2 ................................................................................................................................... 21 Sessions 1.4.3 ..................................................................................................................................... 22 Sectors 1.4.4 ................................................................................................. 22 External OPC Classic Servers 1.4.5 ....................................................................................................... 23 External OPC UA Servers 2.Installation and Setup .................................................................................................................................. 24 2.1Linux System Requirements ................................................................................................................ 24 2.2Windows System Requirements ........................................................................................................... 24 2.3SDG Performance ................................................................................................................................. 24 2.4Installing the SCADA Data Gateway ................................................................................................... 25 2.4.1 .................................................................................................................................. 25 Windows 2.4.1.1Windows Migration Tool ........................................................................................... 26 2.4.2 ....................................................................................................................................... 27 Linux 2.5SCADA Data Gateway Licensing ........................................................................................................ 27 2.6Running the SDG as a Service on Windows (windows only) .............................................................. 28 2.6.1 .............................................................................................. 29 Installation, Removal and Setup 2.6.2 ....................................................................................... 29 Service Events and Status Messages 2.7SDG Web Application Runtime Architecture ...................................................................................... 30 2.7.1 ................................................................................. 30 SDG Web Application System Services 2.7.1.1GTWWebMonitor Services A ................................................................................. 31 2.7.1.2GTWEngine Services B .......................................................................................... 31 2.7.1.3Web Client Services C ............................................................................................ 32 2.7.2 ........................................................................................................... 32 SDG Web Architecture 2.7.3 ..................................................... 33 Determine Engine Health and State ***added manually 3.User Interface ............................................................................................................................................... 34 3.1Accessing the Web UI .......................................................................................................................... 34 3.2Dashboard Management ....................................................................................................................... 34 3.2.1 ................................................................................................................... 35 Default Dashboard 3.2.2 .................................................................................................................... 36 Device Tree View 3.2.3 ................................................................................................................................. 37 Tags Grid 3.2.4 .................................................................................................................................. 39 Mapping 3.2.4.1Single mapping ........................................................................................................... 39 3.2.4.2Multi-mapping ............................................................................................................ 39 3.2.5 ....................................................................................................................................... 40 Editor 3.2.6 ............................................................................................................ 40 Refreshing the Display 3.3Logging ................................................................................................................................................ 40 3.3.1 ................................................................................................................. 41 Filters and Controls 3.3.2 ............................................................................................................. 42 Accessing Log Output 3.3.3 ............................................................................................................. 42 Logging storage limits 3.4Gateway System Configuration ............................................................................................................ 43 3.4.1 ..........................................................................................................44 Workspace Management 3.4.2 ................................................................................................................44 System Management 3.4.3 .................................................................................................................44 Web Server Settings 3.4.4 ................................................................................................................................44 Pagination 3.4.5 ..................................................................................................44 Gateway WebSocket Settings 3.4.6 .................................................................................................................44 Security Parameters 3.4.7 ........................................................................................................................45 Log Parameters 3.5Gtw_config.json file .............................................................................................................................45 3.6Gtw_limits.json file ...............................................................................................................................46 3.7Context Help .........................................................................................................................................47 User Authentication ........................................................................................................................................48 3.7.1 ............................................................................................................................48 Enable Users 3.7.2 .................................................................................................................................50 Edit Users 3.7.3 ........................................................................................................................50 Add New Users 3.7.4 ................................................................................................................................51 User Roles 3.7.5 ..................................................................................................52 User Password Requirements 3.8Audit Logging .......................................................................................................................................52 3.8.1 ...................................................................................................................................52 Auditing 3.9Help Menu ............................................................................................................................................53 3.9.1 ..........................................................................................................................................54 Help 3.9.2 ...............................................................................................................................54 Quick Start 3.9.3 ...................................................................................................................54 Quick Start Pop-up 3.9.4 ...........................................................................................................54 INI File Parameter Help 3.9.5 ..........................................................................................................54 Submit Support Request 3.9.6 .....................................................................................................................................54 License 3.9.7 ..........................................................................................................................54 Request Quote 3.9.8 ......................................................................................................................54 Check for update 3.9.9 .......................................................................................................................................54 About 4.Configuration Reference ..............................................................................................................................55 4.1Tmwgtway.ini Initialization File ..........................................................................................................55 4.1.1 .......................................................................................................55 Tmwgtway.ini Parameters 4.1.2 .............................................................................................................56 Hidden INI parameters 4.1.3 .........................................................................................................57 View All INI Parameters 4.2Point-Mapping File ...............................................................................................................................58 4.2.1 ...................................................................................................59 Saving the INI and CSV files 4.3Tags in the SDG ....................................................................................................................................60 4.3.1 ................................................60 Creating Tags Automatically for DNP and 60870-5 protocols 4.4Master Data Objects (MDOs) ...............................................................................................................61 4.5Slave Data Objects (SDOs) ...................................................................................................................62 4.5.1 .................................................................................................................63 Editing Tag Options 4.6Internal Predefined MDO Points ..........................................................................................................64 4.7Internal User defined MDOs .................................................................................................................64 4.8Equations ..............................................................................................................................................68 4.9Adding Channels ...................................................................................................................................70 4.9.1 ...................................................................................................70 Add TCP/IP Channel Dialog 4.9.2 ............................................................................................................................71 TLS Security 4.9.3 ............................................................72 Redundant Master/Slave 104 Channels/Configuration 4.9.4 ..........................................................................................................................76 Serial Channel 4.9.5 ...............................................................................................77 Add UDP/TCP Channel Dialog 4.10Adding Sessions ....................................................................................................................................79 4.10.1 ........................................................................................................................80 Master Sessions 4.10.1.1IEC 60870-5-101 ........................................................................................................80 4.10.1.2IEC 60870-5-103 ........................................................................................................ 82 4.10.1.3IEC 60870-5-104 ........................................................................................................ 84 4.10.1.4Master DNP3 .............................................................................................................. 86 4.10.1.5Modbus Master ........................................................................................................... 89 4.10.2 ......................................................................................................................... 90 Slave Sessions 4.10.3 ..................................................................................................................... 90 IEC 60870-5-101 4.10.4 ..................................................................................................................... 91 IEC 60870-5-104 4.10.5 ............................................................................................................................... 92 Slave DNP 4.10.6 .......................................................................................................................... 93 Modbus Slave 4.11Adding Sectors ..................................................................................................................................... 94 4.12Adding Data Types ............................................................................................................................... 97 4.13Action Masks ........................................................................................................................................ 97 4.13.1 ..................................................................................................................... 97 IEC Action Mask 4.13.2 ................................................................................................................... 98 DNP Action Mask 4.13.3 ................................................................................................... 100 Modbus Read Action Mask 4.13.4 .................................................................................................. 102 Modbus Write Action Mask 4.13.5 ............................................................................................... 102 Modbus Master Dual Register 4.13.6 ................................................................................................. 103 Modbus Slave Dual Register 4.14Multi Drop Polling strategies for DNP and Modbus .......................................................................... 104 4.14.1 ............................................................................................................. 105 Channel ActionMask 4.14.2 ................................................................................ 105 Prioritization of Unsolicited Responses 4.14.2.1Example 1. ................................................................................................................ 106 4.14.2.2Example 2. ................................................................................................................ 106 4.14.3 ............................................................................................ 106 Disabling Channels and Session 4.14.4 .......................................................... 106 Supporting INI Configuration Parameters and MDOs 4.14.4.1DNPChannelResponseTimeout ................................................................................ 106 4.14.4.2ApplDNPAbsRespTimeout ...................................................................................... 107 4.14.4.3PhysOfflinePollPeriod .............................................................................................. 107 4.14.4.4MDNPunsolHoldOffTime ........................................................................................ 107 4.14.4.5ChannelActiveControl - MDO ................................................................................. 107 4.14.4.6SessionActiveControl - MDO .................................................................................. 107 4.15IEC 61850 Client ................................................................................................................................ 108 4.15.1 ........................................................................ 113 Adding an IEC 61850 Report Control Block 4.15.2 .................................. 117 Adding an IEC 61850 Data Attribute from the Report Control Block 4.15.3 ..................................................................................... 119 Adding an IEC 61850 Control Point 4.15.3.1Changing an IEC 61850 Command Point ................................................................ 121 4.15.4 ........................................................ 121 Other mechanisms to obtain Data from a 61850 Server 4.15.4.1GOOSE Control Blocks ........................................................................................... 122 4.15.4.2Polled Data Sets ....................................................................................................... 122 4.15.4.3Polled Point Sets ....................................................................................................... 122 4.15.5 61400-25 specification) IEC 61400-25 Alarms (Senvion/Re Power Turbine alarms, proposed addition to the IEC ......................................................................................................... 122 4.15.5.1Configuring the SDG to use IEC 61400-25 Alarms ................................................. 123 4.15.5.2Adding the IEC 61400-25 Alarms Node .................................................................. 124 4.15.5.3Adding the IEC 61400-25 Alarms ............................................................................ 125 4.15.5.4Alarm Processing ..................................................................................................... 126 4.16IEC 61850 Server ............................................................................................................................... 127 4.16.1 .............................................................................. 129 Mapping Points to the IEC 61850 Server 4.1760870-6 ICCP (Tase.2) Client and/or Server ..................................................................................... 132 4.17.1 ........................................................................................... 137 Editing/Deleting a Client/Server 4.17.2 .......................................................................................................... 138 Connecting to a Server 4.17.3 ...............................................................................................138 Creating/Editing Client Model 4.17.4 ..................................................................................141 Adding an ICCP Data Set Transfer Set 4.17.5 ..........................................................................................143 Editing an ICCP DS Transfer Set 4.17.6 ....................................................144 Adding an ICCP Data Attribute from the DS Transfer Set 4.17.7 ..............................................................................................146 Adding an ICCP Control Point 4.17.7.1Operating a ICCP Control Point ...............................................................................149 4.17.8 ...................................................................................149 Mapping MDOs into the ICCP Client 4.17.9 .......................................................150 Other mechanisms to obtain Data from an ICCP Server 4.17.9.1Polled Data Sets ........................................................................................................150 4.17.9.2Polled Point Sets .......................................................................................................150 4.18ICCP/Gateway Quality .......................................................................................................................150 4.19Standalone ICCP/ICCP Server ...........................................................................................................151 4.19.1 ....................................................152 Mapping Points from a Master device to the ICCP Server 4.20OPC Alarm and Event Client (windows only) ....................................................................................154 4.20.1 ................................................................................................157 Adding OPC AE Client Items 4.20.2 .................................................................................................158 OPC Alarm and Event MDO 4.20.2.1Event MDO ...............................................................................................................158 4.20.2.2Event Attribute MDOs ..............................................................................................158 4.20.2.3OPC AE MDO Options ............................................................................................160 4.21OPC Alarm and Event Server (windows only) ...................................................................................163 4.21.1 ...................................................................................163 Specifying and using the Event Space 4.21.2 ..........................................................................................................164 Example Configuration 4.22OPC Data Access Client (windows only) ...........................................................................................166 4.22.1 ...................................................................................................................169 OPC XML Client 4.22.2 ...................................................................................................................169 OPC Client Items 4.23OPC UA Client ...................................................................................................................................174 4.23.1 ............................................................................177 Connecting to a server with a known URL 4.23.2 ..............................................177 Connecting to a server using the OPC UA Discovery Service 4.23.3 ............................................................................................178 Verifying the server connection. 4.23.4 ............................................................................................................179 OPC UA Client Items 4.24OPC UA Server ..................................................................................................................................183 4.25OPC UA Server Application and User Security .................................................................................187 4.26Self-Signed Certificate ........................................................................................................................188 4.27ODBC Client .......................................................................................................................................188 4.27.1 ........................................................................................................189 Adding an ODBC Client 4.27.2 ........................................................190 Configuring a Data Source for the SDGs ODBC Client 4.27.2.1Windows ...................................................................................................................190 4.27.2.2Linux .........................................................................................................................193 4.27.3 ...........................................................................................194 Selecting an ODBC Data Source 4.27.4 ........................................................................................................194 Adding an ODBC Query 4.27.5 ...................................................................................................197 Executing an ODBC Query 4.27.6 ..........................................................................................................197 Mapping a query MDO 4.27.7 .......................................................................................................198 Parameter Based Queries 4.27.8 .................................................................199 Mapping Items to Parameters Defined in a Query 4.27.9 ...................................................................................199 "Triggering" the ExecuteQuery MDO 4.27.1 .....................................................................................204 Polling the Data Source for Changes 4.28Mapping Points from Master Components to Slave Components ......................................................204 4.29Mapping Points from Master Components to Master Components ....................................................205 4.29.1 .................................................................................205 Mapping OPC Client points (windows) 4.30Mapping Points from Slave Components to Slave Components ........................................................206 4.31Setting IEC 60870-5-101 and -104 Slave Data Types to use Time Stamps .......................................207 4.32Setting DNP3 Event Types and Default Variations ............................................................................207 4.33Internal OPC Server (windows only) ................................................................................................. 208 4.33.1 ............................................................................ 208 OPC Server Registration for SDG Service 4.33.2 ................................................................................................. 210 OPC Server Point Properties 4.34Mapping Points to an External OPC Server ....................................................................................... 212 4.35Format of the Point-Mapping file ....................................................................................................... 212 4.35.1 ..................................................... 214 Definition of Fields contained in the Point Mapping File 4.35.1.1Point Identification Fields ........................................................................................ 214 4.35.1.2Data Options Field ................................................................................................... 214 4.35.1.3Table of Control Options and the Meaning .............................................................. 215 4.35.1.4LOGMSKxxxx and OPCAE_LOGMSKxxxx Events used in the MDO option field .......................................................................................................................... 224 4.35.1.5Physical Tag Name ................................................................................................... 224 4.35.1.6User Tag Name ......................................................................................................... 225 4.35.1.7Equation Field .......................................................................................................... 225 4.35.1.8Supported Operators Used in Expressions ............................................................... 227 4.35.1.9Examples of Valid Expressions ................................................................................ 227 4.35.1.10Description Field ................................................................................................ 227 4.36Master and Slave Component Columns ............................................................................................. 228 4.37Data Points ......................................................................................................................................... 230 4.37.1 ..................................................................................................... 230 Modbus Point Numbering 4.37.2 ................................................................................................................. 231 Point Initialization 4.37.3 ................... 232 Translating data types between different protocols (Master to Slave mapping) 4.37.4 ............................................................................................ 233 SDG Database Storage Formats 4.37.5 .................................................................................. 234 SDG Database Standard Quality Flags 4.37.6 ................................................................................ 234 IEC 60870-5-101 and IEC60870-5-104 4.37.7 ................................................................................................................... 238 IEC 60870-5-103 4.37.8 238 DNP3 4.37.9 .................................................................................................................................. 240 Modbus 4.37.10 ............................................................................................................................ 241 OPC Reads 4.37.11 ........................................................................................................................... 242 OPC Writes 4.37.12 ........................................................ 243 Predefined Internal MDOs for Monitoring and Control 4.37.12.1Characteristics of Predefined Internal MDOs .................................................... 243 4.37.12.2Action Mask and Period ..................................................................................... 248 4.37.12.3DNP Action Mask .............................................................................................. 248 4.37.12.4DNP Action Now Status ..................................................................................... 249 4.37.12.5DNP Action Mask Examples .............................................................................. 250 4.37.12.6IEC Action Mask ................................................................................................ 251 4.37.12.7Modbus Action Mask ......................................................................................... 251 4.38SDG Clock ......................................................................................................................................... 252 4.38.1 ...................................................................... 252 Event Time Tags and Database Update Times 4.39Sequence of Event Log Files ........................................................................................................... 253 4.39.1 ........................................................................................................... 254 Event Log File Names 4.39.2 ..................................................................................................... 254 Event Log Record Format 4.39.2.1Value Field Names (VFN_Table) ............................................................................ 255 4.39.2.2Update Date and Time Field Names (UDTF_Table) ............................................... 256 4.39.2.3Reported Date and Time Field Names (RDT_Table) ............................................... 257 4.39.2.4Identification Field Names (IFN_Table) .................................................................. 258 4.39.2.5Escape sequences within Event Log Record Formats (ELR_Table) ........................ 259 4.39.3 ........................................................................................................................... 260 Flush Period 4.39.4 ............................................................................................................................... 260 Log Mask 4.40Gateway Properties ............................................................................................................................. 261 4.40.1.1Time Zone Names and Values ..................................................................................266 4.41DNP3 Datasets ....................................................................................................................................267 4.42User Defined Folders ..........................................................................................................................267 4.43GTWSettings command line tool ........................................................................................................268 4.44Import/Export points and mappings ....................................................................................................269 4.44.1 ...................................................................................................................270 Points file format 4.44.2 .............................................................................................................271 Mappings file format 4.45Advanced Parameters ..........................................................................................................................273 5.Troubleshooting ..........................................................................................................................................275 5.1Permissions .........................................................................................................................................275 5.2Running the SDG as Administrator ....................................................................................................275 5.3Running the SDG on a Virtual Machine .............................................................................................275 5.4Errors in the Point Mapping File When Saving the SDG ...................................................................275 5.5Network: Network Driver Appears To Not Be Serving This Directory Dialog .................................275 5.5.1 .......................................................................275 Physical layer error: Error opening channel 5.5.1.1TCP/IP Connection ...................................................................................................276 5.5.1.2Serial Connection ......................................................................................................276 5.5.2 .........................................................................................................................276 Configuration 5.5.3 ............................................................................................................................276 Other issues 5.6Timeout waiting for application layer response fragment or restart." Message is displayed ............276 5.7Missing DLL Files ..............................................................................................................................277 5.7.1 ..............................................................277 Missing Microsoft Visual C Run-time Components 5.7.2 .................277 Error Starting the GTWService - Missing Localization DLL (VC80.MCFLOC) 5.8Using an RS-232/RS-485 Converter with the SCADA Data Gateway ...............................................277 5.9Missing Parameters in tmwgtway.ini file ........................................................................................277 5.9.1 ..................................................................................................279 Other Connection problems. 5.10Exit codes ............................................................................................................................................279 5.11Security tips ........................................................................................................................................279 5.12Missing linux dependency ..................................................................................................................280 5.13Using Cockpit for monitoring on linux ...............................................................................................280 6.SCADA Data Gateway Technical Support ...............................................................................................280 7.Glossary of Terms .......................................................................................................................................283 Table of Figures Figure 1: SCADA Data Gateway .......................................................................................................................16 Figure 2: SDG Software Architecture ...............................................................................................................17 Figure 3: Slave to Master Component Data Collection ....................................................................................18 Figure 4: Commands passed thru the SDG .......................................................................................................19 Figure 5: Connection and Identification of Data Points ...................................................................................20 Figure 6 Communication Channels ..................................................................................................................20 Figure 7 Sessions ..............................................................................................................................................21 Figure 8 Sectors ................................................................................................................................................22 Figure 9 IEC 61850 Client in the SDG ...........................................................................................................123 Figure 10 Add IEC 61400 Alarm Node Menu Item ........................................................................................124 Figure 11 Add IEC 61400 Alarm Node dialog screen ....................................................................................124 Figure 12 Find Alarms Array dialog screen ...................................................................................................125 Figure 13 Alarms Array Node ........................................................................................................................125 Figure 14 Add Alarm Context Menu ...............................................................................................................126 Figure 15 Add Alarm MDO dialog screen .....................................................................................................126 Figure 16 Configured Alarm MDOs ..............................................................................................................126 Figure 17 Example of Master to Slave and Master to Master Association ....................................................229 Figure 18 TMW Simple OPC Client ................................................................................................................232 Figure 19 Event Log Record Format ..............................................................................................................255 1. Overview The Triangle MicroWorks SCADA Data Gateway (SDG) application is a communication protocol driver. It provides the values that the protocol transmits over the wire. It collects data from OPC Server, IEC 60870-5, DNP3, IEC 61850, ICCP/TASE.2, or Modbus Slave devices and then supplies this data to other control systems supporting OPC Client, IEC 60870-5, DNP3, IEC 61850, ICCP/TASE.2, and/or Modbus communication protocols. 1.1 Purpose of this Manual This manual contains the necessary information required to install and configure the SCADA Data Gateway (SDG) and provide descriptions of various aspects of configuring and using the SDG. This manual covers the following topics: Introduction to the SDG Overview of SDG Components Configuration SDG licensing Making changes to the Initialization File tmwgtway.ini What to add to the Point-Mapping File How to use Equations Data Points What are Master and Slave Data Points How to map Data Points from one SDG Component to another How to associate OPC Tag names with Data Points Logging of Data Change Events Glossary of Terms 1.2 Introduction to SDG The SDG is a Windows application that acts as an OPC Client/Server, Data Concentrator, and Protocol Translator. The SDG supports the following protocol components: DNP3, IEC 60870-5-101, IEC 60870-5- 103, IEC 60870-5-104, Modbus, OPC Client, and OPC Server, ICCP/ICCP, 61850 Client, 61850 Server and ODBC Client. In addition, it supports OPC (UA, Data Access, XML Data Access, and Alarm & Events) for interfacing with Windows PC-based SCADA Systems and HMIs. SCADA Figure 1: SCADA Data Gateway The SDG interface retrieves data from external Slave Remote Devices through IEC 60870-5-101, -103, and - 104, DNP3, IEC 61850, or OPC Client Master Components as shown above in Figure 1. The SDG can also issue control commands to remote Slave devices through these Master Components. These data and command points can then be transferred to Control Centers or other Master Devices through Slave Components including OLE for Process Control (OPC) and OPC UA. The SDG comes with all the components depicted in Figure 2. Each component is individually licensed as described below. Only licensed components can be configured and used in a specific SDG installation. SDG architecture 3a Multiple connections to remote master or slave devices can be made with each protocol component. Figure 2: SDG Software Architecture 1.3 SDG Components Master/Client Components of the SDG 1.3.1 Master Components are used to: Read/Receive data from remote Slave/Server devices and to present this data to the SDG database for use by SDG Slave/Server Components Pass event data to the internal database for use by Slave Components, the Sequence-of-event Log File, and the OPC Alarm and Event Server Issue controls from Slave Components and OPC Writes Read and write data from/to external OPC Servers Monitor polling status of remote Slave devices Slave/Server Components of the SDG 1.3.2 Slave Components are used to: Present data from the internal database to remote Master devices Accept controls from remote Master devices, and to pass the controls through the internal database to Master Components slave to master data collection The OPC Alarm and Event Server reports events to OPC Alarm and Event Clients running on the same or a different computer. The OPC Server Component is a specific Slave Component that communicates through the OPC interface to other Windows software applications running on the same or a different computer. The OPC UA Server Component is a specific Slave Component that communicates through the OPC UA interface to other software applications running on the same or a different computer. Note that OPC Classic is only available on Windows. Master / Slave Data Flow 1.3.3 This section describes data flow from the various master and slave devices through the SDG. 1.3.3.1 Sequence from remote Slave devices to Master Component data collection 1. A Master Component collects data from remote Slave devices (e.g. RTUs or IEDs) 2. The Master Component presents the data to the database 3. The Database then presents the data to the Slave Components Figure 3: Slave to Master Component Data Collection 1.3.3.2 Command Sequence from Remote Master Devices 1. The Slave Component will receive the command from a remote Master device. 2. The Slave Component stores the remote Master device command in the database manager. 3. The database manager passes the command to the appropriate Master Component. 4. The Master Component then transmits the commands to the appropriate remote Slave device. commands passed thru SDG Figure 4: Commands passed thru the SDG 1.4 Data Point Addressing The following Point Addressing discussion applies to the DNP3, IEC60870-5-101, IEC60870-5-103, IEC60870-5-104 and Modbus protocols, note that for the other protocols supported the Channel/Session/Sector metaphor does not apply. Points are the most indivisible level of data in the SDG. Master Data Points may be defined automatically for OPC tags when the data is first received from a remote device. This is accomplished by setting the following parameters in the tmwgtway.ini file: Set GlobalCreateTagAutomatic equal to TRUE Set PointMapFile to the desired filename (i.e. PointMapFile="C:\Program Files\Triangle MicroWorks\SCADA Data Gateway\tmwgtway.csv") Slave Data Points are created and associated with Master Data Points or Equations in the Point Mapping file. Master Data Points are placeholders for data calculated by equations or collected by Master Components from Remote Slave Devices. Slave Data Points are placeholders for data in the SDG that are available to remote Masters. The identity of the sector for each data point specified in the Point-Mapping file (channel, link Address, ASDU address) must correspond to a sector defined in tmwgtway.ini. Data points are identified by their physical connection to the SDG and the addressing within the communication protocol configured for the remote device. The following diagram illustrates how the SDG connects to and identifies each data point. While this diagram shows a Master Component of the SDG connected to several remote slave devices, the same point identification scheme is used for Slave Components communicating with remote Master devices. data points com channels Figure 5: Connection and Identification of Data Points Communication Channels 1.4.1 The SDG exchanges data with remote devices through one or more Communication Channels (these may be physically discrete channels, or logically discrete channels that share a physical connection, such as multiple TCP/IP serial pipes, each of which is a separate logical channel, even though they may share a single physical 10BaseT interface). Each communication channel is uniquely identified by a name, for example: COM1, COM2, etc. Figure 6 Communication Channels Each communication channel has a set of parameters, such as: baud rate, parity, and number of data and stop bits. These parameters are set in the Physical Layer section of tmwgtway.ini. sessions IEC60870-5-104 only supports one session per channel; hence the session or link address is not required for this protocol. PC COM ports are typically RS-232 interfaces, which only permit connection to a single remote device. To permit connection to multiple remote devices, as shown in Figure 7, it may be necessary to use some form of multi-drop interface device such as an RS-232 to RS-485 converter. Sessions 1.4.2 A session is a communication connection between the SDG and a remote device. A session is identified by a specific link address on a communication channel. If more than one device is attached to a SDG communication channel (multi-drop topology), the link address is used to select the device or SDG Slave Component that should receive the message. The session establishes a virtual connection between the SDG and a remote master or slave device. Communication protocol specifications supported by the SDG use the ISO protocol standards for the Application (7), Data Link (2), and Physical (1) Layers. It should be noted, the term session in this product and the associated documentation does NOT refer to the defined ISO Session (5) Layer. Figure 7 Sessions The parameters associated with the session include an index to the communication protocol and most of the communication protocol related settings. Each session using the same communication channel must have a unique data link address. Since the IEC60870-5-104 only supports one session per channel, the data link address is not required. For more information, refer to Session Configuration and subsequent sections in tmwgtway.ini. sectors Sectors 1.4.3 In IEC 60870-5 protocols, each session may contain more than one sector. Each Sector is a group of data contained within a single Session. In these protocols, each sector corresponds to a single Common Address of ASDU. DNP3 does not support sectors and hence does not require a Common Address of ASDU. Figure 8 Sectors Each sector is identified in tmwgtway.ini by both its session index and sector index. The sector ASDU Address must be unique within the session. If a remote Slave device or a Slave Component consists of only one sector, the ASDU address is normally set to the same value as the link address. Refer to the Database Configuration section of tmwgtway.ini. In the Point-Mapping file, the following identify the sector: Communication Channel Data Link Address ASDU Address External OPC Classic Servers 1.4.4 Connecting the SDG to an external OPC Server requires the specification of the following parameters in the OPC Client section of the tmwgtway.ini file. OPCserverNode - refers to the machine where the external OPC Server has been installed. This parameter can either be an IP address or the name of the machine as it appears on the network. OPCserverProgID - the Program ID of the external OPC Server. The Program ID of the OPC Server is generally provided in the documentation from the manufacturer. OPCserverName - an optional parameter that may be used to create a user-defined name for the external OPC Server for use in the SDG Point Mapping file (see Section 4.34). If not specified, the OPCserverName defaults to the value specified for the OPCserverProgID parameter. OPCserverReconnectTime - specifies the number of milliseconds to wait until attempting a reconnection with the OPC server should the connection be lost. If this time is set to zero a reconnection will not be attempted. External OPC UA Servers 1.4.5 Connecting the SDG to an external OPC UA Server requires the specification of the following parameters in the OPC UA Client section of the tmwgtway.ini file. OpcUaClientServerUrl - refers to the machine and URL where the external OPC Server has been installed. This parameter can contain an IP address or the name of the machine as it appears on the network. OpcUaClientName - the name used to identify the OPC UA Client in the SDGs name space. 2. Installation and Setup 2.1 Linux System Requirements On Linux the Triangle MicroWorks SCADA Data Gateway (SDG) is supported under the following operating systems (only 64-bit x86 processor) Ubuntu 18.04 and 20.04.05 RedHat Enterprise Linux 8.0 CentOS 8.0 The minimum system requirements for the host computer are: 1.5 GHz processor; recommended: 2.4 GHz multi-core processor 1 GB RAM; recommended: 4GB RAM Note: the minimum requirements are for very small SDG configurations with very limited data throughput requirements. 2.2 Windows System Requirements On Windows the Triangle MicroWorks SCADA Data Gateway (SDG) is supported under the following operating systems (64-bit) Windows 10 Windows 10 Embedded Standard Windows Server 2016 With supported Browser web installed Windows 10 Enterprise IOT The minimum system requirements for the host computer are: 1.5 GHz processor; recommended: 2.4 GHz multi-core processor 1 GB RAM; recommended: 4GB RAM Windows 7 Note: the minimum requirements are for very small SDG configurations with very limited data throughput requirements. 2.3 SDG Performance The performance of the SDG depends on a number of factors, including the number of data points; the number of channels/sessions/sectors; the frequency at which data changes; and the capabilities of the host PC. Also, the number and types of other applications running on the host PC can also affect overall system performance. Moreover, there are several INI configuration parameters that can have positive and negative effects on performance. Consequently, it is difficult if not impossible to characterize the performance on various systems. Listed below is information on the limitations of the SDG to consider when trying to determine system performance: 250 channels (per SDG) 250 session (per SDG) 64 sectors per session (for IEC60870-5) 500 61850 clients 250 61850 servers 250 ICCP clients 250 ICCP servers 25 RCBs per 61850 client 25 GCBs per 61850 client 50 Polled Data Sets per 61850 client 50 Polled Point Sets per 61850 client 20 OPC clients 20 ODBC Database clients 20 OPC-AE clients The 250 session limit is per SDG. For example, you could have one channel with 250 sessions; or 250 channels with one session per channel; or 50 channels with 5 sessions per channel; etc. Note that these limits can be changed by editing the gtw_limits.json file see section on gtw_limits.json. 2.4 Installing the SCADA Data Gateway Note that the Web application that manages the SDG services runs over https. The product comes with a supplied set of certificates to enable https. It is strongly recommended that the customer replace these certificates with their own. The names of the provided certificate files are gtw-https.key and gtw-https.crt. In the gtw_config.json file there are 3 fields that manage the https certificates: "httpsCertificateFile": "gtw-https.crt" "httpsPrivateKeyFile": "gtw-https.key" "httpsPrivateKeyPassPhrase": "" Only "httpsPrivateKeyPassPhrase" should be update if needed manually. The private key need to be encrypted. If the provided certificates need to be replaced the new certificates should be set using provided command line tool GTWSettings. GTWSettings will copy certificates in appropriate folder and encrypt private key file. See the section on GTWSettings tool for details. Windows 2.4.1 The SDG installer will automatically install a 64 bit version on 64 bit operating systems. A 32bit version is not available. After installation, the SDG application (both Monitor (GTWWebMonitor.exe) and Engine (GTWEngine.exe)) will run only with Administrator privileges or by a user that can be elevated to have administrator privileges. Graphical user interface, text, application, email Description automatically generated H G F E D C B A The SDG application is provided as a self-installing executable file (SDGvm.nn.xxxx.exe where m is the major release, nn is the minor release, and xxxx is a build number). To begin installing the SDG application double- click the SDGvm.nn.xxxx.exe file and follow the on-screen instructions. To run the SDG application, typically start it from the service control manager select GTWWebMonitor and GTWService services and start them. Refer to the Quick Start Guides for information on configuring the SCADA Data. To reinstall or remove the SDG application, run the installation process again or select Add/Remove Programs from the Control Panel. When first installed, a default configuration file called gtw_config.json will be placed under the system ProgramData folder (usually C:\ProgramData\Triangle Microworks\SCADADataGateway). This file must remain in this folder and cannot be moved. It is used for both the Monitor and the Engine during startup to define various paths and settings. More discussion on this file is below. If the install is an update (i.e., Older version of SDG were already installed), SDG set-up program should launch Migration Tool. 2.4.1.1 Windows Migration Tool Migration Tool should be launched automatically after install when older SDG version have been detected. It can also be launched manually with a double click on GTWWorkspaceMigration.exe, on SDG install folded. Default SDG install folder is: C:\Program Files\Triangle MicroWorks\SCADADataGateway A. Select the INI file that needs to be migrated to the new Gateway workspace folder B. Once the INI file is selected, the full file path will be shown in this window C. The user can choose a new workspace name or use the default file name as the workspace name (if a workspace already has the same name it might be necessary to rename the workspace) D. This link will open a file explorer for the user to view configuration files such as certificates, private keys, etc E. This link will also open a file explorer for the user to view the existing workspaces available for the Gateway to be run. F. Once the Migrate INI to Workspace button is pressed the INI file and its associated files (CVS, XML, certificates) will be moved to the workspace folder under the chosen name (C.) G. After migration, the tool will display the result of the migration. The results are divided into 3 sections: a. Errors: They need immediate attention (usually without fixing the error the workspace will not run) b. Warnings: The user needs to review the issues, but the workspace might function without any correction c. Information: It is a summary of the files copied to the new workspace Once the workspace is migrated, the user can look at the files of the workspace using the link E. If the workspace is migrated successfully, it can be chosen via the configuration menu in the web interface. Linux 2.4.2 On Linux the SDG application is provided as an installation package appropriate for the specific operating system. We also include the files required to install the licensing system. These files are typically delivered as a zip archive. The installation package files will include the SDG version and build number information in its name. See the document Install Red Hat.txt or Install Ubuntu.txt for complete instructions. Additionnaly Status of GTW Services can be checked remotely using cockpit. See Cockpit site for reference. https://cockpit-project.org/ 2.5 SCADA Data Gateway Licensing The SDG uses a software licensing mechanism, which supports the licensing of individual components of the SDG product. When the SDG is run for the first time, a demo license will be generated which enables all of the SDG components for 21 days. To obtain a permanent license for specific components, please contact Triangle MicroWorks before the end of the evaluation period. The SDG supports the following, individually licensable, components: Component Symbol Status IEC60870-5-101 Master M101 Currently Supported IEC60870-5-102 Master M102 Future IEC60870-5-103 Master M103 Currently Supported IEC60870-5-104 Master M104 Currently Supported IEC60870-5-101 Slave S101 Currently Supported IEC60870-5-102 Slave S102 Future IEC60870-5-103 Slave S103 Future IEC60870-5-104 Slave S104 Currently Supported IEC 61850 Client IEC 61850 Client Currently Supported IEC 61850 Server IEC 61850 Server Currently Supported ICCP/ICCP Client ICCP/ICCP Client Currently Supported ICCP/ICCP Server ICCP/ICCP Server Currently Supported Distributed Network Protocol Master MDNP Currently Supported Distributed Network Protocol Outstation SDNP Currently Supported Modbus Master MMB Currently Supported Modbus Slave SMB Currently Supported OLE For Process Control (OPC) Server OPC Server Currently Supported (Windows Only) OLE For Process Control (OPC) Alarm and Event Server OPC Alarm and Event Server Currently Supported (Windows Only) OLE For Process Control (OPC) Alarm and Event Client OPC Alarm and Event Client Currently Supported (Windows Only) OLE For Process Control (OPC) Data Access Client OPC Client Currently Supported (Windows Only) OPC XML Server XML Data Access Server Currently Supported (Windows Only) OPC XML Client XML Data Access Client Currently Supported (Windows Only) OPC UA Client OPC UA Client Currently Supported OPC UA Server OPC UA Server Currently Supported ODBC Database Client ODBC Client Currently Supported The SDG uses the Sentinel LDK licensing system from Thales, Inc. This is an advanced and full featured licensing system that allows you to manage your license keys for Triangle MicroWorks products. The system provides the capability to transfer licenses to other machines without needing to contact Triangle MicroWorks. All of the capabilities of the system are described in the document called Guide to Triangle MicroWorks Licensing. This document fully describes how to convert your demo version of this product into a fully licensed version. The document is found in the installation directory for the SDG software. 2.6 Running the SDG as a Service on Windows (windows only) This section describes how to run the SDG as a Windows Service. A Windows Service is a process that runs in the background. Typically a service starts when the system boots, although this behavior can be modified through the use of the service management console. Some advantages of running the SDG as a service include: No login required Service can run under a separate user account Application starts automatically when the computer boots Casual users cannot accidentally stop the application The SDG runs as a service. The web application is used to configure the SDG communications parameters, point mapping, etc. This configuration process results in an INI and CSV file that the SDG will later use to configure itself when it starts. The SDG configuration can be created and tested interactively using the web application and then, when the configuration/testing process is complete, the SDG service can be started as a service from the service control panel as outlined below. Installation, Removal and Setup 2.6.1 The service executable files and additional installation setup files are installed as part of the normal installation procedure of the SDG. In addition, you can do the following as needed: 1. Should the SDG service need to be manually registered the following command line options are available. GTWEngine.exe -install this command registers the COM interfaces of the service and install it as a service. Normally these commands are executed for you at SDG install time. 2. To remove the SDG service GTWEngine.exe -remove can be used. This will remove GTWService from the service control panel and as a result GTWEngine.exe will not run as a service any more. 3. GTWWebMonitor.exe install will install the SDG web monitor as a service. GTWWebMonitor.exe remove will un-install the web monitor as a service. 4. Both the web monitor and engine are required for the SDG to operate correctly. Service Events and Status Messages 2.6.2 event viewer Should the SDG service fail to startup or unexpectedly stop. Status and Event messages are logged to the Windows Event Log. The standard windows event viewer is shows below. The Event log along with the protocol analyzer log should be used to troubleshoot problems. 2.7 SDG Web Application Runtime Architecture SDG Web Application System Services 2.7.1 This information is provided to aid in understanding the purpose of the Gateway runtime components; GTWWebMonitor and GTWEngine. Web application services are divided into 3 distinct groups; services the monitor provides, services the engine provides, and Web API calls that the GTWEngine and/or GTWWebMonitor can perform. 2.7.1.1 GTWWebMonitor Services A GTWWebMonitor performs the following processes: Configure System manage the gtw_config.json file. About/Help provide content for the about and help dialogs Messages/Logging support for popup messages and trace logging Stop Monitor stop the monitor process Stop Engine stop the engine process WorkSpace support for creating, selecting and managing the content of a workspace License view and activate a license File upload/download upload and download files to/from the server. Only specific files can be accessed related and restricted to the current workspace. Authentication support user authentication with roles Audit log access and log events related to user authentication 2.7.1.2 GTWEngine Services B GTWEngine performs the following processes: Configure System manage the gtw_config.json file Editors - provide functions to add/del/edit/perform actions/create menus related to protocol components and all related elements Nodes/Tree support tree items/devices in the dashboard, for example: channels, sessions, OPC Server, etc. Paging API to support paging of large amounts of data Health provide health information to caller such as CPU and memory usage Stop Engine stop the engine process. Normally the engine is configured to automatically restart Tags support MDO, SDO, Equations, etc. in the Dashboard. This set of APIs support getting data as well as setting data Messages/Logging - support for popup messages and trace logging Mapping allows creation and display of point mapping 2.7.1.3 Web Client Services C GTWWebMonitor performs the following processes: Configure System manage the gtw_config.json file. Messages/Logging - support for popup messages and trace logging. Health provide health info to caller Authentication support authentication between monitor and engine Stop Engine send message to stop engine SDG Web Architecture 2.7.2 The Web Architecture system consists of 2 processes: 1. GTWWebMonitor - This process is responsible for managing system level activities such as health, licensing, logging, authentication, etc. 2. GTWEngine - This process manages the communication channels using protocols such as DNP3, 61850, OPC, etc. It communicates with remote IEDS/RTUs and performs mapping of data between the protocols. It can act as a data concentrator to provide data to upstream applications such as SCADA systems. TMW envisions multiple GTWEngine processes running on different computers in the future. GTWWebMonitor will be responsible for the integration of the Engines into a coherent system. The two processes are configured as services that restart automatically should they be stopped. Each of these processes has a web client and a web server to facilitate the communication between them. The web application will establish a connection to both the GTWWebMonitor and the GTWEngine. The web application is Single-Page Application based on AngularJS. It utilizes the REST APIs provided by the GTWEngine and GTWWebMonitor as well as WebSockets to keep it current and to manipulate the application configurations. The configuration is stored in several files. GTWWebMonitor stores system configuration in gtw_config.json. The authentication database is stored in gtw_users.db. The auditlog is in gtw_audit_log.db. Logging is in SDGLogs/SDGMonitorError.txt. Licensing logs are in LicenseLogs/Lic- Loglog. GTWEngine also stores its system level configuration information in gtw_config.json. Note that the current active work space is in the gtw_config.json file. In addition, SDG uses a Work space concept to store the protocol configuration related data in a directory under WorkSpaces. Each directory under WorkSpaces represents a complete GTWEngine configuration with all related files such as the INI, CSV, XML, etc. Note that the INI must have the same name as the directory for it to be recognized as a work space. Logging is in SDGLogs/SDGEngineError.txt. An exception to the Work Space storage concept is encryption related files. Public certificates can be stored in the WorkSpace, but private keys are not as they should be secure on the system and should not be generally accessible. Determine Engine Health and State ***added manually 2.7.3 GTWWebMonitor makes a web request to the GTWEngine every few seconds to obtain its health. If there is no response it is assumed the Engine is not running. In addition, the engine will send a message to the web app through a WebSocket when the engine starts and stops. The engine also publishes its current startup state to the web application which can be seen in the status bar. 3. User Interface 3.1 Accessing the Web UI Only Google Chrome, Mozilla Firefox and Microsoft Edge are currently supported web browser. Using the latest version is recommended. It is recommended to only have one tab open with the Gateway WebApp in the browser to improve responsivness. To connect on the local machine, open a web browser and navigate to: https://localhost:58090 To connect remotely to the machine UI, use a web browser and navigate to the SDG machine IP address and port 58090: e.g. https://192.168.1.2:58090 3.2 Dashboard Management Hover the cursor over the Dashboard menu to show the Dashboard Manager: With the Dashboard Manager you can: - Add or Remove Configuration Views to the Dashboard display (max of 3 Views simultaneously) - Add or Remove the Log view (this displays SDG Engine events) - Modify the Dashboard layout using the 3 button options: A. Custom (the user can move and resize the view window - the layout is automatically saved) B. Horizontal (default) C. Vertical Default Dashboard 3.2.1 The diagram above shows the default view of the Dashboard. The Dashboard is comprised of three panels which are labeled with the letters A, B, and C. Device Tree view This area is marked by the letter A in the diagram above. The Device Tree View shows a hierarchy of devices and/or device nodes. Tag Grid This area is marked by the letter B. It is used to displays data point details of the device or device node currently selected in the Device Tree. Log viewer This area is marked by the letter C. The Log viewer shows messages from the Gateway Engine which may come from the processing of protocol messages or may present health and status conditions in the engine. Device Tree View 3.2.2 The diagram above shows the Device Tree View. The diagram also shows the context menu available on the Root Node of the tree. This tree is where the Gateways internally defined devices are represented. By expanding the nodes, the Sectors, Sessions, and Data Point Types can be found. These concepts will be explained later in the document. A. Gateway Root Node B. Root node menu (Right-Click on the Gateway Root Node): Save workspace Enable/disable the Health Warning on the device tree view Edit gateway engine parameter Add new device C. Devices (Right-Click will open context menu) D. Search a device by name E. Special views (click the chevron to display or hide) The Device Tree View also contains a list of Special Views. These selections control what types of items are shown in the Tag Grid which is explained in the next subsection. Warning view will only display tags which are not healthy (the context menu Reset Counter allows some of the health tags to be reset) Health view will show all health tags (healthy and not healthy) Performance view will show all performance tags Important Note: when a device is blinking red, one or multiple health tags indicate an issue with a specific component. Tags Grid 3.2.3 In the Gateway, the term Tag refers to the name and data associated with a MDO, SDO, Internal Point or Equation within the Gateway data base. A. Tags can be ordered by Name or Type B. Context menu actions (if available): Edit tag Delete tag with support for multiple tags Change Value of the tag with support for multiple tags C. The buttons to the right of a tag perform (if available) the same actions as the context menu D. The Mappings of the tags are shown depending on their direction. It also (if applicable) allows users to delete the mappings E. Pagination is available depending on the Gateway Engine configuration (*****Review Configuration Settings HERE*****) F. A Search can be conducted by tag name and type (click the chevron to display or hide): Wildcard searches are enabled, use the asterisk * or money $ characters Only tags for the selected device are searched The Deep Search button searches for tags in all devices Important Note: - VALUE, QUALITY, and TIME are updated in real-time. - Special characters are not supported, like . Multiple tags can be selected or unselected by clicking on the row. The row will turn a different color to indicate selection. Mapping 3.2.4 3.2.4.1 Single mapping To create a mapping, drag and drop a Tag to the devices node destination. 3.2.4.2 Multi-mapping Select multiple tags by clicking on them (or use the context menu) and then drag and drop the selection to the devices node destination. To select several points easily you can use Shift key: Press Shift Key. Click on First MDO to select. Click on Last MDO to select. Release Shift key. Editor 3.2.5 The Editor dialog appears whenever a device or channel is added or needs to be edited from the Device Tree. Quick Help for fields can be accessed by clicking on the blue question mark in the top right corner of the Editor as shown above. Refreshing the Display 3.2.6 In some circumstances the web application may not display the current state of the SDG. Reloading the web application by pressing refresh in the web browser can be used to correct the issue. 3.3 Logging This includes logging of protocol data as well as errors, warnings, and information related to the SDG operation. Various filters are provided to select specific messages. The log can be paused, cleared and downloaded from the log dialog. Filters and Controls 3.3.1 Filters are selected by dropping the dialog down by pressing the chevron. Once the dialog is visible, filter settings can be changed by selecting the SDG, SCL and MMS filter tabs. Inside these tabs you will find the check boxes to select and de-select the various filter categories. Log data is mirrored to a set of log files. Errors and Exceptions are always logged to the files. It is possible to filter the data in the log based on individual devices in the workspace by right-clicking on the device name and selecting Add Device to Logs Filters. One or many devices can be added to the filter. A device can be removed by clicking the X next to the name, or using the Remove All button to clear all devices from the Device Filter. The checkbox Save Log to text file on the SDG Filter panel when enabled will log all filter selected data to the log files. If disabled only Errors and Exceptions are logged. Note that this setting is not persisted between SDG re starts so needs to be enabled after a re-start should all log info be required. The web interfaces log panel supports a 5000 line buffer of the most recent log data received from the server. As a result if any older log data needs to be analyzed the log files should be accesed. Accessing Log Output 3.3.2 The logs can be downloaded for more detailed analysis from the following screen which is accessed by going to Log/Monitor Log. There are separate files for the Engine and the Monitor processes. Logging storage limits 3.3.3 For the engine, by default ten(10) historical log files are kept with 10MB of data in each, this can be changed on the System Configuration page (see below). For the Monitor, two files are kept. The current log file has no time stamp in its name, although historical log files do have a timestamp in their names. As size limits are reached, older files are purged from the system to avoid running out of disk storage. The limits can be changed on the System Settings page in the Log Parameters section. 3.4 Gateway System Configuration Any of these parameters that are also defined in the gtw_config.json file will be kept in sync by the SDG. i.e. the SDG will update the gtw_config.json file with any changes made through the web application. Workspace Management 3.4.1 Selection and creation of a workspace is managed through the Workspace Management section of the Web Client. This section can be used to create a new workspace and to select and run an existing workspace. It also supports uploading and downloading the files related to a workspace. In addition support is provided for download and restore a workspace as a collection in an archive as well as saving a workspace with a different name. System Management 3.4.2 A variety of changes to settings require either the Monitor or Engine to be re-started. This section provides support for re-starting the processes. Many of the fields are identified with an icon to indicate if a re-start is required. Web Server Settings 3.4.3 This section provides configuration settings to manage the IP address and port numbers of the web servers running in the SDG Monitor and Engine. It also provides settings to control various performance settings related to the web servers. Pagination 3.4.4 Paging mode is controlled by Items per page for mappings and tags lists on WEB UI (0 to disable): This setting adjusts the number of items per page when there are a large number of items in a result. Gateway WebSocket Settings 3.4.5 Gateway WebSocket Update Rate (seconds) and Gateway WebSocket Update Block Size: control how often and how much data the WebSockets provide to the web application. When the block size exceeds the specified amount the WebSocket will send, and when the time exceeds the Update Rate it will also send. These 2 settings can be used to throttle the performance of the WebSockets. Web data can be compressed by selecting the Use gzip compression checkbox. This will help performance on slow links. Security Parameters 3.4.6 By default HTTPS protocol for communications between the browser and the Gateway engine is enabled. For this a private/public key file pair is required. We provide a sample of these files but do not recommend their use in a production system. Note: With some browsers (like Firefox 98.0.2) when using a self signed certificate, you need to add an exception for GTWEngine and GTWWebMonitor. For example, with SDG running on computer with IP 192.168.8.152, and SDG using default port: Open Web Page: https://192.168.8.152:58090/#/ Add exception. Then open Web Page: https://192.168.8.152:58080/#/ Add exception. To enable user authentication and roles the Enable User Authentication must be selected. This allows different users to have different levels access to the system. The User Authentication Expiration Timeout (seconds): field will control how long a user remains logged in with no activity. Note there are 4 of these timeouts, one for each user role. A value of zero will disable the timeout making it such that no automatic logout will occur. Allowed IPs is used to specify a comma separated list of IP addresses that can connect to the SDG web server. By default the local machine will always be allowed to connect to the web server. Log Parameters 3.4.7 The Enable Audit Log check box will turn on audit logging. Audit logging currently only logs events related to authentication. When audit logging is enabled, an additional option is available on the Log menu to view and download the audit log. The Generate Full Log on SDG re-start check box will turn on full logging. When this checkbox is enabled after the SDG is re-started the logging text files will receive all logging information specified with the logging filters. This setting will be set back to false (unchecked) when the SDG is shutdown to avoid large amounts of logging information accumulating in the log files. By default only Errors and Exceptions are logged to the files. The Maximum number of log files setting controls how many log files will be retained in the log files directory. For windows this is: C:\ProgramData\Triangle MicroWorks\SCADADataGateway\SDGLogs And linux: /etc/tmw/sdg/SDGLogs For the engine, by default ten(10) historical log files are kept with 10MB of data in each, this can be changed on the System Configuration page (see above). For the Monitor, two files are kept. The current log file has no time stamp in its name, although historical log files do have a timestamp in their names. As size limits are reached, older files are purged from the system to avoid running out of disk storage. The log files can be accesed from the web interface on the Monitor Log panel by going to the menu option Log/Monitor Log as shown below: Note there are 2 sets of files there. SDGEngineError.txt and SDGMonitorError.txt. 3.5 Gtw_config.json file The gtw_config.json file stores the settings described in the section Gateway System Configuration. This file is updated when the save button is pressed. The logging filter settings are also stored in this file. This file is located in the following directory: Windows: C:\ProgramData\Triangle MicroWorks\SCADADataGateway Linux: /etc/tmw/sdg The file contains: { "currentWorkSpaceName": "tmwgtway-logoff", "enableIECFullStackAddressing": false, "fullLogOnRestart": false, "gtwAllowedIPs": "", "gtwAuthExpCONFIGURATOR_ROLE": 600, "gtwAuthExpOPERATOR_ROLE": 600, "gtwAuthExpSU_ROLE": 360, "gtwAuthExpVIEWER_ROLE": 360, "gtwDoAudit": false, "gtwDoAuth": false, "gtwDoFastShutdown": true, "gtwDoValidateConfig": true, "gtwEnableHttpDeflate": true, "gtwHost": "192.168.0.9", "gtwHttpPageBlockSize": 100, "gtwHttpPort": 58080, "gtwLocalIPOnly": false, "gtwMaxLogFiles": 10, "gtwMaxPathLength": 2048, "gtwTzPath": "f:/work/SDG-trunk/thirdPartyCode/TimeZone/cctz/testdata/zoneinfo", "gtwUseWebSSL": true, "gtwWebDir": "f:/work/SDG-trunk/gateway/GTWWebApp", "gtwWsUpdateBlockSize": 100, "gtwWsUpdateRate": 5, "httpsCertificateFile": "gtw-https-default.crt", "httpsPrivateKeyFile": "gtw-https-default.key", "httpsPrivateKeyPassPhrase": "", "maxNumThreads": 100, "monHost": "192.168.0.9", "monHttpPort": 58090, "numThreads": 4, "sclCategoryMask": 524216, "sclSeverityMask": 15, "sdgCategoryMask": -257, "sdgOpcTraceEnable": true, "sdgSeverityMask": 15, "t6CategoryMask": -1, "t6SeverityMask": 7 } 3.6 Gtw_limits.json file The gtw_limits.json file stores the limits for the allowed number of channels, sessions, sectors, clients etc. This file is created if it does not exist with default values. The file can be edited with a text editor i.e. notepad or nano to change the limits. Note this is useful to either expand the limits or decrease them when resources may be more constrained. The file contains: { "AE_SERVER_CLSID": "{5A15D6FA-AC6E-4BCE-9A89-C6372584C9F6}", "AE_SERVER_PROGID": "TriangleMicroWorks.OPCAE", "DA_SERVER_CLSID": "{849B0757-B1DB-4095-AF58-C3E32EE43BEE}", "DA_SERVER_PROGID": "TriangleMicroWorks.OPCDA", "GTKPARAM_MAX_NUM_ACTIONS": 10, "GTKPARAM_MAX_NUM_ACTIONS_PER_SESSION": 25, "GTKTPARM_MAX_ALARMS_PER_CLIENT": 10, "GTKTPARM_MAX_NUM_61850_SERVERS": 250, "GTKTPARM_MAX_NUM_CHANNELS": 250, "GTKTPARM_MAX_NUM_GOOSE_MONITORS": 250, "GTKTPARM_MAX_NUM_GOOSE_SUBSCRIPTIONS_PER_MONITOR": 25, "GTKTPARM_MAX_NUM_I61850_CLIENTS": 500, "GTKTPARM_MAX_NUM_I61850_GOOSES_PER_CLIENT": 25, "GTKTPARM_MAX_NUM_I61850_POLLED_DATA_SETS_PER_CLIENT": 50, "GTKTPARM_MAX_NUM_I61850_REPORTS_PER_CLIENT": 500, "GTKTPARM_MAX_NUM_ODBC_CLIENTS": 20, "GTKTPARM_MAX_NUM_OPC_AE_CLIENTS": 20, "GTKTPARM_MAX_NUM_OPC_CLIENTS": 20, "GTKTPARM_MAX_NUM_OPC_UA_CLIENTS": 20, "GTKTPARM_MAX_NUM_SESSIONS": 250, "GTKTPARM_MAX_NUM_TASE2_CLIENTS": 250, "GTKTPARM_MAX_NUM_TASE2_LDS_PER_SERVER": 64, "GTKTPARM_MAX_NUM_TASE2_POLLED_AND_POINT_DATA_SETS_PER_CLIENT": 50, "GTKTPARM_MAX_NUM_TASE2_RPT_DS_PER_CLIENT": 50, "GTKTPARM_MAX_NUM_TASE2_SERVERS": 250, "GTKTPARM_MAX_QUERIES_PER_ODBC_CLIENT": 200, "GTKTPARM_MAX_SECTORS_PER_SESSION": 64, "GTKTPARM_MAX_USERS_PER_OPC_UA_SERVER": 50, "GTKTPARM_MAX_USERS_PER_SECTOR": 4, "GTKTPARM_MAX_USERS_PER_SESSION": 10 } This file is located in the following directory: Windows: C:\ProgramData\Triangle MicroWorks\SCADADataGateway Linux: /etc/tmw/sdg 3.7 Context Help Pressing the ? Button on a panel will expand the panel to include the help text. This is available on most panels and screens. In addition, tool tip or hover help is provided on many items in the user interface by holding the mouse over an item for a few seconds. User Authentication Enable Users 3.7.1 User authentication is introduced with SDG version 5.0. To enable/disable user authentication, navigate to the Settings page and then to the Security Parameters section. In this section, click on the checkbox for the Enable User Authentication option to enable it (by default during a new install, user authentication is enabled, an admin user is created and the admin user password is set during the installation by asking the user for it). In addition, Audit logging can be enabled to record user access and activity in SDG audit log. After checking the appropriate options, click the Save button at the bottom right of the page. Wait about 30 seconds and refresh the page. You should be prompted to login. Without adding any additional users the default user credentials are: Username: admin Password: <the password specified at install time> After successful login, the admin user will be required to change the password. It is important to not lose this password. After the password is changed, notice there is now a User ID and new buttons in the top navigation bar: Edit Users 3.7.2 To configure and edit user accounts, navigate to the Settings > User page: From here, you can view and manage the user accounts: The two buttons to the right of the user accounts are Edit and Reset Password buttons: Important: Users cannot be deleted but can be deactivated. This is for audit log purposes. Add New Users 3.7.3 To add a new user, simply click the Add New User button on the top right of the Users Management page. A panel will appear requesting information for the new user: Input the user information, set a temporary password and then select the appropriate role. Upon their first login, the new user will be prompted to set their own password. User Roles are explained in detail in the User Roles section below. Important: Here are the Passwords rules: Must contains at least one lowercase characters. Must contains at least one uppercase characters. Must contains at least one digit from 0-9. Must contains at least one special symbols in this list: '@, $, #, !, -, +, *, _, %, &, .' Length must be at least 8 characters and a maximum of 24. User Roles 3.7.4 Each user is assigned a specific role in the SDG. Each role allows specific permissions. There are four User Roles within SDG and each is defined as follows: SUPER USER: Has complete control to modify settings, add users, reset passwords, and configure SDG CONFIGURATOR: Can create and modify SDG configurations. For example; add channel or create new workspace Cannot access User Management Cannot modify SDG UI settings (language, Web Server IP, etc) OPERATOR: Can operate SDG. For example; reset an alert or send a command Cannot change configurations; e.g.; edit channel or data point Cannot modify the current workspace Cannot access Help / License information VIEWER: Can only view the SDG and workspace Can view logs and adjust log filters, as well as clear the log display Cannot modify settings or configurations within the web server or SDG Notes: A single admin user with the Super User role is provided by default. If possible, a second user with the Super User role should be created in case the password for the other super user is lost. Otherwise the password of the default admin user will not be recoverable. The Gateway allows only one user to be logged in at the same time unless the user is using the VIEWER role. If a user is already logged in and a different user with higher or equal role logs in then the previous user will be logged off (this doesnt apply for the VIEWER role). User Password Requirements 3.7.5 Must contain at least one lowercase character. Must contain at least one uppercase character. Must contain at least one digit from 0-9. Must contain at least one special symbol in this list @ $ # ! \- + * _ % & . Length must be at least 8 characters and a maximum of 24. 3.8 Audit Logging Auditing 3.8.1 To view user history and activity, enable the Audit Log. Once enabled, the user Audit Log can be viewed by navigating to Log > Audit Log: On the Audit Log page, a user can search specific parameters such as username and date range. As well, the log can be downloaded: Note: at this time only user authentication events are logged. 3.9 Help Menu Help 3.9.1 Brings up this document which provides comprehensive help for the SDG. Quick Start 3.9.2 Opens a web page that provides some quick start tutorials. Quick Start Pop-up 3.9.3 Opens a dialog that allows playing of various short videos to show how to to do some simple tasks in the SDG. INI File Parameter Help 3.9.4 Provides a summary of all the INI parameters available. Submit Support Request 3.9.5 Use this to get customer support. License 3.9.6 Opens the dialog that is used to manage the license. Request Quote 3.9.7 Opens the dialog that is used to request a sales quotation. Check for update 3.9.8 Allows for a download if the software is not up to date. About 3.9.9 Provides the build and release version information. config parameters 4. Configuration Reference This section discusses the configuration of the SDG. The SDG application configuration is specified in a file, called tmwgtway.ini. 4.1 Tmwgtway.ini Initialization File The SDG is configured using a configuration file. By default, this file is named tmwgtway.ini. The file includes comments describing the purpose of each parameter, the range of values, and the default value. The tmwgtway.ini file is stored as part of a workspace (See Section 4.2.1). Tmwgtway.ini is overwritten with any parameter changes made during the program execution when the SDG saves the configuration. Comments in tmwgtway.ini should not be altered, since they will be set back to the original text when the SDG saves the file. Once you have completed configuring the SDG, Triangle MicroWorks recommends that you make a copy of the configuration files (tmwgtway.ini and tmwgtway.csv) so you can easily restore your system in the event that these files are accidentally lost or overwritten. The following diagram shows how the some of the important configuration parameters in tmwgtway.ini relate to the point-addressing scheme. Tmwgtway.ini Parameters 4.1.1 Tmwgtway.ini may be modified with a standard ASCII text editor (Note: The Web UI can be used to download/upload these files) to match the target application equipment prior to running the SDG although most settings can be modified through SDGs graphical interface. The following table shows basic parameters that should be set prior to establishing a connection. The configuration parameters required depend on the protocol used for each communication session. These parameters are also accessible though the user interface. Parameter DNP 101 103 104 MB Description PhysComChannel X X X X X COM port or remote TCP/IP address PhysComBaud X X X X COM port baud rate PhysComParity X X X X COM port parity PhysComDataBits X X X X Number of COM port data bits PhysComStopBits X X X X Number of COM port stop bits PhysComIpPort X X X TCP/IP port number PhysComIpMode X X X TCP/IP connection mode SessionLinkAddress X X X X Data link address of the slave component or remote device SessionLocalAddress X Data link address of the local device PhysComProtocol X X X X X Protocol for master or slave channel SessionCommIndex X X X X X Index of PhysComChannel for each session LinkConfirmMode X Remote device will send a data link confirm of the last frame sent LinkSizeAddress X X Number of octets (bytes) in link Address field LinkMode X X Balanced or unbalanced data link mode DbasSectorAddress X X X ASDU address of each sector within a session ASDUsizeCmnAddr X X Number of bytes in Common Address of ASDU (sector address) ASDUsizeCot X X Number of bytes in Cause Of Transmission (COT) field of ASDU. ASDUsizeIOA X X Number of bytes in Information Object Address field Hidden INI parameters 4.1.2 These are INI parameters that do not have a direct user interface for configuration (list not complete). Parameter DNP 101 103 104 MB Description MMBSessUpdateTimeOnValueChangeOnly X Only update time if the value of an MDO changes. Session level parameter. DbasSectorFileToExecuteOnReset X X 101/104 Slave parameter. Path to the file to exectute on a reset with qrp equal to one. DoNotChangeQualityOnDisconnect X X Do not change to quality at all when a disconnect occurs. OPCServerTimeZoneName OPC only: TimeZone name for OPC server. OPCServerTimeZoneIgnoreDST OPC only: Ignore DST changes in timezone for the OPC Server times. SessionOutgoingTimeZoneIndex X X X X X Timezone index of slave devices when data changes occur. SessionOutgoingTimeZoneIgnoreDST X X X If true and SessionOutgoingTimeZoneIndex is not 199 changes in Day Light Savings time are ignored for the supplied times of the tags in the session View All INI Parameters 4.1.3 All INI parameters and their descriptions can be viewed by selecting INI File Parameter Help from the Help menu (see below). The INI File parameter Help Panel will pop up with all INI parameter names and descriptions (see below). 4.2 Point-Mapping File The Point-Mapping file (tmwgtway.csv) identifies how the value for each data point in a Slave Component is obtained. This could be a direct mapping to a Master Data Point, or an equation, which incorporates multiple Master Data Points. If the OPC server is the only Slave Component in the target application, it may not be necessary to create a point-mapping file. Creating a point map file with only the required points will improve efficiency. Organization of Point Mapping File Data Saving the INI and CSV files 4.2.1 The INI and CSV file are saved as a pair into a Workspace directory. The name of the workspace directory must match the name of the INI file. All workspaces are stored in a directory named WorkSpaces in the etc/tmw/sdg directory or the ProgramData\Triangle MicroWorks\SCADADataGateway directory on Windows. The System Setting panels as well as the context menu on the SDG root node have options to save these configuration files. New workspaces can be created from the System Settings panel. The web application will prompt at the appropriate times to ask the user to save the INI/CSV files. In addition, when there are outstanding changes the header on the dashboard will turn a light red indicating outstanding changes should be saved. 4.3 Tags in the SDG Tags are added to a data type in the configuration interface. Tags can be Master Data Objects (MDOs), Slave Data Objects (SDOs), Equations, or OPC Client items. SDOs are created indirectly by mapping MDOs, Equations, Internal Points, or OPC Client items to them. Creating Tags Automatically for DNP and 60870-5 protocols 4.3.1 When a Master component is defined in the SDG, sometimes it is convenient to obtain all the tags that are defined in the connected slave device. To automatically create tags, right-click on the session and select the Auto Create Tags context menu option will send a General Interrogation to the Slave device, as the slave point data returns to the Master component, Data Types and Tags are created for each of the points received. This operation is available for both sessions and for sectors on IEC 60870-5-10x masters. 4.4 Master Data Objects (MDOs) Master Data Objects (MDOs) are points that obtain their value from an external Slave device or send commands to an external slave device. To add MDO points to a Master device, right-click on the session and select Add MDO: Next, the Add MDO dialog screen is displayed: Enter the Point Number (1) for this MDO and click the button (2) to select any require options for this MDO. See Section 4.5.1 for a discussion on editing tag options. The User Tag Name (3) and Description (4) parameters are optional and are used to provide further description of the MDO. The MDO is now added and can be viewed, or edited, by clicking the session: 4.5 Slave Data Objects (SDOs) Slave Data Objects (SDOs) are points that provide a value to external master devices. SDOs can also receive commands from external devices. A monitor SDO point receives its value from a mapped MDO point. A command SDO will provide a value to its mapped MDO. Creating SDOs is done indirectly by mapping (drag-and-drop) an MDO on to a Slave Data Type (MDO to a SDO). First, use the mouse to click and hold on the MDO in the Master session (1). Next, drag the MDO on top of the Slave Data Type (2) and release the mouse button to perform the mapping: After the point has been mapped, the Slave Data Object Editor is displayed: Enter the Slave Information Object Address (1) and then select any required options (2) for the Slave Point. Click OK to return to the SDG main window. The SDO is now added to the Slave Data Type: Editing Tag Options 4.5.1 The following dialog screen facilitates editing the options field for tags (i.e. MDO, SDO, OPC Client, Equation). The Option column (1) displays the option name. The Value column (2) is used to set the value of the option. For Boolean options, in order to include the option, the Value must be checked. To edit an option, click in the textbox next to the option to change the value. 4.6 Internal Predefined MDO Points Internal points or predefined points are MDOs that provide certain status and control operations for the SDG. Since many of these points are for status and control of the SDG, they are not editable in the configuration tool. Below are the points that are editable in the configuration tool. 4.7 Internal User defined MDOs Internal User defined MDOS are MDOs that can be used to store temporary point values. Internal User MDOs are added to the configuration by right-clicking on the Gateway node and selecting Add Internal MDO: command sdo This will display the Internal Master Data Object Editor dialog screen. This dialog screen allows you to define the User Tag Name (1), Options (2), Description (3), and the Type (4) for the MDO. The Description parameter (3) is optional: One use of Internal MDOs is to facilitate mapping of SDO points to other SDO points. Another use is to provide a user modifiable source of data in the SDG. To use an Internal MDO to map between two slave points, first create the Internal MDO of the desired type, then drag and drop the internal MDO onto the desired command SDO (this will provide a value for the MDO), and then drag and drop the same internal MDO onto the desired monitor SDO point (this will provide a value for the monitor SDO), see the diagram below. The value of a command SDO will be passed on through the Internal MDO to a monitor SDO. Internal MDOs can be used just like any other MDO type in the SDG. However internal MDOs can accept a value and provide a value to a mapped point. To modify the current value and quality of an Internal MDO, click on the Gateway node and then locate the Internal MDO in the item pane on the right. Next, right-click on the MDO and select Edit Internal MDO: The following dialog boxes are used to manage the value and quality for various data types: To edit an Internal Boolean MDO Value, click the Value checkbox (1) to set the value to TRUE or uncheck the checkbox to set it to FALSE. Click the Quality pull down (2) to edit the MDOs quality. To edit an Internal Analog MDO value, enter a value in the Value textbox (1). Click the Quality pull down (2) to edit the MDOs quality. To edit the quality of an Internal MDO, select the appropriate checkboxes on the Quality Editor pull down control. The quality can be a combination of the check boxes listed above. The numerical value is displayed in the Value box. 4.8 Equations Equations are MDOs that obtain their value based on a calculation performed on one or more other points in the configuration. To add an Equation MDO to the configuration, right-click on the Gateway node and select Add Equation MDO: This will display the Add Equation MDO dialog screen: The equation editor makes it easy to create equations using a drag-and-drop approach. Equation operations and database tags can be dragged and dropped from the Tag List (1) or Operations List (2) into the Equation field (3). The Validate Equation button (4) can be used to verify the syntax of the equation prior to adding it to the point list. One use of equations is to convert the raw value reported by the SDG into an IEC Normalized or Scaled value. For example, to convert the reported value, which has a range of -32768 to 32767 to a Normalized value, which has the range of -1 to +1-2-15, enter an equation to divide the reported value by 32768. See section 4.35.1.7 Equation Field for more information on the options and restrictions for equations. 4.9 Adding Channels Channels have a protocol associated with them; as a result all sessions associated with a channel will have the same protocol. In the user interface, channels are containers for sessions. A channel can be TCP/IP UDP or Serial. Add TCP/IP Channel Dialog 4.9.1 Parameter Description Channel Protocol Type Sets the protocol for the channel. Possible values are 'S101', 'S104', 'M101', 'M103', 'M104', 'MDNP',MMB,SMB or 'SDNP' Physical channel i.e. COM port, Remote TCP/IP or Node Name Sets the Communication Channel. Examples: "192.168.1.25" or "node name" IP Port Number Sets the TCP/IP port number to use Channel Alias Name Name for communications channel, this must be specified Local IP address The Local IP Address to listen or connect through. 62351-3 Security Configuration Check to enable TLS security. Is Redundancy Group Only applies to Slave 104 devices. If checked a redundancy group will be created and redundant channels can be added. TLS Security 4.9.2 Note this dialog is accessed by expanding (press the chevron) the 62351-3 Security Configuration section in the previous dialog. TLS Security is provided by the SDG for DNP, 60870-5, and Modbus TCP protocols. The above dialog is used to configure the certificates and settings. If help is needed to create or manage certificates, please contact Triangle MicroWorks support. Redundant Master/Slave 104 Channels/Configuration 4.9.3 If the Redundant checkbox in the above dialog is checked, a Redundant 104 Redundancy group and corresponding Redundant Master/Slave Channel will be created. The SDG device tree will look like: Slave: Master: One redundant channel is created by default with the same name as the redundancy group. A new redundant channel can be added via the menu Add Redundant Channel in the following popup: Will result in: A redundant channel can be edited/deleted via the menu Edit Channel/Delete Channel in the following popup: Similarly, for the Master 104 redundancy. Otherwise, mapping and creating points for redundant channels are the same as for non-redundant Slave 104 channels. There are some additional MDOs that are supported for redundant channels. The RedundancyGroupActiveControl (shown below) can be used to turn all redundant channels on or off. The CurrentActiveChannel MDO will display the currently active channel name or NONE if no channels are active. The MDO RedundantChannelActiveStatus contains the value of true if the redundant channel is the active channel and false otherwise: Similar MDOs exist for a Master 104 Redundant Channel. All other MDOs are identical for redundant and non-redundant Master and Slave 104 Channels and can be mapped or viewed similarly. Serial Channel 4.9.4 Parameter Description Protocol Sets the protocol for the channel. Possible values are 'S101', 'S104', 'M101', 'M103', 'M104', 'MDNP', or 'SDNP' COM Port Sets the Communication Channel. Examples: "/dev/ttyS0 ", /dev/ttyS1, /dev/ttyS2, etc Baud Rate Sets the baud rate for corresponding serial port. Parity Sets the parity for corresponding serial port. Possible values are 'none', 'even', or 'odd' Data Bits Sets the number of data bits for corresponding serial port. Possible values are '7', or '8' Stop Bits Sets the number of stop bits for corresponding serial port. Possible values are '1', or '2' Flow Control Sets the mode of the serial Channel. Possible values are: none - use no flow control. hardware - use hardware flow control. Hardware flow control is configured by the SDG as shown in the table below windows - use flow control and serial parameters (baud rate, parity, etc.) as specified with the Windows MODE command (For information on the Windows Mode command, type help mode in a Windows Command window). Offline Poll Period The period at which the sessions on the channel are polled if they are offline. This parameter only applies to serial DNP and Modbus master sessions on this channel. A value of zero (0) will disable this feature Alias Name Name for communications channel, this must be specified Add UDP/TCP Channel Dialog 4.9.5 SCADA Data Gateway| Implementers Guide ParameterDescriptionChannelProtocol TypeSets the protocol forthe channel. Possible values are 'MDNP'or'SDNP'Physical channel i.e.COM port, Remote IP / NodeNameSetsthe Communication Channel. Examples:"192.168.1.25" or"node name" TCP/IPModeSetstheconnection mode to use. Possible values are 'server', client, udpor dualendpoint.Typically Slaves are servers and Masters are clientsTCP/IP Port NumberSetstheTCP/IP port number to useLocal TCP/IPaddressOn Master-Address to bind socketto. This allows you to specify whichTCP/IP address to send as source address inTCP messages ifthere are multipleTCP/IP Addresses,for example when there are multipleNetworkInterface Cards (NICs).If"0.0.0.0" isused theTCP stack will choose which TCP/IP Addressto use. If an address that is notpresent is specified, the bind willfailand the TCPstackwillchoose which address.This address is also used for DNP Masterwhen sending UDP datagrams. Binding this address does notguarantee sending on a particular NIC. Thisis determined by theTCP/IP RoutingTable depending on the destinationTCP/IP Address. You can display this table by entering "route print" in a commandwindow. It is possibleto add manual routes to cause aparticularNIC to be used. "route add destIPAddress gateway".Enter "route ?" for more details. On Server-not currently used forlisteners. (Note: this address ISused for DNP Outstation ifconfigured for UDP ONLY) Connection timeout (ms)SetstheTCP/IP connecttimeout to use if PhysComChannel specifies an IP address. For IEC 60870-5-104 this is theT0 parameter. Notethat this parameter shouldbe set tothelowest value that worksreliably. In cases where a connection cannot be establishedthe process will block forthe period specified. Dual End Point IP PortNumberIf Dual End Point is supported alisten willbe done on thePhysComIpPort and aconnection request will besent to this portnumber when needed. Thisshould match ipPort on remote device. Normalstate is listen,connectionwill be made whenthere is data tosend. Local UDP PortLocalport for sending and receiving UDP datagrams on. If thisis setto 0, UDP will not be enabled. For DNP networking UDP should besupported. It is not needed for any ofthecurrent IEC orModbusprotocols. On Master-If this is setto 1,an unspecified available portwillbe used. On Slave -This should be chosen to match the UDPportthatthe master uses tosend Datagrammessages to. This mustnotbe 1 or 2. Destination UDP PortOn Master-if TCP and UDP is configured this specifies the destination UDP/IP port to send broadcast requests in UDPdatagrams to.if UDP ONLY is configured this specifies the destination UDP/IP port to send allrequestsin UDP datagrams to. This mustmatch the "localUDPPort"on the slave. On Slave-if TCPand UDP thisis not used. if UDP ONLY is configured this specifiesthe destination UDP/IP port to send responses to. Can be 2 indicating Page|78 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide use the src port from a UDP requestreceivedfrom master. Unsolicited UDP PortOn master-Not used. On Slave-if TCP and UDP notused. if UDPONLY is configured this specifies the destination UDP/IP port to send the initialUnsolicited Null response to.Afterreceivinga UDPrequestfrom master, destUDPPort(which) may indicate usesrc port) willbe used forall responses. This must not be 0, 1, or 2 for a slavethat supports UDP. Validate UDPAddressWhether or notto validate source address ofreceived UDP datagram. Channel Alias NameName for communicationschannel, this mustbe specified 4.10Adding Sessions The session dialog boxes are protocol specific, only the configuration parametersrequired fortheselected protocolon the Channel configuration dialogbox are displayed. For all Sessiontypes the following fields are defined here: ParameterDescriptionAbsoluteResponse TimeoutDefault application responsetimeout. Thisvalue is themaximumamount of time (in milliseconds)that willbe allowed before acommand is cancelled due to timeout.This time starts when the request is submittedand ends when the final applicationlevelresponse is received.This value can generally be overridden forspecific datapointsby the 'TO' option in the point mapping fileIncremental Response TimeoutMaximum amount of time (in milliseconds) to allow between messages from a remote device when a request is pending to thatdevice.The message neednot be a directresponse tothe pendingrequest. If no message is received from the remote device withinthis period itis assumed thedevice has terminated processing of the request and the requestis cancelleddue to anapplicationleveltimeout.This timer is restarted every time a message is receivedfrom the remote device 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |79 SCADA Data Gateway| Implementers Guide 4.10.1Master Sessions 4.10.1.1IEC 60870-5-101 ParameterDescriptionControlled Station Link AddressData link address ofslave component orremote device. EachAddress identifies a uniquesession, whichis a link layerconnection between a Master and a Slave device. Setto 0xffff(65535) forthe session to be a "broadcast session". Link ModeDatalink transmission mode. Required for each communications channel. Possible valuesare 'balanced', or'unbalanced'Link Max RetriesMaximum number of attemptsto re-transmit data link-layerframes that were not confirmed. This parameter does not applyforlink layer connections (sessions) when the GATEWAY is Page|80 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide acting as an unbalanced slaveLink Confirm TimeoutThe maximum amount of time (in milliseconds)to wait for aconfirmation offrame. For an IEC 60870-5-104 session this is the T1 parameter.This parameter does not apply forlink layerconnections (sessions) when the GATEWAYis acting as anunbalanced slaveLink Address SizeNumber of octets (bytes)in link Addressfield. A value of 0 isonly valid for sessions whose link mode is balanced. Thisparameter is only used for IEC60870-5-101master and slave sessions. Possible values are from 0 to 2Class 2 Poll DelayFor an unbalanced master communication link, the minimumdelay (in milliseconds) after sending request for class 2 data when an application layer response is not pending for thissession. Thisparameter may be used to limit the bandwidth on a shared media like Ethernet or toprevent taxingthe target device with unnecessary communication overhead.This parameter only appliesto IEC 60870-5-101 and IEC 60870-5- 103 Master SessionsClass 2 Pending DelayFor an unbalanced master communication link, the minimumdelay in milliseconds after sendingrequest for class 2 data when an application layer response is pending for thissession. This parameter may be used to limit the bandwidth on a shared media like Ethernet orto prevent taxing the target device withunnecessary communication overhead. This parameter onlyappliesto IEC 60870-5-101 and IEC 60870-5-103 MasterClass Pending CountFor an unbalanced master communication link, thetotalnumber of consecutive class 1 andclass 2request frames thatmay be sentto onedevicewhen an application layer responsemessage is pending from this device before moving on to thenext device on a multi-drop network. This parameter has no effectif only onedeviceisconfigured for a communication channel. If this parameter is setto zero, the device is stillpolled as described for parameterM870CNFG_LINK_CLASS1_POLL_CNT.This parameteronly applies to IEC 60870-5-101 and IEC 60870-5-103 MasterSessionsClass 1 Poll DelayFor an unbalanced master communication link, the minimumdelay in milliseconds aftersending requestforclass 1data when an application layer response is not pending for thissession. Thisparameter may be used to limit the bandwidth on a shared media like Ethernetor toprevent taxingthe target device with unnecessary communication overhead.This parameter only appliesto IEC 60870-5-101 and IEC 60870-5- 103 Master SessionsClass 1 Pending DelayFor an unbalanced master communication link, the minimumdelay in milliseconds aftersending requestforclass 1data when an application layer response is pending for thissession. This parameter may be used to limit the bandwidth on a shared media like Ethernet orto prevent taxing the target device withunnecessary communication overhead. This parameter onlyappliesto IEC 60870-5-101 and IEC 60870-5-103 Master 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |81 SCADA Data Gateway| Implementers Guide SessionsClass1 Poll CountFor an unbalanced master communication link, thetotalnumber of consecutive class 1 request frames that may be sentto onedevicebefore moving on to the next device on a multi- drop network (class 2is always limitedto one request frame unless an application layer response is pending). This parameter has no effectif only one device isconfigured foracommunication channel. In a multi-drop network topology, this parameter isusedto balance polling between all devices and prevent one device from capturing all of the pollingmessages. This parameteronly applies to IEC 60870-5-101 and IEC 60870-5-103 Master SessionsASDU COT SizeNumber of octets (bytes)in Cause of Transmission (COT) field of ASDU. This parameteris only used for masterandslave sessions using the IEC 60870-5-101 orIEC 60870-5-104 protocol profiles. Possiblevalues are 1 or 2ASDU Address SizeNumber of octets (bytes)in Common Address of ASDU(sector address) field. This parameteris onlyusedfor masterand slave sessions using the IEC 60870-5-101 or IEC 60870-5- 104 protocol profiles. Possible valuesare 1 or 2Originator AddressOriginator address (for2-octet COT).This parameteris onlyused formaster and slave sessions using the IEC 60870-5-101 or IEC 60870-5-104 protocol profiles. Possible values arefrom 0 to 255ASDU IOA SizeNumber of octets (bytes)in Information Object Address (pointnumber) field. This parameter is only used for master and slave sessions using the IEC 60870-5-101 or IEC 60870-5-104 protocol profiles. Possiblevalues arefrom 1 to 3 4.10.1.2IEC 60870-5-103 Page|82 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide ParameterDescriptionControlled Station LinkAddressData link address ofslave component orremote device. EachAddress identifies a uniquesession, which isa link layer connection between aMaster and a Slave device. Set to 0xffff (65535)forthesession tobea "broadcast session" Link Max RetriesMaximum number of attemptsto re-transmit data link-layer frames that were not confirmed. This parameter does not apply for link layerconnections (sessions) when the GATEWAYis acting as anunbalanced slaveLink Confirm TimeoutThe maximum amount of time (in milliseconds)to wait for aconfirmation offrame. For an IEC 60870-5-104 session this is the T1parameter. This parameter does not applyfor linklayerconnections (sessions) when the GATEWAYis acting as anunbalanced slaveClass 2 Poll DelayFor an unbalanced master communication link, the minimum delay(inmilliseconds) aftersendingrequest for class 2data whenanapplication layer response is notpending forthis session. Thisparameter may be used tolimit the bandwidth ona shared media like Ethernet orto preventtaxing the target device with unnecessarycommunication overhead. This parameter only applies to IEC 60870- 5-101 and IEC 60870-5-103 Master SessionsClass 2 Pending DelayFor an unbalanced master communication link, the minimum delay in 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |83 SCADA Data Gateway| Implementers Guide milliseconds after sending request for class 2 datawhen an application layer response is pending forthis session.This parametermay be used to limit the bandwidth on ashared medialike Ethernetor toprevent taxing the target device with unnecessarycommunication overhead. This parameter only appliesto IEC 60870- 5-101 and IEC 60870-5-103 MasterClass Pending CountFor an unbalanced master communication link, thetotal number ofconsecutive class 1andclass 2 request frames that may be sentto onedevice whenan applicationlayer response message ispending fromthis devicebefore moving on to the nextdevice on a multi-drop network.This parameter has no effect if only one device is configured for a communication channel. If this parameter isset to zero, the device is still polled as describedfor parameterM870CNFG_LINK_CLASS1_POLL_CNT.This parameter onlyappliesto IEC 60870-5-101 and IEC 60870-5-103Master sessionsClass 1 Poll DelayFor an unbalanced master communication link, the minimum delay in milliseconds after sending request for class 1 data when an application layer response is notpending forthis session. Thisparameter may be used tolimit the bandwidth ona shared media like Ethernet orto preventtaxing the target device with unnecessarycommunication overhead. This parameter only applies to IEC 60870- 5-101 and IEC 60870-5-103 Master SessionsClass 1 Pending DelayFor an unbalanced master communication link, the minimum delay in milliseconds after sending request for class 1 data when an application layer response is pending forthis session.This parametermay be used to limit the bandwidth on ashared medialike Ethernetor toprevent taxing the target device with unnecessarycommunication overhead. This parameter only applies to IEC 60870- 5-101 and IEC 60870-5-103 Master SessionsClass1 Poll CountFor an unbalanced master communication link, thetotal number ofconsecutive class 1requestframes that may be senttoone device before moving on to the next device on a multi-drop network (class 2 is always limited to one requestframe unless anapplication layerresponse is pending).This parameter has no effectif only one deviceis configured for a communication channel. In a multi-drop networktopology, this parameterisused to balance polling between alldevices andprevent one device from capturing all ofthe pollingmessages. This parameteronly applies toIEC 60870-5-101and IEC 60870-5-103Master Sessions 4.10.1.3IEC 60870-5-104 Page|84 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide ParameterDescriptionT1 -Link AcknowledgeTimeoutThe maximum amount of time (in milliseconds)to wait for aconfirmation of frameT2 Send AcknowledgeDelayMaximum time (in milliseconds)to waitto send an AcknowledgeframeT3 Test Frame IntervalTime (in milliseconds) forthe Test Frame intervalK TransmittedMaximum numberof unacknowledged transmit framesW ReceivedMaximum number of unacknowledged receivedframesOriginator AddressOriginator address (for2-octet COT). Possible values are from 0to 255 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |85 SCADA Data Gateway| Implementers Guide 4.10.1.4Master DNP3 ParameterDescriptionSession NameOptional parameter for setting the Session Name to a custom valueSDG Local Link AddressData linkaddress of the localdeviceRemote Device LinkAddressData linkaddress of the remote deviceLink Confirm ModeRequest the remote device to senda data link layerconfirm of thelastframe sent.This setting is independent of whetherthe remote device willrequire this device tosend a data link confirm to frames itreceivesAuto Request ModeEach bit enables (1) or disables (0) an automatic request.This parameter is only used formaster orslave sessions using the DNP3protocolPossible values are: 0x001clear restart upon RESTART IIN0x002integrity poll upon RESTART IIN Page|86 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide 0x004integrity poll upon LOCAL IIN on-then-off0x008integrity poll upontimeout ofprevious request0x010integrity poll upon BUFFER OVERFLOW IIN0x020use delay measurement within time synchronization0x040perform time synchronization upon NEEDTIMEIIN0x080requestclass 1, 2, or 3 dataupon CLASS 1, 2, or 3 IINs0x100enabled unsolicited messaging initial unsolicited0x200disableunsolicited messages upon restart and upon reception of unexpected unsolicited message0x400Enable/Disable automatic generation of application layerconfirmationsEnableUnsolicited Event ClassCreate from DeviceDNP3 SecureAuthenticationEnablesDNP3Secure Authentication for this session. Possiblevalues are 0, 1, 'FALSE', or 'TRUE'. If TRUE, the DNP3 Secure Authentication Settings dialog screen is displayed. 4.10.1.4.1DNP3 Secure Authentication Settings Pressing thechevron next to Authentication Configuration on the previousdialogshows thefollowing. ParameterDescriptionOut Station NameThe Outstation Name of this dnp session. This must beconfigured to match on both the master and theoutstation. 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |87 SCADA Data Gateway| Implementers Guide HMAC AlgorithmHMAC algorithm to be used in challenges. Reply Timeout (ms)How long to wait for any authentication reply. Key Change Interval (ms)For Master: Session key interval.Whentime since lastkey change reaches this value, session keys willbe updated. For systems that communicate infrequently, this may be set to zero, using only themaxKeyChangeCountto determine when to updatekeys. For Slave: Expected session key interval and count. Whenthis amount of time elapses or this quantity ofAuthentication messages are sent or received,the session keys forthis user will be invalidated. Intervaland count should be 2 times the master key changeinterval and count. For systems that communicate infrequently, DNPAuthKeyChangeIntervalmay be setto zero, using only theDNPAuthMaxKeyChangeCountto determine when keys should beconsidered old and should beinvalidated. Max Key Change CountSession Authentication ASDU count since last keychange, When this number of authentication ASDUs istransmitted orreceivedsince the last key change, session keys will be updated. Aggressive ModeEnable Aggressive mode. SecAuth v5Use DNP Secure Authentication Version 5, otherwiseVersion 2 will be usedExtra DiagnosticsOutputextra diagnostics to protocol analyzer. Max Error CountNumber of error messages to be sent before disablingerror message transmission. UserKey(hex 16, 24, or32bytes) UserKey (must be 16, 24, or 32 hex values). For eachkey there must be unique user numberDNPAuthUserNumber. User NumberUser Number: Configuration foreach userspecification says defaultuser number is 1; configures it as first user inarray.Add any other user numbers. For each user number inthe INI file there must be a DNPAuthUserKey. UserNameA globally Unique identifier that will have a usernumber, role, and an update key assigned About users: When DNP Secure Authentication Version 5 is configured, one useris created with the default nameCommon. This userisassigned the User Number 1 and the Single User Role. Otheruserscan be added byupdatingthe tmwgtway.ini file. Theseparametersappearunderthe DNP3 Secure Authentication Users heading and start with DNPAuthUser*. Although the gateway supports multipleusers with varied roles, theiruse in the gateway is discouraged. Only inextremely rare cases shouldthe gateway be configured for a roleother than Single User.This is because the gateways role will almost always require thatit have permissionto read and write all points.Additionally, multi-user has been deprecated in SAv5. 4.10.1.4.2SettingCommandand ControlModes for Binary Output in DNP3 Master Sessions Page|88 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide The SDGsupports severalparameters in the INI file that are usedto setthe command andcontrol modes forbinary output points in Master DNP3 sessions. The OBJ12_use2pass parameter is used toset the control mode. If this parameter is setto TRUE,thentwo-pass control operations will be usedforall initiated "Binary Output" controls. If this parameterisset toFALSE, then single-passcontroloperations will be used for all initiated "Binary Output" controls. The OBJ12_forceDflt parameter is used todetermine ifthe control mode specified by the OBJ12_use2pass parameter will be usedorifthe control mode will be taken from upstream devices. If this parameter is set to TRUE, thenthe default controlcode will always be usedinstead of any control codes forBinary Output controls received from upstream devices. If this parameter is set toFALSE, then the controlcodefor Binary Outputcontrolsreceived from upstream devices willbe used if possible. The OBJ12_dfltCmdMode parameteris usedto setthe command mode. Possible values forthis parameterare LATCH,PULSE, and PAIRED. The default is LATCH. The PulseLength parameter is used to set the length (in milliseconds) of the pulse usedfor allinitiated Binary Output controls. Pulse Length is used when a pulse, trip or close control code isspecified. Thedefault valueis 100 (milliseconds). The values specified for these parameters intheSDGinitialization file apply to all points and devices configured for use with theSDG. For certain pointsand/or devices thatrequire differentcontroland command modes from those specified in the initialization file, thesedefault values can be overridden using the point mapping file.See Section 4.35.1.3Table of Control Options and the Meaningon page215for more information. 4.10.1.5Modbus Master ParameterDescriptionControlled Station SlaveAddressData link address ofremote device. Each Address identifies a unique session, which is alink layer connection between aMaster anda Slave device 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |89 SCADA Data Gateway| Implementers Guide 4.10.2Slave Sessions 4.10.3IEC 60870-5-101 ParameterDescriptionSDG Link AddressData linkaddress of slavecomponentLink ModeDatalink transmission mode. Required for each communicationschannel. Possible values are 'balanced', or 'unbalanced'Link Max RetriesMaximum number of attemptsto re-transmit data link-layer frames that were not confirmed. This parameter does not apply for linklayer connections (sessions) whenthe GATEWAY is acting as anunbalanced slaveLink Confirm TimeoutMaximum time to waitforconfirmation offrame. This parameterdoes not apply for link layer connections(sessions) when the GATEWAY is acting as an unbalanced slaveLink Address SizeNumber of octets (bytes)in link Addressfield. A value of 0 is onlyvalid for sessions whoselinkmode is balancedASDU Address SizeNumber of octets (bytes)in Common Address of ASDU (sectoraddress) fieldASDU COT SizeNumber of octets (bytes)in Cause OfTransmission (COT)fieldofASDUASDU IOA SizeNumber of octets (bytes)in Information Object Address (pointnumber) field Page|90 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide 4.10.4IEC 60870-5-104 ParameterDescriptionT1 -Link AcknowledgeTimeoutThe maximum amount of time (in milliseconds)to wait for aconfirmation of frameT2 Send Acknowledge DelayMaximum time (in milliseconds)to waitto send an AcknowledgeframeT3 Test Frame IntervalTime (in milliseconds)for the TestFrame intervalK TransmittedMaximum number of unacknowledged transmit framesW ReceivedMaximum number of unacknowledged receivedframesNumber of octets (bytes)inCause Of TransmissionNumber of octets (bytes)in Cause OfTransmission(COT) fieldof ASDU. This parameteris only used for masterandslave sessions using the IEC 60870-5-101 orIEC 60870-5-104 protocolprofiles. Originator addressOriginator address (for2-octet COT).This parameter is only usedfor master sessions using the IEC 60870-5-101 or IEC 60870-5- 104 protocol profiles. 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |91 SCADA Data Gateway| Implementers Guide 4.10.5Slave DNP ParameterDescriptionSession NameOptional parameter for setting the Session Name to a customvalueSDG Local Link AddressData linkaddress of the localdeviceRemote Device Link AddressData link address ofremote deviceLink Confirm ModeRequest the remote device to senda data link layerconfirm of the last frame sent. This setting is independent of whether theremotedevice willrequire this device tosend a data link confirm to frames it receivesIncrementalResponse TimeoutEnable DNP3 Secure AuthenticationIf TRUE, the DNP3 Secure Authentication Settings dialog screen is displayed. Page|92 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide 4.10.6Modbus Slave ParameterDescriptionSlave AddressData linkaddress of localdevice. Each Addressidentifies a unique session, which is a link layer connection between a Masterand a Slave device 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |93 SCADA Data Gateway| Implementers Guide 4.11Adding Sectors ParameterDescriptionSector AddressASDUaddress of the sectorEOI Action MaskEach bit enables(1) ordisables(0) an automatic request to be sent as a result of receiving aninitialization message from a slave device. This parameter is only used for IEC60870-5master sessionsPossible values are: 0x1request a general interrogation0x2request a clock synchronization0x4requesta counter interrogationOnlineAction MaskEach bit enables(1) or disables(0) an automatic request to be sent as a result of a slave device coming online.This parameter is only usedfor IEC60870-5master sessionsPossible values are: 0x1request a general interrogation0x2requesta clock synchronization0x4requesta counter interrogationBlocking Action MaskEach bit enables (1) or disables (0) an automatic request to be sent as a result of a slave deviceleaving blockingmode. This parameter isonly used for IEC60870-5-103master sessionsPossible values are: 0x1request a general interrogation0x2requesta clock synchronization0x4requesta counter interrogationEnable Secure AuthenticationEnables Secure Authentication forthesector Configuration for Secure Authentication: Pressing thechevron next to Authentication Configuration on the previousdialogshows thefollowing. Page|94 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide ParameterDescriptionHMAC AlgorithmHMAC algorithm to be used in challenges. Reply Timeout (ms)How long to wait for any authentication reply. Key Change Interval (ms)For Master: Session key interval.Whentime since last key change reaches this value, session keys willbe updated. Forsystems that communicateinfrequently, this may be set to zero, using only themaxKeyChangeCountto determine when to updatekeys. ForSlave: Expected session key interval and count. Whenthis amount of time elapses or this quantity ofAuthentication messages are sent or received,the session keys forthis user will be invalidated. Interval and count should be 2 times the master keychange interval andcount.For systems thatcommunicate infrequently, I14AuthKeyChangeIntervalmay be setto zero, usingonly theI14AuthMaxKeyChangeCountto determinewhen keys should be considered old and should beinvalidated. Max Key Change CountSession Authentication ASDU count since last keychange, When this number of authentication ASDUs is transmitted orreceived since the last key change, session keys will be updated. Key Status Challenge DataLengthLength ofrandom challenge data to send in challenge request. (4-64 bytes). 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |95 SCADA Data Gateway| Implementers Guide Security StatisticsIOAExpected starting IOA for Security Statistics (SITTCpoints) received fromControlled Station (slave). Extra DiagnosticsOutputextra diagnostics to protocol analyzer. Max Error CountNumber oferror messages to be sent before disabling error message transmission. UserKey (hex 32 bytes)UserKey (must 32bytes in length). Foreach key there must be unique user numberI14AuthUserNumber. User NumberUser Number: Configuration foreach userspecification says defaultuser number is 1; configures itas firstuser inarray. Add any otherusernumbers. For each user number in the INI filethere must be a I14AuthUserKey. UserNameA globally Unique identifier that will have a usernumber, role, and an update key assigned Page|96 1994 -2023Triangle MicroWorks, Inc. All rights reserved. 4.12 Adding Data Types A Data Type is a container for MDOs or SDOs. After a sector or session (for DNP and Modbus) is specified a Data Type can be added to it. The list of available types varies depending on the protocol. To add additional data types to the sector or session, right-click on the sector/session and select Add Data Type. 4.13 Action Masks Action mask and action periods are used to control polling/event update behavior for the Modbus, 101, 103, 104 and DNP master protocols. These are stored as MDOs under the channel, session or sector depending on their purpose. The mask is a set of bits that control how the update occurs, and the period is used to specify how often. IEC Action Mask 4.13.1 The IEC Action Mask Editor is used to define the mask and period of each of the IEC Action Masks. It is also used to define the mask for the IECActionNow predefined point. It is accessed by right-clicking on one of the IEC action mask predefined points under an IEC sector, in the configuration window, and then selecting Edit IEC Action: The retry failed transaction part of this dialog screen is used to configure a retry of a timed out transaction, the failed transaction will be retried for Count times at an interval of Period (ms). Once the transaction succeeds again the retry count is reset to 0. This will display the IEC Action Mask Editor dialog screen: DNP Action Mask 4.13.2 The DNP Action Mask Editor is used to define the mask and period of each of the DNP Action Masks. It is also used to define the mask for the DNPSessionActionNow predefined point. It is accessed by right- clicking on one of the DNP action mask pre-defined points, in the configuration window under a DNP session, and selecting Edit DNP Action: SCADA Data Gateway| Implementers Guide This will display the DNPAction Mask Editor dialog screen: The DNPSessionActionNowhas a completion status MDO associated with itDNPSessionActionNowStatus. This MDOis used to display the current statusoftheDNPSessionActionNowrequest.The following describes the meaning of the values: Status ValueDescription0 (success)Indicates the request has completed successfully1 (intermediate)Indicates a response was received but the requested command is not yetcomplete.This could mean the response is part of a multi-fragment responseand did not have the FINAL bitset. Orthis could be a requestsuchas a selectoperate that requires multiple requests and responses. 2 (failure)Indicates that the transmission ofthe request failed3 (mismatch)The response to a selectoranexecute did not echo therequest. 4 (statuscode)Theresponse toa select or anexecute echoed the request, except the statuscode was differentindicating a failure. 5 (iin)The response to the request had IIN bits setindicating the command failed. 6 (timeout)This indicates that the requesthas timed out. Thiscould eitherbe an incremental timeout indicatingwe receivedno link layer frame from the device in the specifiedtime, or anapplication response timeout indicating this particular request did not complete in the specified time. 7 (canceled)This indicates either thatthe user askedthat the request be canceled bycallingdnpchnl_cancelFragment or that a second duplicaterequest hasbeen made and therefore this first one is canceled. 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |99 Typically this can be used to indicate the DNPSessionActionNow has completed and a reason why. This can also be used to sequence a set of commands/requests on a multi-drop connection. Modbus Read Action Mask 4.13.3 The Modbus Read Action Mask Editor is used to define the mask and period of each of the Modbus Action Masks. It is also used to define the mask for the MBActionNow predefined point. It is accessed by right- clicking on one of the Modbus action mask pre-defined points, in the configuration window under a Modbus session, and selecting Edit Modbus Action: This will display the Modbus Action Mask Editor dialog: All data specified in the database will be read at the period specified by the mask period. Transactions will be created based on an optimization algorithm and issued to a slave device. Modbus Write Action Mask 4.13.4 The SDG also has periodic and single write Actions as seen above. Note the indices entered must already exist. Modbus Master Dual Register 4.13.5 The SDG supports Master side Dual Register MDOs to concatenate two 16 bit Holding Registers into a 32 bit integer or floating point number. These points can be mapped and used in equations as any other point. Modbus Slave Dual Register 4.13.6 To create a Modbus Slave 32 bit dual register from two 16 bit Holding Registers, drag the source point into the holding register data type as usual to create a Slave Data Object and mapping. The following dialog will appear: Select the options button and the following window will appear: Now select desired bit order for the registers and the desired type (float32 or int32). You may also choose to make the value a signed value or not. When the bit order and register type are selected, the SDG will automatically create two consecutive Holding Registers that are joined together to represent the 32 bit dual register as seen below, where P2 and P3 are joined together: 4.14 Multi Drop Polling strategies for DNP and Modbus When the SCADA Data Gateway is used in a Multi Drop network configuration the following suggestions may be useful for improved polling of data. This can apply to a mixture of polling and unsolicited reporting when its desired to receive periodic updates based on polling, with preemptive updates based on unsolicited reporting when events occur at a substation. The default polling algorithm in the SCADA Data Gateway can result in many collisions when substation(s) begin sending unsolicited responses, potentially resulting in less data throughput during critical times. These strategies describe changes to the polling algorithm of the SCADA Data Gateway in order to support a more desired operation. In addition, this provides a more intuitive polling configuration for all multi-drop channels, which will simplify configurations using multi-drop connections. This only applies to Modbus and DNP3, as multi drop polling is handled differently in the IEC 60870-5 family of protocols. In particular, the IEC 60870-5 family allows multiple requests to be outstanding at any one time. Channel ActionMask 4.14.1 The SCADA Data Gateway has a set of Action Masks on each session. The ActionMasks are used to schedule various activities. For example, in DNP3, the Action Masks are used to schedule Event Reads, Integrity Polls, etc. For each session, the SDG issues the specified commands at the specified interval. A separate group of ActionMasks is provided for each protocol family: DNPActionMask, IECActionMasks, and MBActionMasks. Within each family, there are five scheduling action masks (e.g., DNPActionMask0, DNPActionMask1, DNPActionMask2, DNPActionMask3, and DPActionMask4). In addition, there is a current action Action Mask (e.g., DNPSessionActionNow) that can be used to issue a command immediately. There is also an ActionMaskOnlineControl for each set of action masks (per device). This control can be mapped and value set to turn the ActionMasks On/Off based on the Boolean value of this control. For example if the ActionMaskOnlineControl is set to off then none of the action masks will fire regardless of the connectivity status of the Device. In general, the ActionMaskOnlineControl needs to be on and connectivity needs to be established for the Actions to fire. With the Channel-based Action Masks, the specified commands will be issued according to the specified schedule, unless the Unsolicited HoldOff timer is configured (see below). Commands will be issued on only a single session at a time. Sessions will be selected on a round-robin schedule. For example, when the Action Mask period timer expires, the specified action will be issued on Session 0 for that channel. The next time the Action Mask period timer expires, the action will be issued on Session 1. The next time the Action Mask period timer expires, the action will be issued on Session 2. When the Action Mask period timer expires after issuing a command for the last session, the next command will be issued on the first session (Session 0) again. Users should configure either the Session-based ActionMasks, or the Channel-based Action Masks, but not both. Five Channel ActionMasks are available (ActionMask0 through ActionMask5). Prioritization of Unsolicited Responses 4.14.2 When using a combination of Unsolicited Responses, polled data, and commands. These messages need to be prioritized in order to minimize collisions on the multi-drop network. The preferred priority (from Highest to Lowest) is: User Commands Unsolicited Responses Polling User commands should have the highest priority. Note that it is not possible to stop the receipt of unsolicited requests; however, when the SDG receives a command (e.g., from the external OPC Client), it will immediately queue the request. When the channel is clear (i.e., no responses are being received), the SDG should send the request. Polling is the lowest priority. Polls should be delayed until (1) there are no outstanding requests queued; and (2) no unsolicited responses have been received for a specified amount of time. This process should run independently on each channel. Within each channel, polling of sessions should be per the Channel ActionMask described above. 4.14.2.1 Example 1. Assume COM15 has been configured to conduct an Event Poll every 60 seconds. If there are no user commands (request messages) queued, and the channel has been quiet for the specified amount of time, then these polls will be conducted. 4.14.2.2 Example 2. Suppose the SDG is configured to conduct an Event Poll every 10 seconds, and it starts polling at 09:00:00. Lets say it completes a poll scheduled for 09:00:00, and receives an unsolicited response at 09:01:05. The SDG would wait an additional amount of time (e.g., 10 seconds) before resuming polling at 09:01:15. To accommodate this, a HoldOff timer is available to the session. This timer will be restarted each time a request is sent or an unsolicited response is processed on that channel. When the channel ActionMask timer expires, if the hold off timer is still running, then the SDG will delay the specified action by the amount of time left on the hold off timer. At the end of the delay, the SDG will again check the hold off timer; if it is still running (as would be the case if a new unsolicited response was received), then the SDG will again delay the action by the amount of time left on the hold off timer. The new Channel-based action timer will run continuously. When the Action Period timer expires, the SDG will check to see if the previous request is still in progress or if the HoldOff timer is not running. If so, then no request will be queued. The next attempt to queue a request will be when the Action Period timer expires again. Otherwise, the specified action will be queued for the next session. Since the Action Period runs continually, the specified actions will occur at regular time intervals (e.g.,on multiples of10 seconds). Note that this algorithm means that it is possible to completely block ActionMask activities if unsolicited responses are received at a rate faster than the hold off timer interval. It is up to the customer to recognize this fact and adjust their configuration or system operation accordingly. The offline poll period currently used on session-based polling should not be used. This period allows sessions that are not online to be polled at a slower rate. This feature is important with the session-based polling scheme, because offline sessions complicate timeout calculations. With the channel-based polling, only the Application Timeout is required, as requests will not be queued at the Application layer if there are prior, incomplete commands. An offline session does not significantly affect polling the offline sessions poll will timeout, and the next session will be polled on the next Action Period interval. [See Note1]. Disabling Channels and Session 4.14.3 It is desirable to have a mechanism to independently and completely disable a channel or a session. When so disabled, the configuration settings will be remembered, but otherwise the SDG acts as if the channel/session does not exist. This feature is very helpful during testing and deployment. For example, using this method, an unused channel may be disabled, and the SDG will not issue Physical Layer Error: Error opening channel messages. Similar, if a session is disabled, it will be omitted from the round robin polling scheme on that channel. Supporting INI Configuration Parameters and MDOs 4.14.4 To support this, the following configuration parameters are required. 4.14.4.1 DNPChannelResponseTimeout This timer exists on the channel. DNP3 allows a single outstanding request on each channel. This timer is started when a request is sent, is canceled when the response is received. 4.14.4.2 ApplDNPAbsRespTimeout This time starts when a request is put in the queue. Multiple (non-identical) requests may be on the queue at any one time. These requests will only be sent one at a time, as prescribed by the DNP3 specification. This timer ensures that requests cannot be stuck in the queue. It should be set larger than (n * DNPChannelResponseTimout), where n is the maximum number of requests that are anticipated to be queued. 4.14.4.3 PhysOfflinePollPeriod This existing parameter determines the period at which sessions are polled if they are offline. This parameter is typically used to slow polling of offline devices so that the repeated timeouts do not adversely affect other sessions. The offline devices are periodically polled (at this reduced rate) so that the SDG can determine when they come back online. 4.14.4.4 MDNPunsolHoldOffTime This timer starts when an unsolicited response is processed. This timer must expire before a new request may be sent on the channel. 4.14.4.5 ChannelActiveControl - MDO When this parameter is set, the SDG will remember the configuration parameters, but otherwise will treat the channel as if it is not present. 4.14.4.6 SessionActiveControl - MDO When this parameter is set, the SDG will remember the configuration parameters for the session, but otherwise will treat the session as if it is not present. Notes: 1. If an offline poll delay is needed, it can be implemented as a counter when a session is offline, it will be skipped for n cycles. 4.15 IEC 61850 Client To add an IEC 61850 Client to your configuration, right-click on the Gateway node and select Add IEC 61850 Client: This will display the in the Add IEC 61850 Client dialog box: SCADA Data Gateway| Implementers Guide Graphical user interface, table Description automatically generated ParameterDescriptionClient NameThe name of the client. Server IP AddressThe TCP/IPaddressof the server to connectto. Server TCP PortTheTCP/IP port of the server to connect to. Client IP AddressSpecifiesthe IP address of this IEC 61850 Client. Thisisuseful toselect a specificnetwork adapterto useto connect to a server. AE Invoke IDASCE AE invokeID. Value from 0 to 65536. Appliesto Client And Server. AE QualifierASCE AE qualifier.Valuefrom 0 to 65536. Applies to Client AndServer. AP Invoke IDInvoke IDASCE AP invokeID. Valuefrom 0 to 65536. Applies to Client And ServerApplication IDACSE AP title of the Server. Applies to Client And Server. Presentation SelectorPresentation selector of the Server. Applies to Client And Server. Session SelectorSession selector of the Server. Applies to Client And Server. Transport SelectorTransportSelector of the Server. Applies to Client And Server. Page|110 1994 -2023Triangle MicroWorks, Inc. All rights reserved. GOOSE Adapter The adapter to use for GOOSE communications. SCL File The SCL/ICD/CID file used to configure the client from. Use SCL File To allow switching between and on-line configuration or an off-line configuration. IED Name The IED Name of the IED in the SCL file to load if using a SCL file. . Applies to Client And Server. Time Zone offset in minutes Integer value of timezone offset. Reconnect Re-try Count How many times to try to reconnect (0 is forever) if cannot connect. If this is set to a nonzero value it can be reset by right clicking on the client node in the main tree and selecting Reset Connect Retry Count from the popup menu. Reconnect Time (msecs) How often to attempt re-connection if a connection fails. Connect Timeout (msecs) The time to wait for a valid connection. Password The password for authentication (if checked) Certificate Whether to use 62351 security (strong security) If Security Setup is selected, the following dialog will appear for setting up the certificate information for 62351 (below). All of the parameters need to be filled in except Certificate Authority Revocation List File and Directory to Certificate Authority. Graphical user interface, application Description automatically generated At a minimum, configure the Name and Server IP Address. If the SDG and IEC 61850 server are running on the same machine, use the default loopback address provided of 127.0.0.1. Click the OK button to continue to add the client to the configuration. If the client is not connected the SDG will display the client name in the main tree prepended with **. Once the client is connected, the ** will go away. This makes it easy to visually see when a client is connected or not. When a client is added the following MDOs are added to indicate status and statistics for that 61850 client. ChannelActiveControl If the channel is active and ready to connect ConnectedToServer On means connected, Off means not connected Disconnect Count NumDisconnects The number of time the client has disconnected from the server NumGooseDrops The number of dropped GOOSE messages NumGooseEvents The number of GOOSE control block updates received NumGooseTimeOuts The number of GOOSE timeouts that have occurred NumIntegrity The number of Integrity updates from the server NumReadResponse The number of reads completed from the server NumReportMessages - The number of report control block updates received ServerOK If true the client is connected to the server TotalBufferOverflows The number of buffer overflows that have occurred in buffered report control blocks Adding an IEC 61850 Report Control Block 4.15.1 To add an IEC 61850 Report Control Block to your configuration, right-click on the 61850Client node and select Add 61850 Report Control Block: Graphical user interface, text, application, chat or text message Description automatically generated This will display the Add 61850 Report Control Block dialog screen: A picture containing application Description automatically generated Select any one of the Report Control Blocks from the Report list box on the Add 61850 Report Control Block dialog box. The corresponding data set and its contents will be shown in the list box below it. Next, click the Ok button to add the report control block. Note that you can create and add your own Data Set to a Report Control block by using the Create new Dataset and assign to selected Control Block button as shown above (select the RCB prior to creating the Data Set). If the Report Control Block is already in use by a client, choose a different report control block or use a Polled Data Set (right click on the IEC 61850 Client node in the tree again and select Add a 61850 Polled Data Set) The Report Control Block is now added and can be viewed by clicking on the 61850Client node. The following status MDOs are added to the client when the Report Control Block is added: RCBName_GI use this MDO to initiate a General Interrogation on the RCB RCBName_PURGE use this MDO to purge the RCB clearing the buffered data Writing any value to these MDOs will initiate the operation. Right clicking on the RCB and selecting Edit RCB button will allow you to change the trigger options and interrogation period of the Report Control Block as shown below. Please see the IEC 61850 specification to understand what each of these options mean. Adding an IEC 61850 Data Attribute from the Report Control Block 4.15.2 Once a Report Control Block is defined we can start adding points (Data Attributes) as MDOs to the SDG. To add an IEC 61850 Item, right-click on the Report Control Block and select Add IEC 61850 Item: Data Attributes can also be added from the keyboard by using the down arrow key to scroll through the list and the enter key to activate the Add Item(s) button. This will display the Add 61850 Item dialog screen: Next, select item(s) from the list. Use the filters at the bottom of the dialog box to limit the number of Data Attributes Shown. Click the Ok button when all items have been added. The Data Attributes now appear in the tree under the Report Control Block node in the list view: Note that the Data Attributes are treated in the SDG as any other Master Data Objects (MDOs). So they can be mapped to other items and are available through the SDGs OPC Server. Adding an IEC 61850 Control Point 4.15.3 To add an IEC 61850 Control Point to your configuration, create an IEC 61850 Client as described above and then right-click on the 61850Client node and select Add IEC 61850 Control Point Set: This will prompt for a name of the control point set. Once the Control Point Set is added select the Add IEC 61850 Control Point menu item on the Control Point Set to add a control point to add a control point. Items that have a 'Command Kind' of Status Only are not writable. This will display the Add a 61850 Command Point dialog screen: Next, select an item from the list by clicking on the desired Item Name. The Item Name should now be displayed in the Tag Name textbox. Click OK to add the IEC 61850 Command Point. The 61850 Command Point now appears in the tree under the 61850Client node in the control point set: 4.15.3.1 Changing an IEC 61850 Command Point To change the value of the command point, right-click on the point and select Change Value of MDO: Note that this command point can be treated as any other command MDO in the Gateway. It can be written to just like any other writable MDO. Other mechanisms to obtain Data from a 61850 Server 4.15.4 As the configuration of the following data gathering methods are very similar to the Report Control Block. We will not waste space with a detailed explanation of how to configure these. Instead we will simply describe their use and operation. It is recommended that the data in the below mechanisms does not overlap with other data. Note that the last mechanism to update the value in the MDO is used as the value of the MDO if the data does overlap. 4.15.4.1 GOOSE Control Blocks In order to use GOOSE Control Blocks a GOOSE adapter must be specified for the client. A GOOSE Control block has a Data Set associated with it that describes the data that is updated by the Control Block. The data attributes in the dataset can be mapped to SDG MDOs and will be updated as the GCB performs updates. GOOSE Control Blocks are typically used for providing high speed control and interlocking between equipment in a substation. As a result they are not as well suited to typical SCADA Process data as a Report Control Block would be. 4.15.4.2 Polled Data Sets Polled Data Sets are a mechanism in the SDG to read data in a specific data set from a server on a periodic basis. The SDG is responsible for issuing a read on the data set in the server to obtain the data. 4.15.4.3 Polled Point Sets The last mechanism to obtain data from a server is a Polled Point Set. This mechanism is very similar to a Polled Data Set except that it does not require a Data Set. This mechanism basically issues a read on all the points in the Polled Point set at the period specified. Note that with this mechanism there is a MDO that can be used to trigger the read operation in addition to the timer. When points are read requests are issued to the server. The outstanding request count is displayed in the RequestQSize MDO of the client. In a situation where more requests are issued than responses received (i.e. the server cannot keep up with the request rate) this MDO will count up to the INI file parameter I61850ClientRequestQMaxSize value which has a default of 50. Should this situation occur, it is recommended that the poll rate of the polled point set is set to a longer value and the I61850ClientRequestQMaxSize value is increased. Alternatively setting the INI file parameter I61850PolledPointSetReadAsArray to true can reduce the amount of traffic on the network and improve this situation. IEC 61400-25 Alarms (Senvion/Re Power Turbine alarms, proposed addition 4.15.5to the IEC 61400-25 specification) Senvion/Re Power turbines have 2 alarm arrays that contain the current alarm and event status of a turbine. There is a requirement to transform these array elements into individual status points. The alarms on the Senvion/Re Power turbine side are essentially delivered in a buffer of 50 alarm statuses and 50 alarm events which come and go as alarms occur. The elements in this buffer need to be translated into equivalent MDOs representing the Status Codes in the buffers. This functionality will allow the end user to create new Alarm Master Data Objects (MDOs) for the Senvion/Re Power REguard Control B Active alarm status arrays. For each Alarm MDO, the user will specify the REguard Status Code that represents the alarm. The REguard active alarm status and event arrays will be processed in such a way that as the Status Code values come and go from the array, the data changes will be generated on the associated Alarm MDOs. The functionality of this feature will be structured according to the following outline: To add an alarm array, right-click on the IEC 61850 Client node and select Add IEC 61400-25 Alarms to display the Add IEC 61400-25 Alarms dialog screen. From this dialog screen, the user can select an alarm status array and alarm event array from the list of available alarm arrays (i.e. WALM1.AlmSt.actSt), provide a unique name for the array, and add a new node to the configuration based on the user-defined name for the array. To add alarm items (i.e. Alarm MDOs), right-click on the new node and select Add Alarms to display the Add Alarm MDO dialog screen. This dialog screen will display the Alarm MDO item properties which allow you to specify the Status Code, Name, and Options as well as other standard SDG MDO options. In addition polling and report control blocks will be setup for the status and event arrays respectively. The following section provides a detailed overview for this configuration within the SCADA Data Gateway. 4.15.5.1 Configuring the SDG to use IEC 61400-25 Alarms In order to add alarms to the SDG configuration, an IEC 61580 Client must already exist in the SDG. In addition the INI file parameter I61850Enable61400AlarmProcessing must be set to TRUE for each 61850 client that will be processing alarms. Note that this INI parameter is FALSE by default and must therefore must be manually set to true if needed. Once the IEC 61850 Client node has been added to the configuration, the next step is to add the IEC 61400-25 Alarms node. Figure 9 IEC 61850 Client in the SDG 4.15.5.2 Adding the IEC 61400-25 Alarms Node To add the IEC 61400-25 Alarms node, right-click on the IEC 61850 Client node and select Add IEC 61400 Alarm Node: Figure 10 Add IEC 61400 Alarm Node Menu Item This will display the Add IEC 61400-25 Alarms dialog screen: Figure 11 Add IEC 61400 Alarm Node dialog screen Clicking the chevron button will display the Alarms Arrays list: Figure 12 Find Alarms Array dialog screen This list allows you to select the desired array from the list. To add an array, simply select it from the list and then click OK. Note, each array must be added to the configuration separately and given a unique name. This will create the alarms node underneath the IEC 61850 Client node based on the name that was provided on the Add IEC 61400 Alarm Node dialog screen: Figure 13 Alarms Array Node 4.15.5.3 Adding the IEC 61400-25 Alarms To add an IEC 61400-25 alarm, right-click on the Alarms Array node and select Add Alarm MDO: Figure 14 Add Alarm Context Menu This will display the Alarm MDO dialog screen which allows you to define the Status Code, Name, and Options for the alarm: Figure 15 Add Alarm MDO dialog screen After the MDOs are added to the SDG configuration, they will be displayed in the GUI as shown below: Figure 16 Configured Alarm MDOs Note that these alarm MDOs can be treated as any other MDO in the SDG as related to mapping, equations, etc. 4.15.5.4 Alarm Processing As changes in the elements of the selected status array (i.e., WALM1.AlmSt.actSt) or event array (i.e., WALM1.AlmEvl.actSt) are received by SDG, the SDG will process these changes and generate change events (set/clear/generate) on the MDOs. These changes will then propagate to SDOs mapped to the MDO. For example, if a change in the array occurs for status code 201 (i.e. an element in the array has the value of 201), then the MDO with the Status Code of 201 will become TRUE. In the next update of the array, if the element with status code 201 is no longer available, the MDO with Status Code 201 will be set to FALSE. In addition, when the MDO is set to TRUE, the timestamp of the array status code will be used. When the status code is no longer in the array, the timestamp of the updated array (i.e. when the change of the array was received) will be used for the timestamp of the Alarm MDO. As a result, there will be one MDO in the SDG for every Status Code of interest in the Senvion/Re Power turbine. In order to start receiving changes on the arrays the SDG must be configured to receive the changes. It is recommended that a Polled Point Set be configured to poll the status array and a report control block configured to receive changes in the event array. Typically the event array has a RCB and Data Set associated with it that can be configured in the SDG as any other Report Control Block would be. With this configuration the status array will be used to initialize the alarms and the event array will be used to receive new alarms and events from the turbine. Please see the appropriate Senvion/Re Power documentation to get more details on how these arrays behave. 4.16 IEC 61850 Server To add a IEC 61850 Server to your SDG configuration, right-click on the Gateway node and select Add IEC 61850 Server: Graphical user interface, application, email Description automatically generated The Create 61850 Server Dialog has tool-tip help for each field. See the 61850 protocol reference manual for more details. Once the server has been created, you will be ready to start mapping MDOs to it. This is done by dragging and dropping MDOs to the server node in the tree: Mapping Points to the IEC 61850 Server 4.16.1 To map points from MDOs to the IEC 61850 Server, simply drag-and-drop an MDO from the onto the IEC 61850 Server node: The following dialog will appear when the point is dropped. Use this dialog to select the Data Attribute in the 61850 model selected with the ICD file by pressing the hi-lite button. Note the point name is pre generated based on its location in the SDG. In this example we select the Phase A current and press OK. Note the Only MX filter was turned on to only display 61850 tags with the MX type functional constraint. After mapping the point, your SDG configuration should look similar to the image below: This shows the point in the 61850 server (i.e. mdnp_L4_T30_P0). Although we dont show it in this image the description will have the path of the 61850 tag. Here we also show the master DNP side with the analog point P0 mapped to the 61850 tag. Mapping additional points is simply a matter of dragging additional MDOs to the 61850 server or by manually adding the mappings in the .csv file. Note that server control points and monitor points are suported. If a control point control value (ctlVal) is used in the server, the IS_COMMAND_SDO option must be set to true. This is accomplished using the edit menu and selecting the options button and set the option to true as below: 4.17 60870-6 ICCP (Tase.2) Client and/or Server The SDG supports ICCP blocks 1, 2, and 5 except for All Changes Reported related to Dataset Transfer Sets. To add an ICCP Client and/or Server to your configuration, right-click on the Gateway node and select Add ICCP Client, Add ICCP Server, or Add IEC ICCP Client/Server: Choosing Add IEC ICCP Client/Server will display the Add ICCP Client/Server screen. For specific help on input parameters press () on any dialog. Parameter Description Name The name of this ICCP client and/or server. Service Role Specifies client, server, or client and server functionality. Selecting Client/Server will create a bidirectional ICCP configuration. ICCP Application Version Specifies the ICCP version to run the current application in ICCP Server IP Address Specifies the IP address that the client will connect to. The address 127.0.0.1 specifies the local machine that the SDG is running on. IP Port (In Client configuration) Specifies the port to which the client will try to connect on the specified Remote IP address. The default is 102. ICCP Client RFC IP Address Specifies the IP address (NIC) that the client will use for connecting to the server. Initiate a connection Applies to Client and Client/Server only. Specifies if the client should initiate a connection upon startup of the SDG application. Sync Data Sets Applies to Client and only. If true, the client will attempt to adjust its local datasets to match servers model. Reconnect Timeout (msecs) How often to attempt re-connection if a connection fails. Should be larger than the Connect timeout. Reconnect Re-try Count How many times to re-try reconnect (0 is forever). MMS Timeout (ms) Specifies the MMS connect timeout for the ICCP Client. After starting a connection attempt this is how int to wait for success. ICCP Server RFC IP Address Specifies the IP address that the Server will listen on for connecting clients. 0.0.0.0 means use local default IP. IP Port (In Server configuration) Specifies the port that the server will listen on for connections. The default is 102. Maximum number of clients If specified, this setting controls how many clients can connect to the server. 0 means no client can connect. Empty means there is no limit. Automatically restart ICCP server when model changes If true, the SDG will automatically restart the ICCP server when its model is modified. Require ICCP server to be stopped for modification If true, the SDG will not allow the ICCP Server to be modified when running. ICCP Security Brings up the security configuration dialog box. Security On Tells the application to connect using the security parameters. If security is turned on, all certificate paths have to be valid except for the revocation list file. Local/Remote AE Invoke ID ASCE AE invoke ID. Value from 0 to 65536 Local/Remote AE Qualifier ASCE AE qualifier. Value from 0 to 65536 Local/Remote AP Invoke ID ASCE AP invoke ID. Value from 0 to 65536 Local/Remote Application ID ACSE AP title Local/Remote Presentation Selector Presentation selector Local/Remote Session Selector Session selector Local/Remote Transport Selector Transport Selector ICCP Server Supported Features The supported features that the server reports to remote clients. The Remote Server 7 Layer Stack section represents the 7 layer stack settings of the remote Server that the client is trying to connect to. The Local 7 Layer Stack section is the local 7 layer stack settings and will designate which server a remote client will connect to and will also identify the local client to the remote server. Remember for the 7 layer stack settings to apply when a remote client is connecting to this devices the Full Stack Addressing option (see section 3.4.1) must be turned on, else 7 layer stack settings are ignored in the SDG when a remote client connects to the SDG. The SDG and ICCP server remote application are running on the same machine, use the default loopback address provided of 127.0.0.1 and make sure the remote server is started first if running in bidirectional mode. Otherwise the ICCP client in the SDG will connect to the ICCP server in the SDG. If bidirectional functionality is desired the Client/Server radio button must be selected. The gateway supports multiple connection profiles for the client to connect to as a type of one sided redundancy. The main field that controls this is the Connecting IP. Multiple values, semicolon separated, can be specified for the seven layer stack and security fields for each IP address specified. The client will cycle through the IP addresses when it loses a connection and will use the corresponding seven layer and security fields. For example, if the IP address field is 192.168.1.134;192.168.1.155 and the Local AP Title is 1,3,9999,1;1,4,5555,2, the client will first try to connect to 192.168.1.134 using 1,3,9999,1. If the client either cannot connect or loses its connection, it will then automatically try 192.168.1.155 using 1,4,5555,2. And then if it cannot connect to the second IP or loses its connection, it will then go back to the beginning. There is no limit as to how many connection profiles can be used. The security dialog box looks like the following: Graphical user interface, application Description automatically generated Use ED1 Compatibility checkbox is to support older edition 1 MMS security implementation that does not comply with the current standard 62351 specification. The rest of the fields should be self- explanatory. All fields are required except Certificate Authority Revocation List File and Directory to Certificate Authority. In the client create/edit dialog above, security cannot be turned on unless all other fields are specified and point to valid certificate files or values. Graphical user interface, text, chat or text message Description automatically generated When finished with security settings click the Ok button and then when finished with the client create/edit settings click the OK button to add the client and/or server to the configuration. When a client is added, the following MDOs are added to indicate status and statistics for that ICCP client. ChannelActiveControl If true, the client is connected to the server. This MDO is writeable and will connect/disconnect the client. NumReadResponse The number of reads completed from the server. NumReportMessages The number of report control block updates received. When a server is added, the following MDOs are added to indicate status for that ICCP Server. ChannelActiveControl If true the server is listening for a client connection. This MDO is writeable and will cause of the server to stop listening if turned off. NumClientsConnected Shows the number of clients connected to the Server. Note that in bidirectional mode, peer clients are only considered connected if they initiate the connection. Editing/Deleting a Client/Server 4.17.1 To edit or delete a client and/or a server, right click on the client or server and select Edit or Delete. For a ICCP server, you will need to Stop the server first. This can be done by right clicking on the server and selecting Stop ICCP Server. Connecting to a Server 4.17.2 To connect the client to an ICCP server, select Connect to Server in the above menu and the ICCP client will initiate a connection to its server establishing either a unidirectional or bidirectional connection. Creating/Editing Client Model 4.17.3 The client should define its own model before connecting to a server by selecting Edit ICCP Client Model which will bring up the ICCP Client Model dialog (below). If no model exists a simple model with just version information will be automatically generated. To use the ICCP Client Model dialog, a user can either add points dynamically or create a .csv file defining the model as follows: Domain/DataPointName,PointType Domain/CommandPointName,CommandPointType,SBO,CheckbackId Domain/BilateralTableId,Bilateral_Table_ID Domain/DataSetName,dataset,<comma separated point list with full path> VMDDataPointName,PointType VMDCommandPointName,CommandPointType,SBO,CheckbackId The Gateway supports the following ICCP data types: Discrete DiscreteExtended DiscreteQ DiscreteQTimeTag DiscreteQTimeTagExtended Real RealExtended RealQ RealQTimeTag RealQTimeTagExtended State StateExtended StateQ StateQTimeTag StateQTimeTagExtended And supports the following ICCP Control types: ControlCommand DiscreteSetPoint RealSetPoint An example CSV file would be: CSV file contents: ICC1/RealPoint,Real ICC1/RealExtendedPoint,RealExtended ICC1/RealQPoint,RealQ ICC1/RealQTimeTagPoint,RealQTimeTag ICC1/RealQTimeTagExtendedPoint,RealQTimeTagExtended ICC1/DiscretePoint,Discrete ICC1/DiscreteExtendedPoint,DiscreteExtended ICC1/DiscreteQTimeTagPoint,DiscreteQTimeTag ICC1/DiscreteQTimeTagExtendedPoint,DiscreteQTimeTagExtended ICC1/ControlCommand,ControlCommand,true,90 ICC1/DiscreteSetPoint,DiscreteSetPoint,true,-8 ICC1/RealSetPoint,RealSetPoint,true,-3 ICC1/CLIENT_BTID,Bilateral_Table_ID VMDRealPoint,Real VMDStatePoint,StateQ VMDDiscreteQPoint,DiscreteQ VMDRealQPoint,RealQ VMDStatePoint,State VMDStateExtendedPoint,StateExtended VMDStateQTimeTagPoint,StateQTimeTag VMDStateQTimeTagExtendedPoint,StateQTimeTagExtended ICC1/DataSet1,dataset,ICC1/RealPoint,ICC1/RealExtendedPoint,ICC1/RealQPoint If the domain is not specified, the Virtual Manufacturing Domain or VCC is assumed. The order that entries are listed does not matter. Once the file is defined it can be loaded into the client model by using the Browse button to find and select the file and then press the Load button. The load will also load an xml file (for example that is created from an SDG/ICCP server). The Clear Model button is used to completely clear the model and start over if a different model is desired. DataSets and DS Transfer Sets are not defined in the .csv file. They are defined via the GUI as described below. The entire model (including DataSets and DS TransferSets) is persisted in a file with the name <clientname>_modeldef.xml and is saved in the standard location. The model is persisted anytime it changes not when the application is saved. NOTE: This XML file can be loaded into other client models using the Browse/Load functionality if needed/desired. The Export Model button will export the model to a chosen .csv file. Graphical user interface, text, application, chat or text message Description automatically generated Adding an ICCP Data Set Transfer Set 4.17.4 To add an ICCP DS Transfer Set to your configuration, you must first connect the client by right-clicking on the client and selecting Connect To Server and then right-click on the ICCP Client again and select Add ICCP Data Set Transfer Set: 00 This will display the Add ICCP Data Set Transfer Set dialog screen: Make sure the name is the desired name. Select the domain to create the DS Transfer Set in. Select any one of the Data sets from the list of available data sets. The corresponding data set and its contents will be shown in the list box on the right. Data Sets must be created from the client. If a new dataset is desired, click the Create New Data Set button which will bring up the Add ICCP Data Set dialog: Make sure the name is the desired name. Select the domain to create the DS Transfer Set in. Select Items to be included in Data sets from the list of available Items. Next, click the Ok button to add the DS. Next, in Add ICCP Data Set Transfer Set dialog screen, click the Ok button to add the DS Transfer Set. The Data Set Transfer Set is now added and can be viewed by clicking on the ICCP client node. The following status MDOs are added to the client when the Report Control Block is added: To modify an existing dataset, select the dataset and press the Manage Selected DataSet button and select the items to remove or add. NOTE: when adding a member to the dataset, the Data Attribute must also already exist on the server or the modify will fail. Modify dataset will preserve existing mappings and will only modify the dataset itself. Note: After creation the Data Set Transfer Set should be Enabled when configuration is done, using a right-click on the Data Set Transfer Set then select Enable Dataset Transfer Set option. Editing an ICCP DS Transfer Set 4.17.5 Right click on the DS Transfer Set and select edit and following dialog will appear: Dataset Transfer Set interval (secs): If nonzero, the ICCP server shall send an information report when the Interval time arrives. If Report by Exception (RBE) is false, then the current state of all objects in the Data Set are reported when the Interval expires. If RBE is true, then the current state of only the changed objects in the Data Set are reported when the Interval expires. Dataset Transfer Set period (secs): If nonzero, the ICCP server shall send the entire dataset when the Integrity time arrives, but only if RBE is true. (If RBE is false, integrity will not return any reports and is not used) Dataset Transfer Set buffer time (secs): The time in seconds to buffer changes before sending a report. If the same point changes more than once during the interval, the last change only will be reported. Dataset Transfer Set Report By Exception: See Above effect of RBE. Adding an ICCP Data Attribute from the DS Transfer Set 4.17.6 Once a DS Transfer Set is defined we can start adding points (Data Attributes) as MDOs to the SDG. To add an ICCP Item, right-click on the DS Transfer Set and select Add ICCP Item: This will display the Add ICCP Item dialog screen: Next, select item(s) from the list and click the Ok button. For each item added, the corresponding domain folder will be created under the client. The Bilateral Table ID will also appear in the name of the Domain. You must select the domain to see the points corresponding to that domain (see below). Note: once Ok is pressed the only way to remove the items is to select it under its respective Domain, right click and choose delete. To select multiple items at once, hold down the control key and click the desired items. To select a range of items, click the first item and hold down the shift key and click the last item. The Data Attributes now appear in the tree under their respective Domain: Note that the Data Attributes are treated in the SDG as any other Master Data Objects (MDOs). So they can be mapped to other items and are available through the SDGs OPC Server. Adding an ICCP Control Point 4.17.7 To add a ICCP Control Point to your client, create a ICCP Client as described above and then right-click on the ICCP client node and select Add ICCP Command Point Set: This will then prompt for a command point set. Once the command point set is added command point can be added to the command point set. This will display the Add ICCP Command Point dialog screen: Next, select an item from the list by clicking on the desired Item Name. The Item Name should now be displayed in the Tag Name textbox. Click OK to add the ICCP Control Point. You can make the control point an SBO and select the checkbackID by pressing the options button: The ICCP Control Point now appears in the tree under its respective Domain node: 4.17.7.1 Operating a ICCP Control Point The normal way to operate the control point is to map it to another slave point. In order to manually operate the control point, right-click on the point and select Operate ICCP Control: The Operate Control Dialog will appear: Put the value to write to the control and press Select And Operate. Select only applies (and appears) for controls that are Select Before Operate (SBO), otherwise only Operate appears. Controls can also be created in the ICCP Server via the normal mapping functionality. Mapping MDOs into the ICCP Client 4.17.8 To map and MDO into the ICCP client, drag and drop any MDO onto the ICCP client node or an ICCP Domain node and you will see the following dialog: A list of existing points will appear and the user can select the desired point to map to. If Ok is selected, the following will occur: SDG will bind the point in the ICCP client model and will automatically map it the point which was dragged. The user will then need to select a control block to put the newly created MDO into (a control block is a polled point set or a polled dataset or a dataset transfer set). The SDG will then automatically add the MDO to the control block and will update any datasets that need updating. NOTE: The new ICCP data attribute must also exist on the server which the ICCP client is connected to or the update will fail. Other mechanisms to obtain Data from an ICCP Server 4.17.9 As the configuration of the following data gathering methods are very similar to the DS Transfer Set, we will not waste space with a detailed explanation of how to configure these. Instead we will simply describe their use and operation. It is recommended that the data in the below mechanisms does not overlap with other data. Note that the last mechanism to update the value in the MDO is used as the value of the MDO if the data does overlap. 4.17.9.1 Polled Data Sets Polled Data Sets are a mechanism in the SDG to read data in a specific data set from a server on a periodic basis. The SDG is responsible for issuing a read on the data set in the server to obtain the data. 4.17.9.2 Polled Point Sets The last mechanism to obtain data from a server is a Polled Point Set. This mechanism is very similar to a Polled Data Set except that it does not require a Data Set. This mechanism basically issues a read on all the points in the Polled Point set at the period specified. Note that with this mechanism there is a MDO that can be used to trigger the read operation in addition to the timer. 4.18 ICCP/Gateway Quality Gateway quality is defined in the following table: IEC 60870-5-101 Quality Descriptor Quality Symbol Meaning Mask Value BL STD_QLTY_BLOCKED Blocked 0x0010 SB STD_QLTY_SUBSTITUTED Substituted (over-ride or forced) 0x0020 NT STD_QLTY_NOT_TOPICAL Not topical (offline/not dated) 0x0040 IV STD_QLTY_INVALID Invalid 0x0080 OV STD_QLTY_OVERFLOW Overflow/rollover 0x0001 EI STD_QLTY_INVALID_TIME Elapsed time invalid 0x0008 - STD_QLTY_REF_ERROR Reference error 0x0100 - STD_QLTY_IN_TRANSIT In transit/chatter 0x0200 - STD_QLTY_UNINITIALIZED Not set since startup 0x0400 - STD_QLTY_TEST Test mode 0x0800 The Gateway maps this quality to ICCP ICCP quality in the following way: ICCP/ICCP Quality Quality Symbol Meaning Mask Value Validity Held STD_QLTY_BLOCKED Blocked 0x0010 Current Source calculated, entered, or estimated STD_QLTY_SUBSTITUTED Substituted (over-ride or forced) 0x0020 Validity Suspect STD_QLTY_NOT_TOPICAL Not topical (offline/not dated) 0x0040 Invalid STD_QLTY_INVALID Invalid 0x0080 Abnormal Value STD_QLTY_OVERFLOW Overflow/rollover 0x0001 Invalid Timestamp STD_QLTY_INVALID_TIME Elapsed time invalid 0x0008 N/A STD_QLTY_REF_ERROR Reference error 0x0100 N/A STD_QLTY_IN_TRANSIT In transit/chatter 0x0200 N/A STD_QLTY_UNINITIALIZED Not set since startup 0x0400 N/A STD_QLTY_TEST Test mode 0x0800 This mapping is necessary because the gateway maps quality between protocols and no two protocols have the same meaning for quality. For server side points, there is an option to interpret the quality value passed to the Slave Data Object as straight ICCP quality. You can edit the SDO and turn INTERPRETQUALITYASTASE2 option on for this functionality. There is no such option on the client side. In general if the ICCP point has a flags attribute or a state attribute the quality will be converted back and forth to and from gateway quality. If the ICCP point does not have flags or state (e.g. a Real or Discrete) the quality with either be Invalid (0x0080) or Good (0x0000). Furthermore if the ICCP point has flags or state the user interface will display both the gateway and ICCP quality and otherwise will only show the gateway quality. The ICCPquality is displayed as (x,x,x,x) which is (Validity, Source, Normal Value, Timestamp Quality). 4.19 Standalone ICCP/ICCP Server To add a standalone ICCP Server see the above instructions and select the Server ICCP Configuration in the Create ICCP Client/Server Dialog Clicking on the ICCP Server node and selecting Add ICCP Logical Device: Enter a Name for the logical device, the DS Transfer Set count to be available to clients and the Bilateral Table ID and then click OK: The ICCP server should now be visible in the SDG configuration: Mapping Points from a Master device to the ICCP Server 4.19.1 To map points from other protocols to the ICCP Server, simply drag-and-drop an MDO from the other protocol onto the Logical Device of the ICCP Server: Next, enter the ICCP Point Name and select a Type from the dropdown list that corresponds to the type of MDO dragged to the logical device. Click OK to continue: After mapping a point, your SDG configuration should look similar to the image below: The final step in the configuration is to restart the ICCP Server. This is done by right-clicking on the ICCP Server node and selecting Start/Restart ICCP Server: The ICCP Server is now ready to be connected to by a ICCP client that will now be able to read the Real point, tase2Real from the Logical Device, ICC1. 4.20 OPC Alarm and Event Client (windows only) To add an OPC AE Client to your configuration, right-click on the Gateway node and select Add OPC AE Client: This will display the Add an OPC AE Client dialog screen as shown below: Next, click Find OPC Server and then select the desired OPC AE Server from the Select OPC AE Server dialog screen. This dialog displays a list of OPC AE Servers available on the Server Node. Selecting one of these servers places the value of the Prog ID field and the Node Name or IP address field into the above dialog box. Click OK to continue. Next, enter an Server Name and then click Select Server and Ok to continue. Parameter Description Node Name or IP Address Specifies the Node name of the OPC AE Server to connect to Prod ID Specifies the PROG ID of the OPC AE Server to connect to Alias Name Optional Name for the OPC AE Server to connect to, if not specified use the value of OPCAEserverProgID. It is strongly recommended that this parameter be defined as an external OPC AE client may not be able to browse for tags in SDG OPC AE server if the OPC AE server name contains one or more period ('.') characters. To work around this issue, define this alias name (with no periods for the external OPC AE server, and reference the server by its alias. Buffer Time (msecs) The buffer time, specified in milliseconds, indicates the number of times the event notifications may be sent to the subscription object. This parameter is the minimum time spread between two successive event notifications. The value 0 means that all event notifications shall be sent immediately from the server. If the parameter MaxSize is greater than 0, it instructs the server to send event notifications quicker in order to keep the buffer size within MaxSizeSpecifies the buffer time for the OPC AE Server subscription. Max Size (events) This parameter is the maximum number of events that may be specified in a call. The value 0 means no restriction for the number of events. Please note that if the value MaxSize is greater than 0, then events can be sent faster from the server than through the BufferTime parameter. Specifies the max size for the OPC AE Server subscription. Reconnect Time (msecs) Specifies the reconnect timeout for the OPC AE Server (0 = no reconnect). Reconnect Re-try Count Specifies the reconnect re-try count for the External OPC AE Server (0 = attempt reconnects for ever)Setting the client refresh MDO will cause the internal limit counter to be re-set to 0resulting in continued connection attempts to the OPC AE server. Adding OPC AE Client Items 4.20.1 To add an OPC AE Client item, right-click on the OPC AE Client node and Select Add OPC AE Client Item. The following dialog screen will be displayed: Not all OPC AE servers support an Area Space. As a result, if the Area Space browsing is not supported event source names will need to be entered manually. If Area Space browsing is supported, then event source items can be selected in the Area Browser to create OPC AE MDOs. By default when a source name is selected, a source_name.Event MDO is created. This MDO will receive events for that source name in the OPC AE Server. The value of this MDO by default will be the message value of the OPC AE event. Time and quality are always obtained from the OPC AE event. To browse for an Event Source, click on the nodes to expand the tree and then click Ok to add the desired item: The event is now added to the configuration: OPC Alarm and Event MDO 4.20.2 An OPC AE MDO represents a source of events in the OPC AE Server. As events are sent on this event source the MDO will be updated with the event message, quality and time stamp. This behavior can be overridden by specifying some of the options below. 4.20.2.1 Event MDO The event MDO (i.e. source_name.Event) represents the base OPC AE event object. This MDO will receive updates as the event source changes in the OPC AE Server. 4.20.2.2 Event Attribute MDOs OPC AE events can have attributes associated with them. OPC AE Attributes are typically defined as properties of Categories in an OPC AE Server. For example, a server may provide an event called right mouse click with attributes x position and y position. To add an OPC AE Event Attribute to your configuration, right-click on the event and select Add OPC AE Attribute MDO: The following dialog screen will be displayed: To browse for an Event Attribute, click on the nodes to expand the tree and then click Add to add the desired item. The event is now displayed in the SDG Configuration window: OPC AE Client MDOs can be created to represent these attributes. The value of these MDOs will be the attribute value (i.e. x position or y position) but the quality and time stamp will be obtained from the event. 4.20.2.3 OPC AE MDO Options To edit the OPC AE MDO options, right-clicking on an MDO and Selecting Edit OPC AE MDO. This will display a dialog screen with selected OPC AE item: To view the available options for this item, click on the button to display the Options Editor dialog screen. Select the desired options, including the Value and the Use parameters. These items are selected by clicking in the appropriate box to change the value. Click OK to continue. *Click on the button to open the LOGMASK dialog screen which displays a list of additional options for these items. Parameter Description ALIAS EVENT_FALSE_MSG used to parse the message string of the event and look for the string specified if the string is found the MDO value will be set to false EVENT_TRUE_MSG used to parse the message string of the event and look for the string specified if the string is found the MDO value will be set to true EVENT_VALUE used to specify/override the default message value for the event. (i.e. EVENT_VALUE true will cause the MDO value to be true when the event is received) The following options are related to the OPC AE Client. However, these options are selected in the OPC DA MDO Options dialog (covered in the next section). Parameter Description AE_INIT_VALUE_MDO used to specify the OPC AE MDO that will be initialized with the value, quality and time stamp (VQT) of the Data Access MDO. Once initialized the VQT will not be applied again unless the OPC AE Client disconnects from its server AE_RESET_VALUE_MDO used to specify the OPC AE MDO that will receive a VQT from the OPC DA MDO if the AE MDO value is different from the DA MDO value AE_RESET_VALUE in association with AE_RESET_VALUE_MDO is used to specify the value of the OPC DA MDO that will cause the OPC AE MDO to be set to the VQT of this OPC DA MDO when an OPC DA update is received for this OPC DA MDO. Valid values for this options argument are 0,false,1,true, numeric value (float or integer) or some string. If this option is specified the difference comparison in the AE_RESET_VALUE_MDO option will not be used 4.21 OPC Alarm and Event Server (windows only) The OPC Alarm and Event Server component provides events to OPC Alarm and Event clients running on this machine or other machines on the network. Events are propagated from Master components in SDG to the OPC Alarm and Event Server Slave component. As events arrive at a Master component they are queued to the OPC Alarm and Event Server. The Server then provides these events to any connected OPC Alarm and Event client. Due to the nature of this queuing process, all data changes received at the Master component will be propagated to the OPC Alarm and Event client. This differs from the operation of the OPC Data Access Server in that it will not provide all changes to a connected client, should these changes occur more rapidly than the polling interval. The time stamp of the event is provided based on the setting of OPCAETimeSource parameter in the INI file (see section 4.38.1 Event Time Tags and Database Update Times on page 252 for additional information). Currently by default, all points are reported as an Event Type of 'Condition', Event Category of 'Discrete', and a condition type of 'COS' (change of state). In addition, the attribute 'CV' (current value) allows the client to see the value at the time the event was reported. Optionally a log mask(s) can be used to enable OPC Alarm and Event Notifications. The OpcAESubscriptionMask parameter in the configuration file controls when events are reported as a result of an item subscription through the SDG OPC Data Access Server. The bits in this mask enable various reasons that an event will be reported. This mask overrides all other OPC Alarm and Event related masks if enabled. If the mask is 0, no OPC Alarm and Event notifications will occur as a result of an OPC Data Access item subscription, although events can be reported for other reasons through the use of the other OPC Alarm and Event masks (see the configuration file, each data type has an OPC Alarm and Event log mask associated with it, the rules associated with this configuration of these masks is similar to the Sequence of Event Log Files section 4.39 page 253). Specifying and using the Event Space 4.21.1 The following INI file parameters can be used to define an Event Space for the SDGs OPC AE Server. OPCAeCatID - Specifies the id of the OPCAE category. Default Cat IDs are from 0 to 4. Start this at 5. OPCAeCatMsg - Specifies the name of the OPCAE category. OPCAeCatCondName - Specifies the name of the OPCAE condition. OPCAeCatSubCondName - Specifies the name of the OPCAE sub condition. OPCAeCatSubCondSeverity - Specifies the OPCAE sub condition severity. OPCAeCatSubCondDescription - Specifies the description of the OPCAE sub condition. OPCAeCatSubCondDefinition - Specifies the definition of the OPCAE sub condition. In addition the CSV file supports the following option fields related to OPC AE Server definition: OPCAESEVERITY the severity of the condition OPCAECONDITIONNAME used to map this MDO to the OPC AE event space. The format of this field is as follows: Category id/condition name/sub condition name CATID:COND:SUB_COND Note the : is used to separate the fields. OPCAEMESSAGE override the default message for this alarm/event with this string OPCAEUSEVALUEASMESSAGE use the value of the MDO as the message for this alarm/event Defining an Event space for the server will allow an OPC AE client to receive additional information associated with the events and alarms by relating the individual events and alarms (defined as MDOs) to the event space through the OPCAECONDITIONNAME option field. This additional information can be used to filter the alarms and events on the client. The structure of this event space definition is as follows (each array element is dimensioned at 10): category[] - ID INI OPCAeCatID[catIndex] - msg INI - OPCAeCatMsg[catIndex] condition[][] - name INI - OPCAeCatCondName[catIndex] [condIndex] - severity CSV - OPCAESEVERITY - msg CSV - OPCAEMESSAGE sub-condition[][][] - name INI - OPCAeCatSubCondName[catIndex] [condIndex] [subCondIndex] - severity INI - OPCAeCatSubCondSeverity[catIndex] [condIndex] [subCondIndex] - description INI - OPCAeCatSubCondDescription[catIndex] [condIndex] [subCondIndex] - definition INI OPCAeCatSubCondDefinition[catIndex] [condIndex] [subCondIndex] Note that in order for the OPC AE Server to publish alarms and events. The following OPTIONS/INI file parameters will have to be configured appropriately. OPCAEUseSimpleEvents ini file parameter OPCAE_LOGMASK MDO specific option Example Configuration 4.21.2 Note this configuration section will need to be manually added to the SDG INI file. Just paste this into the INI file and make any required changes to it. //***************************************************************************** // OPC AE Server //***************************************************************************** OPCAeCatID[0]=5 // Specifies the id of the OPCAE category. // DEFAULT: 5 id // INDICES: [catIndex=0-9] OPCAeCatMsg[0]="msg 1" // Specifies the name of the OPCAE // category. // DEFAULT: "" // INDICES: [catIndex=0-9] OPCAeCatCondName[0][0]=cond1 // Specifies the name of the OPCAE // condition. // DEFAULT: "" // INDICES: [catIndex=0-9][condIndex=0-9] OPCAeCatSubCondName[0][0][0]=sub1 // Specifies the name of the OPCAE sub // condition. // DEFAULT: "" // INDICES: // [catIndex=0-9][condIndex=0-9][subcondIndex=0-9] OPCAeCatSubCondSeverity[0][0][0]=25 // Specifies the OPCAE sub condition // severity. // DEFAULT: 100 num // INDICES: // [catIndex=0-9][condIndex=0-9][subcondIndex=0-9] OPCAeCatSubCondDescription[0][0][0]="sub1 descp" // Specifies the description // of the OPCAE sub condition. // DEFAULT: description // INDICES: // [catIndex=0-9][condIndex=0-9][subcondIndex=0-9] OPCAeCatSubCondDefinition[0][0][0]="sub1 definition" // Specifies the // definition of the OPCAE sub condition. // DEFAULT: definition // INDICES: // [catIndex=0-9][condIndex=0-9][subcondIndex=0-9] As an example of a simple configuration an internal bit can be configured to publish an OPC AE Alarm/Event: Changing the value of the internal MDO bitAlm will then publish an Alarm/Event through the OPC AE Server. 4.22 OPC Data Access Client (windows only) To Add an OPC Data Access Client to your configuration, right-click on the Gateway node and select This will display the Add an OPC Client dialog screen as shown below: Parameter Description Node Name or IP address Specifies the Node name of the OPC Server to connect to. This field is automatically populated when you select a server. Server URL Specifies the URL Address of the OPC Server to connect to. This field is automatically populated when you select a server. Server list Displays the OPC Server list section. Server OPC Version Specifies the version of OPC supported by the server. Force OPC Server Specification This need to be checked for server types that require a strict enforcement of the opc version. Alias Name Optional Name for the OPC Server to connect to, if not specified the value of Prog ID is used. Refresh Rate (ms) Specifies the rate at which the OPC server that the client is connected to will refresh the OPC client. This is a requested update rate and that the server may actually no update the client at the rate requested. Reconnect Time (ms) Specifies the reconnect timeout for the OPC Server (0 = no reconnect). Read Properties Time (ms) Specifies the period at which properties are read (0 = don't read) Reconnect Re-try Count Specifies the reconnect re-try count for the External OPC Server (0 = attempt reconnects forever) Setting the client refresh MDO will cause the internal limit counter to be re-set to 0resulting in continued connection attempts to the OPC server. Reconnect Delay (ms) If nonzero, this specifies the time (in ms) to wait to reset the retry count and continue to try to reconnect. If zero, it is ignored and the retries will not continue past the retry count value. Click the Find OPC Server button to display the Select OPC Server dialog screen: To select a computer that an OPC Server is on enter its ip address or node name in the Node Name (1) field, then select Refresh List (3) to populate the Server List with the available servers. This dialog displays a list of available OPC servers on the Server Node. To select an OPC Server, highlight the server name in the list (4) and then click the OK button to return to the Add an OPC Client dialog screen. Complete the remaining fields, if required, and then click OK to continue. To define the server type select an option (1.0, 2.0, or 3.0) under the Available Servers section (2). Next click the Refresh button (3), if necessary, to refresh the list of available servers. OPC XML Client 4.22.1 If the Server OPC Version is set to XML Client, the Server URL parameter is the URL to the desired OPC XML DA Server. OPC Client Items 4.22.2 OPC Client items are considered MDOs in the SDG. OPC Client items obtain their value from an external OPC server. They are updated as the server provides them to the client. These points can be mapped to other protocols or to another OPC Client. To add an OPC Client item to your configuration, right-click on the OPC Client and select Add OPC Client Item: This will display the Add Item dialog screen: To add items, navigate the item tree (1) to the desired node and then select a value from the list on the right. Click Add Item (2) to add the item to the configuration. Once all items have been added, click Done (3) to return to the SDG main window. The Data Type group box (5) is used to override the SDG data type for the OPC point. If Use native type is selected, the type is set to the type of the point in the OPC server. This ability to override the data type can be useful if a server provides data as one type and it is convenient to change the type in the SDG to a different type. For example, if a server provides data as R8 (double float) and you want to map this to an IEC T50 (floating point set point command) it is useful to be able to change the type of the OPC client point to R4 (float) to facilitate the mapping. Mapping the Refresh tag has to be done manually in the CSV file. When changed, this causes the server to refresh all items in the client. The following is an example of mapping an equation to the OPC Client Refresh tag in the CSV file: myAND,,,,,,,,,test,Refresh,,1,,,,"myPB && 1",,,,OPC Parameter Description myAND the equation name test the OPCClient name Refresh the Refresh tag in the OPC client "myPB && 1" the equation (myPB is an internal Boolean tag) 1 the type of Refresh (i.e. bool) Refer to section 4.34 Mapping Points to an External OPC Server for additional information about OPC Client Items. OPC Client Tag Properties Properties of OPC tags can be used as MDOs. In order to use this feature the OPC Client tag MDO must first exist (as described above). Once this tag is added, right-click on the tag and select Edit OPCDA MDO to add the property as an MDO: The Item Properties dialog screen is now displayed: To add properties, select an item from the list (1) and then click the Add Property button (2). To display the list of available options, click the button (3). There are 2 OPC client tag specific options that are used to cache the current read/write state of the tag. The read/write state is normally obtained from the server but will be cached in the CSV file for performance reasons. If the read/write state of a tag in the server changes these should be updated to reflect that change. o IS_COMMAND_MDO tag can be read and written o IS_NOT_COMMAND_MDO tag can only be read o Not specified read/write state will be obtained from the server To refresh the list of available properties, click the Refresh button (4). To view the attributes of the selected property, click the Attributes button (5). Once the property is added to the SDG, it will appear with an underscore (_) appended to the end of the OPC tag name: This OPC Property can now be used like any other SDG MDO and can be mapped to other SDG points. The description of the new MDO will become the description of the OPC property. OPC properties are updated by setting the INI file configuration parameter OPCserverReadPropertiesTime. This OPC Client configuration parameter can also be set by right-clicking on the OPC Client node and selecting Edit OPC Client. Next, adjust The Read Properties Time field. A value of 0 will disable property reads. A Change in quality or value of the property will cause a propagation of the quality or value to a mapped SDO. 4.23 OPC UA Client To add an OPC UA Client to your configuration, right-click on the Gateway node and select Add OPC UA Client: This will display the Add an OPC UA Client dialog screen as shown below: Parameter Description Alias Name Optional Name for the OPC UA Server. An external OPC UA client may not be able to browse the SDG OPC UA server correctly if the OPC UA server name contains one or more period ('.') characters. To work around this issue, define this alias name (with no periods for the external OPC UA server, and reference the server by its alias. Server Host/URL The full URL of the OPC UA Server to which this client will connect. Session Timeout (msecs) The requested maximum number of milliseconds that a session should remain open without activity. If the client fails to issue a communication within this interval, then the server shall automatically terminate the session Max Notifications Per Publish This parameter sets the maximum number of notifications that a client wishes to receive per publish response. If the client does not want to impose restrictions on the message size of the publish response, this parameter should be set to 0. Publish Queue Size Specifies the maximum number of data change or event notifications that a Monitored Item may hold in its queue. Sampling Interval (msec) Sets the requested sampling interval of the Monitored Item. The server may revise this value when it is connected. Publishing Interval Defines the publishing interval in milliseconds. The publishing interval defines the cycle time in which the server sends notifications (data changes, events or status changes) periodically to the client. Supports Writing timestamps Should be set to true when the server this client is connecting to supports writing timestamps. Supports writing status codes Should be set to true when the server this client is connecting to supports writing status codes. Security Policy Selects the desired security policy for this OPC UA Client connection. Security Mode Selects the desired message security mode for this OPC UA Client connection. Valid options are: None Sign Sign and Encrypt Certificate File The X509 version 3 certificate file encoded using the DER format that contains the public keys for this client. (Unused if Security Policy is set to None) Remote Certificate File The X509 version 3 certificate file encoded using the DER format that contains the public certificate of the server being connected to by this client. (Unused if Security Policy is set to None) Private Key File File containing the clients private key that is associated with the public key. (Unused if Security Policy is set to None) Private Key Pass Phrase The pass phrase used to protect the private key. Auto Rejected Certificates/Trust Selected Certificate The list of rejected server side certificates that this client has rejected. Use the Trust Selected Certificate button to trust any rejected certificate. Certificate Trust Directory The directory where server certificates can be placed for trust. This location is not settable by the user and is hard coded as <data directory>/Workspaces/<workspace_name>/opcua_client_certs/trusted Certificate Reject Directory The directory where server certificates can be placed for rejection. This location is not settable by the user and is hard coded as <data directory>/Workspaces/<workspace_name>/opcua_client_certs/rejected Token Type Selects the security token type for this OPC UA Client connection. Anonymous, User Name/Password, and User Certificate types are supported Token Policy Name The case sensitive policy id name that MUST match the server policy name. Defaults are Anonymous Policy, Username Policy, and Certificate Policy User Name Name of valid user User Password Valid user password Certificate File The user certificate file Private Key File The user private key file Private Key Pass Phrase The user private key file password Reconnect Time (msec) Specifies the reconnect timeout for the OPC UA Client. This parameter should be set to 0 disable reconnects. Reconnect Re-try Count Specifies the reconnect re-try count for the OPC UA Client. If this parameter is set to 0, the client will attempt to connect forever. Connecting to a server with a known URL 4.23.1 If the OPC UAs servers URL is known, enter an Alias Name for the server and its URL. Then click OK to connect to the server. Connecting to a server using the OPC UA Discovery Service 4.23.2 If the OPC UAs servers URL is not known, click Find OPC UA Server and then enter the URL of the server running the OPC UA Discovery service under Server Node. Click Refresh List to populate the list of available servers. Click on the desired server to select it, this will update the Server Name. Click OK to complete the servers selection. Next, enter an Alias Name and then click OK to continue. Then click OK to connect to the server Verifying the server connection. 4.23.3 If the Client is unable to connect to the specified server, errors will appear in the log: The server state can be verified by clicking on the OPC UA Client node and looking at the ServerConnected MDO to verify its value is ON. OPC UA Client Items 4.23.4 OPC UA Client items are considered MDOs in the SDG. OPC UA Client items obtain their value from an external OPC UA server. They are updated as the server provides them to the client. These points can be mapped to other protocols or to another OPC UA Client. To add an OPC UA Client item, right-click on the OPC UA Client node and Select Add OPC UA Client Item. The following dialog screen will be displayed: Click on the + to expand the tree and select the item of interest. Once selected the Items Name, ID, Description, and Type will be populated. Click on the Add Item button to add the monitored item. Select and add all items of interest from this sever then click Done. The selected items will be added to the configuration: OPC UA Client MDO Properties Properties for OPC UA Client MDOs can be modified after the item has been added. Once added, right- click on the MDO and select Edit to edit the MDO. The OPC UA Item Properties dialog screen is now displayed: To display the list of available options, click the button to get the Options Editor dialog screen: NOTEs: Currently OPC UA does not support Tag Names. This will be supported in a future release. Currently the SDG does not support OPC UA Alarm and Conditions. This will be supported in a future release. 4.24 OPC UA Server If the SDG is licensed for an OPC UA Server, it will start a single OPC UA Server when running. Internal MDOs and Data points returned from a remote slave device are automatically shown in an OPC UA Client Browser when it is connected to the SDG OPC UA Server. The default URL for the SDG is opc.tcp://localhost:4885/SDG. The logs in the protocol analyzer log window will also show what endpoints are added to the UA server based on user configuration. These log messages will appear near the beginning of the log when the user interface is first brought up. It is important to look at these logs to make sure the configuration is correct and there are no configuration mismatches. Configuration of the OPC UA Server is performed by right clicking on the OPCUAServer in the main tree and selecting Edit OPC UA Server: This will bring up the following user interface: Parameter Description Opc UA Server Name The name of the SDG that will be used in supported endpoints Add TCP Endpoint,Port,IP Select if a TCP endpoint is needed. Specify the port and IP address for the UA Server to listen on. Add HTTPS Endpoint, Port, IP Select if a HTTPS endpoint is needed (normally associated with higher security). Specify the port and IP address for the UA Server to listen on. Allowable Connection Types Select what connections types your application needs to support along with the policy id names, which must match the server policy ids Supported Message Security Types Select what kind of message encryption is needed Application Certificate The client application certificate. This certificate will likely need to be in the servers trusted directory. This is typically an X509 certificate in the DER format Application Private Key The private key certificate for the client. This is typically a PEM format Application Private Passphrase The private key certificate passphrase corresponding to the private key certificate Certificate Trust Directory The directory location for storing trusted incoming client certificates Certificate Reject Directory The directory location for storing rejected incoming client certificates Discovery Server Certificate The Certificate used to register with the discovery service Supported Application Certificate Security Policy Mask The Certificate algorithm policies that the SDG will support and allow. More information on this below. UA Server Min Sampling Interval The minimum sampling interval (in ms) for data changes for monitored items. UA Server Max Sampling Interval The maximum sampling interval (in ms) for data changes for monitored items. UA Server Min Publishing Interval The minimum allowed publishing interval (in ms) for subscriptions. UA Server Max Publishing Interval The maximum allowed publishing interval (in ms) for subscriptions. UA Server Discovery Server Refresh The refresh period for the OPC UA Discovery Server. UA Server Discovery Server Url If not empty, the SDG will register its OPC UA Server with the OPC UA Discovery Server at this URL. If the discovery service requires a certificate, use the Discover Server Certificate The type of endpoint, connection type, and message security policies should be selected based on the application needs. The Policy names for allowable connection types must match any incoming clients. To understand more in depth what each of these means, it is suggested to reference the UA protocol specification. The typical files used for application and user certificates are X509 version 3 certificate files encoded using the DER format that contains the public keys for the OPC UA Server. The security Policy Mask for both the application and user currently comes from the following choices: This is the algorithm that will be used to encrypt and decrypt both application messages and user based certificates. The user security dialog will pop up when Edit User Security is pressed: The user certificate trust directory and reject directory can be defined for User Certificate based security. And the User Names and Passwords that will be supported by the Server can also be added here. Note the password used for users will be stored in an encrypted format in the ini file and will not be visible in clear text except via this GUI if View Password is checked. The supported user certificate policy mask is the same options as at the application level shown above. 4.25 OPC UA Server Application and User Security Configuration of OPC UA Security can also be performed by editing the following parameters in the INI file: Parameter Description OpcUaSecurityCertificateAuthorityTrustDir Specifies the Directory to search for trusted Certificate Files. This directory is used by both the OPC UA Server and Clients. OpcUaSecurityCertificateAuthorityRejectDir Specifies the directory where rejected client certificates will be stored. OpcUaSecurityCertificateFile The X509 version 3 certificate file encoded using the DER format that contains the public keys for the OPC UA Server. OpcUaSecurityPrivateKeyFile File containing the servers private key that is associated with the public key. OpcUaServerPrivateKeyPassPhrase The pass phrase that was used to protect the private key. OpcUaSecurityDiscoveryServerCertificateFile The X509 version 3 certificate file encoded using the DER format that contains the public keys used to register the OPC UA Server with the OPC UA Discovery server OpcUaServerUserNames An array of user names to be given access to the OPC UA Server OpcUaServerPasswords An array of encrypted user passwords. These cannot be edited by hand. OpcUaSecurityUserCertificateTrustDir The directory associated with the UA Server where trusted user certificates reside. OpcUaSecurityUserCertificateRejectDir The directory associated with the UA Server where certificates associated with unwanted users that will be rejected or have been rejected reside. OpcUaSecurityUserSecurityPolicyMask The mask associated with the encryption types that will be supported for certificate based users. OpcUaServerSecurityUseTCPEndpoint Set to true if TCP based endpoints are to be added to the OPC UA Server. This will be a URL of the form opc.tcp://localhost: OpcUaServerSecurityUseHTTPSEndpoint Set to true if HTTPS endpoints are to be added to the OPC UA Server. This will be a URL of the form https://localhost:... It is best to set these values via the SDG USER INTERFACE rather than trying to edit these in the ini file. Caveat: It is possible to configure the OPC UA server with invalid parameters so that it will not even start and so will not show up in the main tree in the SDG. If this is the case, please look at the startup logs for the errors as to why it will not start and then use the menu Edit OPC UA Server to modify and correct the invalid parameters. 4.26 Self-Signed Certificate To create a Self-Signed Certificate you need to have a licensed OPC UA. In order to create the certificate specify the fields below: At least one Host Name or IP Address must be specified. Once created the Self-Signed Certificate will be available in the security parameter of the desired protocol. Note: A Self-Signed Certificate is not secure and should only be used for testing proposes not in a production environment. 4.27 ODBC Client The ODBC Client allows you to store and retrieve data from any ODBC data source including: SQL Server, Access, Excel, and text files. The following features are currently available: Automatic generation of MDOs related to fields and parameters in a SQL query SQL based query supported Support for SQL Server, Access, Excel, and text files (CSV, tab delimited, text) Supports read and write access to ODBC compliant databases through INSERT, UPDATE, DELETE and SELECT SQL Clauses Ability to single step or directly select a record in a record set Supports parameter substitution for flexible queries Test/Verification mode to help during query development Note that some of the features listed above may not be available on all ODBC Client driver implementations. Adding an ODBC Client 4.27.1 To add an ODBC Client to your configuration, right-click on the Gateway node and select Add ODBC Client. This will display the Add an ODBC Client dialog screen as shown below: SNAGHTML2ceeb94 Configuring a Data Source for the SDGs ODBC Client 4.27.2 4.27.2.1 Windows 1. To configure a data source for use with the SDG ODBC client, you will need to access the Administrative Tools in the Windows Control Dialog. 2. Next, on the ODBC Data Source Administrator dialog screen, select the System DNS tab at the top. 3. Scroll through the list and locate the type of driver for your data source, i.e. SQL Server, and then click Finish. SNAGHTML2cf3c62 SNAGHTML2cfec9d 4. Next, enter the Name, Description, and Server location for the data source and click Next. 5. On the next screen, enter the login credentials and click Next. SNAGHTML2d01c35 SNAGHTML2d046dd 6. Next, check the box for 'Change the default database to' and select your database from the dropdown list. Click Next to continue. 7. Next, select any items that are appropriate for your environment and then click Finish. SNAGHTML2d08209 SNAGHTML2d0af5f SNAGHTML2d0d602 8. Click the Test Data Source button to verify the data source is configured properly. 9. If configured properly, the data source should now appear under the System DNS tab. 4.27.2.2 Linux TMW installs UnixODBC Driver Manager to communicate with specific drivers. The configuration files are located: /etc/tmw/unixODBC/odbc.ini and /etc/tmw/unixODBC/odbcinst.ini. odbcinst.ini Contains driver information. The following shows Microsoft SQL Server [ODBC Driver 17 for SQL Server] Description=Microsoft ODBC Driver 17 for SQL Server Driver=/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.3.so.1.1 UsageCount=1 odbc.ini Contains Data Source Name (DSN) configuration. The following example shows a DSN=sdg_odbc. [sdg_odbc] Driver = ODBC Driver 17 for SQL Server Server = 192.168.1.37 Database = sdg_odbc Selecting an ODBC Data Source 4.27.3 Construct the ODBC connection string. Test ODBC connection string can be used to test the connection. DSN list shows a list of DSN and their descriptions. Enter an Name and then click OK to add the ODBC Client. The ODBC Client should now appear in the tree under the Gateway node. Adding an ODBC Query 4.27.4 After adding an ODBC data source, you will need to define the query for retrieving data from your data source. To add an ODBC query, right-click on the ODBC Client and select Add ODBC Item. This will launch the Add ODBC Item dialog screen. This screen allows you to select a table to build and test your queries against. The Query Name (1) allows you to define the name of the query. This is the name that will be displayed under the ODBC Client node. Please note, the name cannot contain any spaces and once the name has been defined, it cannot be changed. The Table List (3) dropdown box allows you to retrieve data information about a table from your database. The Table Info (5) display the fields defined in the selected table. The read only ODBC connection string (4) is displayed for reference. The SQL query textbox (2) allows you to define your query and offers the following SQL commands: Command Description Example SELECT Retrieves data from a table SELECT * FROM table_name SELECT (with parameters) Retrieves data from a table using a parameter SELECT * FROM table_name WHERE ID = ? INSERT Adds rows to an existing table INSERT INTO table_name (field1, field2, field3) VALUES (test, 3.45, NULL) UPDATE Modifies a set of existing rows in the table UPDATE table_name SET field1=new value WHERE field2 = 3.45 DELETE Removes existing rows from a table DELETE FROM table_name WHERE field2 = 3.45 The Execute/Test Sql Query button (6) executes the SQL commands entered in the SQL statement textbox. The Query Results pane (7) displays the results of the query defined in the SQL statement textbox. The SDG ODBC Client provides pre-defined MDOs that allow you to execute the query and navigate the results. These MDOs are operated on by right-clicking on the desired MDO and selecting Change Value of Internal User MDO. Please note, the CurrentRecord and RecordCount MDOs cannot be operated on. Command Description CurrentRecord Displays the current record number ExecuteQuery Executes the query selected under the ODBC Client node GetNextRecord Gets the next record in the data set MoveToRecord Navigate to the record entered by the user RecordCount Displays the number of records in the data set Executing an ODBC Query 4.27.5 To execute a user-defined query, select the desired query under the ODBC date source. Next, click on the Change Value button () of the ExecuteQuery command MDO. On the Change Boolean Value pop-up dialog screen, click the Value checkbox and then click OK. This will execute the selected query and display the values for each of the query MDOs defined in the database: Mapping a query MDO 4.27.6 Mapping a query MDO is performed the same as mapping other MDOs and points in the SDG. Click and hold on the MDO you wish to map and then drag and drop it to the appropriate node in the tree: Next, set the necessary options and point number of the of the MDO you are trying to map and press OK. The mapping should now be displayed as follows: Parameter Based Queries 4.27.7 The following is an example of a parameter based query that uses values input by the user to select the appropriate rows from the table. This allows you to specify a value for an MDO in the WHERE clause of the SQL statement. The parameter is denoted by a ? in the query. For example: SELECT * FROM sample_data WHERE ID=? Note: Pressing Execute/Test Sql Query button will execute statement with default values. In this example, an MDO named Param0 was created to allow you to specify the value of the ID field in the SELECT statement. Param0 can be written to prior to executing the SQL statement to specify the row that should be returned for a specific value of ID. Note: Some ODBC Driver implementations may not support parameter based queries. To set the value of your parameter, click the Edit() button of Param0 . Enter the desired value on the Mapping Items to Parameters Defined in a Query 4.27.8 You can map MDOs, equations, and points to the parameters created in your query statement in the same way you map other items in the SDG. To map an item, for example an MDO, simply drag-and-drop the MDO on the Gateway node: "Triggering" the ExecuteQuery MDO 4.27.9 You can configure the SDG to automatically trigger the ExecuteQuery MDO using the onchange equation. This is beneficial when you want to capture the data based on the value of a particular point when it changes and update your tables accordingly. The onchange equation has the following structure: onchange(expr1, expr2, expr3) Expr1 is the value that is being monitored for any changes. Expr2 is the deadband for detecting changes. When Expr1 changes by more than Expr2, Expr1 is copied to the output of this equation. The following values can be used for Expr3: Value Description 0 copy value on positive change of Expr1 1 copy value on negative change of Expr1 2 copy value on any change of Expr1 The following example uses an INSERT query to update fields in a database using parameters defined in the query: The full SQL Query: insert into sample_data(value1, value2, value3, myDate, concat, value) values(?, ?, ?, ?, ?, ?) When the value of point (P0) changes, we want the data to be written to the database table for field value1 (param0). The first step is to map point P0 to a parameter in the query; in this example param0: Next, add the onchange equation by right-clicking on the Gateway node and selecting Add Equation MDO. On the Add Equation MDO editor screen, enter the following equation: Select the desired parameter from the ODBC client node that contains the mapping for point P0; in this example Param0 and then define the values for Expr2 and Expr3: In the above equation, when Param0 changes by any value, either positive or negative, we want the query to be executed. Please note, since the onchange equation expects Expr1 to return a double, it may be necessary to 'cast' the return value of Expr1 to a double. This is denoted by double(Expr1). The last step is to map the onchange equation to the ExecuteQuery MDO of the ODBC Client. This is done by dragging and dropping the equation on the ODBCClient.addSampleDataRecord node and then selecting the ExecuteQuery MDO located under the addSampleDataRecord node: The equation (UpdateSQL) is now mapped to the ExecuteQuery MDO: When the value of P0 changes, it will cause Param0 to change, which will trigger the ExecuteQuery MDO and update the appropriate fields in the database table. Polling the Data Source for Changes 4.27.1 To poll the data source for changes, you need to use the Square equation as follows: square( 0,1,5000) Next, map the above equation to the ExecuteQuery MDO of the ODBC Client. 4.28 Mapping Points from Master Components to Slave Components To map points from Master Components to Slave Components, simply drag-and-drop an MDO on a data type of a slave device. When the MDO is dropped on the slave data type, the following dialog box is displayed: This dialog box is used to set the appropriate parameters of the slave data point. 4.29 Mapping Points from Master Components to Master Components Mapping of MDOs to MDOs is only supported for monitor points as the source and command points as the destination. For OPC client points the destination point must be a read/write point. To map points from Master Components to Master Components, drag a monitor MDO to a command data type of a master device. When the MDO is dropped on the master data type the following dialog box is displayed. Use this dialog box to set the appropriate parameters of the master command data point. Mapping OPC Client points (windows) 4.29.1 To map points from the OPC Master Component to other Master Components, drag a monitor MDO to an OPC client. When the MDO is dropped on the OPC Client the following dialog box is displayed. Use this dialog box to set the appropriate parameters of the OPC point. MDOs can also be mapped to internal points (both predefined and user defined). When mapping MDOs to these internal points they must already be defined. Select the source MDO and then drag it to a Gateway, Session, Channel, or Sector node in the configuration tree. When the point is dropped on one of these targets, a Tag Selector dialog will appear: This dialog allows the destination mapped point to be selected. When MDOs are mapped in this way, the destination point must be writeable. Once this operation is completed the source MDO will provide a value to the destination MDO. 4.30 Mapping Points from Slave Components to Slave Components Mapping of SDOs to other SDOs is accomplished by the use of Internal User Defined MDOs. See section 4.7 Internal User defined MDOs for more information. 4.31 Setting IEC 60870-5-101 and -104 Slave Data Types to use Time Stamps The SDG does not currently support setting a data type with time stamp (e.g., Type 58 Single Point with CP56 timestamp) via the user interface. However, the default data type can be set via the INI file. To set the default data type to use a time stamp, select the "base" data type from the user interface (e.g., T1 - Single Point). Then set the corresponding xxx_timeFormat parameter in the INI file. For example, for Single Points, modify the following parameter: MSPNA_timeFormat[*][*]=CP56 // Selects the time format used to report // MSPNA change events. This parameter is // only used for slave sessions using the // IEC 60870-5-101 or IEC 60870-5-104 // protocol profiles. Possible values are // 'CP24', 'CP56', or 'NONE'. // DEFAULT: CP56 // INDICES: // [sessionIndex=0-249][sector=0-63] For Double Points, use MDPNA_timeFormat, for Step Position use MSTNA_timeFormat, etc. 4.32 Setting DNP3 Event Types and Default Variations Default DNP3 event types (e.g., Type 2, Binary Input Events) are configured in the SCADA Data Gateway user interface on the Gateway Properties Editor. These types are automatically determined from the data types of the associated static data types. The DNP3 Default Variation can be set from the user interface. 4.33 Internal OPC Server (windows only) With large point sets it is recommended they be broken into individual OPC groups when an OPC client requests data from the SDG OPC server. This will allow better utilization of resources required by the SDG. There can only be 1 instance of and SDG OPC server on a machine, as a result only 1 instance of SDG is allowed to run. SDG OPC Server can run in either polled or report mode see the INI parameters OPCServerReportMode and OPCServerTagMode. By setting GlobalCreateTagAutomatic = TRUE in the INI file, the data points returned from a remote slave device to the SDG will automatically be shown in an OPC Client Browser that is connected to the SDG OPC Server. Simply define the remote slave device in the INI file, or by using the Configuration User Interface, run the SDG, and perform a general interrogation command (see section 4.37.12 Predefined Internal MDOs for Monitoring and Control). All data points returned from the remote device will then be shown in the OPC Client Browser and will not require an entry in the Point-Mapping file. Since Command points are not received from the remote device, they will not be shown in an OPC Client Browser. Command points may be configured in one of two ways: 1. Manually enter a physical tag name in the OPC client for the command point, for example if the protocol is IEC60870-5-101 the physical tag name might be "COM1.L3.A3.T46.P1142". The value written to this tag will be converted to a 101 Double point command using the command qualifier and select mode (whether a select command is sent prior to the execute) specified in the "Command Qualifiers" section of INI file. This method requires all points of the same type use the same command qualifier and execute mode settings. 2. Create a point-mapping file with only Master component entries for each command point (the Slave component columns are left blank) as outlined later in this section. Use the options field to set the Qualifier of Command and select mode to a different value for each point if necessary. If the options field is left blank, the values specified in the "Command Qualifiers" section of the INI file will be used. Points listed in the point mapping file will appear in the OPC Client Browser followed by the user name in parenthesis (if one was entered). OPC Server Registration for SDG Service 4.33.1 As mentioned in the Quick Start guide the SDG must be registered as an OPC server if you wish to connect to it from an OPC client. Normally this is done by the install. Registration of the SDG as an OPC server can be done from the Windows Command Shell (DOS window) as follows: Open a Windows Command Shell (DOS window) by going to StartRun and typing cmd in the "Open:" field, and then clicking the OK button. When the Command shell is opened, you will typically be in your C: drive. If you have installed the SDG on a hard drive other than the C: drive, then change to that drive by typing the letter for that drive followed by a colon. For example, if you have installed the SDG on your E drive, then type the command "E:" to change into the E drive. Change to the directory where your SDG is installed by typing the command "cd PATH_TO_SCADA_DATA_GATEWAY", where PATH_TO_SCADA_DATA_GATEWAY is the directory where you have installed the SDG. For example, if you installed the SDG in the directory C:\Program Files\TriangleMicroWorks\SDG and you are in the C drive in your DOS window, then type the command "cd C:\Program Files\TriangleMicroWorks\SDG". Depending upon how your system is configured, you may have to use the short directory names, such as "cd C:\Progra~1\Triang~1\SDG". Next, type the command "GTWEngine.exe -install" to register the OPC server and run as a service. This will register the Alarm and Event as well as the Data Access servers. Once registered, the SDG will be available to OPC client software under the name TriangleMicroWorks.OPCDA.1 for the OPC Data server and TriangleMicroWorks.OPCAE.1 for the OPC Alarm and Event server. Un-registration is as above but use " GTWEngine.exe -remove". The SDG service can, and should, also be registered as an OPC server. In case this process has to be done manually, here are the necessary commands: Command Description GTWEngine.exe install setup to run as a service and register opc server GTWEngine.exe remove remove setup to run as service and unregister opc server GTWEngine.exe regopcserver register as opc server and not a service ie command line mode GTWEngine.exe unregopcserver unregister as opc server and not a service ie command line mode DCOM Identity (from the dcomcnfg tool) SNAGHTML151a6b5e When running the SDG engine in command line mode the default setting is The launching user this setting works fine if you are doing simple interactive development with the command line mode although it will cause multiple SDGs to attempt to be launched should you try to connect to the SDG OPC Servers from different user accounts or a service running under a different account. Running multiple instances of the SDG is not supported when using the OPC Servers. When using multiple user accounts be it a service connecting or a different user it is required that either the The interactive user or the This user setting be selected. The interactive user requires that a user is logged in to the station in order to launch the SDG from an OPC client. The This user setting can be used to allow the SDG to launch without a user logged into the station. When running the SDG as a service (i.e. GTWService) it is recommended that the The system account (services only) setting is selected. OPC Server Point Properties 4.33.2 The SDG OPC Server exposes several properties of the Master Data Objects (MDO). These allow access to the quality, description and other properties of a MDO. The following properties are supported: Property Name OPC Property Value Description #DataType OPC_PROP_CDT Provides the OPC Variant data type of the Point #Value OPC_PROP_VALUE Provides the current value of the point (same as the point it self i.e. without the .Value) #Quality OPC_PROP_QUALITY Provides the OPC quality of the point #TimeStamp OPC_PROP_TIME Provides the update time of the point #AccessRights OPC_PROP_RIGHTS Provides the access rights of the point #Description OPC_PROP_DESC Provides a description of the Point (if it was provided) To use a property add it to the end of the tag name. For instance: if the MDO tag being subscribed to by an external OPC Client is named MDNP.L4.T30.P0, to obtain its quality use MDNP.L4.T30.P0#Quality. Simply specifying MDNP.L4.T30.P0 will return the value of this MDO. SDG architecture 3 4.34 Mapping Points to an External OPC Server The OPC Client in the SDG is a master component. It receives data from an external OPC Server and makes it available to the slave components in the SDG. The SDG Point Mapping file is used to describe this data flow. When creating a Point Mapping file for the OPC Client, it is only necessary to define the User Tag Name and Communication Channel fields. The User Tag Name must be the fully qualified path name to the data point in the external OPC Server (such as Device1.Group.Tag1). The Communication Channel must be the OPCserverName from the SDG INI file. If the OPCServerName is not specified in the initialization file, it defaults to the OPCserverProgID. Refer to Section 1.4.4 for more information about these parameters. 4.35 Format of the Point-Mapping file The Point-Mapping data is stored in a Comma Separated Value (CSV) format. It may be derived from existing data files describing the target application or generated from the SDG configuration editor. The following table lists the order and usage of the fields in the Point-Mapping file. Item Component Attribute 1 Master User Tag Name / OPC Tag Name / IEC 61850 Tag Name / IEC 61850 Data Set Alias Name 2 Master Communication Channel / OPC Server Name/ IEC 61850 Client Name / IEC 61850 Data Set Logical Node Name/ ODBC Client Name 3 Master Data Link Address (Not used for IEC 60870-5-104) / IEC 61850 Report Name / IEC 61850 Data Set Name / ODBC Query Name 4 Master ASDU Address (Not used for DNP) / IEC 61850 Quality Name / IEC 61850 Data Set Element Name / ODBC_MDO if ODBC point 5 Master Type ID / OPC type/ IEC 61850 Type / IEC 61850 Data Set Report Control Block Name / ODBC Point type 6 Master Information Object Address/Information Number/ IEC 61850 Time Name / IEC 61850 Data Set GOOSE Control Block Name / ODBC Point name 7 Master Element Index (Only used for IEC 60870-5-103) 8 Master Function Type (Only used for IEC 60870-5-103) 9 Dataset Identifies the dataset present values descriptor element 10 Slave/Master Communication Channel / mapped OPC server 11 Slave/Master Data Link Address (Not used for IEC 60870-5-104) / mapped OPC tag name 12 Slave/Master ASDU Address (Not used for DNP) 13 Slave/Master Type ID / mapped OPC type 14 Slave/Master Information Object Address/Information Number 15 Slave/Master Element Index (Only used for IEC 60870-5-103) 16 Slave/Master Function Type (Only used for IEC 60870-5-103) 17 Master Equation 18 Master Options 19 Slave Options 20 Master Description 21 Mapping Mapping mode (optional) the value can be MDNP_PROTOTYPE, MDNP_PROTO_ELEMENT, MDNP_DESCRIPTOR, MDNP_DESCP_ELEMENT, MDNP_DESCP_ELEMENT_VALUE, SDNP_PROTOTYPE, SDNP_PROTO_ELEMENT, SDNP_DESCRIPTOR, SDNP_DESCP_ELEMENT, SDNP_DESCP_ELEMENT_VALUE,USER, OPC_USER, MDO_USER, OPC_INTERNAL, MDO_INTERNAL, MDO, SDO, OPC, I61850_DS, ICCP_MAPPING or I61850_MDO. If not specified SDO is assumed. Table 1 - Order of fields in the Comma Separated Value format Point-Mapping File The configuration editor is used to build the Point Mapping file using a graphical user interface. The editor provides the tools required to intuitively specify the information required in the Point Mapping file. The Point Mapping file is both written and read by the SDG configuration editor; although it is not recommended a user can choose to modify the file outside of the configuration tool to facilitate a more rapid editing approach. For instance one could create some mappings through the Web interface and then duplicate it many times in a text editor (through copy/paste) while changing some key fields in the Point Mapping file. For DNP datasets the first 3 to 6 fields are overridden with specific dataset descriptor and prototype meanings (i.e. The mapping modes included for data set descriptors and prototypes). Definition of Fields contained in the Point Mapping File 4.35.1 4.35.1.1 Point Identification Fields Data points are identified by their physical connection to the SDG and the addressing within the communication protocol configured for the remote device. The following table applies to both Master and Slave Component points; however, some of these fields are only applicable to specific protocols. 4.35.1.2 Data Options Field The options field allows the default properties of a point to be over-ridden. For some options, a global setting in the INI file defines the default behavior. The options field allows point properties to be specifically enabled on a point-by-point basis. For example, logging of IEC 60870-5-101 single point binary events is controlled by INI file parameter MSPNA_logToFile. If this is set false, specific individual single point event objects will have their events logged by entering LOG in the option field for that point. If MSPNA_logToFile is set true, specific single point objects will be excluded from the event log file by entering !LOG in the option field. The Options field may contain zero, one, or more of the permissible options for the object (e.g., G02 GI). When multiple options are required for one point, each option should be separated by a space. Options are specified on various configuration dialog boxes by clicking the button (1) and selecting the required options on the Options Editor dialog screen: Note the options editor has a tool tip to describe each option. 4.35.1.3 Table of Control Options and the Meaning The option field provides a means of specifying various properties of a point, and has different values depending on the type of point. The options supported for each protocol are listed below: Component Operation Option Name Meaning All protocols: Master Log file LOGMSKxxxx See Table of Events in the section following this section. Record (do not record) changes/events on this point in the log file. Master OPC Alarm and Events OPCAE_LOGMSKxxxx See Table of Events in the section following this section. Provide (do not provide) changes/events on this point through the OPC Alarm and Event Server. Master Control WRITE_PERIOD xxxx **DEPRECATED** Specifies the period xxxx at which to write the control point in (ms). This will cause a write command to be executed for the specific protocol of which the point is a member. This will work for any point that allows control/write operations. Normally a command is only issued when the command points driving point (i.e. the point supplying the value for the command point) changes value or is updated. Master Control 2PASS Control point issues 2-pass control sequence (Select/Execute) 1PASS Control point issues 1-pass control sequence (no Select command) Slave Control 2PASS Control must be performed with a Select/Execute command pair 1PASS (!1PASS) Control will be performed when an Execute command is received (Select is not required) SYNC Control is synchronous. The reply indicating completion of the command is sent after the downstream device completes the command. ASYNC Control is asynchronous. The reply indicating completion is sent immediately to the originating device and is not delayed waiting on the downstream device to report completion of the command. Slave Monitored Data GI (!GI) Point is included in (excluded from) response to General Interrogation (Station Interrogation) request CYC (!CYC) Point is (is not) reported as cyclic data OPCTIMEUPDATE The event time used in OPC updates is the time at which this value was last updated relative to the SDG system clock. 1 OPCTIMEREPORTED The event time used in OPC updates is the reported time from the remote device of the last event that caused this value to change. 1 OPCAETIMEUPDATE The event time used in OPC Alarm and Event updates is the time at which this value was last updated relative to the SDG system clock. 1 OPCAETIMEREPORTED The event time used in OPC Alarm and Event updates is the reported time from the remote device of the event that caused this value to change. 1 Slave Equations EQTIMEUPDATE The event time used for this equation is the time the equation was last calculated relative to the SDG system clock. 1 EQTIMEREPORTED The event time used for this equation is the reported time from the remote device of the last event that caused the equations value to change. 1 UPDATE_IF_NO_CHANGE If this option is specified, when an equation is calculated regardless of whether its resulting value changes or not the resulting value of the equation will be passed on to any mapped points. OPC DA/AE Client Name ALIAS xxxx Specify an ALIAS name for the OPC DA or AE Client point. This is the name that the SDG will use internally for this OPC Client tag. It has the same usage as the User Tag Name. OPC DA Client Write WRITE_INVERTED Specifies that when the OPC Boolean point is written it should be written with a value of TRUE if it is FALSE and FALSE if it is TRUE. OPC DA Client Write WRITE_TRUE_AS_ONE Specifies that when the OPC Boolean point is written it should be written with a value of 1 if it is TRUE instead of -1 which is the numeric value of TRUE in OPC. OPC AE Client Event EVENT_VALUE Used to specify/override the default message value for the event. (i.e. EVENT_VALUE true will cause the MDO value to be true when the event is received) OPC AE Client Event EVENT_TRUE_MSG Used to parse the message string of the event and look for the string specified if the string is found the MDO value will be set to true. OPC AE Client Event EVENT_FALSE_MSG Used to parse the message string of the event and look for the string specified if the string is found the MDO value will be set to false. OPC DA Client Event AE_INIT_VALUE_MDO Used to specify the OPC AE MDO that will be initialized with the value, quality and time stamp of the Data Access MDO once initialized the VQT will not be applied again unless the OPC AE Client disconnects from its server. OPC DA Client Event AE_RESET_VALUE_MDO Used to specify the OPC AE MDO that will receive a VQT from the OPC DA MDO if the AE MDO value is different from the DA MDO value. OPC DA Client Event AE_RESET_VALUE In association with AE_RESET_VALUE_MDO is used to specify the value of the OPC DA MDO that will cause the OPC AE MDO to be set to the VQT of this OPC DA MDO when an OPC DA update is received for this OPC DA MDO. Valid values for this options argument are 0,false,1,true, numeric value (float or integer) or some string. If this option is specified the difference comparison in the AE_RESET_VALUE_MDO option will not be used. IEC 60870-5-101 and IEC 60870-5-104: Master Single Command Qux Specifies Qualifier of Command value to use when qualifier is otherwise unknown or when qualifier is forced (see FQU below). x is a hexadecimal number representing the qualifier value. FQU Force Qualifier of Command to default value specified in .INI file or with QUx option above, even if a qualifier is received from an upstream device (e.g., translate a pulse into a latch). This is mutually exclusive with PQU. PQU Allow Qualifier of Command received from upstream devices to be passed through to downstream devices (e.g., a long pulse received control is passed through to the downstream device even though default command qualifier for point specifies a latch.). This is mutually exclusive with FQU. All Commands TO nnn Application Response Timeout. Maximum amount of time in milliseconds to wait for a response to a command issued to this data point. This option can be used to override the ApplAbsRespTimeout configuration parameter on a per point basis. Binary In Double Point (Type ID 3) PROVIDE_AS_RAW Provide the value of the double point as follows: 0 - "Transit" 1 - "Off" 2 - "On" 3 - "Indeterminate" Normally (when this option is not specified) the value is provided as true/false with the quality describing the additional states. Analog In (Type ID 9,11,13) SCALE rawMin rawMax eguMin eguMax Provide scaling value for the point. The raw analog input value will be scaled as follows: m = (eguMax-eguMin) / (rawMax- rawMin) b = eguMin (m * rawMin) scaledValue = (m * inValue) + b. The resulting value will be provided as a double to any mapped points. Analog In (Type ID 9,11,13) DEADBAND value If this parameter is specified for a master device, the value of an analog point read by the master will have to change by more than this amount to be transferred to a mapped slave point (i.e. causing an event). A value of 0 will disable dead band testing. Step Command (Type ID 46) ALLOW_UP ALLOW_DOWN ALLOW_ANY Allows the configuration of selective blocking of commands passed on to external slave devices. For instance ALLOW_UP will block a down command and ALLOW_DOWN will block an up command. Command values are typically received form mapped slave data objects and passed on to the external slave device. Double Command (Type ID 47) ALLOW_ON ALLOW_OFF ALLOW_ANY Allows the configuration of selective blocking of commands passed on to external slave devices. For instance ALLOW_ON will block an off command and ALLOW_OFF will block an on command. Command values are typically received form mapped slave data objects and passed on to the external slave device. Slave Monitored Data G01, G02, G03, , G16 (!G01, !G02, , !G16) Point is included in (excluded from) response to Group xx Interrogation request (where Gxx identifies group: 01 to 16) Binary Commands (Type ID 45, 46, 47) ALLOW xxxx Control is blocked if a control operation is attempted but not configured in this mask. The reply indicating completion of the command will indicate that this command failed. Analog Commands (Type ID 48, 49, 50) 1PASS (!1PASS) Control will be performed when an Execute command is received (Select is not required) IEC 60870-5-103: General Command (Type ID 20) ACCEPT_AS_RAW Accept the value of the double point as follows: 0 - "Transit" 1 - "Off" 2 - "On" 3 - "Indeterminate" Normally (when this option is not specified) the value is accepted as true/false with the quality describing the additional states. Binary In Double Point (Type ID 1,2) PROVIDE_AS_RAW Provide the value of the double point as follows: 0 - "Transit" 1 - "Off" 2 - "On" 3 - "Indeterminate" Normally (when this option is not specified) the value is provided as true/false with the quality describing the additional states. Master Binary Data (Type ID 1,2) RTZ Return to Zero: generate a 10 transition whenever a 01 transition is received DNP3: Master Binary Control (Type ID 12) LATCH Sets the default command mode for this point to latched. Writing a value of 0 to this point will issue a latch off command, 1 will issue a latch on command. PULSE Sets the default command mode for this point to pulse. Writing a value of 0 to this point will issue a pulse off command, 1 will issue a pulse on command. PRIORITY nnn Sets the priority of the binary or analog write command. 128 is the default if not specified. PULSEWIDTH nnn Sets the pulse width for this point to nnn milliseconds. This option is used in conjunction with the PULSE option. PULSEWIDTH_VAR mdo_name Sets the pulse width for this point to mdo_name milliseconds. This option is used in conjunction with the PULSE option. Mdo_name represents a MDO tag in the SDG whose value is used to specify the pulse width. PAIRED Sets the default command mode for this point to paired. Writing a value of 0 to this data point will issue a trip command, 1 will issue a close command. FCM Force default command mode. The command mode specified in the INI file, or by one of the LATCH, PULSE, or PAIRED, options will override the command mode in an incoming request. PCM Pass command mode. The command mode received from upstream devices will pass through to downstream devices independent of the current default command mode for this data point. Analog In (Type ID 30) DEADBAND value If this parameter is specified for a master device, the value of an analog point read by the master will have to change by more than this amount to be transferred to a mapped slave point (i.e. causing an event). A value of 0 will disable dead band testing. Analog In (Type ID 30) SCALE rawMin rawMax eguMin eguMax Provide scaling value for the point. The raw analog input value will be scaled as follows: m = (eguMax-eguMin) / (rawMax- rawMin) b = eguMin (m * rawMin) scaledValue = (m * inValue) + b. The resulting value will be provided as a double to any mapped SDOs. Slave Monitored Points CLS1, CLS2, CLS3 Specifies which class events for this data point are returned in. This option can be used to override the ObjXXDfltClass INI configuration parameters on a per point basis. Binary Output (Type ID 12) Binary Output (Type ID 41) ALLOW Bitmask to allow/disallow specific control operations: 0x01 No Select Allows single-pass control operations (allows operations that were not previously selected as part of a SBE or 2-pass operation) 0x02 Use Default Allows no specification of operation. This allows pre-defined, fixed, or default operations to take place. It can be used with binary controls (OBJ12) 0x04 Pulse Allows pulse for binary controls (OBJ12) 0x08 Latch Allows latch for binary controls (OBJ12) 0x10 Paired Close Allows paired close for binary controls (OBJ12) 0x20 Paired Trip Allows paired trip command qualifier (OBJ 12) 0x40 Non-standard Allows non-standard command qualifier Modbus: Master Holding Register, Coil PRIORITY nnn Sets the priority of the write command. 128 is the default if not specified. Holding Register, Input Register DEADBAND value If this parameter is specified for a master device, the value of an analog point read by the master will have to change by more than this amount to be transferred to a mapped slave point (i.e. causing an event). A value of 0 will disable dead band testing. Master Holding Register, Input Register SCALE rawMin rawMax eguMin eguMax Provide scaling value for the point. The raw analog input value will be scaled as follows: m = (eguMax-eguMin) / (rawMax- rawMin) b = eguMin (m * rawMin) scaledValue = (m * inValue) + b. The resulting value will be provided as a double to any mapped SDOs. Master Coil write ALLOW_ON ALLOW_OFF ALLOW_ANY Allows the configuration of selective blocking of commands passed on to external slave devices. For instance ALLOW_ON will block an off command and ALLOW_OFF will block an on command. Command values are typically received form mapped slave data objects and passed on to the external slave device. Master Coil write INVERTED_WRITE Inverts the value written to the external slave device connected to SDG Modbus master. For instance if the value received from a mapped SDO is true the value written will be false. Master and Slave Holding Register, Input Register SIGNED Allows the specification of a holding register or input register as a signed 16- bit value. The default is that these types are unsigned 16-bit values. This only changes the way that the SDG interprets the bits in the register. If this option is specified for a point and the point is mapped to a different component the mapped to component will see the value of the point as a signed value. Modbus Slave Holding Register, Input Register FLOAT xxx Allows the specification of 2 consecutive holding registers or input registers as a 32-bit floating point value. The value can be both written and read. The xxx has the following options: 0x01 : 1st register (word) is least significant 0x02 : 1st register (word) is most significant When this option is specified IEEE-754 Floating-Point format is used for the values. When writing a floating point value (i.e. as 2 words) the writing of the 2nd word will cause any up-down stream data to be affected. Table 2 Point-Mapping file options field For a detailed description of reported vs. update time, see section 4.38.1 Event Time Tags and Database Update Times 4.35.1.4 LOGMSKxxxx and OPCAE_LOGMSKxxxx Events used in the MDO option field This table defines the kind of event LOGMSK may use for an individual data object as identified in the above table. These mask values will also be used as defines in the Sequence of Event Log Files and/or the OPC Alarm and Event Server. The Types of Log Masks supported by the SDG can be found in the INI file. Mask Value Type of Event 0x0001 The data is being updated, but the reason for the update is unknown. 0x0002 The data is being updated because it was requested. 0x0004 The data is being updated by the source of data without direct request. No change is necessarily indicated. 0x0008 A change in the data is indicated by the source of data. 0x0010 A change in the data is indicated due to a request through communications. 0x0020 A change in the data is indicated due to action at the device. 0x0040 A control request has been transmitted to a remote device. 0x0080 A control request has been confirmed by a remote device, but is not yet complete. 0x0100 A 1st pass in a 2-pass control operation has been transmitted to a remote device. 0x0200 A 1st pass in a 2-pass control operation has been confirmed by a remote device. 0x0400 A cancel operation has been transmitted to a remote device to abort a 2-pass control operation between the 1st and 2nd passes. 0x0800 A cancel operation has been confirmed by a remote device. 0x1000 A remote device has responded to indicate an error in a control operation. 0x8000 Either the data point or the remote device is operating in a test mode. Table 3 Types of Events used in the SOE Logger and the OPC Alarm and Event Server 4.35.1.5 Physical Tag Name Each row that defines a Master Component data point has a Physical Tag Name field that is automatically generated from the other fields (it is not entered by the user). Slave Component points do not have Tag Names. When a physical tag name is generated, the component fields in the tag name have field identifier prefixes as follows: Field Prefix Example Data Link Address L L3 ASDU Address A A44 Type ID/Object Group T T1 Point Address P P231 Element Index (103 only) (none) 6 Function (103 only) F F160 When a field is not used in the protocol associated with a point, the field does not appear in the tag name. Examples of physical tag names are: The result of all equations is of type double. The type conversion functions are provided to allow changing the equation result into another type. For example, the bool() function can be used to convert the result of one equation into a Boolean value that can be used as an input to an equation that requires Boolean arguments. Tag Name Protocol COM1.L12.A23.T3.P1 (101 protocol) COM1.L12.A23.T3.P146.0.F160 (103 protocol) COM1.A23.T3.P1 (104 protocol) COM1.L3.T1.P0 (DNP protocol) COM1.L3.T0.P0 (Modbus protocol) 4.35.1.6 User Tag Name In addition to the Physical Tag Name, a User Tag Name may be defined for each Master Component Point. Slave Component points do not have Tag Names. The User Tag Name is a user-supplied ASCII text string that identifies the Master Component Point (e.g., BreakerReclose) in a human readable manner. The User Tag Name is case sensitive, should not contain spaces (underscores are allowed), and each one must be unique. Since the User Tag Name is passed through the SDG OPC Server to an external OPC Client application, care should be taken when using characters in tag names that may have special meaning to the OPC client application. Some OPC clients may interpret characters such as (! % @ . etc.), in the user tag name with special meaning. Therefore, please check the manual for the OPC client if these characters are to be used. The User Tag Name is also used to specify the name of a tag in an external OPC Server when configuring a Master Data Point that should connect to the external OPC server. The name specified should be the fully qualified name as would be used to connect to the server using any other generic client (i.e. Device1.Group2.CloseBreaker). The User Tag Name may be systematized to reflect the Master Point addressing if the user does not impose some other naming system. The User Tag Name is optional and this field may be left blank. If a Master Component Point is not assigned a User Tag Name, it may be referred to by its physical tag name, which reflects its communication protocol-specific identifiers. If a Master Component point is associated with more than one Slave Component point, the first instance of a Slave Component point normally appears on the same line as the definition of the Master Component point. Subsequent Slave Component point definitions reference the Master Component point with the User tag Name or Physical tag Name that was assigned to the Master Component point. 4.35.1.7 Equation Field The equation field is used to logically or arithmetically operate on one or more Master Component points referenced in the equation expression. Either a Physical Tag Name or a User Tag Name may be specified in the equation field. The result of evaluating the equation parameter field is used as the value of the Master Point. This value will be assigned to a Slave Component point defined on the same line, or other Slave Component points referring to the assigned User Tag Name. One use of equations is to convert the raw value reported by the SDG into an IEC Normalized or Scaled value. For example, to convert the reported value, which has a range of -32768 to 32767 to a Normalized value, which has the range of -1 to +1-24.35.1.9. -15, enter an equation to divide the reported value by 32768. Please refer to the examples below in Section The equation parser does not support equations in which two operators appear consecutively. For example, an equation of the form (x * -2) is not supported. Generally, you can work around this limitation by rewriting the equation. For this example, the equation can be rewritten as ((0 x) * 2). When an equation argument is a tag and contains characters normally used as operators for calculations (i.e. / + * etc), the tag should be enclosed in (single quote) to facilitate parsing of the tag. The (single quote) operates as an escape character on the tag name. Certain equation operators require that the value of their arguments be able to be converted to a type double (R8) if the argument does not natively/internally support this conversion a casting operator should be used to convert the value to the appropriate type. For example the following will require a cast: If the tag _intR4 is an internal MDO of type R4; Then the equation short(_intR4 * 100) will generate a type mismatch error since the multiply (*) operator requires doubles as its arguments. To get this to work the equation should be written as short(double(_intR4) * 100). The reason for this is that internal MDOs are strictly typed and only return values of the type they are declared as. For example, the following OPC Client Tag: /SUBESTACAO_P/INPUT/00_00_10/Message_2_3.m11 should be enclosed in (single quotes) as follows: /SUBESTACAO_P/INPUT/00_00_10/Message_2_3.m11 This allows it to be used as an argument in an equation. Example: SUM(/SUBESTACAO_P/INPUT/00_00_10/Message_2_3.m11,2) Equations are entered using the equation editor. For more information on using equations, please review section 4.8 Equations. Expression Parameters Definition Physical Tag Name Physical Tag Names previously defined in the Point-Mapping configuration file User Tag Name User Tag Names previously defined in the Point-Mapping file numerical constant Any integer, floating point, or hexadecimal value; (e.g., 23, 14.5e-05, or 0xffa782b respectively) (expression) Any combination of Physical or User Tag Name, and supported operators enclosed within parenthesis. expression op expression Any expression operated with another expression. Only supported operators may be used (the list of supported operators are listed below) function(expression-list) Where function is a supported function as listed below, and expression-list is one or more expressions separated by commas. SCADA Data Gateway| Implementers Guide 4.35.1.8Supported Operators Used in Expressions OperatorEvaluates to: &&Logical andoftheexpressions on eitherside of the operator||Logical oroftheexpressions on either side of the operator&Bit Wise and of the expressions oneither side of the operator|Bit Wise or of the expressions on eitherside of the operator-Arithmetic resultof subtracting the expressions oneither side ofthe operator+Arithmeticresult ofadding theexpressions on either sideof the operator*Arithmetic resultof multiplying the expressionson either sideofthe operator/Arithmetic resultof dividing the expressions on eitherside ofthe operator==Compares the leftandright side ofthe == and returns true ifthey are the same value!=Comparesthe left and rightside of the !=andreturns true if they are notthe same value>Comparesthe left and rightside of the >and returnstrue if the leftsideisgreaterthan the right side<Comparesthe left and rightside of the <and returnstrue if the rightsideisgreaterthan the left side>=Compares the left and rightside of the >=andreturns true if the leftside is greaterthanorequal to the rightside<=Compares the left and rightside of the <=andreturns true if the leftside is greaterthanorequal to theright side (singlequote) Used toallow specialequation operator characters as partofthe name of anMDO that isused asan argumentin the equation. The MDOnamemustbeenclosed in single quotes. 4.35.1.9Examples of Valid Expressions COM1.L2.A2.T1.P48.F128 && COM1.L2.A2.T1.P49.F128 COM1.L2.A2.T11.P300.F128 * 5.43 + 20 OR(COM1.L2.A2.T1.P48.F128, LineEarthFault, BusEarthFault) AND(LineEarthFault,NOT(COM1.L2.A2.T1.P49.F128)) GET_FLAGS(MDNP.L4.T1.P0) TEST_BIT(MDNP.L4.T30.P0,2) RAND(0,100,1000) DIV(COM2.L3.A4.T9.P500, 32768) if(mdnp.L4.T30.P0 > mdnp.L4.T30.P1, mdnp.L4.T30.P0, mdnp.L4.T30.P1) (0 -COM1.L2.A2.T11.P301) * 2 pulse( 0,50,ulong(not(myBool)),ulong(myInt),1) 4.35.1.10DescriptionField The Description field permits comments to be entered aboutthepoint. 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |227 4.36 Master and Slave Component Columns The configuration editor maps data points in Master/Slave Components (shown in the Name column) to data points in Master/Slave Components or command points in Master/Slave Components (shown in the mapping column). The purpose of this is to specify how the value for each data point in a one Component or command point in another Component is obtained. This could be a direct mapping to a Master Data Point, or an equation, which incorporates multiple Master Data Points. This mapping can also include changing fields such as the information object address and even conversion to a different protocol. The data points defined in the Master Components are an image of monitored or controllable data in remote Slave devices. There may be data received by the Master Component which has no Master Point defined. This is not an error condition and the unused data is simply discarded. Since the OPC Server Component is able to automatically generate a list of data received from the remote master device, these points do not require entries in the Master Component column (i.e. They are mapped automatically when an OPC client connects to the SDG or at startup. For more information on mapping points to the OPC Server, see section 0 Default DNP3 event types (e.g., Type 2, Binary Input Events) are configured in the SCADA Data Gateway user interface on the Gateway Properties Editor. These types are automatically determined from the data types of the associated static data types. The DNP3 Default Variation can be set from the user interface. Internal OPC Server or 4.25 OPC UA Server. SCADA Data Gateway| Implementers Guide When mapping monitor master points to control/command masterpointstheNamecolumn represents the control/command point and theMappingcolumn represents the monitor point. The monitor point receives a value from an external slave andthencauses the mapped mastercommand pointto issue a command to its associated slave. The following isread as mmb.L1.T3.P1 provides data to smb.L1.T3.P1. In addition thedata is also providedto theOPCUAserver. In this diagram there is a bi-directional mapping between mmb.L1.T4.P1 and smb.L1.T4.P1 as well astheOPC UAtag. Figure17Example of Master to Slaveand Masterto MasterAssociation Multiple Slave ComponentPoints may be associated witha single Master Component Point.The Slave data points identified in the left-hand columns are data thatwillbe created inSDGSlave Components, and may bereadby remote Masters. While a Slave Component point can only be mapped to one Master Componentpoint, aMaster point may be mapped to zero, one, or more Slave Component points. Ifmapped to zero SlaveComponent points, a Master Componentpoint is only available to theOPCServerComponent. FieldCSV file fieldindexApplies to ProtocolRange ofValuesCorresponding INI file parameter or commentUser Tag1101,103, 104, DNP, OPC Client, OPC AEClient, IECStringSpecifythe fully qualified name of the OPC tag in the externalserver or a userdefined namefor the tag or a fully 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |229 SCADA Data Gateway| Implementers Guide 61850qualified IEC 61850 tag nameChannel / Port2,10101,103, 104, DNP, OPC Client, datasets, OPC AEClient, IEC 61850COM1, COM2,127.0.0.1, PhysComChannel, OPCserverName, PhysComChnlName, I61850ServerNameData LinkAddress3,11101, 103,DNP, datasets, IEC 61850DecimalSessionLinkAddress, I61850ReportControlBlockNameASDU Address4,12101, 103,104, datasets, IEC 61850DecimalDbasSectorAddress if a session (linkAddress) only has one sector (ASDUaddress) defined, the link address and the ASDU address are usually setto the same valueType ID (101,103) Object Group (DNP) 5,13101,103, 104, DNP, datasets, OPC Client, OPCAEClient, IEC 61850DecimalDefines the data type of the point. Mustreference the non-time-tagged Type ID forthe data type. See InteroperabilityDocuments inAppendix for list ofsupported Type IDsfor each protocol. IOA/INF6,14101,103, 104, DNP, datasets, OPC Clientitem properties, IEC61850DecimalPoint Number (DNP) Information Object Address (101,104) Information Number (103) Property ID for OPC client pointElement Index7,15103DecimalZero offset index to identify point withinINF for 103 Type IDs 3 (Measurands I) and 9 (Measurands II), leave thisfieldblankfor all other Type IdsFunction Type8,16103DecimalIt is assumed that a remoteIEC 60870-5-103 slave devicewill report each INFwith only onefunction type. DNP Dataset9DNPIdentifies the dataset present valuesdescriptor element Equation17AnyStringSpecifies the equationTag Options18,19AllStringSpecify options/properties for the tagTag Description20AllStringSpecify a description for the tagMapping Mode21AllStringProvides a hint to the point map parser asto how to interpret the data inthe otherfields. 4.37Data Points 4.37.1Modbus Point Numbering Some Modbus devices use0-based numbering, whereas other devicesuse 1-based addressing. A commonconvention is touse1-based numbering, with a preceding digit to indicatethetype of data. Withthe preceding digit prependedtothe address, the full range is: Page|230 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide AddressType000001 -065535Digital Outputsand internal flags100001 -165535Digital inputs300001 -365535Analog inputs400001 -465535Analog outputsand internal Holding registers The SDGusesthe valuein the protocol, which is 0 65535, with no preceding digits.Thus,values are represented as: Above conventionSDG000001 065535Type x0 65535100001 165535Type y0 65535Etc. 4.37.2Point Initialization When it is first started, theSDGcreates storage space in its internal database forthe points it will receive fromthe remote device. However, since the point values have not yet been received from the device, the SDGsets theinitial value of the points to 0 and setsthequality flag for the points to invalid (STD_QLTY_INVALID). Therefore, the application receiving the data should take into consideration the quality flag ofthe point beforeusing the data. The followsections of thisdocument describe how the values and quality flags ofthe points are translatedbetween the different protocolsso that you can ascertain the certainty of your data. As an example, though, consider the situation where an OPC Client is reading point values from the SDG. In this case, the SDGsSTD_QLTY_INVALID quality flag gets mapped tothe OPC Client withthe statusOPC_QUALITY_BAD. Therefore, when theSDGis started, it is possible tosee results similarto what is shown in Figure18. The point MDNP.L3.T1.P0(Point0) has an initial value ofOff but the OPC quality of thispointis 8 which means that the pointvalue isbad. Once the current value of this point has been received fromthe remote device,theSDGwill update theOPCClient accordingly, and thepoint will have a quality value of 192 which means the point value is good. 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |231 SCADA Data Gateway| Implementers Guide simple OPC item properties Figure18TMW Simple OPC Client 4.37.3Translating data types betweendifferent protocols (Master to Slave mapping) In the Point-Mapping example shown below, the mappingtranslates theIEC 103 protocolto anIEC 101 protocol(1). TheMasterTypeID 20 may receive Slave TypeID 45 datain protocol 101. The following table showsthe datatypes which may be translated from one protocolto another. Summary ofEquivalentData Types forDNP3, IEC 101, IEC 103, Modbus and OPCDescriptionDNP3ObjectIEC101/104 TypeIEC 103 TypeModbusTypeOPC TypeDatabase FormatBinary Input1, 2, 101, 2, 3, 4, 30, 311, 21 (discrete input)VT_BOOLBoolean Page|232 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide Analog Input(Integer) 30, 32, 409, 10, 11, 12, 34, 353, 94 (inputregister)*2VT_UI1, VT_I1, VT_I2 orVT_I4Short orUnsigned LongAnalog Input(FloatingPoint) 13, 14, 364VT_R4Short FloatBinary Output, Step Change (101 only) 1245, 46, 47200 (coil) *1VT_BOOLBooleanCounter Input20, 2115, 16, 37VT_I2 orVT_R8Short orUnsigned LongAnalogOutput(Integer) 4148, 493 (holdingregister)*1,2VT_I2 orVT_I4Short orUnsigned LongAnalogOutput(FloatingPoint) 50VT_R4Short FloatBitstring(Unsigned 32bit) 7, 8, 33VT_UI4 orVT_R8Unsigned Long Table4Summary ofEquivalentData Typesfor DNP3,IEC 101, IEC 103, and OPC *1=also input for Modbus*2=Unsigned Short for Modbus 4.37.4SDG Database Storage Formats TypeRangeConversionBooleanunsigned char0 = FALSE, 1 =TRUEif value=0then FALSEelse TRUEShortsigned 16-bit integer value, -32,768 to 32,767valuetruncated to types rangeif requiredUnsigned Shortunsigned 16-bit integervalue, 0 to 65,535valuetruncated to types rangeif requiredLongsigned 32-bit integer value, -2,147,483,648 to 2,147,483,647valuetruncated to types rangeif requiredUnsigned Longunsigned 32-bit integervalue0 to 4,294,967,295valuetruncated to types rangeif requiredShort Float32-bit short floating pointnumberIEEE Standard 754-3.4 * 1038to +3.4 * 1038fraction = UI23[1..23] exponent = UI8 [24..31] sign = BS1[32] valuetruncated to types rangeif requiredDouble64-bit floating point numbervalue truncated to types range 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |233 SCADA Data Gateway| Implementers Guide IEEE Standard 754-1.7 * 10308to +1.7 * 10308fraction = UI52[1..52] exponent = UI11 [53..63] sign = BS1[64] if required 4.37.5SDG Database Standard Quality Flags IEC 60870-5-101Quality DescriptorQuality SymbolMeaningMaskValueBLSTD_QLTY_BLOCKEDBlocked0x0010SBSTD_QLTY_SUBSTITUTEDSubstituted (over-ride or forced)0x0020NTSTD_QLTY_NOT_TOPICALNot topical(offline/not dated)0x0040IVSTD_QLTY_INVALIDInvalid0x0080OVSTD_QLTY_OVERFLOWOverflow/rollover0x0001EISTD_QLTY_INVALID_TIMEElapsed time invalid0x0008-STD_QLTY_REF_ERRORReference error0x0100-STD_QLTY_IN_TRANSITIn transit/chatter0x0200-STD_QLTY_UNINITIALIZEDNot set since startup0x0400-STD_QLTY_TESTTest mode0x0800 If the same data type is mapped between Master and Slave component points,the database storage format isnot used and the native data type will be mapped exactly. The tables below may be usedto determine howvalues and quality flags are translated when a Slave Component point is mapped to a Master Componentpoint ofa different data type. Simply locate theMaster Component datatype table below and notehow thevalues and quality flags are mapped into theSDGDatabase Storage format. Then locate the Slave Component data type tablebelow and identify how these values are mapped to theSDGDatabase Storage format corresponding to the Master Component Data type. For example:To determinethe value and quality of an OPC Slave Component Boolean point when mapped to an IEC 870-5-101 Master Component Double Point Information (DPI) with a value of 0 (indeterminate orintermediate state), first locatethe DPI table. TheMaster Component column shows a DPIvalue of 0 ismapped to aSDGDatabase Storage value of Boolean 0 and STD_QLTY_IN_TRANSIT isset to TRUE. Then locate the OPC Boolean table. TheSDGDatabase Storage columncontainsa row with Standard Quality Flag STD_QLTY_IN_TRANSIT setTRUE and thisrow in the Slave Component column showstheOPC Quality Flag will be set to UNCERTAIN. The Boolean value will beset accordingly. 4.37.6IEC 60870-5-101 and IEC60870-5-104 Single Point Information (SPI) Type IDs 1 (MSPNA), 2 (MSPTA), 30 (MSPTB) Master component can provide:SPI, booleanSlave component requestorder:SPI, booleanToFromToFromSlave Component orMaster ComponentSingle pointcommandSDG Database StorageMaster Component0 = determined state OFFBoolean= 00 = determined state OFF Page|234 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide 1 = determined state ONBoolean= 11 = determined state ONQuality Descriptors: BL, SB, NT, IVStandard Quality Flags: BL, SB, NT, IVQuality Descriptors: BL, SB,NT, IVIV set TRUESTD_QLTY_REF_ERROR is TRUEIV set TRUESTD_QLTY_UNINITIALIZED is TRUEIV set TRUESTD_QLTY_INTRANSIT is TRUEDouble Point Information(DPI) Type IDs 3 (MDPNA), 4(MDPTA), 31 (MDPTB) Master component can provide: DPI,booleanSlave component requestorder:DPI, booleanToFromToFromSlave Component orMaster ComponentDouble pointcommandSDG Database StorageMaster Component0 = indeterminate orintermediate stateBoolean = 0 STD_QLTY_IN_TRANSIT setTRUE0 = indeterminate or intermediate state1 = determined state OFFBoolean= 01 = determined state OFF2 = determined state ONBoolean= 12 = determined state ON3 = indeterminate stateBoolean = 0 STD_QLTY_INVALID setTRUE3 = indeterminate state or IVQuality Descriptors: BL, SB, NTStandard Quality Flags: BL, SB, NT, IVQuality Descriptors: BL, SB, NTIV set TRUESTD_QLTY_REF_ERROR is TRUEIV set TRUESTD_QLTY_UNINITIALIZED is TRUEMeasured Value (normalized)Type IDs 9 (MMENA), 10 (MMETA), 34 (MMETD) Signed 16 bit integer:range<-1 to 1-2-15> Master component can provide: shortSlave component requestorder: shortToFromToFromSlave Component orMaster ComponentNormalized value commandSDG Database StorageMaster Componentnormalized valueshortnormalized valueQuality Descriptors: BL, SB, NT, IV, OVStandard Quality Flags: BL, SB, NT, IV, OVQuality Descriptors: BL, SB, NT, IV, OVIV set TRUESTD_QLTY_REF_ERROR is TRUEIV set TRUESTD_QLTY_INTRANSIT is TRUE 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |235 SCADA Data Gateway| Implementers Guide Measured Value (scaled)Type IDs 11 (MMENB), 12 (MMETB), 35 (MMETE) Signed 16 bit integer:range<-32,768 to +32,767> Master component can provide: shortSlave component requestorder: shortToFromToFromSlave Component orMaster ComponentScaled valuecommandSDG Database StorageMaster Componentscaled valueshortscaled valueQuality Descriptors: BL, SB, NT, IV, OVStandard Quality Flags: BL, SB, NT, IV, OVQualityDescriptors: BL, SB, NT, IV, OVIV set TRUESTD_QLTY_REF_ERROR is TRUEIV set TRUESTD_QLTY_INTRANSIT is TRUEMeasured Value (shortfloat)Type IDs 13 (MMENC), 14 (MMETC), 36 (MMETF) 32-bit short floating point: range<-3.4*1038to 3.4*1038-1> Master component can provide: short floatSlave component requestorder: short float, shortToFromToFromSlave Component orMaster ComponentFloat valuecommandSDG Database StorageMaster Componentshort float valueshortfloatshort float valueQuality Descriptors: BL, SB, NT, IV, OVStandard Quality Flags: BL, SB, NT, IV, OVQuality Descriptors: BL, SB, NT, IV, OVIV set TRUESTD_QLTY_REF_ERROR is TRUEIV set TRUESTD_QLTY_INTRANSIT is TRUEBitstringType IDs 7(MBONA), 8(MBOTA), 33 (MBOTB) 32 bitunsigned long: range <0 to 2*1032-1> Master component can provide:unsigned longSlave component requestorder:unsigned longToFromToFromSlave Component orMaster ComponentBitstring commandSDG Database StorageMaster Componentunsigned longvalueunsigned longvalueunsigned longvalueQuality Descriptors: BL, SB, NT, IV, OVStandard Quality Flags: BL, SB, NT, IV, OVQuality Descriptors: BL, SB, NT, IV, OVIV set TRUESTD_QLTY_REF_ERROR is TRUEIV set TRUESTD_QLTY_INTRANSIT is TRUE Page|236 1994 -2023Triangle MicroWorks, Inc. All rights reserved. SCADA Data Gateway| Implementers Guide Integrated Totals TypeIDs 15 (MITNA), 16 (MITTA), 37 (MITTB) 32 bit long: range <-2*1031to 2*1031-1> Master component can provide: longSlave component requestorder:longToFromToFromSlave ComponentSDG Database StorageMaster Componentshort float valueshort floatshort float valueQuality Descriptors: BL, SB, NT, IV, OVStandard Quality Flags: BL, SB, NT, IV, OVQuality Descriptors: BL, SB, NT,IV, OVIV set TRUESTD_QLTY_REF_ERROR is TRUEIV set TRUESTD_QLTY_INTRANSIT is TRUESingle Command TypeIDs 45 (CSCNA), 58 (CSCTA) FromToFromToSlaveComponentSDG Database StorageMaster Component0 = command state OFFBoolean= 00 = command state OFF1 = command state ONBoolean= 11 = command state ONQU_USE_DEFAULTBIN_CTRL_MODE_NONEQU_USE_DEFAULTQU_SHORT_PULSE if < max. short pulsetimeBIN_CTRL_MODE_PULSEQU_SHORT_PULSEQU_LONG_PULSE if >= to max short pulse timeBIN_CTRL_MODE_PULSEQU_LONG_PULSEQU_PERSISTENTBIN_CTRL_MODE_LATCHQU_PERSISTENT) Double Command Type IDs 46(CDCNA), 59 (CDCTA) FromToFromToSlaveComponentSDG Database StorageMaster Component1 = command state OFFBoolean= 01 = command state OFF2 = command state ONBoolean= 12 = command state ONQU_USE_DEFAULTBIN_CTRL_MODE_NONEQU_USE_DEFAULTQU_SHORT_PULSE if < max. short pulsetimeBIN_CTRL_MODE_PULSEQU_SHORT_PULSEQU_LONG_PULSE if >= to max short pulse timeBIN_CTRL_MODE_PULSEQU_LONG_PULSEQU_PERSISTENTBIN_CTRL_MODE_LATCHQU_PERSISTENTRegulating Step Command Type IDs47 (CRCNA), 60 (CRCTA) FromToFromToSlave ComponentSDG Database StorageMaster Component1 = command state OFFBoolean= 01 = command state OFF2 = command state ONBoolean= 12 = command state ONQU_USE_DEFAULTBIN_CTRL_MODE_NONEQU_USE_DEFAULTQU_SHORT_PULSE if < max. short pulse timeBIN_CTRL_MODE_PULSEQU_SHORT_PULSEQU_LONG_PULSE if >= to max short pulse timeBIN_CTRL_MODE_PULSEQU_LONG_PULSEQU_PERSISTENTBIN_CTRL_MODE_LATCHQU_PERSISTENTSet-Point Command(normalized value)Type IDs 48 (CSENA), 61 (CSETA) Signed 16 bit integer:range<-1 to 1-2-15> FromToFromToSlave ComponentSDG Database StorageMaster Componentnormalized valueshortnormalized value 1994 -2023TriangleMicroWorks, Inc. All rights reserved.Page |237 SCADA Data Gateway| Implementers Guide Set-Point Command (scaled value)Type IDs 49 (CSENB), 62(CSETB) Signed 16 bitinteger: range <-32,768 to +32,767> FromToFromToSlave ComponentSDG Database StorageMaster Componentscaled valueshortscaled valueSet-PointCommand (short float value) TypeIDs 50 (CSENC), 63(CSETC) 32-bit short floatingpoint:range <-3.4*1038to 3.4*1038> FromToFromToSlave ComponentSDG Database StorageMaster Componentshort float valueshort floatshort float valueBitstringCommand (short float value) TypeIDs 51 (CBONC), 64 (CBOTA) 32 bitunsigned long: range <0 to 2*1032-1> FromToFromToSlave ComponentSDG Database StorageMaster Componentunsigned longvalueunsigned longvalueunsigned longvalue 4.37.7IEC 60870-5-103 Time TaggedType IDs1, 2ToFromSDG DatabaseStorageMaster Component0 = notusedBoolean= 01 = determined state OFFBoolean= 12 = determined state ONBoolean =0STD_QLTY_INVALID setTRUE3 = notusedMeasurandType IDs 3, 913-bitnormalized: range <-1 to 1-2-12> ToFromSDG Database StorageMaster ComponentshortshortTime-TaggedmeasurandType ID 432-bit short floating point: range<-3.4*1038to 3.4*1038> ToFromSDG Database StorageMaster Componentshort floatshort floatCommandType ID20FromToSDG Database StorageMaster Component0 = notusedBoolean= 01 = determined state OFFBoolean= 12 = determined state ON3 = not used 4.37.8DNP3 Binary Input,Object Group 1 and 2 ToFromSlaveComponent orMaster ComponentBinary commandSDG Database StorageMaster Component Page|238 1994 -2023Triangle MicroWorks, Inc. All rights reserved. State = 0 Boolean = 0 State = 0 State = 1 Boolean = 1 State = 1 Restart = 1 STD_QLTY_UNINITIALIZED On-Line = 0, Restart = 1 Communication Lost = 1 STD_QLTY_NOT_TOPICAL Communication Lost = 1 Remote Forced = 1 UPDTRSN_CTRL_BY_COMM Remote Forced = 1 Local Forced = 1 UPDTRSN_CTRL_AT_DEVICE Local Forced = 1 Chatter = 1 STD_QLTY_IN_TRANSIT Chatter = 1 Binary Output Status, Object Group 10 To From Slave Component SDG Database Storage Master Component State = 0 Boolean = 0 State = 0 State = 1 Boolean = 1 State = 1 Restart = 1 STD_QLTY_UNINITIALIZED On-Line = 0, Restart = 1 Communication Lost = 1 STD_QLTY_NOT_TOPICAL Communication Lost = 1 Remote Forced = 1 UPDTRSN_CTRL_BY_COMM Remote Forced = 1 Local Forced = 1 UPDTRSN_CTRL_AT_DEVICE Local Forced = 1 Binary Output Control, Object Group 12 To From Slave Component SDG Database Storage Master Component Code = NUL Boolean = 0 BIN_CTRL_MODE_NONE Code = NUL Code = Latch Off Boolean = 0 BIN_CTRL_MODE_LATCH Code = Latch Off Code = Latch On Boolean = 1 BIN_CTRL_MODE_LATCH Code = Latch On Code = Pulse Off Boolean = 0 BIN_CTRL_MODE_PULSE Code = Pulse Off Code = Pulse On Boolean = 1 BIN_CTRL_MODE_PULSE Code = Pulse On Trip/Close = 1 (Close) Boolean = 0 BIN_CTRL_MODE_PAIRED Trip/Close = Close Trip/Close = 2 (Trip) Boolean = 1 BIN_CTRL_MODE_PAIRED Trip/Close = Trip Analog Input, Object Group 30 and 32 To From Slave Component or Master Component Analog command SDG Database Storage Master Component Long, variation 1, 3 Long Short, variation 2, 4 Long, variation 1, 3 Short float, variation 5 Short Float Short float, variation 5 Restart = 1 STD_QLTY_UNINITIALIZED On-Line = 0, Restart = 1 Communication Lost = 1 STD_QLTY_NOT_TOPICAL Communication Lost = 1 Remote Forced = 1 UPDTRSN_CTRL_BY_COMM Remote Forced = 1 Local Forced = 1 UPDTRSN_CTRL_AT_DEVICE Local Forced = 1 Over Range = 1 STD_QLTY_OVERFLOW Over Range = 1 Reference Check = 1 STD_QLTY_REF_ERROR Reference Check = 1 Analog Output Status, Object Group 40 To From Slave Component SDG Database Storage Master Component 0 Boolean = 0 0 1 Boolean = 1 1 Restart = 1 STD_QLTY_UNINITIALIZED On-Line = 0, Restart = 1 Communication Lost = 1 STD_QLTY_NOT_TOPICAL Communication Lost = 1 Remote Forced = 1 UPDTRSN_CTRL_BY_COMM Remote Forced = 1 Local Forced = 1 UPDTRSN_CTRL_AT_DEVICE Local Forced = 1 Analog Output Control, Object Group 41 To From Slave Component SDG Database Storage Master Component Short, variation 2, 4 Long Long, variation 1, 3 Long, variation 1, 3 Short float, variation 5 Short Float Short float, variation 5 Binary Counters, Object Group 20, 21, 22, and 23 To From Slave Component SDG Database Storage Master Component Long, variation 1, 5 Unsigned Long Short, variation 2, 6 Long, variation 1, 5 Restart = 1 STD_QLTY_UNINITIALIZED On-Line = 0, Restart = 1 Communication Lost = 1 STD_QLTY_NOT_TOPICAL Communication Lost = 1 Remote Forced = 1 UPDTRSN_CTRL_BY_COMM Remote Forced = 1 Local Forced = 1 UPDTRSN_CTRL_AT_DEVICE Local Forced = 1 Over Range = 1 STD_QLTY_OVERFLOW Roll Over = 1 Modbus 4.37.9 Coil, Type 0 From To From To Slave Component or Master Component Coil write SDG Database Storage Master Component State = 0 Boolean = 0 State = 0 State = 1 Boolean = 1 State = 1 Discrete Input Register, Type 1 To From To From Slave Component SDG Database Storage Master Component State = 0 Boolean = 0 State = 0 State = 1 Boolean = 1 State = 1 Holding Register, Type 3 16 bit value From To From To Slave Component or Master Component Holding register write SDG Database Storage Master Component value Unsigned short value Input Register, Type 4 16 bit value To From To From Slave Component SDG Database Storage Master Component value Unsigned short value OPC Reads 4.37.10 The following flags are used with all OLE Variant data types: To From To From OPC Server Slave Component SDG Database Storage OPC Client Master Component OPC_QUALITY_NOT_CONNECTED set TRUE STD_QLTY_UNINITIALIZED is TRUE Any of OPC_QUALITY_NOT_CONNECTED OPC_QUALITY_CONFIG_ERROR set TRUE OPC_QUALITY_OUT_OF_SERVICE set TRUE STD_QLTY_BLOCKED is TRUE OPC_QUALITY_OUT_OF_SERVICE set TRUE OPC_QUALITY_BAD set TRUE STD_QLTY_INVALID is TRUE Any of OPC_QUALITY_BAD OPC_QUALITY_SUB_NORMAL set TRUE OPC_QUALITY_SENSOR_CAL set TRUE STD_QLTY_REF_ERROR is TRUE OPC_QUALITY_SENSOR_CAL set TRUE OPC_QUALITY_EGU_EXCEEDED set TRUE STD_QLTY_OVERFLOW is TRUE OPC_QUALITY_EGU_EXCEEDED set TRUE OPC_QUALITY_LAST_USABLE set TRUE STD_QLTY_NOT_TOPICAL is TRUE Any of OPC_QUALITY_LAST_USABLE OPC_QUALITY_LAST_KNOWN OPC_QUALITY_OUT_OF_SERVICE OPC_QUALITY_COMM_FAILURE OPC_QUALITY_SENSOR_FAILURE OPC_QUALITY_DEVICE_FAILURE set TRUE OPC_QUALITY_UNCERTAIN set TRUE STD_QLTY_IN_TRANSIT is TRUE OPC_QUALITY_UNCERTAIN set TRUE OPC_QUALITY_LOCAL_OVERRIDE set TRUE STD_QLTY_SUBSTITUTED is TRUE Any of OPC_QUALITY_LOCAL_OVERRIDE set TRUE Boolean OLE Variant Type TOleBool To From To From OPC Server Slave Component or OPC Client Master Component SDG Database Storage OPC Client Master Component VT_BOOL Boolean VT_BOOL Short OLE Variant Type Smallint To From To From OPC Server Slave Component or OPC Client Master Component SDG Database Storage OPC Client Master Component VT_I2 Short VT_I2 Short Float OLE Variant Type Single To From To From OPC Server Slave Component or OPC Client Master Component SDG Database Storage OPC Client Master Component VT_R4 short float VT_R4 Double Float OLE Variant Type Double Float To From To From OPC Server Slave Component or OPC Client Master Component SDG Database Storage OPC Client Master Component VT_R8 double float VT_R8 Long OLE Variant Type Longint To From To From OPC Server Slave Component or OPC Client Master Component SDG Database Storage OPC Client Master Component VT_I4 long VT_I4 Unsigned Long OLE Variant Type Unsigned Longint To From To From OPC Server Slave Component or OPC Client Master Component SDG Database Storage OPC Client Master Component VT_UI4, VT_R8 unsigned long VT_UI4, VT_R8 OPC Writes 4.37.11 Boolean OLE Variant Type ToleBool From To From To OPC Server Slave Component SDG Database Storage OPC Client Master Component VT_BOOL Boolean VT_BOOL Short OLE Variant Type Smallint From To From To OPC Server Slave Component SDG Database Storage OPC Client Master Component VT_I2 Short VT_I2 Short Float OLE Variant Type Single From To From To OPC Server Slave Component SDG Database Storage OPC Client Master Component VT_R4 short float VT_R4 Double Float OLE Variant Type Double Float From To From To OPC Server Slave Component SDG Database Storage OPC Client Master Component VT_R8 double float VT_R8 Long OLE Variant Type Longint From To From To OPC Server Slave Component SDG Database Storage OPC Client Master Component VT_I4 long VT_I4 Unsigned Long OLE Variant Type Unsigned Longint From To From To OPC Server Slave Component SDG Database Storage OPC Client Master Component VT_UI4, VT_R8 unsigned long VT_UI4, VT_R8 Predefined Internal MDOs for Monitoring and Control 4.37.12 Predefined Internal MDOs give the OPC Server and other Slave Components access to internal SDG status and operation. The tag name is assigned based on the specific communication channel, session, or sector that it applies to. The format of names for Predefined Internal MDOs is: Channel.LinkAddress.ASDUAddress.Name, shown below as COMx.Ly.Az.Name. 4.37.12.1 Characteristics of Predefined Internal MDOs Predefined Internal MDOs may be read directly in OPC or mapped to other protocols in the Point- Mapping file by listing them in the Master Component User tagName column and leaving the other Master Component Point-Mapping file columns blank. Predefined Internal MDOs may be either type String (Str), Integer (Int) or Binary (Bin). Some Predefined Internal MDOs are specific to a Master or Slave component as shown in the table below, so each session will not have all Predefined Internal MDOs. If a Predefined Internal MDO is read/write (R/W), then it may be written to, or read from. If the Predefined Internal MDO is read-only (R/O) its value may be read from, but not written to. Predefined Internal MDOs marked Error Count are incremented each time an error is detected. They may be preset to a specific value by writing the value to it or cleared by simply writing a value of zero. Reading a R/W Predefined Internal MDO returns the last value written to it. The meaning of each Predefined Internal MDO is explained in the table below. Unless otherwise noted, all Predefined Internal MDOs designated R/W Int may be set to any integer value. The M/S column identifies whether the Predefined Internal MDO is applicable to a Master (M) or Slave (S) component. PREDEFINED INTERNAL MDOS M/S PROTOCOL TYPE DESCRIPTION GatewayOnline M/S N/A R/O Bin True (1) whenever the gateway program is running. Inserts an event into the event buffer indicating the SDGs startup time SOECounter M/S N/A R/W Int SOE Update Counter. Counts the number of entries added to the SOE event log since last reset ChannelsOfflineCounter M/S DNP.101,104,103,MB R/W Int Counts the total number of times all sessions on all channels have gone off line ChannelsOnlineStatus M/S DNP.101,104,103,MB R/O Bin TRUE if all sessions on all channels are on line 61850ClientOnlineStatus M 61850 R/O Int Keeps count of the number of 61850 clients that are online Tase2ClientOnlineStatus M ICCP/TASE.2 R/O Int Keeps a count of the number of ICCP/TASE.2 Clients that ore online DataBaseUpdateQSize M/S DNP.101,104,103,MB R/O Int The current number of items in the data base queue that have not been processed yet DbQsizeExceededErrorLimit M/S DNP.101,104,103,MB R/O Bin TRUE if the DataBaseUpdateQSize exceeds the DbQSizeErrorLimit configuration parameter. If this limit is exceeded items are not added to the data base queue until DataBaseUpdateQSize falls below this limit. In other words if this limit is exceeded data is lost. DbQsizeExceededWarningLimit M/S DNP.101,104,103,MB R/O Bin TRUE if the DataBaseUpdateQSize exceeds the DbQSizeWarningLimit configuration parameter OPCClient_X.CurrentServerTime N/A N/A R/O Str The current time on the OPC Server that this client is connected to. OPCClient_X.LastUpdateTime N/A N/A R/O Str The last time the OPC client was updated by the OPC server. OPCClient_X.NumGroups N/A N/A R/O Int The number of groups created on the OPC server. OPCClient_X.PercentBandWidth N/A N/A R/O Int A performance utilization indicator. OPCClient_X.ServerOK N/A N/A R/O Bin TRUE if the OPC client is connected to the OPC server. OPCClient_X.ServerStartTime N/A N/A R/O Str The time the server was started. OPCClient_X.ServerStatus N/A N/A R/O Str A textual description of the OPC server status. OPCClient_X.VendorInfo N/A N/A R/O Str The vendor of the OPC Server. PREDEFINED INTERNAL MDOS M/S PROTOCOL TYPE DESCRIPTION COMx.NumChecksumErrors M/S DNP.101,104,103,MB R/W Int Error Count. Checksum errors detected on the communication channel COMx.NumConfirmFailures M/S DNP.101,104,103,MB R/W Int Error Count. Data link confirm failures on this channel COMx.NumLengthErrors M/S DNP.101,104,103,MB R/W Int Error Count. Frame length errors detected on the communication channel COMx.NumSyncErrors M/S DNP.101,104,103,MB R/W Int Error Count. Frame synchronization errors detected on the communication channel COMx.NumTimeoutErrors M/S DNP.101,104,103,MB R/W Int Error Count. Data link timeouts on this Session COMx.TotalErrors M/S DNP.101,104,103,MB R/W Int Total number of errors on this channel COMx.NumPhysCharTimeoutErrors M/S DNP.101,104,103,MB R/W Int Total number of Character Timeout Errors on this channel COMx.NumPhysXmitErrors M/S DNP.101,104,103,MB R/W Int Total number of Transmit errors on this channel COMx.NumRxBytes M/S DNP.101,104,103,MB R/W Int Total number of bytes received on this channel COMx.NumTxBytes M/S DNP.101,104,103,MB R/W Int Total number of bytes transmitted on this channel COMx.NumTxFrames M/S DNP.101,104,103,MB R/W Int Count of message frames transmitted COMx.NumRxFrames M/S DNP.101,104,103,MB R/W Int Count of valid message frames received COMx.ChannelSessionsOnLineStatus M/S DNP.101,104,103,MB R/O Bin TRUE if all sessions on this channel are on line COMx.SessionsOfflineCounter M/S DNP.101,104,103,MB R/W Int Counts the number of times any session on this channel has gone offline COMx.T1AckTime M 104 R/O Int The current amount of time elapsed between sending a frame and receiving an acknowledge COMx.T1MaxAckTime M 104 R/W Int The maximum amount of time that has elapsed between sending a frame and receiving an acknowledge. This value can be used to tune the value of T1 (LinkCnfmTimeout). COMx.NumTxFragments M/S DNP R/W Int Number of fragments transmitted on this channel COMx.NumRxFragments M/S DNP R/W Int Number of fragments received on this channel PREDEFINED INTERNAL MDOS M/S PROTOCOL TYPE DESCRIPTION COMx.Ly.OnLineStatus * M/S DNP.101,104,103,MB R/O Bin Status bit that reflects the current online status for this session. A true(1) value means the session is currently online and communicating. A false(0) value means the session if currently offline. This will occur at system startup, when the SessionActiveControl point is set to false(0), or when the master does not receive a data link confirm within the specified timeout period for the specified number of retries. COMx.Ly.SessionActiveControl * M/S DNP.101,104,103,MB R/W Bin Control flag to force a session active or inactive. Write true(1) to this value to attempt to bring the session active. Write false(0) to make a session inactive. Note the session and channel may or may not actually go offline when this is set, ie. The OnLineStatus MDO may or may not go to false when this is set to false. This setting ultimately stops the session from responding on its corresponding link layer and originator address. It does not directly turn off the session or the channel because the protocol does not support such functionality. The lack of response may or may not cause the session and channel to go offline depending on the configuration and protocol. COMx.Ly.SessionOfflineCounter M/S DNP.101,104,103,MB R/W Int Counts the number of times this session has gone offline COMx.Ly.TotalErrors M/S All R/W Int Total number of errors on this session COMx.Ly.NumTxASDUs M/S DNP.101,104,103,MB R/W Int Number of fragments transmitted on this channel COMx.Ly.NumRxASDUs M/S DNP.101,104,103,MB R/W Int Number of fragments received on this channel COMx.Ly.CreateTagsAuto M DNP.101,104,103 R/W Bin Write true (1) to this point to send a general interrogation to the connected slave device and create the data types and tags for this session. The session has to be online and a previous general interrogation cant be pending. This value will be reset to false (0) when the operation completes. PREDEFINED INTERNAL MDOS M/S PROTOCOL TYPE DESCRIPTION COMx.Ly.ColdRestartNow M DNP3 R/W Bin Write true(1) to this value to force a Cold Restart of the corresponding DNP session. COMx.Ly.WarmRestartNow M DNP3 R/W Bin Write true(1) to this value to force a Warm Restart of the corresponding DNP session. COMx.Ly.DNPSessionActionNow M DNP3 R/W Int See the description below for a definition of the bits in this mask. COMx.Ly.DNPActionMask0-4 M DNP3 R/W Int See the description below for a definition of the bits in this mask. COMx.Ly.DNPActionPrd0-4 M DNP3 R/W Int Time between actions defined in the DNPActionMask0-4, specified in milliseconds. COMx.Ly.Az.IECActionNow M 101, 103, 104 R/W Int See the description below for a definition of the bits in this mask. COMx.Ly.Az.IECActionMask0-4 M 101, 103, 104 R/W Int See the description below for a definition of the bits in this mask. COMx.Ly.Az.IECActionPrd0-4 M 101, 103, 104 R/W Int Time between actions defined in the IECActionMask0-4, specified in milliseconds. COMx.Ly.Az.CreateTagsAuto M 101,103,104 R/W Bin Write true (1) to this point to send a general interrogation to the connected slave device and create the data types and tags for this sector. The session has to be online and a previous general interrogation cant be pending. This value will be reset to false (0) when the operation completes. COMx.Ly.Az.Reset101RemoteNow * M 101, 104 R/W Int Write QRP value to send to IEC 60870- 5-101 Slave when issuing application layer command Type ID 105. Refer to IEC 60870-5-101 clause 7.2.6.27. Normally the QRP value will be 1 (reset Slave) or 2 (clear event buffers). COMx.Ly.Az.Reset103RemoteNow M 103 R/W Bin Write true (1) to issue a data link Reset CU command to an IEC 60870-5-103 Slave. COMx.Ly.Az.ClearEventBuffers * S 101, 104 R/W Bin Write true (1) to clear the sectors event buffers. COMx.Ly.Az.ResetCountersNow M 101, 104 R/W Bin Write true (1) to issue an application level reset counters command. COMx.Ly.Az.FreezeCountersNow M 101, 104 R/W Bin Write true (1) to issue an application level freeze counters command. COMx.Ly.Az.TestNow M 101, 104 R/W Bin Write true (1) to issue an application level test command. Write false(0) to issue an application level test command with an invalid sequence number (i.e. test will fail). COMx.Ly.Az.NumMissingResponses * M 101, 103, 104 R/W Int Error Count. Responses to this sector that have been lost. COMx.Ly.Az.TotalEventOverflows * S 101, 104 R/W Int Error Count. Number of events in this sector that have not been reported because of buffer overflow. PREDEFINED INTERNAL MDOS M/S PROTOCOL TYPE DESCRIPTION COMx.Ly.Az.Tt.NumEventOverflows * S 101, 104 R/W Int Error Count. Number of events of this type in this sector that have not been reported because of buffer overflow. Table 5 Predefined Internal MDOs * Link Address not used for IEC 60870-5-104, Sector Address not used for DNP3 4.37.12.2 Action Mask and Period The user can interactively, or periodically, issue various commands from the SDG master components through a set of masks and corresponding periods. The specific commands that can be performed are defined in the tables below. A command, or action, is selected by setting the corresponding bit in the appropriate mask to 1. The IECActionNow and DNPSessionActionNow masks allow the user to execute one or more of the supported actions whenever the mask is modified. These actions are performed once when the mask is updated. Writing the same value to the mask will execute the specified actions again. These masks allow the user to control exactly when an operation is performed. The IECActionMask0-4 and DNPActionMask0-4 masks and their corresponding periods (IECActionPrd0- 4 and DNPActionPrd0-4) allow the user to specify one or more actions to be performed periodically. When the user writes a non-zero value to one of the action periods, the actions selected by the corresponding mask will be performed and a timer is started which will repeat the selected operations at the specified period. The user can change the IECActionMask0-4 or DNPActionMask0-4 masks at any point and the new actions will be performed the next time the period expires or when the user writes a non-zero value to the corresponding period. Writing to the IECAction0-4 or DNPAction0-4 masks will not cause the action to be performed. You can terminate any periodic action by writing a value of 0 to the corresponding action period. 4.37.12.3 DNP Action Mask Bit Position Action 0 (LSB) Read Class 1 Change Events 1 Read Class 2 Change Events 2 Read Class 3 Change Events 3 Read Binary Input Change Events 4 Read Analog Input Change Events 5 Read Running Counter Change Events 6 Read Frozen Counter Change Events 7 9 Not Used 10 Read Class 0 Static Data (See note 1) 11 Specific flag only request points in the SDG database for the data type(s) selected in bits 12-17 (See notes 2, 4, 6) 12 Read Binary Inputs (See notes 2, 6) 13 Read Analog Inputs (See notes 2, 6) 14 Read Running Counters (See notes 2, 6) 15 Read Frozen Counters (See notes 2, 6) 16 Read Binary Output Status (See note 6) 17 Read Analog Output Status (See note 6) 18 20 Not Used 21 Send a Clock Synchronization Request 22 Send a Freeze Counters Command 23 Send a Freeze and Clear Counters Command 24 31 (MSB) Not Used Notes: 1) The DNP specification states a class 0 (static data) poll should be preceded by a class 1,2,3 (event data) poll; hence bits 0, 1, and 2 should be set when bit 10 is set. 2) The DNP specification states a read of static data should be preceded by a read of change events. As an example, if bit 12 is set to read binary inputs, bit 3 should also be set to read the binary input change events. The same logic should be applied to the rest of the data types. 3) At gateway startup, if the action period has a non-zero value, the actions specified by the bit-mask will be performed immediately and repeated at an interval specified by the period in milliseconds. 4) It is recommended that the GlobalCreateTagAutomatic INI file parameter be set to FALSE if the Specific flag is set. Otherwise the SDG will automatically add all data points received from the remote device to the local data base and hence poll for all the data points on the remote device in a very inefficient manner. 5) Action mask command bits are not mutually exclusive; multiple bits can be set to cause the execution of multiple commands. For example if bits 21,0,1,2,10 are set 2 messages will be sent. The first message will be a clock sync, followed by a read of class 1-2-3-0 in the second message. 6) Normally bits 12-17 will cause a read of variation 0 with qualifier code 06 (all points). If the remote device supports the range qualifier codes (00 and 01), the specific flag may be set to reduce the response message size (assuming all the points in the remote device are not defined in the SDG Database). 4.37.12.4 DNP Action Now Status The DNPSessionActionNow has a completion status MDO associated with it called DNPSessionActionNowStatus. This MDO is used to display the current status of the DNPSessionActionNow request. The following describes the meaning of the values: Status Value Meaning 0 (success) Indicates the request has completed successfully 1 (intermediate) Indicates a response was received but the requested command is not yet complete. This could mean the response is part of a multi-fragment response and did not have the FINAL bit set. Or this could be a request such as a select operate that requires multiple requests and responses. 2 (failure) Indicates that the transmission of the request failed 3 (mismatch) The response to a select or an execute did not echo the request. 4 (statuscode) The response to a select or an execute echoed the request, except the status code was different indicating a failure. 5 (iin) The response to the request had IIN bits set indicating the command failed. 6 (timeout) This indicates that the request has timed out. This could either be an incremental timeout indicating we received no link layer frame from the device in the specified time, or an application response timeout indicating this particular request did not complete in the specified time. 7 (canceled) This indicates either that the user asked that the request be canceled by calling dnpchnl_cancelFragment or that a second duplicate request has been made and therefore this first one is canceled. Typically this can be used to indicate the DNPSessionActionNow has completed and a reason why. This can also be useful to sequence a set of commands/requests on a serial multi-drop connection. 4.37.12.5 DNP Action Mask Examples 4.37.12.5.1 Periodic Class Data Polls The following action mask and period settings will cause a periodic integrity data poll (class 1,2,3,0) at a frequency of 60 minutes and a report by exception data poll (class 1,2,3) every 2 seconds. DNPActionMask0[*] = 0x0407 // bits 0, 1, 2, 10 DNPActionPrd0[*] = 3600000 // 60 minutes * 60 seconds * 1000 milliseconds DNPActionMask1[*] = 0x0007 // bits 0, 1, 2 DNPActionPrd1[*] = 2000 // 2 seconds * 1000 milliseconds 4.37.12.5.2 Binary and Analog Points Typically binary input change events are read more frequently than analog input change events. Read all static binary input and analog input points once every 60 minutes: The change event bit is set as well as the static input value bit, as recommended by the DNP specification. DNPActionMask0[*] = 0x3018 // bits 3, 4, 12, 13 DNPActionPrd0[*] = 3600000 // 60 minutes * 60 seconds * 1000 milliseconds Read binary input change events once every 2 seconds: DNPActionMask1[*] = 0x0008 // bit 3 DNPActionPrd1[*] = 2000 // 2 seconds * 1000 milliseconds Read analog input change events once every 20 seconds: DNPActionMask2[*] = 0x0010 // bit 4 DNPActionPrd2[*] = 20000 // 20 seconds * 1000 milliseconds 4.37.12.5.3 Specific Points For DNP applications that do not support report by exception, the recommended way to reduce bandwidth requirements is to use the specific flag. Specific mode only requests data for the current Master data objects in the gateway of the type specified by the bit-mask. The following example will read the specific binary points at a frequency of 5 seconds and the analog points at a frequency of 20 seconds. The corresponding change event bits are not set as the assumption is that the DNP application does not support change events. Read specific binary points once every 5 seconds: DNPActionMask0[*]= 0x1800 // bits 11, 12 DNPActionPrd0[*]= 5000 // 5 seconds * 1000 milliseconds Read specific analog points once every 20 seconds: DNPActionMask1[*]= 0x2800 // bits 11, 13 DNPActionPrd1[*]= 20000 // 20 seconds * 1000 milliseconds 4.37.12.6 IEC Action Mask Bit Position Action 0 (LSB) Send a clock synchronization request 1 Send general interrogation request 2 Send group 1 interrogation request 3 Send group 2 interrogation request 4 Send group 3 interrogation request 5 Send group 4 interrogation request 6 Send group 5 interrogation request 7 Send group 6 interrogation request 8 Send group 7 interrogation request 9 Send group 8 interrogation request 10 Send group 9 interrogation request 11 Send group 10 interrogation request 12 Send group 11 interrogation request 13 Send group 12 interrogation request 14 Send group 13 interrogation request 15 Send group 14 interrogation request 16 Send group 15 interrogation request 17 Send group 16 interrogation request 18 Send general counter interrogation request 19 Send counter group 1 interrogation request 20 Send counter group 2 interrogation request 21 Send counter group 3 interrogation request 22 Send counter group 4 interrogation request 23 31 (MSB) Not Used Notes: At gateway startup, if the action period has a non-zero value, the actions specified by the bit-mask will be performed and repeated at the interval specified by the period in milliseconds. Action mask command bits are not mutually exclusive; multiple bits can be set to cause the execution of multiple commands. For example if bits 0 and 1 are set 2 messages are sent first a clock sync, followed by a general interrogation request. 4.37.12.7 Modbus Action Mask Bit Position Action 0 (LSB) Read Coils 1 Read Holding Registers 2 Read Discrete Inputs 3 Read Input Registers 4 31 (MSB) Not Used Notes: 1. At gateway startup, if the action period has a non zero value, the actions specified by the bit-mask will be performed and repeated at the interval specified by the period in milliseconds. 2. Action mask command bits are not mutually exclusive; multiple bits can be set to cause the execution of multiple commands. For example if bits 0 and 1 are set 2 messages are sent first a read coils, followed by a read holding registers. 4.38 SDG Clock The UseSystemClock configuration parameter allows you to configure the SDG to use the Windows system clock or an internal clock maintained within the SDG. If UseSystemClock is set to TRUE the SDG will always get the current date and time directly from the Windows system clock. This mode is useful if an external clock synchronization mechanism is being used to synchronize the Windows system clock outside the SDG. In this mode of operation clock synchronization requests from remote masters are parsed by the SDG but the results are ignored. The default mode of operation for the SDG is to use an internal clock managed by the SDG. The SDG will set its internal clock from the Windows system clock at startup. After initialization the internal clock is a free running clock that is maintained using a free running millisecond timer. Even though the internal clock is independent from the Windows system clock the two clocks will initially report the same time since the internal clock is updated using the same Windows millisecond timer used to maintain the Windows system clock. The internal clock however will be adjusted by any clock synchronization requests received from any remote master. After an external clock synchronization request the internal clock will remain at a fixed offset from the Windows system clock. Event Time Tags and Database Update Times 4.38.1 There are two ways that the SDG can assign a time to generated events. The first is to record the value of the SDG internal clock when the database object is updated. This is referred to as the Update time. The second option is the Reported time. This is the time reported to the SDG master component as part of the incoming message from remote slave devices. This time is relative to the current time on the remote device and hence there is the possibility of an inconsistent time base when using this approach with multiple slave devices. By default, OPC data values, as well as data points generated by equations, use the Update time as described above. The reason that this is done is to provide a consistent time base for changes whether or not the data was received with a time tag, and whether or not the time in the slave devices has been synchronized. OPC only sees the "current value" of the data points, not an "event sequence", hence the time tag accuracy is not critical. The user can choose to override the default and report the Reported time for OPC data values and equations using the OPCAETimeSource, OPCTimeSource and EquationTimeSource INI file configuration parameters and/or the OPCTIMEUPDATE, OPCTIMEREPORTED, EQTIMEUPDATE, EQTIMEREPORTED point mapping file data point options. For more details on configuration parameters see the INI file description. For more details on data point options see the data options field description in the point configuration file section. The SDG slave components and the sequence of events log file report time only if a valid time is reported by the slave device. Hence, the default time for events to external masters talking to an SDG slave component or a time recorded in a log file will be the Reported time. 4.39 Sequence of Event Log Files The Sequence of Event (SOE) Log file contains records describing events reported to, detected by, and/or generated by the Gateway software. Each record describes one reported, detected, or generated event associated with a Master Component Point. External software applications may actively use the data within the log file simultaneously with the Gateway updating the file. Note: The SOE Queue Feature has been deprecated. The parameters listed in the table below, and further described in subsequent sections of this document, can be used to configure the event log file and queue capabilities of the Gateway. To set or change these parameters, edit the INI file prior to execution of the Gateway software; the parameters are located in the Data Change Event Logging section within the INI file. Parameter Name Description of What Events Are to Be Logged EventLogFileName The name of the file or files into which events are to be logged EventLogRecordFormat The format of records within log files and the SOE queue EventLogFileFlushPrd The maximum amount of time after an event record has been written to a log file in which the log file will be flushed, allowing external software applications to read the updated log file records. Errors_logMask The type of error events to be logged Diagnostics_logMask The type of diagnostic events to be logged GatewayControl_logMask The type of events associates with internal Gateway control points to be logged Equations_logMask The type of events associated with internally calculated equation points to be logged MSPNA_logMask The type of 60870-5-101/104 single point events to be logged MDPNA_logMask The type of 60870-5-101/104 double point events to be logged MSTNA_logMask The type of 60870-5-101/104 step position events to be logged MMENA_logMask The type of 60870-5-101/104 normalized measurand events to be logged MMENB_logMask The type of 60870-5-101/104 scaled measurand events to be logged MMENC_logMask The type of 60870-5-101/104 floating point measurand events to be logged MEINA_logMask The type of 60870-5-101/104 end of initialization events to be logged CSCNA_logMask The type of 60870-5-101/104 single command events to be logged CDCNA_logMask The type of 60870-5-101/104 double command events to be logged CRCNA_logMask The type of 60870-5-101/104 regulating step command events to be logged CSENA_logMask The type of 60870-5-101/104 normalized set point command events to be logged CSENB_logMask The type of 60870-5-101/104 scaled set point command events to be logged CSENC_logMask The type of 60870-5-101/104 floating point set point command events to be logged Parameter Name Description of What Events Are to Be Logged CCSNA_logMask The type of 60870-5-101/104 clock synchronization events to be logged 103_M103Type1_logMask The type of 60870-5-103 type 1 events to be logged 103_M103Type2_logMask The type of 60870-5-103 type 2 events to be logged 103_M103Type4_logMask The type of 60870-5-103 type 4 events to be logged 103_M103Type5_logMask The type of 60870-5-103 type 5 events to be logged 103_M103Type9_logMask The type of 60870-5-103 type 9 events to be logged 103_M103Type20_logMask The type of 60870-5-103 type 20 general commands events to be logged MDNPType1_logMask The type of DNP Binary Input events to be logged MDNPType10_logMask The type of DNP Binary Output Status events to be logged MDNPType12_logMask The type of DNP Binary Output Control events to be logged MDNPType20_logMask The type of DNP Counter events to be logged MDNPType21_logMask The type of DNP Frozen Counter Input events to be logged MDNPType30_logMask The type of DNP Analog Input events to be logged MDNPType40_logMask The type of DNP Analog Output Status events to be logged MDNPType41_logMask The type of DNP Analog Output Control events to be logged Table 6 List of Parameters on what to log for Events Event Log File Names 4.39.1 EventLogFileName defines the file name, including path, base filename, and extension in which to store event log records. The filename can contain the same %xxx property fields defined for event record format, described in Section 4.39.2 below. These fields effectively allow multiple log files; the file in which a specific event will be logged is determined when the event occurs by expanding the %xxx property fields for that event. This feature helps organize Sequence Of Event Log files based on any of the event properties. For example, consider the following: EventLogFileName = c:\mypath\GW%02ry2%02rmr%02rdm.log This will allow event records to be recorded in log files with names such as: c:\mypath\GW090818.log where 090818 represents a date (year is 00, month is 06, and day of month is 09). This example organizes Sequence of Event log files based on the reported dates of events there will be one log file for each day in which an enabled event is reported. Event Log Record Format 4.39.2 EventLogRecordFormat defines the format of event records stored in the Sequence of Event Log files. The format contains %xxx property fields that are expanded when an event occurs to describe a unique property of that event. The field names are included in the format using syntax very similar to the C programming language printf function. More specifically, each property field begins with a % and is followed by an optional field width, and a property name. The tables in this section describe the property field names that can be used in event EventLogRecordFormat (and also in EventLogFileName). The following figure provides an example event log record format, and an example expansion of that format for a specific event. 4 digit year%ry4%02rmr%02rdm%rtm,%val,%qly,%utg, 2000113011:53:58.216,on,0,com1.L12.A41.T1.P100, An example format string: EventLogRecordFormat = %ry4-%02rmr-%02rdm %rtm,%val,%qly,%utg,%dcr %dcrFeeder Breakerreported monthsee RDT_Table reported day see RDT_Table reported timesee RDT_Table hh:mm:ss.msecquality (hex value) see VFN_Tablevalue of the MDO: see VFN_Tableon/off, etcuser tag namesee IFN_Tabledescription for MDOsee IFN_Tablery4rmrrdmrtmvalqlyutgdcrsee RDT_Table Figure 19 Event Log Record Format 4.39.2.1 Value Field Names (VFN_Table) The following fields describe data point properties, which, if changed, usually indicate an event. Field Name Event Property Value Field Names Description val Value The current value of a master data point (after the change which generated the event). The representation of the value depends on the data type of the point. Some values may be represented as character strings (e.g., on, off), while others may be represented with integer or floating-point values. sval Value The current value of a slave data point. The representation of the value depends on the data type of the point. Some values may be represented as character strings (e.g., on, off), while others may be represented with integer or floating-point values. qly Quality A hexadecimal number indicating the quality of the current value of the master data point. The quality can indicate a combination of the following bits: sqly Quality A hexadecimal number indicating the quality of the current value of the slave data point. The quality can indicate a combination of the following bits: 0x0000 Successful no fault or other conditions 0x0001 Overflow or Rollover 0x0008 Elapsed Time Invalid 0x0010 Blocked 0x0020 Substituted (over-ride or forced) 0x0040 Off-line (not topical or not dated) 0x0080 Invalid 0x0100 Reference error 0x0200 In transit 0x0400 Un-initialized (not set since startup) 0x0800 Test mode rsn Update Indicates the reason the event was reported, detected, or generated. Field Name Event Property Value Field Names Description Reason Possible values are: 0x0001 The data is being updated for an unknown reason; e.g., reported through protocols that do not support cause-of- transmission, or reported through IEC 60870-5-103 with a COT of "test mode" 0x0002 The data was requested; no change event is necessarily indicated. 0x0004 The data is being refreshed by the source of data (without request); no change event is necessarily indicated. 0x0008 The source of data indicates this update is a change. 0x0010 The data was changed as a result of a control operation through communications. 0x0020 The data was changed as a result of a control operation executed locally at the device. 0x0040 Used for control points: A control operation has been initiated (e.g., sent to a remote device). This could occur for the second pass of a 2-pass operation, or it could occur for the only pass of a 1-pass operation. 0x0080 Used for control points: A control operation has been confirmed (e.g., the remote device sent confirmation of having received a control operation). However, though confirmed, the control operation may not yet be finished. When it is finished, 0x0010 will be used. 0x0100 Used for control points: The first pass in a two-pass control operation has been initiated (e.g., sent to a remote device) 0x0200 Used for control points: A first pass in a two-pass control operation has been confirmed (e.g., the remote device sent confirmation of having received the first-pass control operation). 0x0400 Used for control points: The de-select operation (cancel of a 1st-pass select operation) has been initiated (e.g., sent to a remote device) 0x0800 Used for control points: A de-select operation (cancel of a 1st- pass select operation) has been confirmed (e.g., the remote device sent confirmation of having de-selected the control operation). 0x1000 Used for control points: An error occurred with the control operation. 0x8000 Used by some protocols to indicate the point or device is operating in a test mode Table 7 Value Field Names - Describes properties which indicate an Event 4.39.2.2 Update Date and Time Field Names (UDTF_Table) The update date and time property fields describe the date and time when an event is detected or updated within the SDG. The timestamp is based on the clock used by SDG. The distinction between the update date and time and the reported date and time, which is described next, is important. The following table describes format fields that correspond to update date and time event properties. Field Name Event Property Update Date and Time Description udt Update Date The update date of the event. Formatted as dd-mmm-yyyy, where dd is day of month, mmm is a 3-character abbreviation for month, and yyyy is the year. utm Update Time The update time of the event. Formatted as hh:mm:ss.iiii, where hh is hour (0-23), mm is minute, ss is second, and iii is milliseconds. uy2 Update 2-digit Year The update year of the event, from 0-99 of the event uy4 Update 4-digit Year The update year of the event, from 1901 to 2099 umr Update Month The update month of the event, from 1 to 12 ume Update Month Name The update month of the event; using 3 character abbreviations for month names udm Update Day of Month The update day of the event, from 1 to 31 udw Update Day of Week The update day-of-week of the event, using 3 character abbreviations for day names. uhr Update Hour The update hour of the event, from 0 to 23 umn Update Minute The update minute of the event, from 0 to 59 usc Update Second The update second of the event, from 0 to 59 ums Update Milliseconds The update milliseconds of the event, from 0 to 999 Utq Update Time Quality A hexadecimal number indicating the quality of the updated date and time of the event. The time quality can indicate any combination of the following bits: (none) Successful no fault or other conditions are associated with the update time. I The update date/time is invalid, which may occur if a particular data point has never been updated. Table 8 Update Date and Time Event Description 4.39.2.3 Reported Date and Time Field Names (RDT_Table) The reported date and time property fields describe the date and time of an event as reported by the device reporting the event. It is based on the clock of the reporting device, which is not necessarily synchronized with the clock used by the SDG. If the reporting device did not include a date and time with the report of the event, then the update date and time will be used. The following table describes format fields that correspond to reported date and time event properties: Field Name Event Property Reported Date and Time Description rdt Reported Date The reported date of the event. Formatted as dd-mmm-yyyy, where dd is day of month, mmm is a 3-character abbreviation for month, and yyyy is the year. rtm Reported Time The reported time of the event. Formatted as hh:mm:ss.iiii, where hh is hour (0-23), mm is minute, ss is second, and iii is milliseconds. ry2 Reported 2-digit Year The reported year of the event, from 0-99 of the event ry4 Reported 4-digit Year The reported year of the event, from 1901 to 2099 rmr Reported Month The reported month of the event, from 1 to 12 rme Reported Month Name The reported month of the event; using 3 character abbreviations for month names rdm Reported Day of Month The reported day of the event, from 1 to 31 rdw Reported Day of Week The reported day-of-week of the event, using 3 character abbreviations for day names rhr Reported Hour The reported hour of the event, from 0 to 23 rmn Reported Minute The reported minute of the event, from 0 to 59 rsc Reported Second The reported second of the event, from 0 to 59 rms Reported Milliseconds The reported milliseconds of the event, from 0 to 999 Rtq Reported Time Quality A hexadecimal number indicating the quality of the reported date and time of the event. The time quality can indicate any combination of the following bits: (none) Successful no fault or other conditions are associated with the update time. A All fields of the date and time were assumed based on the date and time the event was received (the update time was used). H The hour and date were assumed based on the date and time the event was received. The minutes, seconds and milliseconds are as reported from the remote device. -H The event was assumed to have occurred in the hour previous to the one in which the event was received because the reported minutes were significantly greater than the current minutes. +H The event was assumed to have occurred in the hour after the one in which the event was received because the reported minutes were near 0 and the current minutes were near 59. D The date was assumed based on the date and time the event was received. The time is as reported from the remote device. -D The event was assumed to have occurred in the day previous to the one in which the event was received because the reported hours were significantly greater than the current hours. +D The event was assumed to have occurred in the day after the one in which the event was received because the reported hour was near 0 and the current hour was near 23. I The date/time is being reported by the remote device as being invalid. U As reported in DNP3 object 2, variation 3. N Need Time IIN is asserted by remote DNP3 device. R The time was reported with the event. Table 9 Reported Date and Time Event Description 4.39.2.4 Identification Field Names (IFN_Table) The following fields describe properties of a data point that identify the point: Field Name Event Property Identification Field Names Description dcr Description The description as entered for the point in the Point-Mapping configuration file. ptg Physical Tag Name The physical tag name, which is a combination of some or all of communication channel name, link address, application address, information object address (point number), data type, element index, and/or function type. utg User Tag Name The user tag name as entered for the point in the Point-Mapping configuration file. If no user name is entered, the physical tag name is used. prt Communication Channel Name The name of the communication channel associated with this point. lnk Link Address The link address of the device from which the point is communicated. app Application Address The application layer address of the sector from which the point is communicated. Not all protocols use application addressing. ioa Information Object Address The information object address of the point. Some protocols refer to this as point number. typ Data Type The data type, in numerical form, of the point. pcl Protocol The protocol, in string form, through which the point is communicated. eli Element Index The element index of the point. Not all protocols use element indexing. ftp Function Type The function type of the point. Not all protocols use function typing. Table 10 Identification Field Names Event Description 4.39.2.5 Escape sequences within Event Log Record Formats (ELR_Table) Escape sequences are sequences of characters that begin with an escape character (the back-slash \character) followed by one or more other characters. The escape sequences supported within EventLogRecordFormat are listed in the table below. Escape sequences are not supported within EventLogFileName. The reason for this is that escape sequences are not useful in file names, and the escape character (a back-slash) conflicts with file directory separators. Escape sequence Expanded as character code: \a 7 \b 8 (common use: backspace) \f 12 \n 10 (common use: line-feed, or new-line) \r 13 (common use: carriage-return) \t 9 (common use: tab) \v 11 \0mm \1mm Octal equivalent (where m is 0-7) \2mm \3mm \xhh hexadecimal equivalent (where h is 0-9, a-f, or A-F) Table 11 Escape sequences in Event Log Records Flush Period 4.39.3 EventLogFileFlushPrd defines the maximum amount of time after an event is recorded in a log file in which that file will be flushed. This may allow external software applications to access the data stored in the log file soon after the event is generated and stored in the log file. Log Mask 4.39.4 The tttt_logMask configuration parameters (where tttt represents any of the data types listed in the table in Section Sequence of Event Log Files section 4.39 page 253) specify the type of events for all points of each type that are to be logged in Sequence of Event Log Files. (The LOGMSKxxxx point option described in may be used to override tttt_logMask for a specific point.) Logging may be enabled for all kinds of events, enabled for only a selected few kinds of events, enabled for only one kind of event, or disabled for all kinds of events associated with each data type. These combinations are accomplished by adding together any or all of the kinds of events listed in the following table. A value of zero will disable logging. Mask Value Type of Event 0x0001 The data is being updated, but the reason for the update is unknown. 0x0002 The data is being updated because it was requested. 0x0004 The data is being updated by the source of data without direct request. No change is necessarily indicated. 0x0008 A change in the data is indicated by the source of data. 0x0010 A change in the data is indicated due to a request through communications. 0x0020 A change in the data is indicated due to action at the device. 0x0040 A control request has been transmitted to a remote device. 0x0080 A control request has been confirmed by a remote device, but is not yet complete. 0x0100 A 1st pass in a 2-pass control operation has been transmitted to a remote device. 0x0200 A 1st pass in a 2-pass control operation has been confirmed by a remote device. 0x0400 A cancel operation has been transmitted to a remote device to abort a 2-pass control operation between the 1st and 2nd passes. 0x0800 A cancel operation has been confirmed by a remote device. 0x1000 A remote device has responded to indicate an error in a control operation. 0x8000 Either the data point or the remote device is operating in a test mode. Table 12 List of Events and the Mask Value 4.40 Gateway Properties The SDG root node has some global settings for the Gateway. It is accessed as follows: This brings up the following dialog. Property Description Use System Clock If set to true, the date and time will always be read from the Windows system clock as opposed to an internally maintained clock. The internal clock is initialized to the Windows system clock at startup but will be adjusted whenever clock synchronization is received from an external master. You would generally set UseSystemClock to true if you have an external clock synchronization mechanism that synchronizes the Windows system clock outside the SDG. In this case it is advised that AcceptClockSync be set to false. Use Internal/Simulated Clock If set to true, the date and time will always be read from an internally maintained clock. The internal clock is initialized to the Windows system clock at startup but will be adjusted whenever clock synchronization is received from an external master. You would generally set UseSystemClock to true if you have an external clock synchronization mechanism that synchronizes the Windows system clock outside the SDG. In this case it is advised that AcceptClockSync be set to false. Use Display Timezone If set to true, the date and time will always display in the specified timezone and be set from the Windows system clock. Accept Clock Syncs If set to true and UseSystemClock is true, the Windows system clock will be adjusted by a time sync received from an external master device. If set to false, the time syncs will not adjust the Windows system clock. When using a simulated clock, this setting has no effect and clock syncs are always accepted and adjust the simulated clock. Use Reported Time for MDOs If true the client MDOs will display the updated time of the point as reflected by the server. Otherwise the client MDO will display the gateways time of the actual update. Use Specific Time Zone If true display the date and time for the SDG in the specified TimeZoneName. Time Zone Selector (TimeZoneName) The name of the time zone to use for the SDG display time (empty string/default will set to UTC). Note: SDG uses UTC for internal times. UseTimeZoneClock must be true. Ignore DST If true and UseTimeZoneClock is true changes in Day Light Savings time are ignored for display of time Auto Save Period Maximum amount of time between saving the INI and CSV application configuration files. The INI file is only saved if a change has been made. A value of 0 will disable saves. Initial Window Mode Specifies whether the SDG starts minimized normal, or maximized. Equation Time Source - Specifies the source of the time tag for data points which are generated as the result of an equation. The possible values are Update or Reported where Update means the time, relative to the SDG system clock, at which the equation was last calculated, Reported specifies the reported time of the most recent event that caused the equation's result to change. Reported time will be relative to the remote slave device's system clock except on initialization where the SDG's system clock is used until the first event with time is received. It is important to note that static data polling, or received events which do not specify a reported time, may cause a specific data point's value to change without its event time being modified. Based on system polling rates and other parameters this could result in discontinuous times being reported, especially in equations that have inputs from multiple slave devices. Event Log File Name Event Log File Name and Path. See manual for a description of the available %xxx property keywords. Event Log Record Format Event Log record format. See manual for a description of the available %xxx property keywords. Event Log File Flush Period Maximum amount of time between writing to the event log file before it is flushed to disk. Set to zero to inhibit explicit flushing. This parameter may be useful if another application reads the event log file to obtain event data in real time. SOE Queue File Name Sequence Of Events Queue File Name and Path. [DEFAULT: SDG_SOEQueueFile] Enable SOE Queue If set to true, the SOE Queue will be enabled. Possible values are 0, 1, 'FALSE', or 'TRUE'. [DEFAULT: FALSE] OPC Client Status Update Rate (ms) The interval at which an OPC client requests status information from its server. If the client does not wish to request status updates set this to 0. Possible values are from 0 to 30000. [DEFAULT: 1000 ms] OPC Time Source Specifies the source of the time tag for OPC data points. The possible values are Update or Reported. Update means the time, relative to the SDG system clock, at which the data point was last updated. Reported specifies the reported time of the most recent event that caused the data to change. Reported time will be relative to the remote slave device's system clock except on initialization where the SDG's system clock is used until the first event with time is received. Static data polling, or received events which do not specify a reported time, may cause a specific data point's value to change without an event being generated; hence the event time will not change. OPC Event Time Source Specifies the source of the time tag for OPC Alarm and Event data points. The possible values are Update or Reported. Update means the time, relative to the SDG system clock, at which the data point was last updated. Reported specifies the reported time of the event that caused the data to change. Reported time will be relative to the remote slave device's system clock except on initialization where the SDG's system clock is used until the first event with time is received. Static data polling, or received events which do not specify a reported time, may cause a specific data point's value to change without an event being generated; hence the event time will not change. OPC XML DA Server Name (Windows Only) Specifies the name used by an OPC XML DA client to connect to the SDG. With the INI file defaults a local opc xml da client can connect to the SDG as follows: http://localhost:8081/SDG Enable OPC Server login requirement Select this to require a password and username for performing writes to the classic OPC Server OPC Server User Name The user name that will be required to perform writes to the Classic OPC Server OPC Server Password The password that will be required to perform writes to the Classic OPC Server OPC XML DA Server Port (Windows Only) The TCP/IP port for the OPC XML DA server. With the INI file defaults a local opc xml da client can connect to the SDG as follows: http://localhost:8081/SDG Channel Start Delay Delay time in milliseconds to stagger starting all channels (slaves and masters). Only affects startup. Point Map Section Global Create Tags Auto If TRUE, tags (and storage space) will automatically be created upon reception of new data points within incoming response messages from remote devices. Point Map File Specifies Point Data Mapping file. Save Unmapped Points If TRUE, unmapped tags will be saved to the point mapping file. Read Point Map At Startup If TRUE, the point mapping file will be read at startup. Engine Sequence of Events Log Log Masks The log mask is a bitmap of the values. The mask can be applied to the Event Log File and/or the OPC Alarm and Event Server. Possible values are: 0x0001 The data is being updated, but the reason for the update is unknown. 0x0002 The data is being updated because it was requested. 0x0004 The data is being updated by the source of data without direct request. No change is necessarily indicated. 0x0008 A change in the data is indicated by the source of data. 0x0010 A change in the data is indicated due to a request through communications. 0x0020 A change in the data is indicated due to action at the device. 0x0040 A control request has been transmitted to a remote device. 0x0080 A control request has been confirmed by a remote device, but is not yet complete. 0x0100 A 1st pass in a 2-pass control operation has been transmitted to a remote device. 0x0200 A 1st pass in a 2-pass control operation has been confirmed by a remote device. 0x0400 A cancel operation has been transmitted to a remote device to abort a 2-pass control operation between the 1st and 2nd passes. 0x0800 A cancel operation has been confirmed by a remote device. 0x1000 A remote device has responded to indicate an error in a control operation. 0x8000 Either the data point or the remote device is operating in a test mode. The Log Masks are grouped as follows: Group Description Errors Each bit enables (1)/disables (0) a reason to log errors, such as changes in number of link layer checksum failures or in link session on-line/off-line status, in the event log file. If 0, nothing will be logged. Diagnostics Each bit enables (1)/disables (0) a reason to log diagnostic data, such as change to the number of transmitted frames, in the event log file. If 0, no diagnostics data will be logged. Control Each bit enables (1)/disables (0) a reason to log control data, such as changes to pollEnabled or to GeneralInterrogationPeriod, in the event log file. If 0, nothing will be logged. Equations Each bit enables (1)/disables (0) a reason to log results of equations. If 0, no equation results will be logged. OPC Data Access Subscription Each bit enables (1)/disables (0) a reason that an item subscription through the SDG OPC Data Access Server should enable OPC Alarm and Event notifications for the item. This mask overrides all other masks if enabled. If 0, no OPC Alarm and Event notifications will occur as a result of an item subscription. Use OPC Alarm and Event Simple Events If true, all OPC AE events are reported as simple events. Possible values are 0, 1, 'FALSE', or 'TRUE'. 4.40.1.1 Time Zone Names and Values These names apply to both the session time zone as well as the gateway time zone (INI file parameters SessionRemoteTimeZoneName and TimeZoneName). The value specified for the INI parameter should be exactly as specified in the edit combo box in the Gateway Properties dialog and are of the form Country/Region such as UTC or US/Eastern or Europe/Brussels. Note that time zone strings are language specific i.e. English versions are not the same as French versions of time zone strings. Here are some sample string values for Timezone: The timezone information is kept in the zoneinfo directory located in <installdir>/zoneinfo path. If timezone information has changed for a region and needs to be updated, please contact Triangle Microworks and we will help you update this information. 4.41 DNP3 Datasets The current version of the SDG does not support DNP3 datasets. If you have a need for datasets please contact Triangle Microworks Support. 4.42 User Defined Folders User defined folders can be created on the root node of the SDG with the following menu option User defined folders are used to organize internal points and equations when needed. You can drag and drop internal points and equations (ONLY) into user defined folders. User defined folders can be arbitrarily deep and can have any desired name. 4.43 GTWSettings command line tool This tool is a command line utility that enables the ability to change some of the settings in the gtw_config.json file. The following options are available: -h ip : set host in gtw_config.json i.e. -h 192.168.1.151 -u password : set the password for the built in user authentication admin account (this option can not be combined with other options, on linux this option will prompt for the password) -c file : set https public certificate file in gtw_config.json i.e. -c gtw-https.crt -p file : set https private certificate file in gtw_config.json i.e. -p gtw-https.key -v file : set https ca certificate file in gtw_config.json i.e. -v gtw-ca.key -g days : create https certificates valid for days i.e. -g 365 -a country : set the country in certificate i.e. -a US -b company : set the company in certificate i.e. -b Triangle Microworks -f name : set the name in certificate i.e. -c SDG -d email : set the email in certificate i.e. -d support@trianglemicroworks.com -e bits : set the bits in certificate i.e. -e 1024 Notes: To generate https certificates: options -g,-p,-c and -e must be specified at a minimum note that -a -b -f -d are optional and can have empty values If providing user supplied https certificates: only use options -p and -c The tool is located at: Windows: C:\Program Files\Triangle MicroWorks\SCADADataGateway Linux: /usr/bin Note that the tool needs to be run as sudo or administrator. To generate a certificate pair for the web server the following options are required: -c p g e. Optionally a b f d to provide identifying information in the certificate. When creating certificate it will be set as current one. Example of working command: GTWSettings.exe -g 520 -a US -b TotoIsTheBest -f SDG -d toto.is@best.com -e 2048 -p Toto2.key -c Toto2.crt 4.44 Import/Export points and mappings In order to support batch processing of points/tags and or mappings the Import/Export Points feature is available. This allows a user to add/edit mappings and point definitions using a CSV file that can be edited with Excel, a text editor, or a script (user created). There are 2 CSV file formats supported, one for point definitions and a second for mapping definitions. Note that TAB is used in both files as a column delimiter. Note that for import to work properly the devices must exist in the tree i.e client, channel, session, sector etc. For internal MDOs or equations this would not be required as they are stored in the root node. The following menu option is used to start the user interface for import/export of points or mappings. When this option is selected the following dialog will appear. This dialog is used to select the import/export options i.e. file, operation etc. Note that items can be exported recursively from a selected node in the tree i.e. all children within this selected item will be exported. If not recursive only the immediate points/mappings will be exported. Import is only supported at the root node of the Gateway as the file imported will contain the full path of the points or mappings. As shown below points will be exported from the m101.L3 node of the tree. Points can not be exported from slave/server devices. Bringing up the following dialog: Note that when importing mappings or points MDOs can be created. When importing mappings the mapped SDO will be created. When importing points that already exist a duplicate error will be generated Points file format 4.44.1 Columns: tagType contains one of the following to specify the kind of point CLASSIC, - 101,104,103,DNP,MODBUS points INTERNAL, - internal MDO EQUATION, - equation MDO OPCDA, - OPC DA Client point OPCAE,- OPC AE client point OPCUA, - OPC UA client point 61850, - 61850 point ICCP ICCP point valueType - contains one of the following to specify the value data type of the point GTWDEFS_TYPE_UNKNOWN, GTWDEFS_TYPE_BOOL, GTWDEFS_TYPE_SPARE, GTWDEFS_TYPE_CHAR, GTWDEFS_TYPE_UCHAR, GTWDEFS_TYPE_SHORT, GTWDEFS_TYPE_USHORT, GTWDEFS_TYPE_LONG, GTWDEFS_TYPE_ULONG, GTWDEFS_TYPE_SFLOAT, GTWDEFS_TYPE_DOUBLE, GTWDEFS_TYPE_STRING, GTWDEFS_TYPE_TIME, GTWDEFS_TYPE_INT64, GTWDEFS_TYPE_UINT64 tagName the full path of the point i.e. m101.L3.A3.T1.P100 userTagName a user defined name for the tag tagDescription the description of the point i.e. this is point number 100 tagOptions the options of the point control-block-or-equation the name of the control block that the point is in (applies to 61850,ICCP) or the equation if this is an equation tagType auto-map-QT true or false whether to auto create the quality and time points for a 61850 point that can have time/quality associated with it (applies to 61850) # is used in the 1st column to specify a comment Mappings file format 4.44.2 Columns: tagName the full path of one side the mapping i.e. mmb.L1.T4.P0 userTagName a user defined name for the tag tagDescription the description for the tagName i.e. this is point 0 tagOptions the options of the point tagType contains one of the following to specify the kind of point CLASSIC, - 101,104,103,DNP,MODBUS points INTERNAL, - internal MDO EQUATION, - equation MDO OPCDA, - OPC DA Client point OPCAE,- OPC AE client point OPCUA, - OPC UA client point 61850, - 61850 point ICCP ICCP point valueType - contains one of the following to specify the value data type of the tagName GTWDEFS_TYPE_UNKNOWN, GTWDEFS_TYPE_BOOL, GTWDEFS_TYPE_SPARE, GTWDEFS_TYPE_CHAR, GTWDEFS_TYPE_UCHAR, GTWDEFS_TYPE_SHORT, GTWDEFS_TYPE_USHORT, GTWDEFS_TYPE_LONG, GTWDEFS_TYPE_ULONG, GTWDEFS_TYPE_SFLOAT, GTWDEFS_TYPE_DOUBLE, GTWDEFS_TYPE_STRING, GTWDEFS_TYPE_TIME, GTWDEFS_TYPE_INT64, GTWDEFS_TYPE_UINT64 collectionKind the kind of collection the tagName is in TAG_PROPERTY_COLLEC_KIND_MDO, TAG_PROPERTY_COLLEC_KIND_SDO mappedName - the full path of the other side the mapping i.e. smb.L1.T4.P1 mappedDescription the description for the mappedName i.e. this is point 0 mappedOptions the options of the mapped point mappedCollectionKind - the kind of collection the mappedName is in TAG_PROPERTY_COLLEC_KIND_MDO, TAG_PROPERTY_COLLEC_KIND_SDO mappedValueType - contains one of the following to specify the value data type of the mappedName GTWDEFS_TYPE_UNKNOWN, GTWDEFS_TYPE_BOOL, GTWDEFS_TYPE_SPARE, GTWDEFS_TYPE_CHAR, GTWDEFS_TYPE_UCHAR, GTWDEFS_TYPE_SHORT, GTWDEFS_TYPE_USHORT, GTWDEFS_TYPE_LONG, GTWDEFS_TYPE_ULONG, GTWDEFS_TYPE_SFLOAT, GTWDEFS_TYPE_DOUBLE, GTWDEFS_TYPE_STRING, GTWDEFS_TYPE_TIME, GTWDEFS_TYPE_INT64, GTWDEFS_TYPE_UINT64 direction the direction of the mapping BOTH - bidirectional LEFT from mappedName to tagName RIGHT from tagName to mappedName # is used in the 1st column to specify a comment 4.45 Advanced Parameters On the editors that support it there is an Advanced Parameters colapsable panel as shown below. This allows the user to change INI file parameters that previously had to be changed by direct edit of the INI file. Many of these setting are typically left at their default value. In special circumstances they can be changed using this panel. 5. Troubleshooting The following information is provided to assist you with common problems encountered when setting up and configuring the SDG. 5.1 Permissions The SDG reads and writes several configuration files and log files during its course of configuration and operation. The SDG will not function properly if it cannot read from or write to the directories it needs access to. The user of the SDG needs to make sure they have permission to read from and write to the logging directory and the ini file directory. 5.2 Running the SDG as Administrator When using these various operating systems, it is recommended the SDG always run as administrator. It is recommended that running the SDG as administrator be set up automatically for the particular OS that is being used. 5.3 Running the SDG on a Virtual Machine The evaluation versions of the SCADA Data Gateway will not run in a Virtual Machine (VM). Only a fully licensed copy can run in a VM. Timed trials are available on a VM. 5.4 Errors in the Point Mapping File When Saving the SDG These error messages are most likely related to errors discovered in the Point Mapping (CSV) file during startup. Since the file could not be completely read on startup, the SDG will not overwrite it on exit in order to ensure there is no loss of data. To get more information related to these errors, ensure that you have Errors checked in the protocol analyzer log filter and then restart the SCADA Data Gateway. You will most likely get an error message stating that there are errors in the CSV file. However, the errors are now listed in the Protocol Analyzer along with a line number that corresponds to the line in the CSV file where the error occurred. These errors may be resolved by: 1. Stopping GTWEngine 2. Editing either the Point Mapping file (CSV) or the INI file 3. Restarting GTWEngine 5.5 Network: Network Driver Appears To Not Be Serving This Directory Dialog On Windows NT/2000 run the setupex.exe program in the C:\Program Files\Common Files\TriangleMicroWorks directory to install the required drivers for the SDG licensing software. This step is not required for Windows 95/98/ME. Physical layer error: Error opening channel 5.5.1 The message "physical layer error: Error opening channel" indicates that the device is unable to open the specified channel and port. The cause of the message depends on whether the channel is a serial or TCP/IP channel. 5.5.1.1 TCP/IP Connection With TCP/IP, you will normally get this message until the corresponding device also opens the port. It is not unusual to see several of these messages while the other device starts up and opens the port. If you are still getting this message after your corresponding device attempts to open the port, then you should check your configuration of both the Slave and Master device. The typical settings that can cause this problem are: address: the Master (client) must specify the TCP/IP address of the Slave device. The Slave must either specify the TCP/IP address of the client or use an address of "*.*.*.*". (The *.*.*.* address instructs the server to accept a connection from any client.) port: both the Master and Slave must use the same port; for DNP3, you would normally use port 20000 mode: typically, the Slave should be a Server and the Master should be a Client. 5.5.1.2 Serial Connection With serial connections, this message usually indicates that the port cannot be opened. If the specified port is installed in the computer, then this is error is usually caused by another program using the serial port, preventing it from being opened. PDA hotsync programs are frequently the problem. Configuration 5.5.2 Verify the configuration of both the SDG and your remote device: For a serial connection, verify the baud rate, parity, and number of stop bits are configured the same for both devices. For a TCP connection, verify the TCP and port address, as well as the client/server settings. For either type of connection, verify that the link address and common address of ASDU (sector address) match the addresses set in the remote device. Other issues 5.5.3 Verify that the desired protocol is licensed. Verify the physical connection. Turn on the Physical Layer in the Protocol Analyzer window and verify that bytes are sent and received and that the Physical layer error: Error opening channel message is not being issued. 5.6 Timeout waiting for application layer response fragment or restart." Message is displayed The "Timeout waiting for application layer response fragment or restart." message is normal behavior after a warm restart command is sent. If you are seeing this error at other times, it may indicate that the device with which you are communicating is either slow or has not established a connection at all. If you always receive this message, it probably indicates that the connection has not been established. In this case, you should check your configuration. For a serial connection, verify the baud rate, parity, and number of stop bits configured. For a TCP connection, verify the TCP and port address, as well as the client/server settings. For either type of connection, you should verify the link addresses. If the connection is established but you occasionally receive this message, then the device you are connected to is probably slow to respond. You might try changing the ApplResponseTimeout parameter in the INI file. If you will be running the Service Monitor, then your computer must have the Microsoft .NET 4.0 redistributable installed. If it is not installed, the SDG installer will warn you. You can download and install the redistributables from the Microsoft web site: http://www.microsoft.com/en-us/download/details.aspx?id=17718 5.7 Missing DLL Files Missing Microsoft Visual C Run-time Components 5.7.1 If you receive an error message indicating that the msvcrt.dll file is missing, then Microsoft Visual C Run-time Components have not been fully installed on your computer. These files are normally part of all Microsoft Windows Operating System installations. To fix this problem, contact Triangle MicroWorks. We will send you a new msvcrt.dll with instructions for where to load the file. Error Starting the GTWService - Missing Localization DLL (VC80.MCFLOC) 5.7.2 If you are experiencing issues when trying to run the SDG as a service, the localization DLL may be corrupt or missing. To correct this, you will need to download and install the Microsoft Visual C++ 2010 SP1 Redistributable Package using the following URL: http://www.microsoft.com/en-us/download/details.aspx?id=8328 5.8 Using an RS-232/RS-485 Converter with the SCADA Data Gateway The SDG is compatible with any RS-232/RS-485 converter using Send Data (SD) control. If you experience communication problems when using the SDG with an RS-232/RS-485 converter, make sure that your converter is configured for Send Data control. If your converter does not support Send Data control, you will need to acquire a different converter before using the SDG in your project. Triangle MicroWorks has tested the SDG using the 485SD9TB Port Powered RS-232 to RS-485 Converter from B&B Electronics: http://www.bb-elec.com/Products/Serial-Connectivity/Serial-Converters/Port-Powered-RS-232-to-RS- 485-Converters.aspx 5.9 Missing Parameters in tmwgtway.ini file Note that not all parameters are written to the INI file by default. For some parameters, if the current setting is the default setting, the parameter is not written to the INI file. To change the value of these parameters, you may simply add the parameter to the INI file. A complete list of INI file configuration options is available in the SDG Application by selecting HelpINI File from the menu. The order is not important; when you save the INI file from the SDG, it will reorganize the parameters and add the comments. Other Connection problems. 5.9.1 An incorrect firewall configuration can also prevent connections from occurring. Here is a list of commonly used port which need to be opened depending of used protocols. SDG Monitor 58090 SDG Engine 58080 OPC UA 4885 OPC Classic (Windows Only) 8081 ICCP (Tase2) 102 61850 102 60870-5-104 2404 60870-5-101 2404 Modbus 502 DNP3 20000 License 1947 5.10 Exit codes These are the exit codes when monitor or engine fails to start. Code Explanation 0 Success 1 General failure 2 Exception thrown 3 Configuration validation failed 4 Web server failed to start non HTTPS 5 Web server initialization failed non HTTPS 6 Web server failed to start HTTPS 7 Web server initialization failed HTTPS 8 Failed to create the WorkSpaces directory 9 INI file failed to load 10 CSV file pointmap file failed to save 11 Initialization of INI/CSV failed 12 OPC Classic startup failed 13 Load Point map file failed 14 Create of trial license failed 15 No license found 16 No INI file found 17 Invalid Monitor location 18 OPC Classic registration failed 19 Invalid monitor network address 129 HTTPS Security key problem 5.11 Security tips Turn on https with proper certificate files. Turn on User Authentication and create appropriate accounts with strong passwords. Make sure the host system has proper access control limits setup, with protection for sensitive files. Isolate the system from the internet if possible. Set appropriate automatic logoff timeouts for the various roles. Utilize the security features of the process protocols. Backup your important data. Utilize anti-virus software. Make sure your site is physically secure. Avoid wi-fi if possible. Properly secure it if needed. Create a security plan and educate your employees. ??? 5.12 Missing linux dependency To resolve dependencies issue (e.g. libltdl7 on Ubuntu): sudo apt-get -f install 5.13 Using Cockpit for monitoring on linux Cockpit can be very useful to check what is happening: https://cockpit-project.org/running 6. SCADA Data Gateway Technical Support The best method for receiving technical support for the SDG is to complete a support request ticket on the Triangle MicroWorks website. This request is submitted directly to the technical support department. To access this page, click on HelpSubmit Support Request from the SDGs main menu (requires an internet connection). To access this page directly, please use the following link: http://www.trianglemicroworks.com/SDGSupportRequest.aspx Please be sure to complete all the requested information and to include your INI and CSV files for the configuration along with the log files. The INI and CSV files are located in the WorkSpaces directory of the SCADA Data Gateway; by default: etc/tmw/sdg/WorkSpaces. They can be retrieved on the Settings/Sytem Settings dialog by using the Manage Workspace Files controls. To save a log to a file, goto Log/Monitor Log and use the Download Selected Log control. 7. Glossary of Terms Term Description APCI (Application Protocol Control Information) The transport interface is a stream-oriented interface, which does not define any start or stop mechanism for the ASDUs of IEC 60870-5-101. In order to detect the start and end of the ASDUs, each APCI includes delimiting elements (i.e., a start char, specific length of the ASDU, and control field). APDU (Application Protocol Data Unit) Contains an APCI and an ASDU. The 1st octet, which is subsequently the 1st octet of the APCI, contains the length of the APDU (max 253). Application Layer Layer 7 of the ISO model and one of the layers used in the EPA model. The application layer contains a number of application functions that involve the transmission of ASDUs between source and destination. ASDU (Application Service Data Unit) The application user layer contains a number of Application Functions that involve the transmission of ASDUs between source and destination. The ASDU is composed of a Data unit identifier and one or more information objects ASDU Data Unit ID Data Unit Type Information Object Information Object ID Time Tags Information baseband A type of digital data transmission in which each medium (wire) carries only one signal, or channel, at a time. In contrast, broadband transmission enables a single wire to carry multiple signals simultaneously. Client A device that receives data on a communication link. Also referred to as Master or SCADA node. Comma Separated File (CSV) A file format in which each object in the file is separated by a comma. When an object is to be omitted then two commas are side by side with no space in between. Communication Channels These may be physically discrete ports, or logically discrete ports that share a physical connection, such as multiple TCP/IP serial pipes, each of which is a separate logical port, even though they may share a single physical 10BaseT interface. control direction The direction of transmission from the controlling station to a controlled station. DNP or DNP3 Distributed Network Protocol EPA (Enhanced Performance Architecture) A protocol reference model that provides (compared to the 7 layer ISO model architecture) a three-layer architecture for obtaining faster response times for the critical information but with service limitation. Defined to use Layers: 1 (Physical), 2 (Data Link Layer) and 7 (Application Layer), along with a User defined Layer at the top. Ethernet A local-area network (LAN) protocol developed by Xerox Corporation in cooperation with DEC and Intel in 1976. Ethernet uses a bus or star topology and supports data transfer rates of 10 Mbps. The Ethernet specification served as the basis for the IEEE 802.3 standard, which specifies the physical and lower software layers. A newer version of Ethernet, called 100Base-T (or Fast Ethernet) now exists. F-Code Function code FUN Function type Function Type The 1st octet of the information object identifier/data unit identifier defines the function of protection equipment. IEC (International Electrotechnical Commission) The IEC is a worldwide organization for standardization comprising all national electrotechnical committees. The object of the IEC is to promote international co-operation on all questions concerning standardization in the electrical and electronic fields. IED Intelligent Electronic Device IEC 101 IEC 60870-5 protocol standard Information Object A component of the ASDU. Consists of an information object identifier, which may include the information object type, information object address, and a set of information elements along with a time tag of information object. IOA (Information Object Address) Specifies the associated address of the first information element of the sequence of information elements in the ASDU. INF (Information Number) The INF is used when IEC60870-5-103 protocol is selected. The INF is an octet, which is part of the Information Object Identifier in the Information Object of the ASDU. interoperability The ability of software and hardware on different machines from different vendors to share data. ISO (International Organization for Standardization) Communication protocol widely used for SCADA and process control applications originally developed by Modicon. The Modbus protocol is now in the public domain. Monitoring direction The direction of transmission from the controlled station to a controlling station. OLE Microsoft Windows platforms Object Linking & Embedding OPC OLE for Process Control OSI (Open System Interconnection) The reference model of ISO-CCITT, which subdivides communication into seven layers. EPA uses a three-layer approach. Physical Tag Name Each row in the Point-Mapping file has a Physical tagName field that is generated from other entries (it is not entered by the user). When making reference to a Master Component Point, either the User Tag Name (if one was entered) or the Physical Tag Name may be used interchangeably to identify the point. Point A data entity. The lowest level of indivisible data collected or sent by the SDG. RTU Remote Terminal Unit SCADA (Supervisory Control and Data Acquisition) Unbalanced transmission procedures are used in SCADA systems in which a Master station controls the data traffic by polling outstations sequentially. In this case the Master station is the primary station that initiates all message transfers while the outstations are secondary stations that may transmit only when they are polled SDG SCADA Data Gateway TCP/IP (Transmission Control Protocol/Internet Protocol) A protocol developed for the Internet to get data from one network device to another; "TCP uses a retransmission strategy to insure that data will not be lost in transmission". Type ID Is a code, which unambiguously identifies the ASDUs type within the collection of all possible types, for a system or a profile. The Type ID enables the receiving application service to send each data unit to the correct application process for handling the indicated type of data unit. It also enables the receiving application process to see what type of data is contained in the data unit and to determine its structure from a local table. User Tag Name The User Tag Name is a user-supplied text label that identifies the Master Component Point. The User Tag Name should not contain spaces. Also beware of using characters in tag names that may have special meaning to OPC clients. 10 Base T One of several adaptations of the Ethernet (IEEE 802.3) standard for Local Area Networks (LANs). The 10Base-T standard (also called Twisted Pair Ethernet) uses a twisted-pair cable with maximum lengths of 100 meters. The cable is thinner and more flexible than the coaxial cable used for the 10Base-2 or 10Base-5 standards. Cables in the 10Base-T system connect with RJ-45 connectors. A star topology is common with 12 or more computers connected directly to a hub or concentrator. The 10Base-T system operates at 10 Mbps and uses baseband transmission methods. 100 Base T A networking standard that supports data transfer rates up to 100 Mbps (100 megabits per second). 100Base-T is based on the older standard. 

**********SCL Navigator User Manual************
 TRIANGLE MICROWORKS, INC. SCL Navigator User Manual Version 1.3.5 July 2023 Property of Triangle MicroWorks, Inc. This Documentation and its associated distribution media contain proprietary information of Triangle MicroWorks, Inc. and may not be copied or distributed in any form without the written permission of Triangle MicroWorks, Inc. Copies of the source code may be made only for backup purposes. 2000 - 2023 Triangle MicroWorks, Inc. All rights reserve TRIANGLE MICROWORKS, INC. LICENSE AGREEMENT IMPORTANT THIS IS A LEGAL AGREEMENT BETWEEN YOU (YOU) AND TRIANGLE MICROWORKS, INC. (TRIANGLE MICROWORKS). TRIANGLE MICROWORKS IS WILLING TO LICENSE THE TRIANGLE MICROWORKS SOFTWARE AND RELATED DOCUMENTATION PURCHASED BY YOU AND KNOWN AS SCL Navigator (THE SOFTWARE) TO YOU ONLY UPON THE CONDITION THAT YOU ACCEPT ALL OF THE TERMS CONTAINED IN THIS LICENSE AGREEMENT (THE "AGREEMENT"). PLEASE READ THE TERMS AND CONDITIONS CAREFULLY BEFORE CLICKING ON THE I ACCEPT THE LICENSE AGREEMENT BUTTON, AS CLICKING ON THIS BUTTON WILL INDICATE YOUR ASSENT TO THEM. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, THEN CLICK ON THE CANCEL BUTTON. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, TRIANGLE MICROWORKS IS UNWILLING TO LICENSE THE SOFTWARE TO YOU. 1. Subject to the following terms and conditions, Triangle MicroWorks grants to you a nonexclusive, nontransferable limited license to install and use the Software solely for your internal business purposes and subject to the limitations in this Agreement. There are three mechanisms to license the Software. The Seat License is a PC based software license that is installed on a single computer and is solely for the use of the Software by you on a single computer at any one time. The Hardware Dongle License is a license that is stored on a dedicated USB hardware device that can be shared between computers by physically moving the Hardware Dongle License. The Hardware Dongle License is solely for the use of the Software by you on a single computer at any one time. The Network License is a license that is stored on a computer that can be shared across a local area network. The Network License is solely for the use of the Software by you on a single computer at any one time. The Network License may be installed on a corporate network and shared locally. The Network License cannot be shared outside of your company. You may physically transfer or copy the Software on to another computer; provided, however that each license instance may not be used on more than one computer or device at any given time. Subject to the foregoing limitation, you are entitled to copy the Software into any machine-readable or printed form for backup purposes in support of the use of the Software on a single computer by you. Any copy of the Software permitted by the preceding provision shall include a copyright notice of Triangle MicroWorks. 2. You may use the Software solely for the applicable term (i.e., period of time) as set forth below. If you have licensed the Software for a free trial period (a Trial License), the term of the Software license will be for 21 days. You understand and acknowledge that after the 21- day period and unless otherwise extended by Triangle MicroWorks, in its sole discretion, the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software. At the conclusion of the Trial License, you may purchase a Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have licensed the Software for a fixed period of time and not on a trial basis (a Term License), the term of the Software license will be the period of time agreed to by you and Triangle MicroWorks. At the end of that period of time, your use of the Software will terminate, and the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software except as explicitly set forth herein. At the conclusion of the Term License, you may purchase an additional Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have paid a one-time fee for a perpetual license, then your term of the Software License is perpetual (a Perpetual License) and subject to termination only as set forth herein. 3. You may not transfer the Software to another entity or person. You shall not modify, translate, reverse assemble or reverse compile in whole or in part the Software. You shall not rent, sell, time-share, lease, sublicense, transfer, copy, disclose, display or otherwise make available the Software or copies thereof to others, except as provided herein. Any breach of the above terms and conditions shall immediately terminate your license under this Agreement. You hereby certify and agree that the Software will not be shipped, transferred, or re-exported, directly or indirectly, into any country prohibited by the United States Export Administration Act and the regulations thereunder, and the Software will not be used for any purpose prohibited by the same. Neither you nor any of your subsidiaries, as applicable, or to your knowledge any of their directors, officers, or employees, is a person or entity that is directly or indirectly owned or controlled by any person, entity or country currently included on the List of Specially Designated Nationals and Blocked Persons or the Foreign Sanctions Evaders List maintained by the U.S. Treasury Departments OFAC, or is directly or indirectly owned or controlled by any person or entity that is located, organized, or resident in a country or territory that is, or whose government currently is, the target of countrywide sanctions imposed by any U.S. government Sanctions Authority. Neither you nor any of your subsidiaries, as applicable, or to your knowledge any of their directors, officers, or employees is a person or entity that is currently the subject of any Sanctions investigation, or is directly or indirectly owned or controlled by any person, entity or country that is currently the subject of a Sanctions investigation. 4. You may not erase, delete or modify any copyright, trademark or other proprietary notices included on the Software. You agree that if you infringe Triangle MicroWorks intellectual property rights or exceed the scope of permitted use of this license, Triangle MicroWorks will be irreparably injured and may obtain a court order to enjoin you from further use of the Software. 5. This license is not a sale. Title, ownership rights, and intellectual property rights in and to the Software shall remain in Triangle MicroWorks. You agree to abide by the copyright laws and all other applicable laws of the United States and international treaties. You acknowledge that the Software in source code form remains a confidential trade secret of Triangle MicroWorks. Failure to comply with the above restrictions will result in automatic termination of this Agreement and will make available to Triangle MicroWorks other legal remedies. 6. Notwithstanding anything herein to the contrary, you acknowledge and agree that Triangle MicroWorks may terminate your license to the Software if you have failed to comply with any of your obligations under this Agreement. You shall not be entitled to a refund in the event of termination by Triangle MicroWorks pursuant the foregoing sentence. 7. By purchasing a Term License or a Perpetual License to the Software you are automatically subscribed in the Software Maintenance & Enhancement Plan (the Plan) for a period of one (1) year following the date of purchase (the Purchase Date) of the license, or if shorter, the term of your Term Licensee (such initial period referred to as the Initial Maintenance Period). Upon expiration of the Initial Maintenance Period, the Plan may be renewed on an annual basis subject to payment by you of the applicable then-current annual fee. This Plan may provide a cost effective method for upgrading to the most recent release version of the Software. The Plan also provides technical support by telephone, fax, or email. The Initial Maintenance Period for Perpetual Licenses is free. The annual fee for the Plan for Perpetual Licenses after the first year is 25% of the Perpetual License purchase price; provided, however that Triangle MicroWorks, in its sole discretion, may adjust the annual fee at any time. The annual fee for the Plan for Term Licenses depends on the specific terms of the Term License. Although Triangle MicroWorks has no obligation to provide upgrades to the Software under the Plan, Triangle MicroWorks may from time to time provide upgrades. Any upgrades to the Software shall be deemed to become part of the Software and shall be subject to the terms and conditions of this Agreement. All Plan annual fees are nonrefundable. 8. For Term Licenses and Perpetual Licenses only, Triangle MicroWorks warrants that the Software shall perform substantially as described in its documentation for a period of sixty (60) days from the Purchase Date. In the event of a breach of the foregoing warranty, the sole obligation of Triangle MicroWorks and your sole remedy is that Triangle MicroWorks will provide you with a refund of the most recent license fee paid for the Software. Triangle MicroWorks, as a condition to providing such refund, may require that you sign an affidavit certifying that you have deleted or destroyed all copies of the Software in your possession. Triangle MicroWorks makes no warranties with respect to Trial Licenses. Except as set forth in this Section 8, THE SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTIES OF ANY TYPE, AND TRIANGLE MICROWORKS HEREBY EXPRESSLY DISCLAIMS ALL WARRANTIES, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY, TITLE, NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE. 9. TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL TRIANGLE MICROWORKS BE LIABLE FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL OR PUNITIVE DAMAGES WHATSOEVER, INCLUDING ANY CLAIMS FOR LOST PROFITS, BUSINESS INTERRUPTION, LOST INFORMATION OR OTHER DAMAGES ARISING OUT OF THE USE OF INABILITY TO USE THE SOFTWARE. IN NO EVENT SHALL TRIANGLE MICROWORKS HAVE ANY LIABILITY FOR ANY DATA STORED OR PROCESSED WITH THIS SOFTWARE, INCLUDING THE COST OF RECOVERING ANY LOST DATA. IN ALL EVENTS, TRIANGLE MICROWORKS SOLE OBLIGATION OR LIABILITY UNDER THIS AGREEMENT IN THE AGGREGATE IS A REFUND OF THE MOST RECENT LICENSE FEE PAID FOR THE SOFTWARE. THE PARTIES AGREE THAT THESE LIMITATIONS ARE AN ESSENTIAL PART OF THE AGREEMENT AND THAT TRIANGLE MICROWORKS WOULD NOT ENTER INTO THE AGREEMENT WITHOUT THESE LIMITATIONS. THIS LIMITATION OF LIABILITY CLAUSE WILL SURVIVE FAILURE OF ITS ESSENTIAL PURPOSE. 10. You may terminate the license granted under this Agreement at any time by destroying or deleting all copies of the Software in your possession or control. In the event of termination of this Agreement for any reason, all the sections of this Agreement will survive except for Sections 1 and 2. 11. This Agreement shall be governed by and construed in accordance with the laws of the State of North Carolina, without regard to the choice of law provisions thereof. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to this Agreement. Any dispute or claim arising out of, or in connection with, this Agreement shall be finally settled by binding arbitration in Raleigh, North Carolina, in accordance with N.C. Gen. Stat. 1-569.1 et seq. (the Uniform Arbitration Act) and the then-current rules and procedures of the American Arbitration Association by one (1) arbitrator appointed by the American Arbitration Association. The arbitrator shall apply the law of the State of North Carolina, without reference to rules of conflict of law or statutory rules of arbitration, to the merits of any dispute or claim. Judgment on the award rendered by the arbitrator may be entered in any court of competent jurisdiction. The parties agree that, any provision of applicable law notwithstanding, they will not request, and the arbitrator shall have no authority to award, punitive or exemplary damages against any party. In the event that any arbitration, action or proceeding is brought in connection with this Agreement, the prevailing party shall be entitled to recover its costs and reasonable attorneys' fees. In the event that the above arbitration provision is held invalid or unenforceable, any dispute with respect to this Agreement shall be brought and heard either in the North Carolina state courts located in Raleigh, North Carolina or the federal district court, located in Raleigh, North Carolina. In such event, the parties to this Agreement each consent to the in person jurisdiction and venue of such courts. The parties agree that service of process upon them in any such action may be made if delivered in person, by courier service, by telegram, by facsimile or by first class mail, and shall be deemed effectively given upon receipt. 12. This Agreement sets forth the entire agreement between you and Triangle MicroWorks pertaining to the licensing of the Software and supersedes in its entirety any and all written or oral agreements previously existing between the parties with respect to such subject matter. If any provision of this Agreement is held invalid or unenforceable, such provision shall be revised to the extent necessary to cure the invalidity or unenforceability, and the remainder of this Agreement shall continue in full force and effect. In the event of any conflict between any provision of this Agreement and any applicable law, the provision or provisions of this Agreement affected shall be modified to remove such conflict and permit compliance with such law and as so modified this Agreement shall continue in full force and effect. This Agreement may not be assigned without the consent of Triangle MicroWorks. Triangle MicroWorks, Inc. 2840 Plaza Place, Suite 205, Raleigh, North Carolina 27612 USA (919) 870-5101 1 INTRODUCTION ........................................................................................................................................................ 9 2 SCL BASICS ................................................................................................................................................................ 9 2.1 THE ELEMENTS OF AN SCL FILE......................................................................................................................... 9 2.2 VARIANTS OF SCL FILES ..................................................................................................................................... 9 3 SCL NAVIGATOR MENUS AND VIEWS .................................................................................................................... 10 3.1 WINDOWS AND VIEWS LAYOUT ..................................................................................................................... 10 3.2 MENUS ............................................................................................................................................................ 11 3.2.1 File Menu ................................................................................................................................................. 11 3.2.2 Edit Menu ................................................................................................................................................ 12 3.2.3 Tools Menu .............................................................................................................................................. 13 3.2.4 View Menu .............................................................................................................................................. 15 3.2.5 Window Menu ......................................................................................................................................... 15 3.2.6 Preferences Menu ................................................................................................................................... 16 3.3 QUICK ACCESS BAR ......................................................................................................................................... 18 3.4 EDITOR TABS WINDOW AND NAVIGATION PANE .......................................................................................... 19 3.4.1 Editor Tabs ............................................................................................................................................... 19 3.4.2 Line Presentation In The Editor ............................................................................................................... 19 3.4.3 Context Menu .......................................................................................................................................... 20 3.4.4 Editor Navigation Pane ............................................................................................................................ 20 3.4.5 Search Box ............................................................................................................................................... 21 3.5 EXTRA VIEW PANE .......................................................................................................................................... 21 3.5.1 SCL Verify ................................................................................................................................................. 21 3.5.2 RiseClipse Verify ...................................................................................................................................... 23 3.5.3 Model Viewer .......................................................................................................................................... 24 3.5.4 Model Compare ....................................................................................................................................... 25 3.5.5 Open Read Only Files and Fragments ..................................................................................................... 29 3.6 OUTPUT VIEW ................................................................................................................................................. 30 3.7 LN LIBRARY VIEW ............................................................................................................................................ 30 3.8 SHORTCUT KEYS .............................................................................................................................................. 31 4 SCL EDITING IN NAVIGATOR .................................................................................................................................. 31 4.1 FILE CREATION ................................................................................................................................................ 31 4.2 DATA TYPE TEMPLATES .................................................................................................................................. 34 4.3 CONFIGURING INSTANCE DATA ..................................................................................................................... 35 4.4 CHANGING IDENTIFIERS ................................................................................................................................. 37 4.5 DATA TYPE TEMPLATE IMPORT/EXPORT ................................................................................................................. 39 4.5.1 Export ...................................................................................................................................................... 39 4.5.2 Import ...................................................................................................................................................... 39 4.5.2.1 XML Import ........................................................................................................................................... 39 4.5.2.2 NSD Import ........................................................................................................................................... 40 4.7 MODIFYING DO TYPES .................................................................................................................................... 44 4.8 MODIFYING ELEMENTS IN THE SCL FILE ......................................................................................................... 46 4.9 BASIC EDITING OF THE FILE ............................................................................................................................ 46 4.11 PRE-CONFIGURATION OF CONTROL BLOCKS ............................................................................................... 48 4.12 SADDRESS IMPORT/EXPORT ................................................................................................................................ 50 1 INTRODUCTION SCL Navigator is a tool to work with IEC 61850 Substation Configuration Language (SCL) files. It provides the following main functionalities: - Create and modify Intelligent Electronic Device (IED) related SCL files (IED Capability Description (ICD), Instantiated IED Description (IID) or future ISD) - Analyze and edit any kind of IEC 61850 SCL file. SCL Navigator supports Edition 1 (Schema 1.7), Edition 2 (Schema 2007B) and Edition 2.1 (Schema 2007B4) of IEC 61850-6. The data model when using Edition 1 schema needs to be Edition 1; when using 2007B or 2007B4 it needs to be Ed 2. 2 SCL BASICS Below is a high-level description of SCL. Note that this is just a summary. 2.1 THE ELEMENTS OF AN SCL FILE An SCL file is an XML file (for an introduction to XML see e.g. http://www.w3schools.com/XML/default.asp) An SCL file has the following top-level XML Elements (called section in SCL): - Header: contains information of the SCL file including versioning support - Communication: collects all the information related to communication network related parameters - Substation: describes the topology and functional requirements of the substation - IED: describes capabilities, communication configuration (control blocks, data sets) and the data model with possible pre-configuration of values of the IED; can be present multiple times (once for each IED in the scope of the SCL file) - Data Type Templates: hierarchically describes the detailed structures of the data models for all IEDs and functional requirements in an SCL file; can contain templates for data objects types, data attribute types, and enumeration types. 2.2 VARIANTS OF SCL FILES Various SCL file types exist depending on the engineering stage. Different file types have different presence of the top level SCL elements. All file types have a header section. - SSD Substation specification description: Used to describe the substation functional requirements; has as a minimum one substation section - ICD IED Capability Description: used to describe the capabilities and the data model of an IED, the IED name is set to TEMPLATE in these types of files; may include preconfigured communication configuration; has as a minimum of one IED section and a data type template section - IID Instantiated IED Description: used to describe an IED that has at least a partial configuration for a specific role in a project; as such an IED name is configured; otherwise as ICD - CID Configured IED Description: used to describe the full configuration of an IED; includes the complete configuration of the communication (control blocks, data sets and communication parameters) as well as everything the IED needs to know about the configuration of other devices in the system, to be able to perform its functionality; has the IED section related to itself and the data type template section; has a part of a communication section with the relevant information and additional IED sections from the IEDs where this IED needs to receive messages - SCD Substation Configuration Description: this is the complete SCL file that describes the substation; has all sections including one IED section per IED in the system - SED System Exchange Description: used to exchange information between two SCL projects 3 SCL NAVIGATOR MENUS AND VIEWS 3.1 WINDOWS AND VIEWS LAYOUT SCL Navigator layout The user interface of SCL Navigator is divided into multiple areas. Those areas are number in image below and described in the following sections. The areas are identified as: 1. Menus 2. Quick access bar 3. Editor tabs window (each SCL file has its own tab) 4. Editor Navigation pane 5. Extra view pane (For Verify and the Model View) 6. LN Library view 7. Output view 3.2 MENUS SCL Navigator Menu 3.2.1 File Menu 3.2.1.1 Create a file Select File New which Edition of IEC 61850 the file should be based on. When asked, enter the file name and select a folder where the file will be stored. Enter the desired file extension by default the tool will create a file with the extension ICD. File creation 3.2.1.2 Open a file Select File Open Select the folder where the file is stored. By default, the tool will look for files with the IEC 61850 SCL Files extension (*.icd, *.iid, *.cid, *.scl, *.scd, *.ssd, *.sed) File opening 3.2.1.3 Save files Three options are available for saving files. The user can directly save the currently selected file, save the currently selected file to directory and/or name and save all the files currently open in SCL Navigator. File saving 3.2.2 Edit Menu 3.2.2.1 Format file The Format File menu option will fix the indentation of the currently selected file and remove unnecessary carriage returns if needed. 3.2.2.2 Clean template The Clean Template menu option will remove any unused templates in the currently selected file. If two templates are identical, this command will also merge them into one template and fix any reference to this template. 3.2.2.3 ICD Schema Conversion If the current ICD files schema is 2007B (Edition 2) the user will have the ability to convert the files schema to 2007B4 (Edition 2.1). 3.2.2.4 Undo/Redo The Undo and Redo menu options allow the user to revert or re-apply any changes to the file made in the same editing session. Because of resource constraint, SCL Navigator will not provide the Undo/Redo options for files bigger than 22 MB. 3.2.2.5 Search capability The Search menu option allows the user to search for any string in a SCL file. The results of the search are displayed in the output view. Clicking on any search result in the output view will bring the user to the specific line containing the string in the Editor. SCL Navigator allows the user to quickly go to a specific line by its number. Because the editor is displaying a condensed version of the SCL file, the Go To Line Number will go to the closest line displayed if the line is not on the screen. 3.2.3 Tools Menu 3.2.3.1 Open Model View The user can open a SCL Model View of the currently selected file in the extra view pane. For more detail on the SCL Model View please refer to the Model Viewer Section. 3.2.3.2 SCL/RiseClipse Verify The user can open a SCL/RiseClipse Verify panes for the currently selected file by selecting this menu option. SCL/RiseClipse Verify can be used to perform a verification on the SCL file to determine possible problems in the syntax and with the formulation of the SCL in the file. For more detail on the SCL Verify please refer to 3.5.2 Section and 3.5.3 Section for RiseClipse Verify. 3.2.3.3 Comparing SCL file The Compare menu option will show the differences in SCL files. SCL Navigator offers two options for comparing files: The user can compare the current and unsaved version of the file against its saved version. The user can also compare the current and unsaved version of the file against a different file present on their system. 3.2.3.4 SAddress Import/Export SAddress Import/Export allows to edit, import and export SAddress in xlsx format of any IED from file loaded into Navigator. 3.2.3.5 Model Compare Model Compare allows to compare IEC 61850 Object and Communication models from two different scd, ssd, icd, iid, cid, scl files. 3.2.3.6 Manage IEDs IPs This utility is used to quickly reassign IP Addresses, Subnet Masks, and Gateway Addresses to multiple IEDs simultaneously. To reassign in bulk, select the Checkbox of the target IEDs in the grid, fill in desired fields in the Bulk Reassignment pane, and select Apply. Blank fields will remain unchanged. In addition to bulk editing using the upper pane, address fields in the lower pane may be edited individually. Export to CSV/Import Selected is used to save all, or reload the selected IP Addresses, Subnet Masks and Gateway Addresses. Manage IEDs IPs Window 3.2.3.7 Manage User LN Library This command allows the user to add custom User LN Library to the LN Library from NSD files from a Specific folder. Once the import is completed the new User LN Library is created as an XML file in the userLNLibrary folder. The user can add/remove imported files from this folder to manage his User LN Libraries. Manage User LN Library 3.2.3.8 Create DNP3 Point Mapping The goal of this screen is to create a file containing a list of points that can be mapped to DNP3 devices. The corresponding mDNP3 points can be retrieved by either the sAddress or a custom attribute in the instance data, depending on the configuration of the SCL file. Before selecting the desired file format, users can choose the DNP3 class. Point searching can be performed for all the IEDs in the file or just one (using the IED context menu). Create DNP3 Point Mapping 3.2.4 View Menu 3.2.4.1 Output The user can hide or show the Output View. For more detail on the Output View please refer to the Output Section. 3.2.4.2 LN Library The user can hide or show the LN Library View. For more detail on the LN Library View please refer to the LN Library Section. 3.2.5 Window Menu A user can open multiple files in one SCL Navigator instance. Each file corresponds to a tab in the Editor window. If multiple files are open, the windows menu allows the user to quickly select a specific file from the list. The same functionality is present on the top right side of the screen. Window Menu 3.2.6 Preferences Menu 3.2.6.1 Font Size To improve the readability and adjust to different screen resolutions, the user can adjust the font size of the Editor. 3.2.6.2 Theme Two Editor themes are available to improve the user readability of the Editor: The default SCL Navigator theme - White/Blue/Red. The low brightness theme - Black/Light Blue/Orange. 3.2.6.3 Configure File Comparison Tool SCL Navigator allows the use of an external tool to compare versions of the current file. This feature works with third party file comparison tools which can be called from a command line and accept file paths to the two files being compared as command line arguments. To use this functionality, the user must configure the command line parameters to access the external comparison tool: Use the ellipsis button to browse to the executable path of the file comparison tool. The Parameters line allows SCL Navigator to input the file path to the comparison tool. The parameter {0} represents the selected file displayed in the Editor. The parameter {1} is the file path to be compared to. File Comparison Tool Configuration popup There are two options available for comparing SCL files. In the first option, the currently selected file is compared to the last saved version of the file. File opening In this case, the currently selected file path is passed to the comparison tool in the {0} parameter and the previously saved file path is passed in the {1} parameter. In the second option, the currently selected file is compared to a file of the users choice. In this case, the currently selected file path is passed to the comparison tool in the {0} parameter and a file selection dialog box is displayed to allow the user to pick a file which is passed to the comparison tool in the {1} parameter. 3.2.6.4 Expert Mode The Expert Mode is disabled by default. Once enabled it allows the user to have full control of the file edition. For example, the Expert Mode will grant the user the ability to create Child DO in a LN Template or edit a SCL line. 1 2 3 4 5 6 7 8 9 3.3 QUICK ACCESS BAR Quick Access Bar 1. Go Back/Go Forward These buttons allow the user to navigate back and to navigate forward to previously selected elements in the selected file. 2. File Management These buttons allow the user to respectively Open, Save, and Save as a SCL file. 3. Undo/Redo These buttons allow the user to access the Undo and Redo functions for the selected file (please refer to the Undo/Redo Section). 4. Model View This button opens a SCL Model view for the selected file (please refer to the Model View Section). 5. Run SCL Verify This button runs the SCL Verify command against the selected file (please refer to section to the SCL Verify Section). 6. Format file This button launches the Format file command for the selected file (please refer to the Format file Section). 7. Clean Templates This button launches the Clean template command for the selected file (please refer to the Clean template Section). 8. Output The Output button toggles the visibility of the Output View (please refer to the Output Section). 9. LN Library The LN Library button toggles the visibility of the LN Library View (please refer to the LN Library Section). Moving a T ab 1 2 3 4 5 6 7 3.4 EDITOR TABS WINDOW AND NAVIGATION PANE 3.4.1 Editor Tabs Each open SCL file is a tab in the Editor. The tabs can be re-ordered by dragging and dropping them. If a tab is closed and its corresponding SCL file still has pending changes, a popup will be displayed asking the user whether to save its pending changes. In addition to saving its changes, the user can also format the file and clean its data type templates. Save Changes popup 3.4.2 Line Presentation In The Editor Editor Line 1. Line Number 2. Collapse/Expand elements children 3. Attribute present in the element 4. List of possible values for an attribute (accessible with the Control Key) 5. Default Value Attribute (green color) 6. Optional Attribute (gray color) 7. Mandatory Attribute (orange color) When editing an attribute value, the Escape Key allow you to revert to the previous attribute value. If no value is present in the attribute, the Delete Key will remove the attribute completely from the element. 3.4.3 Context Menu The common Editor context menu is available by right clicking on any element in an editor window. The functionalities available on the context menu are: Expand and collapse child or sibling of the selected element Delete the selected element Open the Model View for this specific element Open the selected element as fragment in a new read-only window (see the Open Read Only Files And Fragment Section) Other functionalities will be available depending on the element selected. Editor context menu 3.4.4 Editor Navigation Pane The Editor Navigation pane is a hierarchical tree of the SCL File. It provides the following functionalities: Quick navigation to an element in the SCL file by clicking on the logical node name. Logical Node drag and drop: - Re-order of a Logical Node inside a Logical Device. - Transfer of a Logical Node between Logical Device. Editing a Logical Node. Deletion of a Logical Node. Expand and collapse child or sibling of the selected element File Searching Navigation Pane context menu 3.4.5 Search Box At the bottom of the Navigation pane the user can find the search box. This performs a textual search of the file and displays the results in the Output View. 3.5 EXTRA VIEW PANE 3.5.1 SCL Verify With SCL Verify, the capability to verify the consistency of any SCL file is built into the tool. The verification of the currently selected file is started using the menu item Run SCL Verify SCL Verify tool SCL verify window A new window opens, showing the results. The results are classified as critical, errors, warnings and information. The results table can be rearranged to help analyze the findings. A double click on an entry in the SCL Verify window sets the focus to the corresponding line in the SCL file. The way errors are reported and categorized can be modified by the user by editing the following file: C:\ProgramData\Triangle MicroWorks\Common\SCLVerify\SCLVerifyConfigSources\TMWSCLVerifyConfig.xml. The user can change the category and severity on each error code. It is preferable to backup the file before making any changes. TMWSCLVerifyConfig.xml SCL verify window User frie ndly error description this text is not ( displayed by SCL Verify). F ile E xtension MaskO ut determine which fi le extension will be excluded from the verification ( icd, scl ) . S everity can be change by the user . C ategory can be change by the user . C ode can t be changed . If an entry is removed from the file , t he error will disappear from the verif y results. 3.5.2 RiseClipse Verify With RiseClipse Verify, the tool can compare the SCL file to a set of OCL Rules and NSD. The verification of the currently selected file is started using the menu Tools Run RiseClipse Verify. For more Information about RiseClipse see Official Web page: https://riseclipse.github.io RiseClipse Verify Data M odel V iew Like SCL Verify, the results table can be rearranged to help analyze the findings. A double click on an entry in the RiseClipse Verify window sets the focus to the corresponding line in the SCL file. 3.5.3 Model Viewer By clicking on the Model View menu item, an additional window with a Model View appears: The Model View has two parts, the navigation on the left side allows the user to navigate to individual data objects; in the right side of the window, the complete hierarchical model is visible (definitions from the data type templates are used to create the LN structure). The right side also displays preconfigured values. The user can open the Model View for a portion of the SCL file by right clicking on an object and selecting Open Model in New Window. (only available on IED, AccessPoint, LDevice and LN) Open the Model View for a part of the file Note -The Model View is only updated, when a change to the file has been saved. 3.5.4 Model Compare Model Compare allows to compare IEC 61850 Object and Communication models from two different scd, ssd, icd, iid, cid, scl files. User can launch model compare by clicking toggle button on toolbar or clicking menu item in Tools menu. Model Compare tool bar item. Model Compare initial view. All files which have been loaded into Navigator would populate under Source and Target File drop down list. When the user opens Model Compare View the active file in the Navigator would populate as selected file in source file drop down list. Model Compare Target File selection. User can trigger model compare by selecting Target File. If both source and target files are present and the selection change of either of the file would show initial comparison view. If only one file loaded into Navigator, User would get an option to select target file using browse window. Model Compare Select Target File using browse window. Model compare has two sections communication and object model. Each section has detail error messages on right hand side. Communication section compares Subnetwork, ConnectedAP, Address. User can see detail error messages by selecting compare node in result tree. Model Compare Communication compare result. The initial view of Object Model comparison would display all matching and missing IEDs from selected source and target files. User can compare source IED with any of the target IED by selecting item from drop down list. Model Compare Object Model Initial View. User can compare IEDs in 3 different ways. Compare all IEDs - By clicking on Compare All button would compare all IEDs and the detail error messages would be shown in error messages view on the right hand side. Compare matched IED By clicking on icon on any row would compare source and target IED and the detail error messages would be shown in error messages view on the right hand side. Compare different IED - By selecting different IED from target drop down would trigger comparision. and the detail error messages would be shown in error messages view on the right hand side. Model Compare Object Model Compare Result View. Each icon in compare result would denotes the status of comparison result. ICON Compare Result Relevant to Missing in Source or Target All Elements in Object and Communication model. Matched All Elements in Object and Communication model. Value has been modified Attribute Value in Object Model. sAddress value in Object Model Attribute value in communication model. Order is different Logical Node Type templates Data Object Type templates Model Compare Compare Result Icons. File open in read only copy User can expand collapse children and siblings by using context menu as shown in below. Model Compare Compare Result context menu. 3.5.5 Open Read Only Files and Fragments If the user opens the same file twice, the second instance of the file will be displayed as a tab in the Extra View Pane as a read-only file. The shaded background indicates the read-only state. If the user uses the context menu command Open In New Window, the element will be displayed as a read only fragment in a new tab in the Extra View Pane. Fragment in read only 3.6 OUTPUT VIEW The Output View allows the user to see the results of the search (see the Search Box Section) and schema errors. Each line can be clicked to bring the user to that specific line in the Editor panel. Search file result in the Output View 3.7 LN LIBRARY VIEW New Logical Nodes can be created from the LN Library. To make the LN Library View visible, click on the menu item LN Library. The LN Library View will display the Edition of the corresponding SCL file (please refer to the Adding additional Logical Nodes Section). Hide or unhide the LN Library View 3.8 SHORTCUT KEYS List of the shortcut keys: Create a SCL Schema 1.7 (Ed 1) File Control 1 Create a SCL Schema 2007B (Ed 2) File Control 2 Create a SCL Schema 2007B4 (Ed 2.1) File Control 3 Open a file Control O Save a file Control S Undo changes Control Z Redo changes Control Y Go to line number Control G Search text Control F Increase font size Control + Decrease font Size Control Open a list of possible values for an attribute Control 4 SCL EDITING IN NAVIGATOR 4.1 FILE CREATION Select File New which Edition of IEC 61850 the file shall be created. When asked, enter the file name and select a folder where the file shall be stored. Enter the desired file extension by default the tool will create a file with the extension ICD. Although the tool will create an ICD file by default (IED Name = TEMPLATE), it is possible to edit the IED name and as such create an IID file. The future ISD file (IED specification) as currently discussed within the standards development can be created as well. The tool supports the reuse of customized data model elements (Logical nodes, common data classes). This supports an efficient creation of the file. Next the tool will ask you to provide the File context this information is used for populating some information in the file and to set the default for analog values. This window is also accessible at any time by the SCL tag context Menu. SCL context information The IED Context Information dialog box will be displayed. IED Context Information popup SCL File ID appears in the header: SCL File ID Vendor and SW revision appear in the logical device name plate as preconfigured values for the DA vendor and swRev. Additionally, vendor appears as manufacturer information of the IED and in the physical device name plate. Preconfigured values for logical device name plate based on capturing of IED context Vendor information of IED Vendor information in the physical device name plate of the IED The selection of the Analogue value type definition is used as a default to create data structures for analogue values. Now the tool has created a first skeleton of a ICD file with an IED called TEMPLATE (which is the requirement for the IED name of an ICD file), with one logical device CTRL and the logical nodes LLN0 and LPHD1. Navigation to the LN type definition Navigat ion to the usage of a template 4.2 DATA TYPE TEMPLATES Data structures are defined in SCL as data type templates hierarchically organized. With SCL Navigator, it is easy to move between the different hierarchical levels of templates. With a right click on a logical node, it is possible to move to the definition of the logical node type: In the same way, it is possible to navigate within the Logical Node type from a DO to its DO type or within the DO type from a structured DA to its DA type. The reverse direction is supported in a similar way. A right click on the type definition shows the places where the type is used and allows navigation to one: 4.3 CONFIGURING INSTANCE DATA Pre-configuration of values can be made either in the data type template or in the instance of a logical node. To pre-configure values for logical node instances, with a right click on the logical node instance, instance data can be added. Pre-configure values at logical node instance SCL Navigator has now added placeholders for instance data at each DO for each DA that is present in the model. Placeholders for instance data added by SCL Navigator Pre - configuring a value for instance data The element Val to configure a value can be added with a right click on the selected DAI. Configuring a value for instance data Now the value can be typed in Once the required values have been configured, unused instance data can be removed. Adding a preconfigured value to a data attribute Removal of unused instance data To add pre-configuration at the template level, a value can be added to a selected DA by using its menu. A bulk add of instance value is also possible by selecting Add Value Element To Children in the DOType menu. This will create an empty value for all the DOType children without already an existing value. Bulk add of Instance Value. The recently added value can then be configured Configuring the value Note that all the empty values will be deleted by using the context menu option Delete Unused Value Element In Children or by using the Clean Templates command. 4.4 CHANGING IDENTIFIERS It is possible to change names of data type templates. This needs to be done where the template is defined. References to that template are then automatically updated. The renaming is initiated by a right click on the line where the template is defined. Renaming a template A new id can now be entered. Changing a data type template id Note that SCL Navigator does allow the user to change the id attribute by simply typing into it as shown below. However, in that case, the reference will not be updated. Overwriting an id will not update the references 4.5 DATA TYPE TEMPLATE IMPORT/EXPORT The Data Type Template can be fully or partially imported or exported using the methods below. Data Type Template Import/Export menu 4.5.1 Export Two export formats are available depending on the need of the user NSD or XML. Both Export screens work the same. The template can be fully or partially exported by choosing specific Logical Nodes in the list. A pre- selection can also by applied to the Logical Nodes list by selecting a previous exported file. After clicking Export, the Logical Node template is exported with all its dependent children (DOType, DAType ). Export specific LN of a Data type Template 4.5.2 Import Logical Node Templates and Libraries can be imported from XML or NSD file formats. 4.5.2.1 XML Import The Logic Nodes templates contain in the XML file can be fully or partially imported by choosing specific Logical Node in the list. A pre-selection of Logical node can also by apply to the Logical Nodes list by selecting a previous exported file. Import specific LN of a Data type Template A Results window will display the result of an import. The imported Logical Nodes can be added to the existing Data Type Templates or replace the existing Logical Nodes with the same type if any by checking the checkbox. 4.5.2.2 NSD Import This screen allows the user to import NSD files as new custom LN library. First the user needs to select a folder where NSD files are stored and enter a name for the new LN library. After the folder selection, a list of NSD files present in the selected folder will be displayed in the NSD Files Table. The results of the import are shown in the Results Table. After the import Navigator needs to be restarted to show the new LN library in the LN library tab. Import of NSD Files H ide or unhide the LN Standard L ibrary View 4.6 ADDING ADDITIONAL LOGICAL NODES New logical nodes can be created from the LN Library. To make the LN Library View visible, click on the menu item LN Library. The LN Library View will display the Edition of the corresponding SCL file. That information is visible in the attributes originalSclRevision and originalSclVersion. 2007B is the Ed 2 of IEC 61850-6. SCL version As mentioned in the introduction, currently the data model version and the SCL version need to be the same. That may be changed in the future when the tool will support the handling of the model namespace automatically. The user can add a logical node by dragging a LN from the LN Library View into a logical device in the navigation part of the window. Adding a LN by drag and drop A window pops up, that allows the user to select a prefix for the LN name, enter the instance number and select whether the analog data attributes shall be floating point or integer. Adding a new logical node In the next step (after clicking next), it is possible to select which data objects shall be added. By default, the mandatory DOs are already selected. For DOs that can have multiple instances, to the user can provide the instance number. Selecting the DOs to be inclu ded New LN CSWI Clicking finish will create the new LN. A new type will be created with the default name LNXX_TYPE (where LNXX is the LN class name). For the various DOs, if one or more DO types for that CDC already existed in the file, the first one that matches the DO is used. If no DO type for that CDC already exists, a new DO type with the name of the CDC is created which includes all mandatory elements of the standard based on the selection for analog values. For controllable CDC, the default is the model for status-only. If a new logical node is added where there is already an instance of the selected class available in the file, it is possible to choose that the new LN is using the type of the already existing LN instead of starting to configure it from scratch. DO types for CDC ENS Possibility to select an already existing LN Type for creating the new instance 4.7 MODIFYING DO TYPES It is possible from the view shown in image below, to customize the CDC type by clicking on the ID (which refers to a DO type). A window pops up that shows the CDC types from the LN Library as well as all other CDC types from the file that are applicable for that DO. The CDC type from the LN Library contains all the mandatory data. An existing CDC type can be selected (the selected one will be shown with a highlighted border) or a new CDC type can be created in the most left column. To create a new CDC type, select in the most left column the DAs that shall be included and provide a name for the new CDC type. Create a new CDC type By default, for controllable objects, a variant of the CDC that is status only will be proposed. When creating a new CDC, it is possible to select a control model. Based on the selection, data attributes are selected or not in the proposed Custom CDC type and the value for the DA ctlModel will be configured accordingly. The configuration of the control model 4.8 MODIFYING ELEMENTS IN THE SCL FILE It is possible to modify a LN type with a right click on the LN instance and selecting Edit template. The same window as shown in Pre-configuring a value for instance data image appears an allows to edit the LN and the DO Types as explained earlier. Editing the LN type. Additional modifications of the file are possible using the basic editing functionality explained in the next section. 4.9 BASIC EDITING OF THE FILE Besides the wizards described so far, SCL Navigator supports a basic editing of the SCL file within the constraints of the SCL schema: - Values for XML attributes can be changed by clicking into the value and overwriting it. (figure below) - When an element is selected, attributes that are not yet in the file are shown in grey. By clicking between the quotes, it is possible to enter a value. - Deleting an element is possible with a right click. - With a right click on a selected element, it is possible to add child elements. SCL Navigator will provide a selection of elements that are allowed according to the schema. Changing values for XML attributes Adding a DO to a LN ty pe Creating a dataset By default, some features are not shown, because the Expert Mode is disabled. The expert mode, allows the user to have full control of the file. For example, the Expert Mode will grant the user the ability to create Child DO in a LN Template, which requires knowledge of IEC 61850. The normal mode is safer for user with less experience with SCL 61850. 4.10 PRE-CONFIGURATION OF DATASETS A dataset can be created with a right-click on the logical node and selecting Add DataSet. Figure 68. The dataset that has been created A pop-up window allows the selection of the elements to be included in the dataset. The attributes are grouped according the functional constraints. It is possible to show all FCs or only ST, MX, SR, SP and SG. Selecting the members of a dataset A name for the dataset shall be entered. 4.11 PRE-CONFIGURATION OF CONTROL BLOCKS Control blocks can be added in an equivalent way as datasets using a right click on the LN and selecting which Dataset the Control Block will be associated with. Since some of the controls can only be added to a LLN0, the selection of options will be different for a right click on a LN other than a LLN0. Adding elements to LLN0 For example, a Report Control element was added with these child elements and parameters pre-filled. Report Control Block element added by the menu To add a GOOSE control block, two options exist: Add GSEControl and Add GSEControl and GSE Communication Entry. With the first option, an empty GSEControl element is created. With the second option, a GSEControl element with some attribute values filled in as defaults as well as a GSE element in the communication section are created. The GSEControl element created with Add GSEControl and GSE Communication Entry The GSE element of the communication section added with Add GSEControl and GSE Communication Entry 4.12 SADDRESS IMPORT/EXPORT sAddress Import/Export allows to edit, import and export sAddress in xlsx format of any IED from file loaded into Navigator. User can launch sAddress Import/Export view by clicking menu item in Tools menu. sAddress Import/Export Initial view. All files which have been loaded into Navigator would populate under Selected File dropdown list. Edit or Export: To edit or export sAddress, user need to select IED on left hand side (From image bellow) which would populate list of sAddress to the table in right hand side. sAddress Import/Export Export or Edit view. Note: To save any edit operations on sAddress, User need to click on Save Change To SCL File button. To export sAddress into xlsx format user need to click on Export To Excel. sAddress Import/Export Save export file. Import: To import xlsx file, User need to click on Import button which would populate xlsx file contents into table on the right-hand side. Save Changes To SCL File would trigger save imported contents into selected file. If the imported IED is not present in Selected file user would get an alert message as shown in Figure Below. sAddress Import/Export Error import file. 

**********DTM Users Guide************
 logo TRIANGLE MICROWORKS, INC. Distributed Test Manager (DTM) Users Guide Version 1.6.0 May 2023 Property of Triangle MicroWorks, Inc. This documentation and its associated distribution media contain proprietary information of Triangle MicroWorks, Inc. and may not be copied or distributed in any form without the written permission of Triangle MicroWorks, Inc. 2000 - 2023 Triangle MicroWorks, Inc. All rights reserved. READ THIS BEFORE INSTALLING THE SOFTWARE AND BEFORE ACCEPTING THE LICENSE AGREEMENT THIS SOFTWARE LICENSE AGREEMENT IS BEING PROVIDED TO YOU AS PART OF A TRIAL FOR REVIEW BEFORE PURCHASE. THE FOLLOWING LICENSE AGREEMENT IS A CLICKTHROUGH LICENSE AGREEMENT THAT IS SHOWN DURING THE INSTALLATION OF THE SOFTWARE. YOU MUST AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT IN ORDER TO INSTALL THE SOFTWARE. THEREFORE, IF YOU DO NOT AGREE TO ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT, DO NOT INSTALL THE SOFTWARE. TRIANGLE MICROWORKS, INC. License Agreement and Warranty THIS IS A LEGAL AGREEMENT BETWEEN YOU (YOU) AND TRIANGLE MICROWORKS, INC. (TRIANGLE MICROWORKS). TRIANGLE MICROWORKS IS WILLING TO LICENSE THE TRIANGLE MICROWORKS SOFTWARE AND RELATED DOCUMENTATION PURCHASED BY YOU AND KNOWN AS DISTRIBUTED TEST MANAGER (THE SOFTWARE) TO YOU ONLY UPON THE CONDITION THAT YOU ACCEPT ALL OF THE TERMS CONTAINED IN THIS LICENSE AGREEMENT (THE "AGREEMENT"). PLEASE READ THE TERMS AND CONDITIONS CAREFULLY BEFORE CLICKING ON THE I ACCEPT THE LICENSE AGREEMENT BUTTON, AS CLICKING ON THIS BUTTON WILL INDICATE YOUR ASSENT TO THEM. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, THEN CLICK ON THE CANCEL BUTTON. IF YOU DO NOT AGREE TO THE BELOW TERMS AND CONDITIONS, TRIANGLE MICROWORKS IS UNWILLING TO LICENSE THE SOFTWARE TO YOU. 1. Subject to the following terms and conditions, Triangle MicroWorks grants to you a nonexclusive, nontransferable limited license to install and use the Software solely for your internal business purposes and subject to the limitations in this Agreement. There are three mechanisms to license the Software. The Seat License is a PC based software license that is installed on a single computer and is solely for the use of the Software by you on a single computer at any one time. The Hardware Dongle License is a license that is stored on a dedicated USB hardware device that can be shared between computers by physically moving the Hardware Dongle License. The Hardware Dongle License is solely for the use of the Software by you on a single computer at any one time. The Network License is a license that is stored on a computer that can be shared across a local area network. The Network License is solely for the use of the Software by you on a single computer at any one time. The Network License may be installed on a corporate network and shared locally. The Network License cannot be shared outside of your company. You may physically transfer or copy the Software on to another computer; provided, however that each license instance may not be used on more than one computer or device at any given time. Subject to the foregoing limitation, you are entitled to copy the Software into any machine-readable or printed form for backup purposes in support of the use of the Software on a single computer by you. Any copy of the Software permitted by the preceding provision shall include a copyright notice of Triangle MicroWorks. 2. You may use the Software solely for the applicable term (i.e., period of time) as set forth below. If you have licensed the Software for a free trial period (a Trial License), the term of the Software license will be for 21 days. You understand and acknowledge that after the 21-day period and unless otherwise extended by Triangle MicroWorks, in its sole discretion, the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software. At the conclusion of the Trial License, you may purchase a Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have licensed the Software for a fixed period of time and not on a trial basis (a Term License), the term of the Software license will be the period of time agreed to by you and Triangle MicroWorks. At the end of that period of time, your use of the Software will terminate, and the Software will automatically deactivate and cease to operate. You agree that you are solely responsible for backing up all data used in connection with the Software and that Triangle MicroWorks will not be responsible to you for any damages, claims, loss or expenses whatsoever in connection with deactivation of the Software except as explicitly set forth herein. At the conclusion of the Term License, you may purchase an additional Term License or a Perpetual License, as agreed to by you and Triangle MicroWorks. If you have paid a one-time fee for a perpetual license, then your term of the Software License is perpetual (a Perpetual License) and subject to termination only as set forth herein. 3. You may not transfer the Software to another entity or person. You shall not modify, translate, reverse assemble or reverse compile in whole or in part the Software. You shall not rent, sell, time-share, lease, sublicense, transfer, copy, disclose, display or otherwise make available the Software or copies thereof to others, except as provided herein. Any breach of the above terms and conditions shall immediately terminate your license under this Agreement. You hereby certify and agree that the Software will not be shipped, transferred, or re-exported, directly or indirectly, into any country prohibited by the United States Export Administration Act and the regulations thereunder, and the Software will not be used for any purpose prohibited by the same. Neither you nor any of your subsidiaries, as applicable, or to your knowledge any of their directors, officers, or employees, is a person or entity that is directly or indirectly owned or controlled by any person, entity or country currently included on the List of Specially Designated Nationals and Blocked Persons or the Foreign Sanctions Evaders List maintained by the U.S. Treasury Departments OFAC, or is directly or indirectly owned or controlled by any person or entity that is located, organized, or resident in a country or territory that is, or whose government currently is, the target of countrywide sanctions imposed by any U.S. government Sanctions Authority. Neither you nor any of your subsidiaries, as applicable, or to your knowledge any of their directors, officers, or employees is a person or entity that is currently the subject of any Sanctions investigation, or is directly or indirectly owned or controlled by any person, entity or country that is currently the subject of a Sanctions investigation. 4. You may not erase, delete or modify any copyright, trademark or other proprietary notices included on the Software. You agree that if you infringe Triangle MicroWorks intellectual property rights or exceed the scope of permitted use of this license, Triangle MicroWorks will be irreparably injured and may obtain a court order to enjoin you from further use of the Software. 5. This license is not a sale. Title, ownership rights, and intellectual property rights in and to the Software shall remain in Triangle MicroWorks. You agree to abide by the copyright laws and all other applicable laws of the United States and international treaties. You acknowledge that the Software in source code form remains a confidential trade secret of Triangle MicroWorks. Failure to comply with the above restrictions will result in automatic termination of this Agreement and will make available to Triangle MicroWorks other legal remedies. 6. Notwithstanding anything herein to the contrary, you acknowledge and agree that Triangle MicroWorks may terminate your license to the Software if you have failed to comply with any of your obligations under this Agreement. You shall not be entitled to a refund in the event of termination by Triangle MicroWorks pursuant the foregoing sentence. 7. By purchasing a Term License or a Perpetual License to the Software you are automatically subscribed in the Software Maintenance & Enhancement Plan (the Plan) for a period of one (1) year following the date of purchase (the Purchase Date) of the license, or if shorter, the term of your Term Licensee (such initial period referred to as the Initial Maintenance Period). Upon expiration of the Initial Maintenance Period, the Plan may be renewed on an annual basis subject to payment by you of the applicable then-current annual fee. This Plan may provide a cost-effective method for upgrading to the most recent release version of the Software. The Plan also provides technical support through email and Triangle MicroWorks website. The Initial Maintenance Period for Perpetual Licenses is free. The annual fee for the Plan for Perpetual Licenses after the first year is 25% of the Perpetual License purchase price; provided, however that Triangle MicroWorks, in its sole discretion, may adjust the annual fee at any time. The annual fee for the Plan for Term Licenses depends on the specific terms of the Term License. Although Triangle MicroWorks has no obligation to provide upgrades to the Software under the Plan, Triangle MicroWorks may from time to time provide upgrades. Any upgrades to the Software shall be deemed to become part of the Software and shall be subject to the terms and conditions of this Agreement. All Plan annual fees are nonrefundable. 8. For Term Licenses and Perpetual Licenses only, Triangle MicroWorks warrants that the Software shall perform substantially as described in its documentation for a period of sixty (60) days from the Purchase Date. In the event of a breach of the foregoing warranty, the sole obligation of Triangle MicroWorks and your sole remedy is that Triangle MicroWorks will provide you with a refund of the most recent license fee paid for the Software. Triangle MicroWorks, as a condition to providing such refund, may require that you sign an affidavit certifying that you have deleted or destroyed all copies of the Software in your possession. Triangle MicroWorks makes no warranties with respect to Trial Licenses. Except as set forth in this Section 8, THE SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTIES OF ANY TYPE, AND TRIANGLE MICROWORKS HEREBY EXPRESSLY DISCLAIMS ALL WARRANTIES, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY, TITLE, NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE. 9. TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL TRIANGLE MICROWORKS BE LIABLE FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL OR PUNITIVE DAMAGES WHATSOEVER, INCLUDING ANY CLAIMS FOR LOST PROFITS, BUSINESS INTERRUPTION, LOST INFORMATION OR OTHER DAMAGES ARISING OUT OF THE USE OF INABILITY TO USE THE SOFTWARE. IN NO EVENT SHALL TRIANGLE MICROWORKS HAVE ANY LIABILITY FOR ANY DATA STORED OR PROCESSED WITH THIS SOFTWARE, INCLUDING THE COST OF RECOVERING ANY LOST DATA. IN ALL EVENTS, TRIANGLE MICROWORKS SOLE OBLIGATION OR LIABILITY UNDER THIS AGREEMENT IN THE AGGREGATE IS A REFUND OF THE MOST RECENT LICENSE FEE PAID FOR THE SOFTWARE. THE PARTIES AGREE THAT THESE LIMITATIONS ARE AN ESSENTIAL PART OF THE AGREEMENT AND THAT TRIANGLE MICROWORKS WOULD NOT ENTER INTO THE AGREEMENT WITHOUT THESE LIMITATIONS. THIS LIMITATION OF LIABILITY CLAUSE WILL SURVIVE FAILURE OF ITS ESSENTIAL PURPOSE. 10. You may terminate the license granted under this Agreement at any time by destroying or deleting all copies of the Software in your possession or control. In the event of termination of this Agreement for any reason, all the sections of this Agreement will survive except for Sections 1 and 2. 11. This Agreement shall be governed by and construed in accordance with the laws of the State of North Carolina, without regard to the choice of law provisions thereof. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to this Agreement. Any dispute or claim arising out of, or in connection with, this Agreement shall be finally settled by binding arbitration in Raleigh, North Carolina, in accordance with N.C. Gen. Stat. 1-569.1 et seq. (the Uniform Arbitration Act) and the then-current rules and procedures of the American Arbitration Association by one (1) arbitrator appointed by the American Arbitration Association. The arbitrator shall apply the law of the State of North Carolina, without reference to rules of conflict of law or statutory rules of arbitration, to the merits of any dispute or claim. Judgment on the award rendered by the arbitrator may be entered in any court of competent jurisdiction. The parties agree that, any provision of applicable law notwithstanding, they will not request, and the arbitrator shall have no authority to award, punitive or exemplary damages against any party. In the event that any arbitration, action or proceeding is brought in connection with this Agreement, the prevailing party shall be entitled to recover its costs and reasonable attorneys' fees. In the event that the above arbitration provision is held invalid or unenforceable, any dispute with respect to this Agreement shall be brought and heard either in the North Carolina state courts located in Raleigh, North Carolina or the federal district court, located in Raleigh, North Carolina. In such event, the parties to this Agreement each consent to the in personam jurisdiction and venue of such courts. The parties agree that service of process upon them in any such action may be made if delivered in person, by courier service, by fax, or by first class mail, and shall be deemed effectively given upon receipt. 12. This Agreement sets forth the entire agreement between you and Triangle MicroWorks pertaining to the licensing of the Software and supersedes in its entirety any and all written or oral agreements previously existing between the parties with respect to such subject matter. If any provision of this Agreement is held invalid or unenforceable, such provision shall be revised to the extent necessary to cure the invalidity or unenforceability, and the remainder of this Agreement shall continue in full force and effect. In the event of any conflict between any provision of this Agreement and any applicable law, the provision or provisions of this Agreement affected shall be modified to remove such conflict and permit compliance with such law and as so modified this Agreement shall continue in full force and effect. This Agreement may not be assigned without the consent of Triangle MicroWorks. Triangle MicroWorks, Inc. 2840 Plaza Place, Suite 205, Raleigh, North Carolina, 27612 USA (919) 870-5101 Table of Contents Introduction ................................................................................................................................................................... 15 Getting Started .............................................................................................................................................................. 16 Important Concepts .................................................................................................................................................... 17 Overview ................................................................................................................................................................ 17 The DTM Network.............................................................................................................................................. 17 3.2.1 DTM Master Computers ......................................................................................................................... 17 3.2.2 DTM Slave Computers ............................................................................................................................ 18 3.2.3 Adapters ....................................................................................................................................................... 18 3.2.4 Firewalls ....................................................................................................................................................... 18 3.2.5 Time Synchronization ............................................................................................................................. 18 3.2.5.1 NTP.......................................................................................................................................................................................................... 19 3.2.5.2 PTP .......................................................................................................................................................................................................... 19 3.2.5.3 GPS .......................................................................................................................................................................................................... 19 Programs and Services ..................................................................................................................................... 20 3.3.1 DTControl.exe ............................................................................................................................................ 20 3.3.2 DTMgr.exe .................................................................................................................................................... 20 3.3.3 DTHost.exe .................................................................................................................................................. 20 3.3.4 DTMAdmin.exe .......................................................................................................................................... 21 3.3.5 DTExec.exe .................................................................................................................................................. 21 Workspaces .......................................................................................................................................................... 21 3.4.1 Computers ................................................................................................................................................... 21 3.4.2 DTHosts ........................................................................................................................................................ 22 3.4.3 Folders .......................................................................................................................................................... 22 3.4.4 Resources ..................................................................................................................................................... 22 3.4.5 Devices .......................................................................................................................................................... 23 3.4.6 Channels ....................................................................................................................................................... 23 3.4.7 Sessions ........................................................................................................................................................ 23 3.4.8 Sectors ........................................................................................................................................................... 23 3.4.9 IED/Functional Simulation ................................................................................................................... 23 3.4.10 Auto data changes .................................................................................................................................... 23 InSight ..................................................................................................................................................................... 23 3.5.1 Scripts ............................................................................................................................................................ 23 3.5.2 Displays......................................................................................................................................................... 23 3.5.3 Flowcharts ................................................................................................................................................... 24 3.5.4 IEC 61131..................................................................................................................................................... 24 Architecture ................................................................................................................................................................... 25 DTM Network ...................................................................................................................................................... 25 DTM Licensing ..................................................................................................................................................... 26 DTM Applications ............................................................................................................................................... 26 4.3.1 DTM Client Applications ........................................................................................................................ 26 4.3.1.1 Distributed Test Manager Administration Tool (DTMAdmin.exe) .......................................................................... 26 4.3.1.2 DTExec (DTExec.exe) ..................................................................................................................................................................... 27 4.3.2 DTM Services .............................................................................................................................................. 27 4.3.2.1 DTControl ............................................................................................................................................................................................ 28 4.3.2.2 DTManager .......................................................................................................................................................................................... 28 4.3.2.3 DTHost................................................................................................................................................................................................... 28 User Interface ................................................................................................................................................................ 30 Menus and Toolbars .......................................................................................................................................... 30 5.1.1 File Menu ...................................................................................................................................................... 30 5.1.1.1 Load Workspace ............................................................................................................................................................................... 31 5.1.1.2 Restore Workspace ......................................................................................................................................................................... 31 5.1.1.3 Save Workspace ................................................................................................................................................................................ 31 5.1.1.4 Save Workspace As ......................................................................................................................................................................... 31 5.1.1.5 Save and Export Workspace ....................................................................................................................................................... 31 5.1.1.6 Import Workspace ........................................................................................................................................................................... 31 5.1.1.7 Import Substation ............................................................................................................................................................................ 31 5.1.1.8 Close Workspace .............................................................................................................................................................................. 31 5.1.1.9 Exit .......................................................................................................................................................................................................... 31 5.1.2 Tools Menu .................................................................................................................................................. 31 5.1.2.1 New Point List.................................................................................................................................................................................... 31 5.1.2.2 InSight Graphic Object Palette ................................................................................................................................................... 32 5.1.2.3 InSight Graphic Object Properties ........................................................................................................................................... 32 5.1.2.4 InSight Global Properties ............................................................................................................................................................. 32 5.1.2.5 JavaScript Editor/Debugger ....................................................................................................................................................... 32 5.1.2.6 Configure DTM Administration Tool .................................................................................................................................. 32 5.1.2.7 Configure DTM Services ............................................................................................................................................................ 34 5.1.2.8 Start DTM Services .......................................................................................................................................................................... 35 5.1.2.9 Stop DTM Services ........................................................................................................................................................................... 35 5.1.3 Views Menu ................................................................................................................................................. 35 5.1.3.1 Network ................................................................................................................................................................................................ 35 5.1.3.2 Resources ............................................................................................................................................................................................. 35 5.1.3.3 Workspace ........................................................................................................................................................................................... 35 5.1.3.4 Output .................................................................................................................................................................................................... 35 5.1.3.5 IEC 61850 / ICCP Reports ............................................................................................................................................................ 35 5.1.3.6 IEC 61850 GOOSE ............................................................................................................................................................................ 35 5.1.3.7 IEC 61850 Sampled Values .......................................................................................................................................................... 36 5.1.3.8 IEC 61850 Logs ................................................................................................................................................................................. 36 5.1.3.9 CSV State Collections ...................................................................................................................................................................... 36 5.1.3.10 DTM Event Log .................................................................................................................................................................................. 36 5.1.4 Windows Menu .......................................................................................................................................... 36 5.1.4.1 Layout .................................................................................................................................................................................................... 36 5.1.4.2 Show ....................................................................................................................................................................................................... 37 5.1.5 Help Menu .................................................................................................................................................... 37 5.1.5.1 Check for Updates ............................................................................................................................................................................ 37 5.1.5.2 Manage License ................................................................................................................................................................................. 37 5.1.5.3 Collect Log Files ................................................................................................................................................................................ 38 5.1.5.4 Show InSight Demo ......................................................................................................................................................................... 38 DTM Start Page .................................................................................................................................................... 39 Workspace View ................................................................................................................................................. 40 5.3.1 Node Selection ........................................................................................................................................... 41 5.3.2 Workspace Indicators ............................................................................................................................. 41 5.3.3 Workspace View Context Menu .......................................................................................................... 42 5.3.3.1 Manage .................................................................................................................................................................................................. 43 5.3.3.2 Show ....................................................................................................................................................................................................... 45 5.3.3.3 Commands ........................................................................................................................................................................................... 46 5.3.3.4 Simulate ................................................................................................................................................................................................ 46 5.3.3.5 InSight.................................................................................................................................................................................................... 47 5.3.3.6 Operations on InSight Scripts .................................................................................................................................................... 47 5.3.3.7 Operations on InSight Displays ................................................................................................................................................. 48 5.3.3.8 Operations on InSight Flowcharts ........................................................................................................................................... 48 5.3.3.9 Expand/Collapse All ....................................................................................................................................................................... 48 Resource View ..................................................................................................................................................... 48 5.4.1 Access Resource View ............................................................................................................................. 49 5.4.2 Resource View Context Menu .............................................................................................................. 49 5.4.2.1 Import Resource ............................................................................................................................................................................... 49 5.4.2.2 Export Resource ............................................................................................................................................................................... 49 5.4.2.3 Update Resource .............................................................................................................................................................................. 50 5.4.2.4 Monitor/Un-Monitor Resource for Changes ....................................................................................................................... 50 5.4.2.5 Delete Resource ................................................................................................................................................................................ 50 Network View ...................................................................................................................................................... 50 5.5.1 Network View Context Menu ............................................................................................................... 51 5.5.1.1 Manage .................................................................................................................................................................................................. 51 5.5.1.2 Show Statistics ................................................................................................................................................................................... 52 5.5.1.3 Expand/Collapse All ....................................................................................................................................................................... 52 Model Viewer ....................................................................................................................................................... 52 Point List Viewer ................................................................................................................................................ 54 DTM Event Log .................................................................................................................................................... 55 Protocol Analyzer ............................................................................................................................................... 56 Output ..................................................................................................................................................................... 57 Reports View ........................................................................................................................................................ 57 GOOSE View .......................................................................................................................................................... 58 Sampled Values View ........................................................................................................................................ 58 IEC 61850 Logs View ........................................................................................................................................ 60 CSV State Collections ........................................................................................................................................ 60 Turnkey Mode...................................................................................................................................................... 61 Functionality .................................................................................................................................................................. 63 Computers and Hosts ....................................................................................................................................... 63 6.1.1 Adding a Computer to the DTM Network ....................................................................................... 63 6.1.2 Removing a computer from the DTM Network ............................................................................ 63 6.1.3 IP Addresses and Ports .......................................................................................................................... 63 6.1.4 Host Management Mode ........................................................................................................................ 64 6.1.5 Adding a DTHost to the DTM Network ............................................................................................ 65 6.1.6 Starting DTHost(s) ................................................................................................................................... 65 6.1.7 Stopping DTHost(s) ................................................................................................................................. 65 6.1.8 Deleting DTHost(s) .................................................................................................................................. 66 6.1.9 Showing Statistics ..................................................................................................................................... 66 6.1.10 Move a Simulated Device from One Host/Computer to Another ......................................... 66 Workspaces .......................................................................................................................................................... 66 6.2.1 Layouts .......................................................................................................................................................... 66 6.2.2 Loading a Workspace .............................................................................................................................. 67 6.2.3 Saving a Workspace ................................................................................................................................. 68 6.2.4 Saving a New Workspace ...................................................................................................................... 68 6.2.5 Deleting a Saved Workspace ................................................................................................................ 69 6.2.6 Exporting a Workspace .......................................................................................................................... 69 6.2.7 Importing a Workspace.......................................................................................................................... 70 6.2.8 Restoring a Workspace .......................................................................................................................... 70 6.2.9 Closing the Current Workspace .......................................................................................................... 70 6.2.10 Sample Workspaces ................................................................................................................................. 70 Folders .................................................................................................................................................................... 71 6.3.1 Creating a New Folder ............................................................................................................................ 71 6.3.2 Editing an Existing Folder ..................................................................................................................... 72 6.3.3 Resetting the Contents of a Folder .................................................................................................... 72 6.3.4 Deleting an Existing Folder .................................................................................................................. 72 Resources .............................................................................................................................................................. 72 6.4.1 Importing Resource ................................................................................................................................. 73 6.4.2 Updating Resource ................................................................................................................................... 73 6.4.2.1 Manual Resource Update ............................................................................................................................................................. 73 6.4.2.2 Monitored Resource Update ....................................................................................................................................................... 73 6.4.3 Deleting Resource..................................................................................................................................... 73 IP Addressing ....................................................................................................................................................... 73 6.5.1 Creating IP addresses ............................................................................................................................. 74 6.5.2 Removal of DTM-created IP addresses ............................................................................................ 76 Creating and Managing Devices ................................................................................................................... 76 6.6.1 Adding Device ............................................................................................................................................ 77 6.6.1.1 Using a Configuration Wizard .................................................................................................................................................... 77 6.6.1.2 Building Individual Components .............................................................................................................................................. 78 6.6.2 Editing Existing Device/Channel/Session/Sector ...................................................................... 78 6.6.2.1 Single Device ...................................................................................................................................................................................... 79 6.6.2.2 Single Channel/Session/Sector ................................................................................................................................................. 79 6.6.2.3 Multiple Devices/Channels/Sessions/Sectors................................................................................................................... 79 6.6.3 Enabling/Disabling a Device/Channel ............................................................................................. 80 6.6.4 Deleting a Device/Channel/Session/Sector .................................................................................. 80 6.6.5 Managing DNP3 Devices ........................................................................................................................ 80 6.6.5.1 DNP3 Device Wizard ...................................................................................................................................................................... 80 6.6.5.2 DNP3 Channel Editor ..................................................................................................................................................................... 81 6.6.5.3 DNP3 Master Session Editor ....................................................................................................................................................... 82 6.6.5.4 DNP3 Outstation Session Editor ............................................................................................................................................... 83 6.6.5.5 DNP3 Dual End Point ..................................................................................................................................................................... 83 6.6.5.6 DNP3 over UDP ................................................................................................................................................................................. 84 6.6.5.7 DNP Data Types supported in DTM ........................................................................................................................................ 85 6.6.6 Managing Modbus Devices ................................................................................................................... 85 6.6.6.1 Modbus Device Wizard.................................................................................................................................................................. 85 6.6.6.2 Modbus Channel Editor................................................................................................................................................................. 86 6.6.6.3 Modbus Master Session Editor .................................................................................................................................................. 86 6.6.6.4 Modbus Slave Session Editor...................................................................................................................................................... 87 6.6.6.5 Modbus Data Types supported in DTM ................................................................................................................................. 87 6.6.7 Managing IEC 61850 Devices .............................................................................................................. 88 6.6.7.1 IEC 61850 Client Editor ................................................................................................................................................................ 88 6.6.7.2 IEC 61850 Server Editor ............................................................................................................................................................... 89 6.6.7.3 Import Substation ............................................................................................................................................................................ 90 6.6.7.4 IEC 61850 Data Types supported in DTM ............................................................................................................................ 92 6.6.8 Managing ICCP (TASE.2) Devices ...................................................................................................... 92 6.6.8.1 ICCP Wizard ........................................................................................................................................................................................ 92 6.6.8.2 Editing Object Model ...................................................................................................................................................................... 93 6.6.8.3 Saving or Exporting Object Model. .......................................................................................................................................... 94 6.6.8.4 ICCP Data Types supported in DTM ........................................................................................................................................ 94 6.6.9 Managing IEC 60870 Devices .............................................................................................................. 94 6.6.9.1 IEC 60870-5-101 Channel Editor ............................................................................................................................................. 94 6.6.9.2 IEC 60870-5-104 Channel Editor ............................................................................................................................................. 95 6.6.9.3 IEC 60870-5-101 Master/Slave Session Editor ................................................................................................................. 95 6.6.9.4 IEC 60870-5-104 Master/Slave Session Editor ................................................................................................................. 96 6.6.9.5 IEC 60870-5-101 Master/Slave Sector Editor ................................................................................................................... 96 6.6.9.6 IEC 60870-5-104 Master/Slave Sector Editor ................................................................................................................... 97 6.6.9.7 IEC 60870 Data Types supported in DTM ............................................................................................................................ 97 6.6.10 Managing Data Devices .......................................................................................................................... 98 6.6.10.1 DTM Data Device Editor ............................................................................................................................................................... 98 6.6.10.2 DTM Data Device Data Types ..................................................................................................................................................... 99 6.6.11 Editing Data Points .................................................................................................................................. 99 Viewing Data ..................................................................................................................................................... 100 6.7.1 Models ........................................................................................................................................................ 100 6.7.2 Point Lists ................................................................................................................................................. 100 6.7.3 InSight Displays and Flowcharts ..................................................................................................... 101 6.7.4 Reports (IEC 61850 and ICCP only)............................................................................................... 101 6.7.5 GOOSE messages (IEC 61850) ......................................................................................................... 101 6.7.6 Sampled Values (IEC 61850) ............................................................................................................ 101 6.7.7 IEC 61850 Logs ....................................................................................................................................... 101 IED Functional Behavior Simulation ....................................................................................................... 102 6.8.1 Creating a Substation ........................................................................................................................... 103 6.8.2 Functional Nodes ................................................................................................................................... 107 6.8.2.1 Mode/Behavior and Functional Nodes............................................................................................................................... 108 6.8.3 Simulated Data Points .......................................................................................................................... 108 6.8.4 Connecting Simulated Data Points in the Substation ............................................................. 110 6.8.5 Connecting Simulated Data Points to IEC 61850 Models...................................................... 112 Simulating Data ................................................................................................................................................ 115 6.9.1 Changing Data from the DTM User Interface ............................................................................. 115 6.9.2 Automatic Data Changes ..................................................................................................................... 115 6.9.2.1 Creating Automatic Data Change Nodes ............................................................................................................................ 115 6.9.2.2 Working with the Automatic Data Change nodes ......................................................................................................... 117 6.9.3 Changing Data via CSV Files .............................................................................................................. 117 6.9.3.1 CSV File Format .............................................................................................................................................................................. 118 6.9.3.2 Loading CSV Files .......................................................................................................................................................................... 120 6.9.3.3 Editing CSV File within DTM.................................................................................................................................................... 122 6.9.3.4 Validating CSV File ........................................................................................................................................................................ 122 6.9.3.5 Applying CSV File States ............................................................................................................................................................ 123 6.9.3.6 CSV State Collections ................................................................................................................................................................... 124 6.9.4 Manipulating Data Using InSight ..................................................................................................... 125 DTM Test Managers ....................................................................................................................................... 125 6.10.1 Creating a new Test Manager ........................................................................................................... 126 6.10.2 Test Manager Views ............................................................................................................................. 127 6.10.3 Running Tests ......................................................................................................................................... 129 6.10.4 Action Based Test Manager ............................................................................................................... 130 6.10.4.1 Default Test Actions ..................................................................................................................................................................... 132 6.10.4.2 Populating Test Action data ..................................................................................................................................................... 132 6.10.4.3 Access To Protocol Specific Test Actions ........................................................................................................................... 136 6.10.5 Scripted Test Manager ......................................................................................................................... 138 6.10.5.1 Implementing Tests ..................................................................................................................................................................... 138 6.10.5.2 Add User Feedback to a Test ................................................................................................................................................... 143 6.10.6 Gateway Testing Considerations ..................................................................................................... 145 Subscribing to Reports (IEC 61850 and ICCP) .................................................................................... 145 Configuring and Subscribing to GOOSE (IEC 61850) ....................................................................... 146 6.12.1 Publishing GOOSE .................................................................................................................................. 147 6.12.2 Subscribing to GOOSE .......................................................................................................................... 149 6.12.3 Simulated GOOSE Streams ................................................................................................................. 152 Subscribing to Sampled Values (IEC 61850) ....................................................................................... 153 Commands.......................................................................................................................................................... 154 6.14.1 Adding Commands ................................................................................................................................ 154 6.14.2 Command Execution and Scheduling ............................................................................................ 155 6.14.3 Common Protocol Specific Commands ......................................................................................... 156 6.14.3.1 DNP Commands ............................................................................................................................................................................. 156 6.14.3.2 Modbus Commands...................................................................................................................................................................... 157 6.14.3.3 IEC60870 Commands.................................................................................................................................................................. 157 6.14.3.4 IEC61850 Commands.................................................................................................................................................................. 159 6.14.3.5 ICCP (Tase.2) Commands.......................................................................................................................................................... 159 Controlling DTM from the Windows Command Line ....................................................................... 160 6.15.1 DTMAdmin Command Line Arguments ....................................................................................... 160 6.15.2 DTExec ....................................................................................................................................................... 160 6.15.2.1 DTExec command line arguments ........................................................................................................................................ 160 6.15.2.2 Examples of using DTExec commands with options ................................................................................................... 161 6.15.2.3 Examples of .bat files ................................................................................................................................................................... 162 InSight ............................................................................................................................................................................ 164 Paths ..................................................................................................................................................................... 164 Scripting .............................................................................................................................................................. 165 Data Type Formats Accepted from Scripts ........................................................................................... 166 7.3.1 DNP Formats ........................................................................................................................................... 166 7.3.2 Modbus Formats .................................................................................................................................... 167 7.3.3 IEC60870-5-101 and IEC60870-5-104 Formats ...................................................................... 167 7.3.4 IEC61850 Formats ................................................................................................................................ 169 7.3.5 ICCP Formats ........................................................................................................................................... 171 Flow Charting .................................................................................................................................................... 173 Custom Displays .............................................................................................................................................. 173 InSight Files ....................................................................................................................................................... 173 7.6.1 External Files ........................................................................................................................................... 173 7.6.2 Workspace Files ..................................................................................................................................... 173 IEC 61131 Application (Structured Text) ............................................................................................. 173 Troubleshooting ........................................................................................................................................................ 178 DTM Connection Issues ................................................................................................................................ 178 8.1.1 Firewall configuration ......................................................................................................................... 179 8.1.2 Adapter Configuration ......................................................................................................................... 179 8.1.3 Using DNS ................................................................................................................................................. 179 8.1.4 Disable Computer Power Save ......................................................................................................... 180 8.1.5 Resetting DTM ........................................................................................................................................ 180 8.1.6 Missing DTM Slave Computer........................................................................................................... 180 8.1.7 Disconnected DTM Slave Computer............................................................................................... 181 8.1.8 Disconnected DTHost .......................................................................................................................... 182 IEC 61850 Messaging Issues ....................................................................................................................... 182 8.2.1 Troubleshooting GOOSE Messaging ............................................................................................... 182 Appendices .................................................................................................................................................................. 184 Grid View Sorting ............................................................................................................................................ 184 Multi-Cell Editing ............................................................................................................................................. 184 Test Manager Actions .................................................................................................................................... 185 Introduction Thank you for your interest in the Triangle MicroWorks Distributed Test Manager (DTM), a tool which supports simulation of SCADA systems. These systems range from a few devices in a small substation to many devices in a large distribution network. Depending on the required tasks as well as the load, DTM can be configured to run on a single computer or distributed across a network. Tests can be configured from any computer on the network and left to run for extended periods of time (days or, perhaps, weeks), and then monitored and/or reconfigured from a different computer. Typical scenarios in which DTM is used include, but are not limited to: Load testing of a device or specific component. Validating and testing a communications infrastructure. Validating a substations configuration and interoperability. Personnel training The DTM Users Guide manual is organized in the following way: Section 2 lists references which can help the user get started with DTM. Section 3 provides a brief overview of some important topics and terminology used throughout the manual. Section 4 describes DTM architecture and the concept of operations. Section 5 describes the DTM user interface. Section 6 examines available functionality and provides detail on how to perform specific tasks. Section 7 provides details about the InSight capabilities, specifically with regards to how it is presented and used in the DTM1. 1 The DTM shares the InSight visualization and scripting functionality with other Triangle MicroWorks products. As mentioned above, Section 7 describes how this functionality is presented in DTM and how to use it. General information on this functionality can be found in the InSight Product Documentation available from the DTMs Help InSight Users Guide menu. Section 8 is a troubleshooting section which includes a few topics on what to do if one encounters connectivity issues. Getting Started Information required to install, configure, and get started using the DTM can be found in the following documents: For information on how to install, license, and configure the DTM see the DTM Installation and Configuration Guide. For information on how to perform several common tasks using the DTM see the DTM Quick Start Guide. Important Concepts This section provides an overview of DTM with respect to terminology, important concepts, and capabilities which will be referenced throughout this document. Most of these topics will be discussed in more detail in other chapters. However, some of the basic information described in here will help the user to understand and follow the rest of the manual. Overview Distributed systems frequently include many different components that must not only communicate with each other but also interact and respond to a variety of inputs and situations in a controlled and deterministic way. Thus, testing a distributed system represents a challenging endeavor. In a typical SCADA system, where downtime and any unexpected behavior are highly undesirable, validation and testing become critical. DTM is a distributed system made up of several client applications and services which support such testing by providing a framework for management of multiple simulated devices across a network. DTM does this by splitting the configuration, management, and simulation into DTM client and DTM services components which can be distributed as needed for a particular scenario. The configuration and monitoring of the DTM system are performed by a DTM client which is called the DTM Administration Tool (DTMAdmin.exe, or simply DTMAdmin). This is a Windows application that includes the main DTM user interface. The real time and persistent management of the overall system is performed by a DTM service called DTManager (DTMgr.exe). Simulations are contained in a DTM service called DTHost (DTHost.exe). A DTM service called DTControl (DTControl.exe) manages both DTManager and DTHost services on a given computer, starting and stopping them as needed. DTControl.exe is intended to be run as a Windows service but can be started and stopped manually if desired. The DTM Network All applications and services comprising the DTM system can be run on the same computer or spread across a Local Area Network, or LAN. The term DTM Network is used to describe all computers that are currently participating in the DTM system. A few important aspects of configuring the DTM Network which need to be considered are outlined below. 3.2.1 DTM Master Computers For every DTM Network, one computer must be configured as the DTM Master. If the entire DTM Network is to be run on a single computer, that computer should be configured as the DTM Master. If the DTM Network contains multiple computers, one of them is configured as the DTM Master and the rest as DTM Slaves. The DTM Master performs licensing, discovery, and management for the entire DTM system. As such, it would typically be the main server or a machine that will be available and stable most of the time. The DTManager service will typically be run on the DTM Master but this is not required. Also, the DTM Master may run one or more DTHost services, but again this is not required. 3.2.2 DTM Slave Computers It may be desirable to distribute the load (if simulating hundreds or, even, thousands of devices) or to test the actual network hardware which would require simulating the devices on different computers and, thus, simulating real network traffic. If additional computers are needed to participate in the DTM Network, they should all be configured as DTM Slaves. A DTM Slave computer runs additional DTHost services which are used to simulate devices. If being used to simulate real network traffic, the network adapter(s) of the DTM Slave computer will need to be configured as part of the simulated SCADA network. The simulated devices will be configured to use the appropriate adapters. 3.2.3 Adapters The DTM processes described in the installation guide and throughout this document must communicate with each other over the DTM Network/LAN. During installation of DTM on a given computer, the DTM Configuration dialog will be displayed that allows the user to select the default adapter, and address, used by the DTM processes to communicate. This adapter, or more specifically the address, must be the address that other machines on the network will use to communicate with the computer being configured. 3.2.4 Firewalls Independent of whether DTM is run on a single computer or spread across multiple computers it will be necessary to configure any firewalls to allow DTM traffic. The first step is to determine which firewall, if any, is running on the computer. If no firewall is active, no action is required. If the computer is running a firewall and it is active, it will need to be configured. Frequently, firewall configuration is as simple as allowing network access for the DTM services when they are first run on a given computer. If this does not work, refer to Section 8.1.1 which deals with firewall configuration issues. 3.2.5 Time Synchronization When working with multiple computers or devices it is important to consider time synchronization across all devices. Information collected and distributed between devices will frequently be tagged with the time it was acquired. In order to properly utilize this information, it is important that the times are synchronized with each other. There are many ways to synchronize time across multiple devices and ultimately the decision on which to use will be based on the situation. This section will attempt to introduce the most common techniques. 3.2.5.1 NTP The Network Time Protocol, or NTP, is the most common time synchronization mechanism in use today. NTP is commonly used to synchronize the clocks of computers within a local network and over the internet. NTP can usually maintain time to within tens of milliseconds over the public Internet, and can achieve better than one millisecond accuracy in local area networks under ideal conditions. NTP utilizes one or more servers, available on the network, and NTP client software on the device to be synchronized. All versions of Windows include NTP client software and all Windows Servers include NTP server software and can optionally act as NTP servers for the local network. If the public internet is available from all the computers in the network, and the 10- millisecond accuracy is sufficient, simply configure each computer to synchronize with one of the many available NTP servers available on the internet. This can be accomplished from the Windows Control Panel or the Window Clock display on the toolbar. If 10 millisecond accuracy is not adequate, or the public internet is not available, configure one of the computers in the network as an NTP server. This is done using the Windows Control Panel and is slightly different depending on the version of Windows. Then configure all the other computers to synchronize to the local server similar the same way you would configure them to use an internet time server. 3.2.5.2 PTP The Precision Time Protocol, or PTP, can achieve clock accuracy in the sub-microsecond range on local networks. Note that Windows is not a real time system and does not really require the accuracies of PTP. The main motivation for synchronizing Windows computers with PTP is to leverage existing infrastructure and/or synchronize with other real time devices on the network. A network using PTP to perform time synchronization will have a PTP reference (called the PTP grandmaster) which is usually a hardware device of some type. To allow Windows computers to synchronize to PTP time a PTP client application will need to be installed on configured. Installation and configuration of a PTP client is dependent on the specific client being used. Note that the most recent versions of Windows and Windows Server have built in support for PTP and NTP. Information on how to configure either of these protocols is widely available online. 3.2.5.3 GPS Another option to provide time synchronization is the Global Positioning System or GPS. The GPS system has a built-in clock signal accurate to 10 nanoseconds although most GPS receivers will only provide an accuracy of 100 nanoseconds to 1 microsecond. If a GPS receiver is available, the Windows clock can be synchronized to the GPS time. Exactly how this is accomplished is dependent on the software being used to read the GPS signal from the receiver. Programs and Services DTM is a distributed system that includes several components running on the DTM Network. Understanding their use in the system can help when developing and configuring various test scenarios. The main components are DTControl.exe, DTMgr.exe, DTHost.exe, and DTMAdmin.exe. DTM services (DTControl.exe, DTMgr.exe, and DTHost.exe) are typically managed by the DTM client (via DTMAdmin.exe) and do not require direct management. Another DTM client application (DTExec.exe) is a command line tool which is also a part of the DTM system. 3.3.1 DTControl.exe The DTControl.exe application manages licensing and discovery for the DTM system. In addition, it starts and stops DTMgr.exe (on the DTM Master computer) and DTHost.exe (on both the DTM Master and DTM Slave computers) as required. One instance of DTControl.exe must be running on each computer that will be part of the DTM network. This program will typically be run as a Windows service (configured during installation) but can also be invoked directly from the DTMAdmin.exe user interface or the Windows command line if desired. When the user configures DTM, one computer must be designated as the DTM Master, all other computers on the DTM network must be configured as DTM Slaves. As mentioned above, the instance of DTControl.exe running on the DTM Master will manage licensing and discovery. The instance of DTControl.exe running on a DTM Slave computer will coordinate with the DTControl on the Master and start and stop local services as required. 3.3.2 DTMgr.exe The DTMgr.exe application manages workspaces and coordinates all activities within the DTM Network. Only a single instance of this service needs to be running in the entire DTM Network and is usually started automatically by the DTControl service running on the DTM Master computer. 3.3.3 DTHost.exe The DTHost.exe application hosts simulated devices in DTM which runs in one of the three different host management modes: one-host-per-computer, one-host-per-device, and user-managed (for details, see section 6.1.4.). At least one DTHost application is required to run to instance any DTM simulated devices. If the host management mode allows, multiple copies of DTHost.exe can be run on the same computer or distributed across many computers on the DTM Network as required. If in the user-managed mode, in order to start simulating a device the user decides which DTHost that it should run on (based on the load, network connectivity, etc.) and, then, creates the device on that instance of DTHost. If in the one-host-per-computer or the one- host-per-device modes, the user decides which computer (each computer in DTM Network runs one copy of DTControl) that it should run on and, then, creates the device on that computer. In the latter case, the DTHost will be chosen automatically. 3.3.4 DTMAdmin.exe The DTMAdmin.exe application (Distributed Test Manager Administration) is the main user interface for the DTM system. It allows the user to create and manage simulated devices, load and run scripts or displays, and monitor the behavior of the system. The DTMAdmin.exe application is launched from the Windows Start Menu, or Command line. The DTMAdmin.exe application can be started and stopped at any time. When the DTMAdmin.exe application starts, it queries the DTMgr.exe service for the current state of the DTM Network. Note, that exiting the DTMAdmin.exe application does not automatically shut down any simulated devices or scripts unless specifically configured to do so. 3.3.5 DTExec.exe The DTExec.exe, a DTM client application, is a Windows command line tool which can be used to control DTM without DTMAdmin. The DTExec.exe process is described in detail in sections 4.3.1.2 and 6.15. Workspaces DTM manages its current configuration as a Workspace which can be modified, saved and restored, and exported to a zip file and imported from a zip file. Workspaces may include folders, simulated devices, scripts, displays, etc. When you save or export a workspace, DTM saves the current state of all components of the DTM system. Restoring a workspace will reset the DTM system to the state it was in when the workspace was last saved. Loading or importing a workspace will replace the current workspace with another desired one. See section 6.1 for detail. Sections 3.4.1 through 3.4.9 introduce the concepts of every major component which could be a part of a workspace. The components of a workspace are visualized by the Workspace View (section 5.2), Resource View (section 5.4), and Network View (section 5.5). Note, that some of the components could be visualized in only one of the three views mentioned, some in two or all three. 3.4.1 Computers When a workspace is saved/exported, DTM saves the information about the computers and the IP addresses employed by the workspace. If the user loads a workspace or imports a workspace from a zip file, some computers may not be available (missing) in the DTM Network at that moment. Although it is the users responsibility to find out why they are not available and take an appropriate corrective action, DTM will make a best effort to load the workspace and make it function. DTM will offer alternative computers currently available in the DTM Network during the workspace loading process. If the user skips selecting an alternative computer and lets the workspace load, he/she can still move the DTHosts/devices that were previously on the missing computer to one that is currently available and reconfigure them as required after the workspace load is complete. 3.4.2 DTHosts When loading a workspace, DTM will attempt to distribute the simulated devices and other resources the same way they were distributed when the workspace was saved. In order to accomplish this, various DTHost services will need to be available. If they are not, DTM will attempt to create/start them. In the user-managed DTHost management mode (for reference, see section 6.1.4), DTM can be configured to automatically start DTHost services when loading a workspace and shut down these services when closing a workspace. This will impact the workspace load time but removes the burden of having to manage the DTHosts by the user. If the automatic start/stop option is turned off, all missing DTHosts upon loading a workspace will not be started but corresponding icons will be displayed in the Network view where DTHosts can be started manually. In the one-host-per-computer or the one-host-per-device modes, DTHosts are started and stopped automatically. Depending on the Host Management Mode DTM is running in, simulated devices can be moved to another existing (and available) DTHost. The latter is similar to redistributing components of the workspace to different computers described in 3.4.1. More information on different Host Management Modes can be found in section 6.1.4. 3.4.3 Folders DTM folders are used in the Workspace tree browser to organize the current DTM configuration in a logical way. A folder is simply a container for other folders, files, devices, etc. The DTM Workspace is itself a specialized folder. When saving a workspace, all folders will be saved. When loading a workspace, all the folders will be restored. 3.4.4 Resources DTM resources are files which have been imported and are being managed by DTM. These files include configuration files, spreadsheet CSV files, InSight displays, script files, certificates, etc. Once imported into DTM, resource files can be shared and linked within the workspace as required. DTM will make sure the most recent version of each resource file is deployed to the various DTHosts as required. Such resources as spreadsheet (playback) CSV files, InSight displays, and Insight scripts can be a part. 3.4.5 Devices A DTM device is used to represent either a single physical or simulated device in the system. Note, a single simulated device can support multiple protocols by including different channels for each protocol. 3.4.6 Channels A DTM device can have one or more channels. A channel represents a physical (or logical) connection to the system. This could be a serial port, TCP Server, etc. 3.4.7 Sessions Some protocols allow multiple sessions on a single connection (i.e. channel). 3.4.8 Sectors Some protocols allow multiple sectors on a single session. 3.4.9 IED/Functional Simulation DTM 1.5 adds the ability to implement the functional behavior of a simulated IED using an approach based on the IEC 61850 object model. Note that this technique uses the IEC 61850 object model but will work with any of the supported protocols. This technique, described in more detail below, creates a substation topology based on the IEC 61850 object model. This topology can be created within DTM or imported from an IEC 61850 SCD file. 3.4.10 Auto data changes Automatic data change nodes are a special type of node allowing for predetermined data changes on the relevant database nodes (channel, session, sector depending on protocol). InSight InSight is a suite of functionality from Triangle MicroWorks, Inc. that supports programmability and display of information/data. 3.5.1 Scripts InSight supports scripting using the JavaScript language. Access to DTM functionality is provided via a set of JavaScript methods that are available to the InSight JavaScript engine. InSights scripting capability can be used to implement simulated devices, setup and manage tests, etc. 3.5.2 Displays InSight includes a graphical display capability that allows the user to build a variety of interactive displays that can be used to monitor and manipulate data in the system. Displays can range from simple button, dials, and gauges, to complete Single Line Diagrams for the system being tested. 3.5.3 Flowcharts As an alternative to traditional scripting, InSight provides a flowchart functionality that allows the user to specify operations to perform graphically as opposed to in a script. Again, this functionality can be used to setup and manage tests or simulate devices. 3.5.4 IEC 61131 DTM 1.5 adds limited IEC 61131 support to InSight. Specifically, DTM will begin support Structured Text which is one of the five languages specified in IEC 61131. This feature is an early access to allow users to experiment with it and provide feedback. Currently DTMs Structured Text interpreter does not implement the entire Structured Text specification. Future versions of DTM will add better Structured text support as well as additional IEC 61131 features. Architecture DTM is a system for simulating, testing, and validating the communications within a SCADA system. The SCADA system might range from a small substation with a few devices to a large distribution system with hundreds or thousands of devices. The purpose of using DTM is to run configuration validation, load testing, training, or a variety of other reasons. Attempting to simulate a system using tools designed to simulate/test a single device is cumbersome at best. Running a hundred or more instances of an application and attempting to coordinate the actions is not feasible. DTM allows the user to manage and coordinate the simulation of many devices from a single point. Given that SCADA systems are inherently distributed, a tool for simulating and validating them must be distributed as well. DTM is a distributed system made up of a several applications that can be configured to satisfy a wide range of requirements. The main components are shown in Figure 1. DTHost.exe DTMAdmin.exe DTControl.exe DTM Client DTM Serv ice DTM Communication Protocol Specific Communication DTMgr.exe DTHost.exe DTHost.exe Figure 1: DTM Architecture DTM Network DTM Network consists of all computers that are currently participating in the DTM system. Computers which are in the DTM Network need to run DTM applications (DTM clients, services, or both). The DTM applications use standard TCP sockets to communicate. All DTM services register at startup with the DTControl service running on the computer that has been configured as the DTM Master. DTControl manages a list of available services and how to connect to them. DTM clients connect to the DTControl service running on the DTM Master to determine what services are available and use that information to make the appropriate connections. DTM clients can query for the available services as well as subscribe to be notified when new services are available. DTM Licensing The DTM licensing is managed by the DTControl service running on the computer that is configured as the DTM Master. At startup, this instance of DTControl verifies that a valid DTM license exists, then reads the available options, number of connections (channels) which can be licensed, etc. Whenever a DTHost attempts to start a channel it first sends a request to DTControl to acquire a license for that connection (channel). When DTHost disables/deletes a licensed channel it sends a request to DTControl to release that license. Only the computer configured as the DTM Master needs to be licensed. Other DTM services will utilize the license and licensing options from there. DTM Applications As shown in Figure 1, the DTM components can be broken into two groups: clients and services. DTM clients include the DTMAdmin and DTExec applications. The DTM services are the DTControl.exe, DTMgr.exe, and DTHost.exe applications. These applications provide the core functionality for the DTM system. 4.3.1 DTM Client Applications The DTM clients are applications that connect to DTM services and are used to configure and manage the DTM system. The DTM clients include the DTM Administration Tool (DTMAdmin.exe) and the DTExec.exe command line tool. 4.3.1.1 Distributed Test Manager Administration Tool (DTMAdmin.exe) The Distributed Test Manager Administration Tool (DTMAdmin) is the main user interface for the DTM system. This application is a traditional Windows application that can be launched from the Start menu, Desktop, or Windows Explorer. When DTMAdmin starts up it attempts to connect to the DTControl on the computer configured as the DTM Master. If this fails it will display a dialog giving the user, the option to: Configure the local computer as the DTM Master and start DTControl (and DTManager etc.) on it. Specify which computer is the DTM Master and attempt to connect again. If the DTMAdmin is running on the DTM Master, the user has the option of configuring it to shut down the DTM services when DTMAdmin exits. By configuring DTMAdmin to start up and/or shut down the services DTM will behave more like a traditional application (i.e. it starts when the user launches it from the Start menu and shuts down when the user chooses to exit the application). Alternatively, the user can choose to run the DTM services as Windows services using the DTM Configuration tool during installation or launched from the Start menu. In this case, the DTM services, and any associated simulated devices, scripts, etc. continue to run independent of whether DTMAdmin is running. Once DTMAdmin successfully connects to DTControl it requests the existing services and then attempts to connect to DTManager and the DTHost services in the entire system. Once connected to each of these, it queries the services for its respective state and updates the DTMAdmin user interface accordingly. Once the state of the system has been retrieved and displayed, DTMAdmin allows the user to configure the system by sending requests to the appropriate services etc. DTMAdmin stays updated by subscribing for changes. Given the subscription nature of the updates, multiple copies of DTMAdmin are allowed within DTM Network. However, it needs to be noted that only one copy of DTMAdmin can be run on a given computer at the same time. Nevertheless, DTMAdmin copies on different computers always stay in sync. Also, note that changing configuration from multiple locations simultaneously is highly discouraged. Depending on how DTMAdmin is configured the DTM services will or will not remain running after DTMAdmin exits. If DTM services are left running, tests can be configured and then left to run for an extended time. When the user wants to check their status and/or reconfigure them, the user just needs to run DTMAdmin. 4.3.1.2 DTExec (DTExec.exe) DTExec is a command line application which can be launched from the Windows command line or another external application/tool and is used to invoke a script in the DTManager service. This can be used, for example, to control DTM via external applications and tools or from an external script (DOS Batch File, PowerShell, Python, etc.). In order for DTExec to run its commands, DTM Services must be running on the machine. The user has various options for starting DTM Services automatically or manually (via DTMAdmin.exe or DTMConfig.exe) which requires users input via graphical user interface, or simply launching DTControl.exe. For more information on available DTExec command options, as well as an example of a .bat, file see section 6.15 Controlling DTM from the Windows Command Line. 4.3.2 DTM Services The DTM services perform most of the functions within DTM. The services are designed to run all the time but can be configured to start and stop when the user runs DTMAdmin. DTControl, DTManager, and DTHost are considered as DTM services. 4.3.2.1 DTControl DTControl performs discovery, licensing, and starting and stopping other DTM services (DTManager and DTHost) as required. One instance of DTControl must run on each computer in DTM Network. Note that discovery and licensing are currently only done in the DTControl service running on the computer configured as the DTM Master. Also, DTManager can only be started on the computer configured as the DTM Master. Other computers in DTM Network, configured as DTM Slaves and running DTControl, can only start and/or stop DTHost services. 4.3.2.2 DTManager DTManager maintains the current DTM Workspace. All requests to update or reconfigure the workspace are sent to DTManager. DTManager then forwards the appropriate requests to all DTHost services. It also updates any other clients (DTMAdmins and DTExecs) that are currently connected and subscribed to DTManager. Note, that multiple DTM clients can connect simultaneously. For as long as DTManager is running, DTM clients can connect and disconnect. Yet, the DTM Network continues to simulate the current devices and run scripts etc. as configured in the current workspace. Stopping a client (including DTMAdmin) has no impact on the currently loaded workspace unless instructed/configured to do so. 4.3.2.3 DTHost The DTHost service runs all the simulated devices in DTM. Any DTM Network will have at least one DTHost service but can have any number. All DTHost services can run on a single computer or be distributed across a network. When DTHost starts, it first registers itself with the DTControl running on the DTM Master using the name provided by the user (the default name is DTHost). If a DTHost with that name already exists DTControl simply adds a number at the end to create a unique name (i.e. DTHost1, DTHost2, etc.). Once registered, DTHost waits for requests from other components of the system. Most requests will come from DTManager (as the result of some user action) but DTMAdmin will also query and subscribe directly to DTHost to get state changes and data. DTHost requests support the following actions on devices: Adding/remove devices and their associated channels, sessions, and sectors. Editing devices and their associated channels, sessions, and sectors. Managing resources needed by these devices. Handling asynchronous events returned from simulated devices. Configuring and running automatic data changes. Configuring and collecting statistics Running pre-configured data changes via CSV file playback. User Interface DTM user interface and its components are handled by Distributed Test Manager Administration Tool (DTMAdmin.exe) which is used to configure and manage the Distributed Test Manager. Figure 2 shows a snapshot of the user interface with one of the sample workspaces. Note, only a few of the available panes are shown in Figure 2. The user has a flexibility to open, close, and rearrange data panes based on ones needs and taste. Individual components of the user interface are discussed in the following sections. Figure 2: DTM User Interface Menus and Toolbars This section will go through the main menu and toolbar of the DTM user interface and briefly discuss each option. Details on functionality with respect to specific menu options are discussed in Section 6 Functionality. 5.1.1 File Menu The File menu performs actions related to saving and loading workspaces, importing and exporting workspaces as zip files, importing substation configuration, etc. 5.1.1.1 Load Workspace Loads a previously saved workspace. For more information on Workspaces see section 6.1 below. 5.1.1.2 Restore Workspace Restores an auto-recovery workspace. For more information on Workspaces see section 6.1 below. 5.1.1.3 Save Workspace Saves the current workspace. For more information on Workspaces see section 6.1 below. 5.1.1.4 Save Workspace As Saves the current workspace with a user supplied name. This can be used to create a new workspace or make a copy of an existing one. For more information on Workspaces see section 6.1 below. 5.1.1.5 Save and Export Workspace Saves and exports the current workspace to a standard .zip file. For more information on Workspaces see section 6.1 below. 5.1.1.6 Import Workspace Imports a workspace from a .zip file created using the Export Workspace option. For more information on Workspaces see section 6.1 below. 5.1.1.7 Import Substation Imports a substation from an IEC 61850 Substation Configuration File. For more information, see section 6.6.7.3 below. 5.1.1.8 Close Workspace Closes the current workspace, deleting all resources, folders and devices and creates a new empty workspace. For more information on Workspaces see section 6.1 below. 5.1.1.9 Exit Exits the Distributed Test Manager Administration (DTMAdmin) tool. Depending on the configuration settings, this may or may not shut down the DTM services on the computer which run DTMAdmin. For more information, see the section on Distributed Test Manager Settings. 5.1.2 Tools Menu 5.1.2.1 New Point List Displays a new point-based Point List. DTM Point Lists can be tied to a node in the DTM Workspace in which case all the data points owned by that node will be displayed in the list, or point based in which case only points specially added to the point list will be displayed. For more information see section 5.7 Point List Viewer below. 5.1.2.2 InSight Graphic Object Palette Displays the InSight Graphics Object Palette view. This view allows the user to create new graphic objects in an InSight Display. For more information, see the InSight Users Guide. 5.1.2.3 InSight Graphic Object Properties Displays the InSight Graphics Object Properties view. This view allows the user to edit properties of the selected InSight Graphic Object. For more information, see the InSight Users Guide. 5.1.2.4 InSight Global Properties Displays the InSight Global Properties view. This view allows the user to configure global InSight properties. For more information, see the InSight Users Guide. 5.1.2.5 JavaScript Editor/Debugger Open a new InSight script editor. For more information, see the InSight Users Guide. 5.1.2.6 Configure DTM Administration Tool This option displays the Distributed Test Manager Administration Tool (or, shortly DTMAdmin) Settings editor (shown in Figure 3) and allows the user to change DTMAdmin settings. Figure 3: Distributed Test Manager Administration Tool (DTMAdmin) Settings It is important to note that the DTMAdmin is a client to the DTM services and as such its configuration and settings are independent of the DTM Services configuration on the same machine. Typically, DTMAdmin will be configured to talk to the DTM Master computer on the DTM Network the computer is configured to be part of, but this is not required. DTMAdmin can talk to any DTM Master and hence any DTM Network desired, independent of the DTM services configuration on the same computer. Settings which can be changed/modified include: DTM Master Specify which DTM Master this DTMAdmin should attempt to connect to. This setting must point to a valid DTM Master, even if you are running the DTMAdmin on a DTM Slave computer. The DTM Master does not have to be the same DTM Master of the DTM Network the computer is configured to be part of (the computer may not even have the DTM Services installed). DTM Services As mentioned the DTM Services configuration is independent of the DTMAdmins configuration but DTMAdmin can be used to help manage the DTM Services on the local machine. For instance, the DTMAdmin can be configured to automatically start the DTM Services on the current machine if they are not already running whenever it starts. In addition, the DTMAdmin can be configured to stop the DTM services on the local computer when DTMAdmin is closed. Note that this will terminate all simulated devices and running tests on this computer. If you want the DTMAdmin to perform the specified actions whenever it starts without prompting uncheck the Show DTM Connection Dialog check box. Host Management Mode Three available modes (only one can be active at the same time) allow for different ways to distribute the simulation load in the DTM Network. General Settings General settings used to control the behavior of the DTM Administration Tool can be edited. Double Click Actions The double click action editor (shown in Figure 4) allows the user to specify what actions are performed in different views when the user double clicks with a left mouse button on a component in that view (for instance, in the Workspace view double clicking a command may execute the command or edit the command). To configure double click actions, choose the View first from the top of the dialog, then select an Action for each Target. When done, simply press Close. The chosen settings will be retained in DTM configuration settings. Figure 4: Double-click action settings dialog. Logging Sets logging options for DTM. If the Log To File checkbox is checked each DTM application will log to a designated subfolder in the specified Log File Folder. Errors and Warnings are always logged. Additional diagnostic information can be logged if desired using the Debug Level (the higher the level the more information will be logged). Log files are deleted automatically after the period of time passed set by the Delete log files after: time frame option. 5.1.2.7 Configure DTM Services Runs the DTM Configuration tool (DTMConfig.exe) to configure or reconfigure the DTM Services on the computer from which the DTMAdmin has been launched. For more information on configuration of DTM Services and the DTM Configuration tool see the DTM Installation Guide. 5.1.2.8 Start DTM Services Starts the DTM Services as configured on the computer from which the DTM Administration tool has been launched. If the computer is configured as a DTM Master all required services for the DTM Master will be started (DTManager, DTControl, and DTHost). If the computer is configured as a DTM Slave all DTM Slave services (DTControl and DTHost) will be started. Note that this does NOT impact the current DTMAdmin or DTM Services configuration. 5.1.2.9 Stop DTM Services Stops all the DTM Services on this computer. Note that this does NOT impact the current DTM Admin or DTM Services configuration. 5.1.3 Views Menu The Views menu allows the user improve experience with DTM navigation by showing or hiding various DTM panes (or, views). For example, one can show a view that has been previously closed or is covered up by other views. 5.1.3.1 Network Show/Hide the DTM Network view. When checked, this option shows the Network Browser pane. See Section 5.5 for specific information on Network Browser. 5.1.3.2 Resources Show/Hide the DTM Resources view. When checked, this option shows the Resources Browser pane. See Section 5.4 for specific information on Resources Browser. 5.1.3.3 Workspace Show/Hide the DTM Workspace view. When checked, this option shows the Workspace Browser pane. See Section 5.2 for details on the Workspace Browser. 5.1.3.4 Output Show/Hide the DTM Output view. For more information on the Output view, see section 5.10. 5.1.3.5 IEC 61850 / ICCP Reports Show/Hide the DTM Reports view. For more information on the Reports view, see section 5.11. 5.1.3.6 IEC 61850 GOOSE Show/Hide the DTM IEC 61850 GOOSE log view. For more information on the IEC 61850 GOOSE Log view, see section 5.12. 5.1.3.7 IEC 61850 Sampled Values Show/Hide the DTM Sampled Values view. For more information, see the IEC 61850 Sampled Values View section 5.13. 5.1.3.8 IEC 61850 Logs Show/Hide the DTM IEC 61850 Log view. For more information, see the IEC 61850 Log View section 5.14. 5.1.3.9 CSV State Collections Show/Hide the DTM CSV State Collections view. For more information, see the CSV State Collections View section 5.15. 5.1.3.10 DTM Event Log Show/Hide the DTM Log Viewer. For more information, see the Log Viewer section 5.8. 5.1.4 Windows Menu The Windows Menu keeps track of the various windows that have been opened in the DTM and can be used to set focus to a particular window view (point lists model views). 5.1.4.1 Layout A DTM Layout includes the current state and position of all the views in the DTM user interface. This includes tools such as the Workspace, Network, and Resource views as well as documents such as the Point Lists, Model Views, Logs, etc. 5.1.4.1.1 Save Layout Saves the current layout. Saved layouts can be restored later (Restore Layout) which allows user to return DTM to the current set of views and their state. 5.1.4.1.2 Restore Saved Layout Restores a previously saved layout (Save Layout). 5.1.4.1.3 Restore Workspace Layout Restores the default layout for the current workspace. The default layout is the set of views and their configuration when the workspace was saved. 5.1.4.1.4 Restore To Default Layout Resets the current DTM layout to the default layout. 5.1.4.1.5 Save Default Layout Saves the current DTM layout as the default system layout. 5.1.4.1.6 Reset To Factory Layout Resets the current DTM layout to the factory supplied default system layout. 5.1.4.2 Show The Show option displays a list of all DTM Windows. Selecting one of the Windows will make that window active and bring it to the front of all other DTM Windows. 5.1.5 Help Menu The Help menu provides access to DTM user guides and manuals, links to various web resources, checks for DTM software updates, gathers log files to send to Triangle Microworks, manages the DTM license, etc. Most of the Help Menu options simply open various documents or link to information on the web. Additionally, the following functionality is available from the Help Menu. 5.1.5.1 Check for Updates Checks for updates on the TMW web site and displays the status in the DTM Updates dialog shown below. Figure 5: DTM Updates Dialog 5.1.5.2 Manage License Displays the Manage License dialog as shown below and allows the user to update the current DTM license. Figure 6: Distributed Test Manager License Manager 5.1.5.3 Collect Log Files Collects the most recent log files from all the DTM services and creates a .zip file that can be uploaded to the TMW support website. This can be useful when attempting to diagnose issues. 5.1.5.4 Show InSight Demo Displays the InSight Demo dialog that can be used to navigate through a variety of InSight and DTM displays, videos, etc. Figure 7: InSight Demo Menu Window DTM Start Page When DTMs main window (DTMAdmin) launches, a DTM Start Page is always brought up to the users attention (see Figure 8). The Start Page is like other view panes in DTM. The Start Page can be used to easily access certain DTM-related features, documents, and resources such as loading or importing workspaces, manuals, technical support, instructional videos, and other useful information contained on the TMW website. Note that if, at any time, the Start Page is closed, it can always be shown back again by using the main window menu Views Start Page. To load an existing workspace or import a workspace from a file, use the Open section in the Start Page. The Recent section allows for a quick load of a favorite workspace or a workspace which has recently been opened. The Help Center section contains such important documents as the Users Manual and the Licensing Guide, the tutorial videos, and other various resources such as the DTM product page, the technical support page, etc. Figure 8: DTM Start Page Workspace View The Workspace View (an example is shown in Figure 9) displays the current DTM workspace. Workspaces can be structured using folders (for example, Master and Outstation in the figure). Folders can contain devices, files, commands, etc. The Workspace View is the main view for creating and editing the workspace, including adding, configuring, and removing devices, folders, files, auto data change nodes, scripts, etc. Figure 9: Workspace View 5.3.1 Node Selection To select a node in the Workspace View, simply click on a node. Multiple node selection is possible but only if the nodes are of the same type. There are a couple ways to select multiple nodes. The easiest is to simply hold the Ctrl key down while clicking on successive nodes. If the new node is of the same type as the previously selected node(s), the new node will be added to the selection. If not, it will deselect the previously selected node(s) and select the new node. A second way to select multiple nodes is to select any node in the tree and then hold the Shift key down while clicking on another node (this node does not necessarily have to be of the same type). All the visible nodes between the first and last nodes whose type matches the last node will be selected. For instance, right-click on the Workspace node and select Expand All to make all the nodes visible, select the Workspace node using the left mouse button. Scroll to the end of the workspace and click on a node while holding down the Shift key. All the nodes in the workspace that match the type of the last node clicked will be selected. 5.3.2 Workspace Indicators Nodes in the Workspace View display indicators that can provide information about the nodes status. Indicators are small icons displayed directly underneath the nodes name. Hovering on an indicator will provide additional information. The current set of indicators include: - Locked status indicator. This status indicator appears when the node is being edited. A lot of action items in the tree is disabled/locked until the node editor is closed. - Connected status indicator. This status indicator shows if the node is in the connected state. If the connection is broken, it will turn into Disconnected status indicator. - Disconnected status indicator. This status indicator shows if the node is in the disconnected state. If the connection is established, it will turn into Connected status indicator. - Disabled warning indicator. This warning indicator shows that the node is currently not in its normal operating state because it is not enabled. The warning will go away automatically if the condition that is causing the warning is resolved. - Error error indicator. This indicator appears if the node has one or more errors. Errors are added to the list as they occur and are only cleared by the user. Error indicators do not go away automatically even though the error condition may have been corrected. - Executing status indicator. This indicator shows that the corresponding node is currently in executing or active state. Scripts and CSV playback files will display this indicator. - Data Changing status indicator. This indicator will appear if the data in this node is being changed by DTM. This might be the result of a script, auto data changes, CSV file playback, etc. Indicators are displayed hierarchically. That is an indicator on a node in the Workspace View will be displayed on that nodes parents recursively. This way indicators will be visible even though the portion of the tree that contains the source of the indicator might be collapsed. 5.3.3 Workspace View Context Menu The Workspace View displays a wide variety of nodes in a tree. Nodes include folders, devices, channels, scripts, etc. The Workspace View supports a context sensitive menu that displays options appropriate for the node the user clicked on. To display the context menu, click on the desired node with the right mouse button. Context menus will update accordingly if multiple items are selected (i.e. options not available to be applied for multiple items will be disabled). The Workspace context menu options are listed here (only the appropriate options for the selected node(s) will be shown and/or enabled in the menu): 5.3.3.1 Manage The Manage submenu supports a variety of options for managing the state and content of the selected node. File-type nodes (scripts, CSV files, displays) as well as commands, auto data change nodes and test manager nodes have their own Manage context menus. The details can be found in section 6 where functionality of the mentioned nodes is discussed in detail. The subsections 5.3.3.1.1 - 5.3.3.1.13 describe options for the Manage submenu available on nodes that can be containers for other nodes (folders, devices, channels, sessions). 5.3.3.1.1 Add The Add submenu displays a list of nodes that can be added to the selected node. Items that may be added, depending on the selected node, include: Protocol specific master/outstation If the selected node is a folder or workspace the Add submenu will include options that display a create-device wizard tailored to configure a master/outstation/client/server for each supported protocol. Channel/Session/Sector If the selected node is a device, channel, or session the Add submenu will include the appropriate options for creating children of that node, i.e. channel, session, or sector, respectively. Substation Adds a Substation node (IED functional node simulation) to the workspace. Only one substation node is allowed in the workspace. For details, refer to section 6.8 IED Functional Behavior Simulation. Test Manager - Creates a DTM Test Manager which is used to manage and execute a list of tests. DTM supports two variations, the first based on pre- defined actions, the second uses JavaScript to manage and run the tests. For more information see section 6.10 below. Data Device (available on a folder level only) Adds a Data Device which is a standalone, protocol independent, pseudo-device which can be used as a data storage or support sharing variables between different parts of the system. Device (available on a folder level only) Creates a generic device with no channels. Channels can then be added independently via Add menu option. Folder (available on a folder level only) Creates a new folder as a child of the selected folder. For details, see section 6.6.1 as well as sections 6.6.5 through 6.6.9 or protocol specific information. Details on Data Devices are provided in section 6.6.9.7. 5.3.3.1.2 Edit (available on folder level only) The Edit menu, opened at a folder level, contains an intuitive filter and allows the user to edit all children nodes of the specified type (channels/session/sectors) using a multi-device editor. For more information on multi-device editor, see section 6.6.2.3 on editing multiple nodes in one grid. Note that editing nodes of different type is disallowed. 5.3.3.1.3 Edit Device/Channel/Session/Sector Allows the user to edit the selected node. The device editing options are limited to renaming the device and/or moving it to a different DTHost. However, many changes can be done to the channel/session/sector. For details, see section 6.6.2 for general common information as well as sections 6.6.5 through 6.6.9 for details on a specific protocol. 5.3.3.1.4 Configure Statistics A simple dialog with a list of available statistics will open. All statistics that can be collected on the node clicked as well as the nodes children will be seen in a list with a checkbox next to each item. Check/uncheck boxes to select/de-select specific statistics items. When done, click OK button. 5.3.3.1.5 Clear Statistics Clears statistics configured to be collected on given node and all its children. 5.3.3.1.6 Enable/Disable (All) Devices (available on folder level only) Performs the specified action (enable or disable) on all devices in the folder recursively. 5.3.3.1.7 Enable/Disable Device/Channel Disables the selected device and all its children. Disabling a device will free up any licenses allocated to that device. 5.3.3.1.8 Delete Folder/Device/Channel/Session/Sector Permanently deletes the selected node. 5.3.3.1.9 Edit Data Points (available on session or sector level only) For nodes that contain a simulated database (either sessions or sectors, depending on protocol) the Edit Data Points option allows the user to edit the points in the simulated database. 5.3.3.1.10 Reset Workspace/Folder (available on folder level only) Resets the entire workspace. Deletes all current workspace content and returns the workspace to its default (empty) state. 5.3.3.1.11 Import Node (available on folder level only) Imports a node (folder or device) from an XML file created via the Export option (see 5.3.3.1.12). When importing a node from the XML file, all naming restrictions will apply (i.e. duplicate names are not allowed). 5.3.3.1.12 Export Folder/Device (available on folder and device level only) Exports the current node to an XML file which can then be imported to another workspace. When a node is exported all folders, devices, files, and resource contained in that node are exported. Use this feature in conjunction with the Import Node feature described in 5.3.3.1.11. 5.3.3.1.13 Clear Errors Clears the error buffer. Correspondingly, all the errors indicated in the workspace tree with an icon will be cleared. 5.3.3.2 Show The Show submenu is used to open different views of the data. 5.3.3.2.1 Model Show the simulated device database as a hierarchical model. The Model View is limited to devices which support either IEC 61850 or ICCP protocol. See section 6.10 for more information. 5.3.3.2.2 Data Points Show the simulated device(s) database as a point list. If the selected node is a container (e.g., a Folder) the Point List will display the points for all compatible devices contained by the selected node. See section 5.7 for more information on the point list viewer. 5.3.3.2.3 Protocol Analyzer Show a channels low-level protocol traffic. Currently the Protocol Analyzer does not display a protocol trace for IEC 61850 devices. See the Protocol Analyzer section 5.9 for more information. 5.3.3.2.4 Statistics Show statistics which is currently configured to be collected on the clicked node and all its children. Depending on the clicked node, the statistics point list will contain items pertinent to the node level. Note, the statistics is shown using the same format as data points in a point list (see section 5.7 for more information on the point list viewer). To reconfigure the list of collected statistics, use option Configure Statistics described in section 5.3.3.1.4. 5.3.3.2.5 Browse Files On nodes which support browsing files on the remote device (currently IEC 61850 clients) this option displays the DTM File Browser dialog show below. This dialog allows the user to view, upload, download, and delete remote files. Figure 10 DTM File Browser Dialog 5.3.3.3 Commands The Commands submenu is used to add commands to the workspace. For more information on commands see section 6.14 below. Commands added to a folder can be applied to all compatible devices contained in the folder recursively. Commands added to a specific node (channel, session, or sector) will apply to that node. Clicking on a command in the Workspace view will allow the command to be executed, edited, or deleted. 5.3.3.4 Simulate The Simulate submenu configures various simulation tools and features within DTM. 5.3.3.4.1 Add Automatic Data Changes Adds an Automatic Data Change node. Automatic Data Change nodes are used to change data periodically on one or more devices. Like commands an automatic data change node in a folder will potentially be applied to any of the compatible children. An automatic data change node in a specific channel, session, or sector will apply to that node. For more information, see the section on Automatic Data Changes below. 5.3.3.4.2 Configure Sampled Value Generation Allows the user to configure Sampled Values Generation for IEC 61850 Servers that are configured to public sampled values. For more information, see the section on Sampled Values below. 5.3.3.4.3 Add CSV File Adds a CSV File node. DTM uses Comma Separated Value (CSV) files to change specific data points when requested by the user, periodically, or based on the acquisition times stored in the CSV file. As with commands and automatic data change nodes CSV files can be applied hierarchically from a folder or on individual nodes. For more information see the CSV File section 6.9.3 below. 5.3.3.5 InSight The InSight submenu allows you to create and manage InSight files (scripts, flowcharts, and displays) in the DTM workspace. For more information, see the InSight section below and the InSight Users Guide. 5.3.3.5.1 Add (Script, Flowchart, Display) Allows the user to add a new InSight script, flowchart, or display as a child of the selected node. For more information, see the InSight section below and the InSight Users Guide. 5.3.3.5.2 Start/Stop All Scripts Allows the user to conveniently start/stop all scripts in a folder (recursively). 5.3.3.6 Operations on InSight Scripts This section described menu options specific to InSight scripts. 5.3.3.6.1 Edit Script Edit the selected script using the InSight Script Editor. 5.3.3.6.2 Execute/Stop Script Starts or stops the execution of the selected script. The script will display an indicator that indicates the script is running. When the script terminates, either normally, abnormally, or stopped by the user the indicator will go away. Figure 11: Script Indicator Example 5.3.3.6.3 Edit Settings Edits script settings. For details see the Insight Scripts section below. 5.3.3.6.4 Delete Script Delete the selected script from the current workspace. 5.3.3.7 Operations on InSight Displays This section describes menu options specific to InSight displays. 5.3.3.7.1 Edit/View Display Opens the selected display for edit/viewing. 5.3.3.7.2 Delete Display Delete the selected display from the current workspace. 5.3.3.8 Operations on InSight Flowcharts This section describes menu options specific to InSight flowcharts. 5.3.3.8.1 Edit Flowchart Open the selected flowchart for edit/viewing. 5.3.3.8.2 Delete Flowchart Delete the selected flowchart from the current workspace. 5.3.3.9 Expand/Collapse All Expands/collapses the selected node and all its children recursively. Resource View DTM manages files that are part of the current workspace as Resources. The Resource View, shown in Figure 12, can be used to view and manage resources currently utilized by the DTM system. Figure 12: Resource View 5.4.1 Access Resource View The DTM Resources view, if closed, can always be accessed (re-opened) via the main application menu Views Resources. When checked, this menu option shows the Resources View pane. 5.4.2 Resource View Context Menu The Resource View supports a context sensitive menu that displays options appropriate for the node the user clicked on. To display the context menu, click on the desired node with the right mouse button. The Resource View does not currently support multiple node selection. The available options are listed below (only the appropriate options for the selected node will be shown in the menu): 5.4.2.1 Import Resource Allows the user to import a new resource into the workspace. Usually, resources are added via the various InSight as well as device configuration tools throughout DTM but occasionally it is desirable to add a resource directly that will be used in the future, or the user simply wants the file to be maintained (and possibly distributed) with the workspace. 5.4.2.2 Export Resource Export selected resource as a regular file in the local file system. This can be used to share the resource with other tools. An exported resource can be edited externally and then used to update other similar resources in the workspace (see section 5.4.2.3). 5.4.2.3 Update Resource A DTM resource file can be updated to match the change or a different file altogether. For example, this usually happens after an exported configuration file has been edited with an external third-party tool. Note that if the resource is being used by DTM it may sometimes be required to refresh components using that file and reload the workspace before the updated file will take effect. 5.4.2.4 Monitor/Un-Monitor Resource for Changes DTM resource files are often created from usual files on the users hard drive, or other media. If such an original file changes for any reason, sometimes it is important to incorporate that change into the DTM resource as well. If this feature is available from the context menu, the clicked-on resource can be monitored for changes. When this menu option is chosen, a standard open file windows dialog opens from which the user can pick a file which is to be monitored. Press Open the file selection is done. DTM will automatically detect changes done to this file (when these changes are saved), and then update the DTM resource being monitored. If automatic monitoring is no longer desired, the user may right-click on the resource and chose Un-Monitor Resource for Changes to cancel monitoring. 5.4.2.5 Delete Resource Deletes selected resource. Although resources are typically managed by DTM, in some cases the user may want to delete a resource directly. Network View As described above the DTM Network is made up of one or more computers one of which is the DTM Master, and the rest are DTM Slaves. Each computer will typically have at least one instance of the DTHost service but may have several. Any simulated devices must run on a DTHost. The Network View is used to facilitate the user to visualize the DTM Network. The Network View in the user-managed mode is shown in Figure 13. The Network View in the one-host-per-device and one-host-per-computer is shown in Figure 14. The context menus in the Network View may be accessed by right-clicking on a node and will contain pertinent menu items depending on the node clicked (see Section 5.5.1 for detail). Similar to the Workspace View, the Network View supports multiple selection of nodes of the same type. Figure 13: Network View in user managed mode Figure 14: Network View in one-host-per-device or one-host-per-computer modes 5.5.1 Network View Context Menu The Network View supports a context sensitive menu that displays options appropriate for the node the user clicked on. To display the context menu, click on the desired node with the right mouse button. The available options are listed below. Note that not all options will be available for all nodes. The available options may also change depending on the Host Management Mode (refer to section 6.1.4) DTMAdmin is running in. 5.5.1.1 Manage The Manage submenu supports some basic options for managing the state and content of the selected node. 5.5.1.1.1 Enable/Disable/Delete All Devices Performs the requested action on all devices recursively. 5.5.1.1.2 Start/Stop/Delete All Hosts Performs the requested action on all DTHosts recursively. 5.5.1.1.3 Add Host(s) Adds a new host on select computer. When this menu option is chosen, a dialog will open where the user will have an opportunity to enter a desired host name. Please note that if such a name already exists in the DTM system, the name will be automatically adjusted to make it unique. The Add Hosts(s) dialog also contains a numeric input which allows the user to start a few hosts simultaneously. The new hosts will be named using the input desired name appended with a number to ensure uniqueness. 5.5.1.1.4 Start Host(s) Start a DTHost or DTHosts that are not currently running. 5.5.1.1.5 Stop Host(s) Stop a DTHost or DTHosts that are currently running. Any simulated devices on each DTHost will stop but remain part of the workspace. 5.5.1.1.6 Delete Host(s) Delete a DTHost or DTHosts, terminating all pertinent service(s). Any simulated devices on each DTHost will stop but remain part of the workspace. 5.5.1.2 Show Statistics DTM gathers statistics for certain nodes in the workspace. The Show Statistics option displays the statistics for the selected node(s). If the selected node is a container (a DTHost, a computer, etc.) the Show Statistics option will show the statistics for all nodes that are children of the selected container. 5.5.1.3 Expand/Collapse All Expands/collapses the selected node and all its children recursively. Model Viewer The Model Viewer displays the data in a device as a hierarchical model. IEC 61850 and ICCP devices can be viewed using the Model Viewer. Models from multiple nodes can be displayed in one Model Viewer pane. To show multiple models in one Model Viewer pane, multi-select appropriate nodes in the Workspace Browser, then chose from the context menu Show Model(s). The Model Viewer pane (shown in Figure 15) consists of a few parts: Figure 15: Model Viewer Header The header of the model viewer pane can be customized by clicking on the icon in the top left corner of the pane as shown in Figure 16. A simple dialog which opens will allow the user change both the title and the tooltip information. Figure 16: Customizing Model Viewer pane header Search expander Use this feature to quickly locate the desired data attribute in the model(s). Bookmarks control Use this feature to help quickly navigate through the model. Some bookmarks (such as Data Sets and various Control Blocks) are predefined. The user also has an opportunity to add user-defined bookmarks. Model tree view The model is represented as a hierarchical tree whose leaf nodes can be expanded and collapsed as desired. Certain operations on the model nodes can be performed by right- clicking on the node and invoking a context menu. Point List Viewer The Point List Viewer displays the data in a device as a list of points. The point list viewer (shown in Figure 17) consists of a few parts: Figure 17: Point List Viewer Header The header of the point list viewer can be customized in the same way as the header of the model viewer. As such, see description of the header feature in section 6.10 for detail. Point list grid view Data points are shown in a spreadsheet (grid) format with one row corresponding to one data point. The columns in the grid can be dragged-n-dropped at the top level to group data points. Filters at the header of each column can be used to sort data within each column as well. Select View control Very often, the user will apply column filter and/or grouping. To save filtering and grouping use Select View feature and save the view(s) under desired name(s). Trash can Data points can be selected and dragged in the trash can. The points will be removed from the list. Note, this operation does NOT affect the points in the database, just the points currently being displayed. DTM Event Log The DTM Event Log (shown in Figure 18) is displayed by checking View DTM Event Log from the main DTM menu. Alternatively, one can press the Show DTM Event Log button in the upper right corner of the DTM Administration Tools user interface. The DTM Event Log displays information, warning, and error messages generated by the DTM system. The DTM Event Log supports filtering, grouping, and sorting as desired. Figure 18: DTM Event Log By default, the diagnostics is turned off as it mostly contains information useful to developers. If, for any reason, diagnostics is turned on, the user has an option to set the level at which diagnostics information is collected. The higher the level number, the more information will be logged. New messages in the DTM Event Log are shown highlighted. When the user clicks on a message, it indicates the user read (acknowledged) the message, and the highlight is turned off. The user has an option to acknowledge reading all messages at once by pressing the Mark All As Read button. Messages that have been read may be hidden by pressing Hide Read Events. To clear the DTM Event Log, one would press Clear Event Log. If, at any time, the user is in need to contact Triangle MicroWorks, Inc. for assistance and the event logs are of importance to resolve a problem, the DTM Event Log data can be packed into a zip file. For this, follow the menu option Help Collect Log Files. Event log information from DTMAdmin, DTManager, DTControl, and all available DTHosts will be collected. The user is referred to section 5.1.2.5 for more information on the event logging settings. Protocol Analyzer The DTM Protocol Analyzer displays diagnostic and low-level protocol traffic on the specified channel(s). As can be seen below the Protocol Analyzer can display General diagnostic information as well as protocol specific information in the same view. This helps understand the order of events throughout the system. Note that since DTM is a distributed system, entries in the protocol analyzer may arrive at slightly different times and will not always occur in time order. Low level protocol traffic is not currently available for IEC 61850 channels. Figure 19: Protocol Analyzer The information displayed in the Protocol Analyzer may be filtered using the Filter expander on the right-hand side of the analyzer pane. How information is displayed can be customized by the user by checking or unchecking appropriate check boxes and choosing a desired text color. Note that some of the filters are relevant for certain protocols only. Output The Output view (see Figure 20) displays an output log that can be used to monitor messages associated with major events or actions as well as errors or warnings that have occurred in the DTM system. Note, if one wants to save logs for later use and/or analysis, one is encouraged to have such a feature as Log To File from the Distributed Test Manager Administration Tools Settings editor enabled (see section 5.1.2.5). Figure 20: Output Log View Reports View The Reports View, shown in Figure 21, displays IEC 61850 and ICCP (Tase.2) reports. The reports can be sorted, grouped, and filtered as desired. To apply sorting, use the funnel icon available on each column as well as drag-n-drop feature at the top of the pane. The expander on the right allows the user to select which of the available columns are displayed. Any report can be expanded to show the actual content of the message, i.e., the data attributes with its values. For more information on IEC 61850 or ICCP reports, see the Section 6.11. Figure 21: Reports view. Relevant for IEC 61850 and ICCP protocols only. GOOSE View The IEC 61850 GOOSE View shown in Figure 22 displays selected IEC 61850 GOOSE messages. The expander on the right allows the user to select which of the available columns are displayed. Each data attribute received in a GOOSE message is displayed in its own row. The GOOSE messages can be sorted, grouped, and filtered as desired. To apply sorting, use the funnel icon available on each column as well as drag-n-drop feature at the top of the pane. For instance, grouping by Control Block and State Number as shown in Figure 22 will list a sequence of events for a specific GOOSE channel. For more information, see the section 6.12 on IEC 61850 GOOSE Messaging. Figure 22: GOOSE View. Relevant for IEC 61850 protocol only. Sampled Values View The Sampled Values View, shown in Figure 23, displays IEC 61850 Sampled Values. This view displays the data received by an IEC 61850 Sampled Values subscriber. The data can be displayed as analog signals, phasors, and/or in a tabular display. Figure 23: Sampled Values View. Relevant for IEC 61850 protocol only. The Sampled Values Settings editor is displayed in Figure 24 below. The user can show or hide the labels, analog signals, legend and/or phasor display. Fix Axes fixes the axis limits at their current value. This can improve performance since the extents do not have to be calculated for each frame. The Auto Trigger feature will sync the acquisition to a zero crossing which makes the display more readable. The user can choose to read from the sampled values stream when directed using the Get Buffer button, periodically by checking the Poll checkbox, or when an external trigger is detected. If polling is enabled the Polling Rate determines how often the sampled values stream is read. The Buffer Size determines how many samples are read. Figure 24: Sampled Values Settings. The External Trigger editor below allows the user to configure an external trigger to use to trigger an acquisition. Specify a data point that will trigger the acquisition when it changes. The number of samples before and after the trigger can also be configured. Figure 25: Sampled Values External Trigger Editor. For more information, see the section 6.13 on IEC 61850 Sampled Values. IEC 61850 Logs View The IEC 61850 Log view, shown in Figure 26, displays IEC 61850 log data. The expander on the right allows the user to select the logs to display. The selected logs can be queried manually using the Read Log(s) button or polled at the rate specified by checking the Poll Log(s) checkbox. Use the Clear List button to clear the IEC 61850 Log view. Figure 26: IEC 61850 Logs view. CSV State Collections The CSV State Collections view, shown in Figure 27, is a view which is often used in conjunction with the CSV file playback feature to change data attributes in a preconfigured way. The CSV State Collections view allows the user to manage data attribute states using different CSV files and apply them at the same time. For more information, see the sections 6.9.3 and, specifically, 6.9.3.6 on how to change data using CSV files and how to use CSV State Collections, respectively. Figure 27: CSV State Collections Turnkey Mode DTM supports a Turnkey mode which can be configured using the DTM Administrators Configuration dialog as shown below. Turnkey mode is entered/exited using the F11 function key in the DTM Administration Tools window. Turnkey mode can also be invoked at startup using the -turnkey command line option when starting the DTMAdmin.exe application. Figure 28 DTM Administration Settings The following options are available in Turnkey mode: Hide Application Title Bar Hides the bar along the top of the DTM Administration Tools main window, including the minimize/maximize and close buttons. Display Application Full Screen Forces the DTM Administration Tools main window to full screen, covering the Windows task bar. Keep Application On Top Of All Other Windows Forces the DTM Administration Tools main window on top of all other windows. The user will no longer be able to display any windows on top of the DTM Administration Tools window. Allow User To Move Views Within the Application Allows users to dock/undock/layout panes within the DTM Administration Tools window. Unlock Password Specifies an optional password that will have to be entered to leave Turnkey mode. Functionality This section describes functionality available in the DTM system and how to use it. Computers and Hosts In DTM, any simulated device is run in a service called DTHost which, in its turn, runs on a computer which is a part of the DTM Network. Any DTM Network can have several DTHosts spread across different computers. How the DTHosts are distributed depends on the users preference (usually driven by load distribution requirements) as well as the Host Management Mode (see section 6.1.4) configured by DTMAdmin. 6.1.1 Adding a Computer to the DTM Network Computers are added to the DTM Network by installing DTM on each PC and then configuring it to connect to the computer configured as the DTM Master. For more information, see the DTM Installation Guide. 6.1.2 Removing a computer from the DTM Network To remove a computer from the DTM Network, simply shut down all the DTM Services currently running on that computer. Note that stopping the DTControl process will stop all other DTM services. Any computer, which is part of DTM Network but no longer runs DTM Services, will be marked by the sign in the Network View (described in section 5.5). If DTM is no longer needed on the computer, one can optionally uninstall DTM application from that computer. For more information, see the DTM Installation Guide. 6.1.3 IP Addresses and Ports When configuring a simulated device, the user must select where the simulation will be running. This selection is sometimes dependent on the configuration options for that device such as IP Addresses, Serial Ports, or other system requirements. DTM will allow to choose an existing IP address as well as temporarily assign a desired IP address to an adapter of choice on a given computer on which the simulated device will run. For more information on configuring devices and assigning IP addresses refer to sections 6.5 and 6.6. DTM also allows use of special IP addresses such as the Loopback Address 127.0.0.1 (supported by most systems) and Any Address (the device can be configured to listen on address 0.0.0.0) which means it will accept connections for any address the computer supports. When a server is configured to listen on a specific address, any client/master needs to be configured to connect to that (server) specified address. Servers configured to listen on the Loopback Address will only accept connections from masters/clients on the same computer. On Windows, a network adapter can frequently be configured to support multiple IP Addresses. This is done by editing the properties of the specific network adapter in the Windows Network and Sharing Center. If configured, each IP Address will be available to the simulated servers running on that DTHost. This can be used to run multiple servers on the same computer listening on different IP Addresses. 6.1.4 Host Management Mode Three Host (DTHost) Management Modes are available to choose from: One-host-per-computer: simulated devices on a given computer must be run on one single DTHost. This mode generally minimizes the workspace load time since there is no requirement to start/stop extra hosts. One-host-per-device: each simulated device is run on its own individual DTHost. It takes longer to load workspaces when in this mode. In addition, the computer memory requirements are somewhat higher than for other host management modes. However, the simulation of every device is completely independent from others running DTHost services. User-managed: the user decides how to distribute the load of handling simulated devices across DTHosts, including creating and stopping individual DTHosts. This mode is the most flexible as far as the computer workload management is concerned. By default, DTM is set upon fresh installation to the one-host-per-device mode. To change the mode, by open the DTM Administration Settings dialog via the main application menu Tools Configure DTM Administration. Note, that both the one-host-per-computer and the one-host-per-device modes impose certain restrictions on how devices are distributed across DTM Network. Depending on the chosen Host Management Mode, the DTM Network View pane will either show or not show DTHosts (see Figure 13 and Figure 14). The Network View lists all computers currently in the DTM network (Eugene-PC and Eugene-HP in Figure 13 and Figure 14). In addition, the Network View in the user-managed mode lists each instance of DTHost running on a computer as a direct child of that computer. Each DTHost displays its current memory usage and CPU utilization. Please note that if DTM is installed as a 32-bit application, when the memory usage exceeds a safe level (set at 750 MB) the status turns yellow, if it exceeds 1GB it turns red. If DTM is installed as a 64-bit application, the memory usage is restricted by the physical memory of the computer on which the given DTHost is running. Under each DTHost, the Network View will list any device and/or resource (if configured to do so via DTM Administration Settings tool) that is currently deployed to/running in that DTHost. Which devices and resources are deployed to a DTHost is managed by the DTM system and/or specified when you create the resource or the device. Devices can be moved from DTHost to DTHost by dragging them in the Network View and dropping them on the desired DTHost. 6.1.5 Adding a DTHost to the DTM Network If DTM is configured to run in either the one-host-per-computer or one-host-per-device mode, adding DTHosts is done automatically without users input. Adding DTHosts manually is allowed only in the user-managed mode. See section 6.1.4 for more detail on Host Management Modes. If in the user-managed mode, in order to add a DTHost to a computer in the DTM Network simply click the right mouse button on the target computer and select the Manage Add Host menu option. Specify the desired name for the DTHost and optionally how many to create. Note: DTM will add an index to DTHost names to force them to be unique. Most networks have hosts named DTHost, DTHost1, DTHost2, etc. Note: The number of devices that can be reliably simulated in a single DTHost varies with protocol and configuration. A general rule is to create the desired number and watch the memory usage of the DTHost. 6.1.6 Starting DTHost(s) A DTHost service, which is not currently running, is indicated by an in the Network View. Under normal operation, this is usually caused by the user manually stopping the DTHost service. Other reasons include the service shutting down prematurely (crash), or a loaded workspace that included DTHost services which are not currently available, e.g. the computer on which the DTHost service was running when the workspace was created is off network. From the context menu in the Network browser, the menu option Manage Start All Hosts can be used to start all the hosts in the entire DTM system, or on a specific computer. Alternatively, the Manage Start Host could be used to start an individual DTHost service. 6.1.7 Stopping DTHost(s) To stop a DTHost, in the Network View; right-click the target DTHost and from the context menu, select Manage Stop Host. To stop all DTHosts in a given computer, right-click that computer and from the context menu select Manage Stop All Hosts. Note, stopping a DTHost is sometimes desirable when the user wants to leave a DTHost configured in the DTM Network, yet the user would like to free up any resources the DTHost is using. 6.1.8 Deleting DTHost(s) If a DTHost, or DTHosts, are no longer needed they can be deleted using the Manage Delete All Hosts or Manage Delete Host menu options. This option deletes the DTHost from the DTM network and shuts down the DTHost service. 6.1.9 Showing Statistics DTM manages statistics for all computers, DTHosts, and devices managed by the system. Statistics for the entire system, a specific computer, or a specific DTHost can be displayed using the Show Statistics menu option in the Network View. Which statistics to collect and show for individual node elements can be configured using Configure Statistics option found in the node context menus in the Workspace View. See 5.3.3.1.4 for more detail. 6.1.10 Move a Simulated Device from One Host/Computer to Another To move a simulated device from one DTHost/computer to another simply drag the device and drop it on the desired DTHost/computer. This will delete the device from the original DTHost/computer and add it to the destination DTHost/computer. Note: This may require reconfiguration of the device (like the IP address and/or port) based on the hardware resources available on the destination. Workspaces The current DTM configuration, including all devices, resources, scripts, etc. is maintained as a DTM Workspace. Workspaces are stored within the DTM system and can be saved and restored as desired. They can also be exported as a .zip file and imported to a different DTM system. In addition, the current workspace is periodically saved as an automatic backup which can be restored as needed. Workspaces are stored in the DTManager service as opposed to the local file system, so they are available anywhere on the DTM Network. If workspace sharing across different DTM systems is desirable, use the Workspace Import/Export functionality to create a .zip file. 6.2.1 Layouts The current location of all DTM panes and views is called a layout. The default DTM layout can be restored using the Windows Layout Restore Default Layout menu option. When a workspace is saved, the current layout is saved as that workspaces default layout. The default workspace layout can be restored using the Windows Layout Restore Workspace Layout menu option. Additional layouts can be saved in the workspace using the Windows Layout Save Layout menu option. This displays a dialog that allows the user to select a layout to overwrite or type in a new layout name. Note layouts can also be deleted from this dialog. Saved layouts can be restored using the Windows Layout Restore Saved Layout menu option and selecting the desired layout. 6.2.2 Loading a Workspace To load a workspace, it has to exist first. A few sample workspaces are provided upon a new installation of DTM and should be available for any user as at the start. To initiate the workspace loading process, select the File Load Workspace menu option or simply click the button on the toolbar. This displays the Select Workspace to Load dialog, shown in Figure 29. This dialog contains a folder/file browser in which the workspaces (i.e., workspace files) are organized within a customizable folder structure. Both DTM sample workspaces as well as customer-saved workspaces are shown. Note that the user can sort the content using the funnel icon in the corresponding column header. Select the desired workspace, then click OK to load that workspace. Figure 29: Select Workspace dialog. One exception to the workspace loading procedure is loading a new (empty) workspace for which the Select Workspace to Load dialog will not be shown. To load an empty workspace, simply press the button on the toolbar. This will clear all current workspace content (folder, devices, resource files) and load a New Workspace. 6.2.3 Saving a Workspace Workspaces are saved by the DTManager service. This makes them available to any member of the DTM Network. To save an existing workspace, select the File Save Workspace menu option or click the button on the toolbar. This will save the current workspace using the current workspace name. Note that if the current workspace name is not specified Save Workspace behaves exactly as the Save Workspace As menu option discussed below. 6.2.4 Saving a New Workspace To initiate the new workspace saving process, select the File Save Workspace As menu option. This displays the Save Workspace As dialog like the Select Workspace dialog in Figure 29. The Save Workspace As dialog (shown in Figure 30) allows the user to create and delete workspace browser folders as well as to create and delete workspaces. Figure 30: Save Workspace As dialog: creating new workspace browser folder To create a new workspace folder, invoke a context menu by right-clicking on any desired parent folder (see, for example, the Create New Folder menu option shown in Figure 30). When the Create New Folder option is selected, a field opens where the user can give the new folder a name. Pressing Enter key or simply clicking outside will finish creating a new folder. Select an existing workspace to overwrite it. If the user enters a new name, and optional description and keywords, a new workspace in the same browser folder will be created. Note that if the user selects a browser folder and enters a new workspace name, the workspace will be created into that folder. Finally, click Save to save the workspace. See Figure 31. Note, that if DTM Host Management Mode is set to either one-host-per-computer or one- host-per-device mode, creating and saving a workspace in any of these two modes may require DTM to be in the same Host Management Mode for this workspace to be properly loaded later. Figure 31: Save Workspace As dialog: selecting the workspace name 6.2.5 Deleting a Saved Workspace To delete a previously saved workspace, one would need to open either the Select Workspace to Load dialog or the Save Workspace As dialog (see sections 6.2.2 or 6.2.4). The user will have to invoke a context menu by right-clicking on a workspace and choose Delete Selected Workspace menu option. A confirmation dialog will open and, if acknowledged, delete the workspace. 6.2.6 Exporting a Workspace To export a workspace to a zip file that can be moved to a different DTM Network or shared with other users, select File Export Workspace menu option. This displays a standard Windows file browser that allows the user to select a new or existing .zip file and exports the current workspace to the selected file. This file can then be imported to any DTM system using the File Import Workspace option. 6.2.7 Importing a Workspace To import a previously exported workspace, select the File Import Workspace menu option. This displays a standard Windows file browser that allows the user to select a previously exported workspace and import it into DTM. 6.2.8 Restoring a Workspace DTM creates an Auto Recovery workspace every time the DTManager service is started and updates this periodically while the system is running. DTM saves the last ten Auto Recovery workspaces. The File Restore Workspace menu option displays the available recovery workspaces in the Select Workspace dialog and allows the user to load one of them to restore the system to the previous state. 6.2.9 Closing the Current Workspace To close the current workspace, deleting all folders and their contents, and setting the system back to its original state, select the File Close Workspace menu option or click the button on the toolbar. This will display a confirmation dialog shown in Figure 32 asking if you want to save the workspace before closing. Click Yes to save the workspace, click No to close without saving, or Cancel to cancel closing the current workspace. Uncheck the Always ask to save before closing checkbox to avoid this dialog from popping up in the future and always close the workspace without saving it. Figure 32: Save Current Workspace confirmation dialog. 6.2.10 Sample Workspaces Several sample workspaces are distributed with DTM. For additional information on any of the sample workspaces use the File Load Workspace menu option and select the desired workspace. A description of the workspace will be displayed in the Select Workspace dialog. Examples of the sample workspaces include: DNP3 Master and Outstation Creates a DNP3 Master and Outstation. IEC 60870-5-101 Master and Slave Creates an IEC 60870-5-101 Master and Slave. IEC 60870-5-104 Master and Slave Creates an IEC 60870-5-104 Master and Slave. IEC 61850 Client and Server Creates an IEC 61850 Client and Server. ICCP Unidirectional Client and Server Creates Unidirectional ICCP (TASE2) Client and Server. ICCP Bidirectional Client and Server Creates Bidirectional ICCP (TASE2) Client and Server. Modbus Master and Slave Creates a Modbus Master and Slave. Folders Folders are used to structure the DTM workspace both to make the workspace easier to navigate as well as to group similar functions. A DTM Workspace is a specialized folder and as such inherits a folders functionality. Folders can be created using the Workspace Browsers context menu. Folders can be nested inside other folders. Folders can contain devices, resources, commands, etc. In addition to partitioning a workspace for readability or ease of use a folder can be used to limit the scope of many DTM operations. For instance, creating a command in a folder automatically applies that command recursively to any compatible children of that folder. 6.3.1 Creating a New Folder To create a new folder, right-click over the desired parent folder in the Workspace Browser and select Manage Add Folder as shown on the top portion of Figure 33. This will display the Edit Folder dialog as shown at the bottom of Figure 33. Figure 33: Creating/Editing a new folder. To create a folder, one must at least enter a name. Note that folder names must be unique within the parent folder. If desired, a short description of the folder content and/or purpose can be added by typing in the info into the Description text box. Enter a unique name for the new folder and optional description, then click OK to create the folder. 6.3.2 Editing an Existing Folder To edit an existing folder, right-click over the folder to edit and select Manage Edit Folder. This will display the Folder Properties dialog as shown in Figure 33. Edit the folder properties, such as the name and description, then click OK to update the folder. 6.3.3 Resetting the Contents of a Folder Resetting a folder deletes all the contents of a folder and resets the folder back to its default state. To reset a folder, right-click over the folder to be reset and select Manage Reset Folder. This will display a confirmation dialog. If confirmed DTM will reset the selected folder. 6.3.4 Deleting an Existing Folder To delete an existing folder, right-click on the folder in the Workspace Browser and select Manage Delete Folder. This will display a confirmation dialog. If confirmed, DTM will delete the selected folder and all its content. Resources DTM manages files as resources and distributes them as necessary based on the current configuration. For example, if a configuration file is required for a particular device that file is automatically copied to the DTHost where the simulated device is running. Resources are shown in the DTM Resource View (refer to section 5.4). Resources are usually managed by other parts of the DTM user interface, but certain functions can be performed in the Resource View when required. 6.4.1 Importing Resource It may be desired to add files as resources to a workspace simply to make sure that file is maintained and easily accessible within the workspace. Alternatively, it may be desirable to add all the require resources at once. To import a resource, click the right mouse button on the Resources node and select the Import Resource menu option. This displays a file browser that allows the user to select a file to import into the current DTM Workspace. 6.4.2 Updating Resource Sometimes the original file, from which a DTM resource was imported from, changes. If this change is desired to be reflected in the resource as well, the DTM resource needs to be updated. For instance, if an external configuration tool is used to edit a configuration file the user needs to update that configuration files resource. 6.4.2.1 Manual Resource Update To update a resource, right-click over the desired resource and select the Update Resource menu option. Once updated the workspace should be saved and re-loaded to make sure all references are using the updated resource. 6.4.2.2 Monitored Resource Update Optionally, the user can initiate automatic monitored updates of resources. If this option is turned on, any saved change to a file which is being monitored automatically updates the resource. By default, this option is turned off. In order to monitor a resource, right-click over the desired resource, then chose the file which you wish to monitor. 6.4.3 Deleting Resource DTM attempts to manage resources without users interference. Deleting a resource manually is not required. In fact, the user should not delete resources manually unless he/she is sure DTM is not using them. Some resources, however, which were imported directly by the user and are no longer required can be deleted by right-clicking over the resource and selecting the Delete Resource menu option. IP Addressing As of the v1.4 release, DTM has the capability to temporarily reconfigure available adapters allowing to use IP addresses which currently do not exist. See section 6.5.1 on how to create IP addresses. Section 6.5.2 lists the ways to remove DTM-created IP addresses. Note, DTM currently supports adding only IPv4 addresses (32-bit decimal written as four octets in the dot-decimal notation). Important: For DTM to be able to handle IP addresses, DTM Services must run with administrative privileges. The user has two options how to do this: If DTM Services are launched via the Tools Configure DTM Services As Administrator, proceed to the Services tab, press Stop DTM Services to stop DTM Services running in regular mode, then press Start DTM Services to start DTM Services with windows administrative privileges. If DTM Services are launched using the DTMConfig application, right-click on the DTMConfig.exe in the Windows File Explorer, then choose Run as administrator. If an DTM Services applications are started individually, right-click on the corresponding .exe file (like DTControl.exe, DTManager.exe) in the Windows File Explorer, then choose Run as administrator. 6.5.1 Creating IP addresses To add an IP address to an existing adapter, the user must provide the IP address, the subnet mask, and the computer name. This is typically done when the devices are created or edited via protocol dependent configuration wizards (see sections 6.6.5 through 6.6.9 for a specific protocol of choice). An example of how to assign an IP address from the Modbus configuration wizard for a single device is shown in Figure 34. For other protocols, it will look similar in many respects. The Local Address property, besides giving the user the ability to pick an existing address from a drop box, allows to enter a new IP address (10.10.10.10 shown in red in the figure). After the Assign IP Addresses button is pressed the user will be asked for the subnet mask then prompted to the next stage, where he/she will see the dialog as the one shown in Figure 36. Note that an existing address, regardless of whether it was created by DTM or existed before, will be shown in green color. Non-existing addresses will be shown in red. Figure 34: An example of IP address assignment for single device. An example of the IP address assignment from the Modbus configuration wizard for multiple devices is shown in Figure 35 (similar for other protocols). The user can edit each individual Local IP cell by either picking an existing address from a drop-down menu or manually entering the desired IP address. Addresses which do not currently exist are shown in red. Select (multi-selection is allowed) the Local IP address cells for the addresses to be added, then press the Assign IP Addresses button. The Configure Network Adapters dialog ( Figure 36) will be shown unless there are any warnings regarding the configuration. Figure 35: An example of IP addresses assignment for multiple devices. Figure 36: Configure Network Adapters dialog. In the Configure Network Adapters dialog, only cells in the Internet Adapter column can be modified. The user can pick adapters via a drop box selection. No adapter will be modified until button Assign is pressed. Multiple cells can be edited via the Edit Selected Adapters box (see section 9.2). If the Subnet Restriction mode in the Configure Network Adapters dialog is set to Yes, then the grid will group all the IP addresses belonging to the same subnet. Then, after the Assign button is pressed, the chosen adapter will be configured with these addresses added to it. If the Subnet Restriction mode in the Configure Network Adapters dialog is set to No, then all the IP address entries will be shown in the grid individually. The dialog also allows to apply the same adapter to multiple entries. One can also monitor the existing and to-be-added addresses via the IP Addresses in DTM Network expander. 6.5.2 Removal of DTM-created IP addresses In a Workspace, when a device which uses a DTM-created IP address is disabled the IP address will be removed from the network adapter configured for this IP address. Re- enabling this device will restore the IP address on that adapter. When a device, which uses a DTM-created IP address, is deleted, DTM will automatically deleted the IP address. If this address is currently being used by any other device in the workspace, a warning will be shown. When a workspace is reset, all network adapters in the DTM network will be returned to their original state. The user should save the workspace before resetting if they desire to quickly restore these IP addresses when this workspace is next loaded. When the DTM application is exited, all network adapters in the DTM network will be returned to their original state. Creating and Managing Devices In DTM simulated devices are managed using the DTM Workspace Tree View. Some of the operations are common to all protocols and include adding, editing, and deleting devices, enabling, and disabling channels, etc. The following is the list of sub-sections with protocol specific operations related to creating and editing of devices: Section 6.6.5 Managing DNP3 Devices Section 6.6.5.5 - Managing Modbus Devices Section 6.6.7 - Managing IEC 61850 Devices Section 6.6.8 - Managing ICCP (Tase.2) Devices Section 6.6.9 - Managing IEC 60870 Devices 6.6.1 Adding Device There are two major ways (except the IEC 61850 protocol which has additional capabilities) to add devices to the DTM Workspace. The first way, and the easiest, is to use one of the protocol specific configuration wizards to define the device, channel, and session and sector (for some protocols) in one step. The second way is to create each of the components individually. 6.6.1.1 Using a Configuration Wizard To use the device wizard to create a complete device, right-click on the desired folder and select the Manage Add protocol Client/Server or Master/Outstation menu option. This will display a device configuration wizard which will allow user to configure the most important parameters such as names, IP Addresses, Ports, etc. For the details and variations for each protocol see the protocol specific sections 6.6.5 through 6.6.9. The wizards break out each component such as Device, Channel, Session (if relevant), and Sector (if relevant) of a device node as tabs which present the most frequently used attributes for each component. The tab content is tailored for the protocol chosen. The component specific tabs present the most frequently changed parameters for that component. The Advanced tab will display all configurable parameters in a property editor and allow the user to change any configurable parameter. For DNP3, Modbus, and IEC 60870 protocols, each wizard also includes a Database tab to allow the user to configure the simulated database. The Multiple Devices tab (an example is shown for the DNP3 protocol in Figure 37) attempts to make it easier to create many similarly configured devices at once. The configuration options from the other tabs are repeated for each device while varying only a couple critical parameters such as the device name and the Port number. The common features of the Multiple Device tab will be the Single/Multiple selectors, Available Properties, Load Configuration, Export Configuration buttons, and the actual device grid allowing for alterations to the individual device configuration parameters. The default value is for the Single/Multiple selectors is Single. If Multiple is chosen, the numeric dial will be enabled, and the user will have the ability to pick the number of devices to create at once. Available Properties click to open a simple dialog where the user can choose which properties to show in the grid. Load Configuration click to load a multiple device configuration from a CSV file. Use this feature in conjunction with the Export Configuration feature. Export Configuration click to export a multiple device configuration to a CSV file. One can edit the parameters in the CSV file using any external software. Use Export Configuration in conjunction with the Load Configuration feature. Figure 37: Example - Multiple Devices Tab. When creating multiple devices in the Multiple Devices tab, the user may change automatically generated parameters by simply clicking within the cell the value of which is to be changed and type in the desired value. The parameter values may also be configured via an external CSV file. This file must have the same spreadsheet structure as the grid representing parameters for the multiple devices to be created in the Multiple Devices tab. Usually, such a file is created using the Export Configuration feature. Press the Load Configuration to load desired configuration. 6.6.1.2 Building Individual Components The second, more flexible but more time consuming, way to create a device is to create the individual components. To start, right-click on the desired folder, where the device will reside, then select the Manage Add Device menu option. Create the device, then right-click on this device in the Workspace browser, then select Manage Add channel menu option to add a specific channel. In a similar fashion, continue with adding session and sector, if protocol-appropriate. For details see protocol specific sections below. 6.6.2 Editing Existing Device/Channel/Session/Sector Depending on whether the user chose to edit a single node or multiple nodes, there will be differences in how the editing is handled in DTM. Subsections 6.6.2.1 through 6.6.2.3 list possible options. Note that editing any component of a device will cause the entire Workspace tree branch to lock until editing is complete, at which point locking will be lifted and the device will restart. 6.6.2.1 Single Device To edit a single device, right-click on the desired device in the Workspace browser and chose Edit menu option. A simple dialog will open where the user will be able to change the name of the device and the DTHost on which the device is running. Note that editing a device is different from creating a device for which DTM uses protocol specific wizards. 6.6.2.2 Single Channel/Session/Sector To edit a component of a device, right-click over the desired component and choose the Edit menu option. Then use the editor to update that component. The editor which the user is going to use to edit a component will look exactly like the respective tab in the protocol specific wizard. For details, see protocol specific sections 6.6.5 through 6.6.9 below. 6.6.2.3 Multiple Devices/Channels/Sessions/Sectors To edit multiple components (components must be of same type) one needs to multi- select the components in the Workspace browser, as explained in section 5.3.1, then right-click and select the Edit menu option. Figure 38 shows an example of a multi- device editor which allows for the components to be edited in a grid. Figure 38: Multi-device editor. Only a few important basic parameters are shown as columns in the grid. In order to access more parameters available for editing, click on Available Properties, then check the properties desired to be viewed and edited in the multi-device editor grid. The user can change each parameter in a cell individually. The user also has an option to export the grid to a CSV file for editing using an external software. Press Export Configuration to export the grid to a file. Once edited, the CSV file can then be loaded back via Load Configuration. Multiple cells can be assigned the same value with just a few clicks. For this, multi-select multiple editable cells in the grid using the standard Windows shift-click technique. Then, select a desired new value in the Edit Selected Cells box and press the Apply Value to Selected Cells button. For more information refer to section 9.2. Note, Available Properties, Export Configuration, and Load Configuration are the same features used when multiple devices are created via Multiple Devices tab in the device creation wizard (see section 6.6.1.1). When editing is complete, press OK to apply changes. 6.6.3 Enabling/Disabling a Device/Channel An entire device or an individual channel can be enabled and disabled as needed. Enabled devices will attempt to connect and operate normally. Disabled devices or channels will not connect and remain idle. Disabling a device or channel allows the configuration to remain part of the workspace but not tie up resources. To Enable or Disable a device or channel, right-click on the desired node in the Workspace Browser and select the Enable or Disable Device/Channel menu option as appropriate. 6.6.4 Deleting a Device/Channel/Session/Sector To delete an existing device, channel, session, or sector simply click the right mouse button over the desired node and select the Manage Delete menu option. 6.6.5 Managing DNP3 Devices This section discusses configuration specific to DNP3 devices. For a list of DNP data types supported in DTM, refer to section 6.6.5.7. 6.6.5.1 DNP3 Device Wizard As mentioned above the easiest way to create a DNP3 Master or Outstation is to invoke the appropriate device wizard. Right-click on a folder where the device will reside, then chose Manage Add DNP3 Master/Outstation. A dialog similar to the one shown in Figure 39 will open. Figure 39: DNP3 Device Wizard. DNP3 outstation shown. The wizard has the following tabs: Device configure such attributes like the device name and the Computer/DTHost. Channel configure the most often changed DNP3 channel attributes. See information on the DNP3 Channel Editor in section 6.6.5.2. Session configure the most often changed session attributes. See information on the DNP3 Master/Outstation Session Editor in sections 6.6.5.3 and 6.6.5.4. Database define simulated database for DNP devices. The file(s) which are used for this purpose are previously created via the Export Database to CSV File feature. For more information, see section 6.6.11. Advanced configure all attributes using a property editor. The Advanced tab, will display all configurable parameters and allow the user to change any configurable parameter. Multiple Devices configure multiple devices at once. By default, the Device tab is shown first. Here, one can choose the name for the new device as well as the name of the DTHost the new device will be simulated on. Import DNP3 Device Profile button will allow user to import an xml file describing the DNP3 device profile. 6.6.5.2 DNP3 Channel Editor The DNP3 Channel Editor tab shown in Figure 40 allows the user to change the channel name, the behavior, the connection type, and connection properties. Figure 40: DNP3 Channel Editor. 6.6.5.3 DNP3 Master Session Editor The DNP3 Master Session Editor tab shown in Figure 41 allows the user to change the session name, the link layer addresses, enable and disable unsolicited reports by exception. Secure Authentication can also be set up in the tab. If Secure Authentication is enabled, user credentials can be updated via clicking the Edit Users button. Figure 41: DNP3 Master Session Editor. 6.6.5.4 DNP3 Outstation Session Editor The DNP3 Outstation Session Editor tab shown in Figure 42 allows the user to change the session name, the link layer addresses, enable and disable unsolicited messages. Secure Authentication can also be set up in the tab. If Secure Authentication is enabled, user credentials can be updated via clicking the Edit Users button. Figure 42: DNP3 Outstation Session Editor. 6.6.5.5 DNP3 Dual End Point DNP3 supports a dual end point configuration in which the master and outstation are both a TCP client and TCP server. Requests will be sent from the master to the outstation using the masters client and unsolicited responses will be sent from the outstation to the master using the outstations client. To configure a DNP3 Master to use dual end point use the Advanced tab of either the DNP3 Master Configuration dialog or the DNP3 Channel Editor and configure the following parameters on the channel: TCP Properties->TCP Mode to Dual. TCP Properties->Port to the port for this channel to listen on. TCP Properties->DualEndPointIpPort to the port the remote channel is listening on. To configure a DNP3 Outstation to use dual end point use the Advanced tab of either the DNP3 Outstation Configuration dialog or the DNP3 Channel Editor and configure the following parameters on the channel: TCP Properties->TCP Mode to Dual. TCP Properties->Port to the port for this channel to listen on. TCP Properties->DualEndPointIpPort to the port the remote channel is listening on. 6.6.5.6 DNP3 over UDP DNP3 over ethernet generally uses a TCP connection where the Master is a TCP client, and the outstation is a TCP server. Broadcast requests are sent over UDP via UDP broadcast messages. DNP3 can be configured to run entirely over UDP. To configure a DNP3 Master to run over UDP only use the Advanced tab of either the DNP3 Outstation Configuration dialog or the DNP3 Channel Editor and configure the following parameters on the channel: DNP3 Connection Type to UDP_TCP Network Type as UDP_ONLY TCP Properties->DestUDPPort to the UDP port on the outstation. TCP Properties->LocalUDPPort to the UDP port of this master. TCP Properties->Local IP to the master IP address. TCP Properties->Remote IPs to the IP address of the outstation. TCP Properties->TCP Mode to UDP To configure a DNP3 Outstation to run over UDP only use the Advanced tab of either the DNP3 Outstation Configuration dialog or the DNP3 Channel Editor and configure the following parameters on the channel: DNP3 Connection Type to UDP_TCP Network Type as UDP_ONLY TCP Properties->DestUDPPort to the UDP port on the master. TCP Properties->InitUnsolUDPPort to the UDP port on the master. TCP Properties->LocalUDPPort to the UDP port of this outstation. TCP Properties->Local IP to the outstation IP address. TCP Properties->Remote IPs to the IP address of the master. TCP Properties->TCP Mode to UDP Note that if both master and outstation are on the same computer the master UDP port and outstation UDP port must be different. 6.6.5.7 DNP Data Types supported in DTM [1] Binary Inputs, Abbreviation = "BI", ObjectId = 1 [3] Double Bit Inputs, Abbreviation = "DBL", ObjectId = 3 [10] Binary Output Statuses, Abbreviation = "BO", ObjectId = 10 [20] Running Counters, Abbreviation = "CNTR", ObjectId = 20 [21] Frozen Counters, Abbreviation = "FRZCNTR", ObjectId = 21 [30] Analog Inputs, Abbreviation = "AI", ObjectId = 30 [40] Analog Output Statuses, Abbreviation = "AO", ObjectId = 40 [110] String Data, Abbreviation = "STR", ObjectId = 110 [112] Virtual Terminal Output, Abbreviation = "VTERM", ObjectId = 112 [114] Extended String Data, Abbreviation = "ESTR", ObjectId = 114 [121] Secure Authentication Stat, Abbreviation = "SecAuth", ObjectId = 121 6.6.6 Managing Modbus Devices This section discusses configuration specific to Modbus devices. For a list of Modbus data types supported in DTM, refer to section 6.6.6.5. 6.6.6.1 Modbus Device Wizard To invoke a Modbus Master or Outstation device wizard, right-click on a folder where the device will reside, then chose Manage Add Modbus Master/Outstation. A dialog similar to the one shown in Figure 43 will open. Figure 43: Modbus Device Wizard. The wizard has the following tabs: Device configure the device name and the Computer/DTHost which this device will be simulated on. Channel configure the most often changed Modbus channel attributes. See information on the Modbus Channel Editor in section 6.6.6.2. Session configure the most often changed session attributes. See information on the Modbus Master/Outstation Session Editor in section 6.6.6.3 and 6.6.6.4. Database define simulated database for Modbus devices. The file(s) which are used for this purpose are previously created via the Export Database to CSV File feature. For more information, see section 6.6.11. Advanced configure all attributes using a property editor. The Advanced tab will display all configurable parameters and allow the user to change any configurable parameter. Multiple Devices configure multiple devices at once. By default, the Device tab is shown first. 6.6.6.2 Modbus Channel Editor The Modbus Channel Editor tab shown in Figure 44 will allow user to change the channel name, the behavior, the connection type, and connection properties. Figure 44: Modbus Channel Editor. 6.6.6.3 Modbus Master Session Editor The Modbus Channel Editor tab shown in Figure 45 allows the user to change the session name and the slave address. Figure 45: Modbus Master Session Editor. 6.6.6.4 Modbus Slave Session Editor The Modbus Channel Editor tab shown in Figure 46 allows the user to change the session name and the unit identifier. Figure 46: Modbus Slave Session Editor. 6.6.6.5 Modbus Data Types supported in DTM [0] Coils, Abbreviation = "COIL", ObjectId = 0 [1] Discrete Input Registers, Abbreviation = "DREG", ObjectId = 1 [3] Input Registers, Abbreviation = "IREG", ObjectId = 3 [4] Holding Registers, Abbreviation = "HREG", ObjectId = 4 [43] Device Identification Objects, Abbreviation = "DIR", ObjectId = 43 [100] 32 Bit Input Registers, Abbreviation = "IREG32", ObjectId = 100 [101] 32 Bit Holding Registers, Abbreviation = "HREG32", ObjectId = 101 6.6.7 Managing IEC 61850 Devices The 61850 Client and the 61850 Server device wizards are slightly different from device wizards for DNP3, IEC 60870, and Modbus protocols. The 61850 device wizards do not have a specific Device tab. However, both the DTHost and the device name can be chosen from either Server or Client tabs (depending on which device wizard is used). For more information, see sections 6.6.7.1 and 6.6.7.2. IEC 61850 devices can be configured in more than two ways indicated in section 6.6.1. Besides using a 61850 wizard and creating a device by component (i.e., creating an empty device first and then adding either a Client or a Server channel), one also has an option to import 61850 IEDs from an SCD file using a substation wizard. For more information, see section 6.6.7.3. 6.6.7.1 IEC 61850 Client Editor The IEC 61850 Client device wizard is shown in Figure 47. Figure 47: IEC 61850 Client Configuration To initialize the client, the user has two options: Discovery process: By checking the checkbox Use Discovery, the user acknowledges that the object model for the client will be adopted from the server the client connects to. The connection parameters are set in the Connection Properties group box and include the Server IP Address (or name), and the port number the server is set to listen on. Also, if discovery is used, the user is responsible for giving the device and the channel proper names. Using SCL file: If the Use Discovery checkbox is left unchecked, the user must provide a proper ICD file to initialize the client. Use the Select SCL File button (similar to the one shown in Figure 48 describing the IEC 61850 Server editor) to choose an ICD file. If using an SCL file an optional Read command can be configured on the device to read all critical information from the server after a connection is established. For example, Read all Control Blocks. This will read the desired information from the server after each connection. This will synchronize this information with the current state of the server. Note that future changes to the information on the server will not update the client model unless the change is configured for reporting or it is specifically read again. A lot of configurational adjustments can be made, if necessary, by making changes in the Advanced tab. Note, when choosing a GOOSE adapter, if needed, the same rules will be applied as described for the IEC 61850 servers described in section 6.6.7.2. If multiple clients need to be created, use Multiple Devices tab. From Multiple Devices tab the user can choose to either create multiple clients which would connect to a single server, or multiple clients which would connect to different servers. 6.6.7.2 IEC 61850 Server Editor The IEC 61850 Server device wizard is shown in Figure 48. Select an SCL file to initialize the important IED parameters of the IEC 61850 server. After that, the user can change such parameters as the channel name, local IP address, port number, etc. Additional adjustments can be made, if necessary, by making changes in the Advanced tab. If multiple servers need to be created, use Multiple Devices tab. Figure 48: IEC 61850 Server Configuration. Although duplicated in the Advanced tab, the Server tab has a drop-down to select an adapter for GOOSE communications. If None adapter is selected, DTM will not configure the created server to send or receive GOOSE messages. If an adapter is selected, DTM will use this adapter to send or receive GOOSE messages. If an adapter is not selected, or a previously selected adapter does not exist, DTM will try to use the default GOOSE adapter chosen when DTM was installed (note, the default GOOSE adapter can always be set later by running the DTMConfig.exe application). 6.6.7.3 Import Substation Clicking File Import Substation from the DTMAdmins main menu will display a file browser that allows the selection of an appropriate IEC 61850 SCL file (usually an SCD file). The Import Substation dialog will be displayed that lists all the IEDs found in the SCL file along with their Role, Type, Manufacturer, Default DTHost or Computer, IP Address, and Port. See Figure 49. The first column Importable is a read-only column where a check mark is indicated whether a given IED is available to be imported into the current workspace. The Import Substation dialog can be operated in one of three modes available: SCL file restricted, user-defined, and loopback. Switching between these modes will impose certain restrictions on the ability to customize/reconfigure individual IED parameters. The IP Addresses and Ports are initialized from the SCD file if the SCD file specifies them, otherwise they default to reasonable values. It is the users responsibility to ensure the specified IP Address and Port are available in the DTM Network. IEDs can be configured based on the available DTHosts and corresponding IP Addresses on the system. The host for an IED can be specified by selecting the Computer/Host cell for each device and using the drop-down menu to select the desired Computer/DTHost for that IED. Once the host is selected the IP Address can be set by selecting the IP Address cell and using the combo box to select the desired IP Address. Note that the IP Addresses in the combo box will be limited to those available on the specified Computer/DTHost. Finally enter the desired port number for each IED. Figure 49: Import Substation dialog. To configure multiple IEDs at once, the user can utilize the group-edit option (see Figure 50). For this, multi-select desired cells from the list, then enter a new value in the Edit Selected Cells box, then press Apply Value to Selected Cells. See section 9.2 for more detail. Figure 50: Substation Browser - editing multiple IEDs at once. Once the IEDs have been configured as desired they can be added to the workspace by dragging them off the Import Substation window and dropping them onto the desired folder in the Workspace View as shown in Figure 51. Figure 51: Substation Browser - dragging IEDs onto the Workspace. 6.6.7.4 IEC 61850 Data Types supported in DTM The data types supported in DTM for the IEC 61850 protocol are the data types defined by the IEC 61850 standard. 6.6.8 Managing ICCP (TASE.2) Devices ICCP (TASE.2) devices can be either unidirectional or bidirectional. In DTM, the unidirectional device is shown as a TASE2 channel containing either one Server or one Client session. The bidirectional device is represented by a TASE2 channel containing both Server and Client sessions. 6.6.8.1 ICCP Wizard The ICCP (TASE.2) configuration wizard has a few tabs. The default tab is shown in Figure 52. The user can choose the Computer/DTHost, the device name and the channel name, and decide on whether the device will be bidirectional acting as both server and client, or it will be either server or client. Depending on the Type chosen, the Server and Client configuration boxes will be enabled or disabled. Figure 52: ICCP device wizard. If the Server configuration box is enabled, the user can choose the name of the Server session and the object model file. If the object model file does not exist, one can check the Use Empty Model Template checkbox and build the model afterwards. Note, in order to make changes to the existing model, one would have to disable the channel first after it has been created. When creating a Server, the user also must specify the local IP address and the port number. If the Client configuration box is enabled, the user can choose the name of the Client and the object model file. If the object model file does not exist, the user may attempt discovery by checking the Use Discovery checkbox. When creating a Client, the user has to specify the server remote address and the port that server listens on. Also, if the Initiate On Start checkbox is checked, the Client will immediately attempt to connect to the server using parameters specified. If this checkbox is left unchecked, the user will have to manually connect the Client by using the Connect menu option from the clients context menu in the Workspace View. 6.6.8.2 Editing Object Model Whether an ICCP server was started using an empty model template or a user-provided object model xml file, DTM provides a way to make changes to the object model. For this, right-click on the ICCP server in the Workspace View and choose Show Model. Once the Model Viewer has opened, right-click on the ICCP server in the Workspace View and choose Manage Stop Listening and Disconnect. The user can now add or delete model nodes in the Model Viewer. For example, when Add New Node(s) option is chosen by right-clicking on a logical device node, the user will see a dialog which will allow to pick the type of the node, the name of the node, and (in case if multiples are desired) the quantity of the nodes and an optional append index. See Figure 53. A screenshot of a social media post Description automatically generated Figure 53: Making changes to the ICCP object model 6.6.8.3 Saving or Exporting Object Model. DTM offers the capability to Save or Export the object model. To access these options right-click on the ICCP sesson in the Workspace View and choose Manage Save Model or Manage Export Model. Saving the model will result in DTM updating its resource model file for the ICCP session for which the save model operation was requested. Exporting the model will result in the model to be exported to a system .xml file. 6.6.8.4 ICCP Data Types supported in DTM The data types supported in DTM for the ICCP (Tase.2) protocol are the data types defined by the ICCP standard. 6.6.9 Managing IEC 60870 Devices This section discusses configuration specific to IEC 60870 devices. For a list of IEC 60870 data types supported in DTM, refer to section 6.6.9.7. 6.6.9.1 IEC 60870-5-101 Channel Editor The IEC 60870-5-101 Channel Editor tab shown in Figure 54 allows the user to change the channel name, the behavior, the connection type, and other connection properties. Figure 54: IEC 60870-5-101 Channel Editor. 6.6.9.2 IEC 60870-5-104 Channel Editor The IEC 60870-5-104 Channel Editor tab shown in Figure 55 allows the user to change the channel name, the behavior, the connection type, and other connection properties. Figure 55: IEC 60870-5-104 Channel Editor. 6.6.9.3 IEC 60870-5-101 Master/Slave Session Editor The IEC 60870-5-101 Master Session Editor tab shown in Figure 56 allows the user to change the session name and a few connection parameters. Figure 56: IEC 60870-5-101 Master or Slave Session Editor. 6.6.9.4 IEC 60870-5-104 Master/Slave Session Editor The IEC 60870-5-104 Master Session Editor tab shown in Figure 57 allows the user to change the session name and one connection parameter. Figure 57: IEC 60870-5-104 Master or Slave Session Editor. 6.6.9.5 IEC 60870-5-101 Master/Slave Sector Editor The IEC 60870-5-101 Master/Slave Sector Editor tab shown in Figure 58 allows the user to change the sector name, the ASDU address, and (for slave only) the cyclic message interval. Secure Authentication can also be set up in the tab. If Secure Authentication is enabled, user credentials can be updated via clicking the Edit Users button. Figure 58: IEC 60870-5-101 Master or Slave Sector Editor. 6.6.9.6 IEC 60870-5-104 Master/Slave Sector Editor The IEC 60870-5-104 Master/Slave Sector Editor tab shown in Figure 59 allows the user to change the sector name, the ASDU address, and (for slave only) the cyclic message interval. Secure Authentication can also be set up in the tab. If Secure Authentication is enabled, user credentials can be updated via clicking the Edit Users button. Figure 59: IEC 60870-5-104 Master or Slave Sector Editor. 6.6.9.7 IEC 60870 Data Types supported in DTM [1] Single Point Information, Abbreviation = "MSPNA", ObjectId = 1 [3] Double Point Information, Abbreviation = "MDPNA", ObjectId = 3 [5] Step Position Information, Abbreviation = "MSTNA", ObjectId = 5 [7] Bitstring of 32 Bits, Abbreviation = "MBONA", ObjectId = 7 [9] Measured Value, Normalized Value, Abbreviation = "MMENA", ObjectId = 9 [11] Measured Value, Scaled Value, Abbreviation = "MMENB", ObjectId = 11 [13] Measured Value, Short Floating-Point Number, Abbreviation = "MMENC", ObjectId = 13 [15] Integrated Totals, Abbreviation = "MITNA", ObjectId = 15 [21] Measured Value, Normalized Value without Quality Descriptor, Abbreviation = "MMEND", ObjectId = 21 [41] Integrated Totals containing time-tagged security statistics, Abbreviation = "SITTC", ObjectId = 41 [110] Parameter of Measured Value, Normalized Value, Abbreviation = "MMETD", ObjectId = 110 [111] Parameter of Measured Value, Scaled Value, Abbreviation = "MMETE", ObjectId = 111 [112] Parameter of Measured Value, Short Floating Point Number, Abbreviation = "MMETF", ObjectId = 112 6.6.10 Managing Data Devices DTM supports a protocol independent device called a Data Device that stores and manages a list of data points. A Data Device can be used to share information between components in a DTM network and/or to provide temporary storage for data. Data Devices can be run on any DTHost in the system. For a list of IEC 60870 data types supported in DTM, refer to section 6.6.10.2. 6.6.10.1 DTM Data Device Editor To create a Data Device, click the right mouse button on the desired folder in the Workspace View and select Manage Add Data Device menu option. This will display the Data Device Editor. Select the Host you want the Data Device to run on and enter a unique name for the device. Configure the database using the Edit Database editor described in section 6.6.11. Then click OK to create the device. A screenshot of a social media post Description automatically generated Figure 60: DTM Data Device Editor 6.6.10.2 DTM Data Device Data Types Binary, Abbreviation = "BIN", ObjectId = 0 Double Bit, Abbreviation = "DBL", ObjectId = 1 Counter, Abbreviation = "CNTR", ObjectId = 2 Analog, Abbreviation = "ANLG", ObjectId = 3 String, Abbreviation = "STR", ObjectId = 4 6.6.11 Editing Data Points Note: This information in this section applies only to the DNP3, IEC 60870, and Modbus devices. Editing data points (attributes), via editing the object model, for the ICCP devices is described in section 6.6.8.2. Editing data points (attributes) for the IEC 61850 is not supported by DTM. Please contact Triangle Microworks, Inc. for information on other applications designed to edit IEC 61850 object models. The user can edit the data points for any device that manages their data points as a point list. To edit the point list, right-click on the point list container (channel, session, or sector depending on protocol) and select the Manage Edit Data Points menu option which opens a data point editor shown in Figure 61. Editing point lists is usually done when a device is created or changed. Figure 61: Database editor. Point database configuration is done by utilizing the following features in the editor window: Point list display. The display shows current data points. Points can be filtered, sorted, or grouped as desired. Properties tab. The tab displays the detailed properties for the selected data point. These properties can be edited as desired. Add Points panel. The panel allows the user to add one or more points of the specified type at the specified start index. Note that some protocols require contiguous point lists in which case the Start index will be read only. Reset Database button. Pressing the button will delete all the points in the current point list. Create Default Database button. Pressing the button will create a default database appropriate for the device. Delete Selected Data Points button. Pressing the button will delete all the data points currently selected in the point list. Import Data from CSV File button. Pressing the button will attempt to open a CSV file and read a point list from that file. Export Database to CSV File button. Pressing the button will export the current point list to a CSV file. Viewing Data The DTM includes several options to view data in simulated devices. Depending on requirements and protocol of choice, some options could be preferable than the others. 6.7.1 Models Model Viewer allows to view data as a standard model (for IEC 61850 and ICCP models only). To view the model (or models, if multiple nodes selected) using the model viewer, right- click on the node which is a model container (channel for IEC 61850, session for ICCP/Tase.2), then chose Show Model(s). For details on the Model Viewer, the user is referred to section Error! Reference source not found.. 6.7.2 Point Lists Points List Viewers allow to view data in a table. To show data as data points in a Point List Viewer, right-click on the node which is a container for the database or a model container, then chose Show Data Points. For details on the Point List Viewer, the user is referred to section 5.7. 6.7.3 InSight Displays and Flowcharts The InSight functionality allows to view data as custom displays and flowcharts. The user is referred to the InSight Users Guide for detail on how to use displays and flowcharts. 6.7.4 Reports (IEC 61850 and ICCP only) One of the major components of IEC 61850 and ICCP protocols is reporting. For both IEC 61850 and ICCP protocols, once reports are enabled (section 6.11) they can be viewed in DTMs Reports View (section 5.11) upon arrival. Reports can be filtered, grouped, and sorted using the column headers like other grids in DTM. 6.7.5 GOOSE messages (IEC 61850) Besides traditional MMS-based client/server communication, the IEC 61850 standard defines GOOSE messaging. GOOSE messaging is a broadcast technique that is implemented at the Ethernet layer (does not use the Internet Protocol (IP) as the underlying transport layer). It was designed to be more deterministic and faster than traditional IP based communication. GOOSE is typically used to provide peer to peer communication between IEC 61850 devices for protection and other time critical functions. How one can configure GOOSE messaging is described in section 6.11. In DTM, GOOSE messages received by any GOOSE subscriber (IEC 61850 Client or Server) can be displayed using the DTM GOOSE View. For more details on GOOSE View the user is referred to section 5.12. 6.7.6 Sampled Values (IEC 61850) The Sampled Values View is used to view the Sampled Values data (Figure 23). Select the desired Sampled Values Control Block using the Settings editor. Whenever the user unsubscribes from that Sampled Values Control block the buffer will be displayed in the graph. The user can expand the time axis using the mouse wheel (hold the ctrl key down to expand or compress the x axis, use the scroll bar or mouse wheel without the ctrl key to move forward or back along the x axis). 6.7.7 IEC 61850 Logs IEC 61850 Servers can be configured to generate logs of events such as attribute value changing, etc. These logs are configured by the SCD file used to instance the server and require no additional configuration by DTM. A simulated IEC 61850 Client can read the logs from the attached server and display them in the Logs View shown in Figure 62. Select which devices/logs are to be viewed and click the Refresh button to read the current logs from the server. The logs will be read and displayed in the list. Expanding a log will display the logs content. Figure 62: IEC61850 Logs View. IED Functional Behavior Simulation DTM supports the simulation of multiple IEDs. This includes the configuration of the IEDs data model or point list, communications, and basic functionality. Frequently a more complete simulation of an IEDs functional behavior in a system is necessary. DTM 1.5 introduced a new mechanism, based on the IEC 61850 data model, to implement a simulated IEDs function behavior. While this technique is based on the IEC 61850 data model it can be used for any protocol. The IEC 61850 data model describes a substation as shown in Figure 63. The basic elements include the Substation, Voltage Levels, Bays, Breakers, Switches, Current/Voltage Transformers, etc. These elements are organized in a hierarchical fashion based on the topology of the substation. Figure 63 Substation Layout Two of the elements used to describe the substation are Conducting Equipment (breakers, switches, etc.) and LNodes (logical nodes). In the above example the CircBrk1 is a conducting equipment and the XCBR1 is an LNode. These elements are collectively referred to as functional nodes. They define their inputs, outputs, and a behavior that implements the required functionality for that node. The DTM functional simulation allows the user to specify the substation topology, how the functional nodes are connected, and the behavior of each functional node. Adding functional simulation to a workspace in DTM can be done by either manually adding special behavior nodes to the workspace tree or by importing the Substation section of SCL that contains a well-defined Substation section. This Section will describe both processes and will explain the concepts necessary to perform simulations as well as providing the information necessary to modify the provided functional behavior to suite specific cases. The behavior of a substation or a portion of a substation is defined by adding Simulation Nodes (SimNodes) to the Workspace tree. The SimNodes correspond to the elements available in the Substation section of an IEC 61850 Substation Sections. At the top level, a Substation node is always present. Under the Substation, a Voltage Level node is defined. Under the Voltage Level, the Bays and their children are defined. The Figure below shows a simple example of a Substation in a DTM workspace tree. The example in the figure above shows a substation with a voltage level of 34.5kV containing a single Bay. Within the bay there is a single Circuit Breaker which is represented by an XCBR LNode. Note that the XCBR is an LNode and not a Logical Node. The substation section of an SCL file uses LNodes although the LNode can be linked to a Logical Node in the model of a server. This will be discussed in more detail later. The Circuit Breaker is a type of node called ConductingEquipment. Like LNodes, ConductingEquipment is part of the definition of the Substation section of an SCL file. Within the DTM workspace tree, both LNode and ConductingEquipment nodes are of special importance because it is these nodes that contain the functional behavior definitions. While all nodes in the Substation portion of the tree are considered SimNodes, the LNode and ConductingEquipment nodes are also Functional Nodes because of their place in performing the functional simulation of the substations behavior. Later well look at how the Functional Nodes behavior is defined and how they can communicate among themselves and with attributes of a servers model. 6.8.1 Creating a Substation As mentioned in the introduction, there are two ways to add a Substation to a DTM workspace. The Substation can be created manually by adding each node one at a time, or the Substation can be imported from an SCL file. The SCL file can be an SCD file containing the IED definitions, or it can be an SDD file containing only the Substation section. This section will show how to manually create the Substation section. Starting from a new Workspace, right click on the New Workspace item in the workspace tree and navigate to the Substation context menu item following Manage>Add>Substation as shown below. Figure 64: Add Substation Menu Item. This menu item will bring up the Power Sim Editor Dialog box which allows the Substation to be named. A description of the SimNode can also be added if desired. This same dialog box is displayed anytime a node is added to the Substation except for Functional Nodes. The dialog box that is displayed for Functional Nodes has a few additional options and will be explained later. Figure 65: Naming the Substation. Once the Substation has been added, the remainder of the SimNodes can be added to describe the substation structure. Any SimNode that allows children nodes will have a context menu that includes the option Manage>Add>nodename where nodename is an allowable SimNode child. Many of the SimNodes allow specific SimNodes to be added as children. The table below lists the SimNode types as well as their allowable children. SimNode Type Allowable Children Functional Node Substation Power Transformer, Voltage Level no Power Transformer Transformer Winding, Sub Equipment, Equipment Function, LNode no Transformer Winding Tap Changer, Equipment Function, LNode no Tap Changer Sub Equipment, Equipment Function, LNode no Voltage Level Voltage, Power Transformer, Bay, LNode no Voltage - no Bay Power Transformer, Conducting Equipment, LNode, Function, Connectivity Node no Conducting Equipment Sub Equipment, Equipment Function, LNode, Terminal Yes Sub Equipment Equipment Function, LNode, Terminal Yes Equipment Function Sub Equipment Function, LNode no Equipment Subfunction LNode no LNode - Yes Connectivity Node - no Terminal - no Figure 66: Table of Simulation Nodes. For example, adding a Conducting Equipment node to the Bay would result in the Context Menu shown in Figure 67. Figure 67: Adding Conducting Equipment. Since Conducting Equipment is not only a SimNode, but also a Functional Node, the Power Sim Editor Dialog box contains extra fields as shown below. Figure 68: Configuring the Conducting Equipment. In addition to the Name and Description fields, information about the equipment must be specified. The Equipment combo box provides a way to specify what type of conducting equipment is being modelled. The combo box provides a list of equipment codes as defined by IEC 61850 as shown below. In the example, the code CBR is being selected which indicates the Conducting Equipment node will represent a circuit breaker. Figure 69: Choosing the Equipment Type. The selection of CBR in the above example causes the Equipment Variation to be populated. Continuing with this example, the Equipment Variation combo box now appears as shown below. Figure 70: Choosing the Equipment Variation. The Equipment Variation field defines how the circuit breaker will be simulated. This is covered in the next section. 6.8.2 Functional Nodes The primary purpose of creating or importing a Substation into a DTM workspace is to simulate the behavior of the substation design. The task of performing the simulation is achieved through the Functional Nodes in the Substation tree. While the role of the non- functional SimNodes is to describe the organization of the substation, the Functional Nodes do the work of responding to data changes and executing the logic that models the nodes behavior. Specifically, the Functional Nodes differ from the other SimNodes in that they possess a set of input and output points as well as a behavior script. Each Functional Node type has a set of files that control its operation. The files are organized by variations. One Functional Node type can have many variations. An example of a Functional Node type would be CTR or Current Transformer. The CTR Functional Node type comes with a default variation called CTR.default. In addition to the default Functional Node variation, other variations may be supplied with DTM, and custom variations can be created. Every Functional Node type variation consists of two files. The first file is a definition file and, in the case of the CTR example, would be named CTR.default.xml. The second file is the behavior script which is named CTR.default.js. Currently the only supported format for behavior files is JavaScript. A second variation for the CTR Functional Node type could be called SinglePhase. This variation would have two files called CTR.SinglePhase.xml and CTR.SinglePhase.js. The XML definition file in a variation describes the data points used by the behavior script included in that variation. At the time it is created, a Functional Node is assigned a set of input data points and output data points as described in the XML definition file of a given variation. When a CRB Functional Node is added to the Substation tree, the data points are created based on the variation that is selected in the Power Sim Editor Dialog box. The variation selection also specifies the behavior script that will run for that node when the simulation is started. When DTM is installed, a default set of variation files for all the support Functional Node types are included. These files can be found in ProgramData\TriangleMicroworks\DTM\DTAdmin\Simulation. An explanation of how to create a custom variation is provided in a later section. In addition to defining the input and output data points for a Functional Node, the XML definition file also includes association rules for each point that allows DTM to automatically make connections between input points on the node and output points on other Functional Nodes in the substation. 6.8.2.1 Mode/Behavior and Functional Nodes IEC 61850 defines a Mode/Behavior functionality that is used to support testing in a live system. An IEDs mode can be changed either locally, or by remote command, which in turn updates the IEDs behavior. A detailed description of Mode/Behavior is beyond the scope of this document. See the IEC 61850 specification for more information. The DTM Functional Node implements most of the required behavior for the simulation based on the current logical node behavior and the functional node inputs quality. For functional node inputs: If the logical nodes behaviour is set to off, no inputs shall be processed. If the logical nodes behaviour is set to not off and the test bit of the inputs quality is 0 the input will be processed normally. If the logical nodes behaviour is set to test or test/blocked and the test bit of the inputs quality is 1 the input will be processed normally. If the logical nodes behaviour is set to on or blocked and the test bit of the inputs quality is 1 the input will be considered invalid. The inputs quality will be marked invalid, and the functional node behavior invoked. The functional node behavior must perform the required behavior as for any invalid input. For commands: If the logical nodes behaviour is set to on or blocked commands with service parameter Test = false will be processed. Otherwise, they will be refused. If the logical nodes behaviour is set to test or test/blocked commands with service parameter Test = true will be processed. Otherwise, they will be refused. For functional node outputs: If the logical nodes behaviour is set to test or test/blocked, the test bit in the outputs quality will be set to 1, otherwise it will be 0. If the logical nodes behaviour is set to test/blocked or blocked, physical outputs shall not be activated. When blocked, physical outputs will be set to their nominal values. If the logical nodes behaviour is set to off, no outputs are activated and the validity bits in each functional nodes output will be set to invalid. 6.8.3 Simulated Data Points The behavior of a Functional Node as part of a Substation simulation is controlled by the logic in the behavior script assigned to the Functional Node. For the behavior script to operate, it needs to receive data from one or more of these sources: 1) other Functional Nodes; 2) from user interactions; 3) from the IED server models; 4) from other layers of the simulation which are calculating physical properties of the equipment including voltages and currents. This data is presented to the behavior script through simulated input points. The values received by the simulated input points are processed by the Functional Nodes behavior script and the results are represented by the simulated output points owned by the Functional Node. As previously noted, the simulated input and output points are defined in the XML definition file for the nodes type and variation. The input and output points on a Functional Node are visible by using the Show>Data Points option on the context menu of the node. Figure 71: Data Points Menu Item. In this example, the data points are shown for a Conducting Equipment node called CircuitBreaker1. The result of selecting the Show>Data Points option is a Data Points Pane displayed in the workspace as shown. Figure 72: Functional Node Data Points List. The Direction and Category columns provide information specific to the simulated data points found on Functional Nodes. The Direction value indicates if the point is an input point or an output point. The Category value describes how the point is used in the simulation script. In the Data Point list example in Figure 72, the first 5 rows show input points named OpenCoil1 through OpenCoil5. The 5 points are an array of points. They represent 5 possible connection points to the OpenCoil input of the Functional Node. So up to 5 boolean signals can be connected to the OpenCoil input. The connections are typically logically ORd together although this is a function of the behavior script. 6.8.4 Connecting Simulated Data Points in the Substation Each Functional Node in the Substation has a behavior script and input and output data points. A behavior script can write values to its simulated output data points, and it reacts to changes in the values of its simulated input data points. For the values of the input points to change, the input points must be connected to a data source. Possible data sources are: 1) output points on other Functional Nodes; 2) external references on an IED 61850 server modelled in DTM; 3) graphical user input events from Insight displays (buttons, check boxes, etc.); 4) input points in outstations modelled in DTM using protocols other than IEC 61850. When a change to a simulated input data point occurs, the behavior script is notified, and the scripts logic processes the change and potentially modifies values of its output data points. The connections between a Functional Node and other Functional Nodes are made in two ways. There is a mechanism to automatically make connections when a Functional Node is added to the substation based on a set of rules. And connections can be made manually using a Substation Connection Editor dialog. The manual method will be described first. To display the Substation Connection Editor dialog, right click on the node to edit and select the context menu Manage->Manage Substation Connections as shown below. Figure 73: Manage Substation Connections Menu Item. The following dialog box is then displayed. Figure 74: Manage Substation Connections Dialog Box. There are three buttons at the top of the Manage Substation Connections dialog box which are used to limit the choices in the drop-down box where the data source is selected. The data source can be selected from the entire substation, from the bay where the selected Functional Node is located, or from just the Functional Nodes within the parent Conducting Equipment. The grid on the left side of the dialog box shows the output points for the node selected in the drop-down box. In the example above, the selected node is CirciutBreaker3. The grid shows the data output points on CircuitBreaker3. The grid also shows the data type of each point. These points are now available to be connected to the data input points shown in the grid on the left side of the dialog box. In the example in Figure 74, some input points on xcbr_1 already have connections as shown on the grid on the right side of the dialog box. For example, Trip.general1 is connected to OpOpn.general on cswi_1. Therefore, when the simulation is running, the input point Trip.general1 will be updated any time the value of the output point OpOpn.general changes on cswi_1. To remove a connection, click the X at the end of the row and the two points will no longer be connected after the OK button is clicked. To create a new connection, drag an output point from the grid on the left side of the dialog box to the appropriate row on the right side of the dialog box. To successfully make the connection, the data types of the two points much match. All new connections will be established when the OK button is clicked. 6.8.5 Connecting Simulated Data Points to IEC 61850 Models. In addition to connecting the input data points of a Functional Node to the output of other Functional Nodes, a Functional Node that represents an LNode can also connect its inputs and outputs to the model of IEC 61850 server. This is only true for LNodes as Conducting Equipment and Conducting Sub Equipment cannot be connected to server models. The reason for this is that the LNode in the substation section is a representation of a Logical Node in a server model. The Conducting Equipment in the substation represents the physical hardware which is not included in the server model. Typically, an LNode in the Substation will only have connections to a single Logical Node in one server model. This relationship is called an Association. When an LNode is associated with a Logical Node, the behavior script of the LNode can react to signals sent to the Logical Node through GOOSE and the script can cause the Logical Node to send GOOSE and MMS messages. In this way, the simulation induces communications through the simulated IEDs and can interact with real devices outside of DTM. In DTM, the Substation representation can be added to the workspace tree manually at any time. If the workspace does not contain IED servers, then the simulation must depend on the connections between the Functional Nodes. Only when the workspace contains IED servers can the Functional Nodes be associated to them. In order to make connections between a Functional Node and an IED server, right click on the Functional Node to edit and select the context menu Manage->Manage IED Association as shown below. Figure 75: Manage IED Association Dialog Box. The Manage IED Association dialog box is then displayed as shown in Figure 76. Figure 76: Manage IED Association Dialog Box. While the general layout of this dialog box is similar to the Substation Connections Dialog box, an important difference is that there are four tabs labeled Inputs, Outputs, Parameters, and Controls. These represent the four types of connections a Functional Node may have with an IED server. When connecting two Functional Nodes, only inputs to the Functional Node can be connected. When connecting a Functional Node to an IED server, there are two additional types of input called Parameters and Controls that can be wired up. Additionally, Outputs from the Functional Node can be connected to data attributes on the IED server. The IED drop-down box at the top of the dialog box provides a list of all server IEDs in the workspace. The grid on the left side of the dialog box lists all of the external references available in the selected IED. On an IED server, it is the external references that describe values being input to the server. If a connection is made from an LNode to an external reference, then when the external reference value changes, that value is also transmitted to the input of LNode. To effectively model the behavior of substation described by an SCD file, the IED server models must contain external references (ExtRefs) indicating the data they are subscribing to. The Outputs, Parameter, and Controls Tabs on the IED Association require a Logical Node to be selected from the Logical Node drop-down box. The drop-down box will contain a list of the names of the Logical Nodes on the selected IED. When a Logical Node is selected from the list, the appropriate data attributes are displayed on the three tabs and made available to be connected to simulated data points on the LNode. Next to the Logical Node drop-down is a button labeled Associate Logical Node to Functional Node. If this button is clicked, DTM will attempt to automatically connect the points on the Local Node to the points on the LNode. This connection is performed by looking with points and attributes with the same name. For example, if the LNode has a point called OpOpn and the Logical Node has a data attribute with the same name and it is of the same type, then a connection will be made. If any points are found to already have connections, then DTM will ask if the existing connections should be replaced. The Output Tab only shows output data points on the LNode along with all data attributes on the Logical Node. Any LNode output can be connected to any data attribute on the Logical Node. If a connected data attribute is part of data set in an enabled control block, then an appropriate GOOSE or Report message will be sent from the server when the simulation changes the simulated data output point on the LNode. The Parameters Tab shows input data points on the LNode that are categorized as parameter values. These are values that the behavior script uses such as timeouts or retry counts. These values have default values in the simulation but can be overwritten by connecting the parameter point to data attribute in the server. Most of the parameters in the default behavior scripts have corresponding values in the Logical Node definitions. The parameter values that are connected to the model are read at the start of the simulation. If the values are changed in the server while the simulation is running, the new values will be transmitted to the LNode. The Controls Tab shows input data points on the LNode that are categorized as control values. The tab also shows controllable data objects on the Logical Node. A controllable DO such as Pos can be connected to the corresponding Pos control input point on an LNode. The interaction of the controllable points is more complex than the connections between parameter and output points due to the various select and operate schemes available. As a result, the behavior script itself manages the sequence of the control operation. Simulating Data The DTM includes several mechanisms to manipulate data in the simulated devices. These include: Changing individual data values from the DTM user interface. Automatically changing data values randomly at fixed time intervals. Changing data based on preconfigured values and time intervals using spreadsheet files (CSV files) that have been previously captured or generated. Changing data using InSight functionality such as scripting, flow charting, or a custom InSight display. 6.9.1 Changing Data from the DTM User Interface Whether the simulated devices database is displayed using the DTM Point List or Model View, an individual data point/attributes value can be changed, if allowed, by double clicking the value in the display. An editor will be presented based on the type of value being edited. The new value is set in the simulated device when the user presses the Enter key on the keyboard or simply clicks outside the value editor. 6.9.2 Automatic Data Changes All simulated servers/outstations support changing the applicable data values automatically at a user specified rate. Boolean values are toggled, counters and analog values are incremented for each user specified period. Sections 6.9.2.1 and 6.9.2.2 provide detail on how to implement automatic data changes. 6.9.2.1 Creating Automatic Data Change Nodes Automatic data changes are configured and managed using Automatic Data Change Nodes in the workspace. These nodes are created and managed directly on the target channel/session/sector or at a folder level in which case they apply to all the appropriate children of that folder recursively. To create an Automatic Data Change Node, click right on the target node in the workspace and select Simulate Add Automatic Data Changes from the context menu. This will display the Automatic Data Change Editor as shown below. Note that by default all compatible devices will be selected, and a default set of data points will be preselected for all devices. Note, that by a device here one implies a target node (channel/session/sector) which supports either a data model or a point database. Figure 77: Automatic Data Change Editor. Enter a unique name and a description for this auto data change node using the Name and Description pull-downs. Select the desired devices using the Select Device(s) pull-down. Note, this pull-down will be disabled if the target node is a single device. Specify a period at which to change the selected data points. Specify the percentage of the selected points to change each interval. For example, 100% will change every selected point each interval, 50% will change half of the points chosen at random. Use the protocol specific tabs to select the desired points/attributes to change. Points can be chosen using the following mechanisms: Select the desired points in a DTM Point List or Model View and drag them over the appropriate protocol specific grid in the Auto Data Change Editor. Use the protocol specific Insert pull-down to add predefined groups of points appropriate for the protocol (i.e., Dataset points for IEC 61850, Class 1 points for DNP3, etc.). The Apply button will create/update the Automatic Data Change Node(s) in the workspace. The OK button will create/update the Automatic Data Change Node(s) in the workspace and close the Automatic Data Change Editor. The Close button closes the editor without creating/updating the Automatic Data Change Node(s). 6.9.2.2 Working with the Automatic Data Change nodes Once an Automatic Data Change Node is created, the user can perform certain operations on the node (and its children) from the context menu. Figure 78: Automatic Data Change Context Menu. Manage - is used Automatic Data Change Node. This option is only available on the parent Auto Data Change Node and, when invoked, opens the Automatic Data Change Editor, shown in Figure 77. From there, the user can make required modifications to the node. Change Values Once is used to change all the selected points once. This can be used to interactively change the data based on user input. Start/Stop Auto Data Changes is used to start and stop periodic data changes based on the period specified in the node. Note that whether the user choses the Change Values Once option or the Start Auto Data Changes option, the Percent parameter is honored to control what percentage of the selected points is changed. 6.9.3 Changing Data via CSV Files DTM supports the use of Comma Separated Value (CSV) files to specify a sequence of states for a simulated system. This allows the user to change data precisely at the desired moments with precisely desired values. The basic idea is that each state in the CSV file represents a different state of the system and defines specific data values for the attributes of the system in that state. Data change CSV files can be created (pre-configured) using most spreadsheet applications or a standard text editor. A data change CSV file can then be loaded into a simulated server/outstation and individual states can be applied or played back. 6.9.3.1 CSV File Format As mentioned above, CSV Files can be created using a variety of tools from a standard text editor to a spreadsheet editor that has an ability to export to CSV file format (i.e. Microsofts Excel). CSV files are made up of rows, and entries within each row are separated by commas. There are four important elements in a DTM CSV file: State Names the user defined names of states of data point values. An example of state names would be State1, State2, State3, etc. Timestamps time stamps corresponding to each State Name. An example of a time stamp would be 7/1/2013 10:00:15. Point Paths DTM point paths to the data points whose values are defined by each state. A path may be relative or absolute. Relative paths point to a point in the local device but do not specify the device. Relative paths may apply to multiple devices that have a similar database. o Name of channel o Name of session (if appropriate) o Name of sector (if appropriate) o The data type preceded by a T o The point index preceded by a P For protocols, other than IEC61850 and ICCP, the relative path contains the following fields separated by a .: For example, for a DNP3 CSV file a relative path might be: sDNPChannel.sDNPSession.T20.P10 where SDNPChannel is the name of the DNP3 Channel, sDNPSession is the name of the DNP3 Session, T20 indicates object type 20 (i.e. Counter), and P10 indicates the 10th counter. An absolute path is the DTM Path (i.e. /folder1/folder2/device/channel) followed by &&& followed by the relative path described above. For instance, the DNP3 example above might look like: /MyFolder/sDNPChannel/sDNPSession&&&sDNPChannel.sDNPSession.T20.P10 For ICCP protocol, the relative path is the ICCP path to the attribute the point corresponds to. For example: ICC2/rq.Value The corresponding absolute path for the ICCP attribute would be, for example: /MyFolder/Device1/ICCPchannel/ICCPserver&&&ICC1/rq.Value For IEC61850 protocol, the relative path is constructed using an attribute path preceded by a wildcard %%%IED%%%: %%%IED%%%Q/T1MMXU1.A.phsB.cVal.mag.f DTM will try to find all IEDs in the workspace where an attribute with given path can be found. If an absolute path is to be used for IEC61850 protocol in the csv simulation file, it should look like this: /BayController_0/AP&&&BayController_0Q/T1MMXU1.A.phsB.cVal.mag.f where BayController_0 is the name of the IED. Data Values values of each data point corresponding to the respective state. The value type for each point determines which values are accepted in the CSV file. Note that as of DTM v1.2, the components of a DTM CSV file can be organized in two different ways: column-oriented format (older) or in row-oriented format (newer). Regardless of format, both ways are equally supported for playback, although the user is highly encouraged to use the newer format. Column-oriented old format: The first row of a valid column-oriented DTM CSV file is the header which contains the names for each state. The first item of the header will always be Path and the rest are the names of the various states stored in this CSV file. If included, the next row defined the time stamps for each state. The time stamp row is identified by the label $$$snap_time$$$ in the path column. The rest of the entries in this row are the time stamps for each state defined in the first (header) row. Time stamps can be used to play back a CSV file in real time (i.e. the same rate at which it was acquired). The rest of the rows in the CSV file are the specific data points or attributes that will be updated. The first column of each of these rows is the path to the data point or attribute. The additional columns are the value for that point for the specified state. If the file in the column-oriented format is opened in any spreadsheet application, one can easily see that the states are arranged as columns, thus the term column-oriented. An example of a DTM CSV file in column-oriented format is given below: Path, State1, State2, State3, State4 $$$snap_time$$$, 7/1/2013 10:00:00, 7/1/2013 10:00:05, 7/1/2013 10:00:10, 7/1/2013 10:00:15 /sDNP_0/sDNP/sDNP&&&sDNP.sDNP.T1.P0,on,off,on,off /sDNP_0/sDNP/sDNP&&&sDNP.sDNP.T3.P0,intermediate,off,on,indeterminate /sDNP_0/sDNP/sDNP&&&sDNP.sDNP.T20.P0,1,2,3,4 /sDNP_0/sDNP/sDNP&&&sDNP.sDNP.T30.P0,11,22,33,44 /sDNP_0/sDNP/sDNP&&&sDNP.sDNP.T110.P0,text 11,text 22,text 33,text 44 This example has four states arranged as columns (if viewed in a spreadsheet format) labelled State1, State2, State3, State4 as can be seen in the first row. The second row is the acquisition time identified by the $$$snap_time$$$ label. The remaining rows identify five data points, one Binary Input (T1), one Double Point Input (T3), One Counter (T20), one Analog Input (T30), and one String (T110). P0 indicates that it is the first data point of each type. The rest is the data based on the data type for each point whose path is given by the first item in a row. Row-oriented new format: The first row of a valid row-oriented DTM CSV file is the header which contains the word State followed by the time stamp label $$$snap_time$$$ followed by the paths identifying data points. All other rows use the first row as a guide. The first item in this row would be the name of the state followed by the time stamp followed by the values for each respective data point defined in the first (header) row of the file. If the file in the row-oriented format is opened in any spreadsheet application, one can easily see that the states are arranged as rows, thus the term row-oriented. An example of a DTM CSV file in row-oriented format is given below: State,$$$snap_time$$$,/sDNP_0/sDNP/sDNP&&&sDNP.sDNP.T1.P0,/sDNP_0/sDNP/sDNP&&&sDNP.sDNP.T3.P159,/sDNP_1/sDNP/sDNP&&&sDNP.sDNP.T20.P0,/ sDNP_3/sDNP/sDNP&&&sDNP.sDNP.T30.P0,/sDNP_0/sDNP/sDNP&&&sDNP.sDNP.T110.P0 State1,7/1/2013 10:00:00,on,intermediate,1,11,text 11 State2,7/1/2013 10:00:05,off,off,2,22,text 22 State3,7/1/2013 10:00:10,on,on,3,33,text 33 State4,7/1/2013 10:00:15,off,indeterminate,4,44,text 44 The information in the file is the same as it would be in the older format except that the different states are stored as different rows instead of different columns. This allows new states to be added by simply adding a new row at the end of the file. Also, states in the file can easily be replaced with other states. The row-oriented format is now the default format. 6.9.3.2 Loading CSV Files A CSV file can be added to either a valid CSV file host (i.e. simulated channel, session, or sector depending on the protocol) or a folder by right clicking on the device or folder and selecting Simulate Add CSV File from the context menu. If loaded into a folder the file is applied to multiple devices simultaneously. When applied to multiple devices relative points will be applied to all devices which have a point corresponding to the relative path. When Add CSV File context menu option is chosen, this displays the DTM Resource Browser that allows the user to select an existing CSV file resource or import a new one. After the desired resource is selected, one needs to click OK to display the Edit CSV Playback dialog shown in Figure 79. Figure 79: Edit CSV file dialog. Edit the Name and Description using the appropriate pull-downs. This will modify the DTM resource file but in no way affect the original CSV file stored on the users computer. If the CSV file is being applied to more than one device use the Select Device(s) pull down to select which devices to apply it to. If the CSV file is only being added to a single node this pull-down will be disabled. Specify a noise level to be applied to analog signals. This parameter specifies noise as a percent of the current value of the signal. Specify parameters to be used during playback of the CSV file. Check the Restart playback when complete check box to automatically restart playback of the CSV file when the end of file is reached. Use the Delay between iterations time span to specify how long to wait after the last state is applied before starting back at the beginning of the file. States can be applied periodically at the period specified using the Period time span. If this option is active, the time stamps in the CSV file will be ignored. Alternatively, if the CSV file includes acquisition times (time stamps) and the user wants to honor this time stamps, the states can be played back relative to the times at which they were acquired sped up or slowed down by a scale specified by Scale. Click Apply to create a new or update the existing CSV file resource. Click OK to create a new or update the existing CSV file resource and close the dialog. Click Close to close the dialog without making any changes. 6.9.3.3 Editing CSV File within DTM After a CSV file has been loaded, it can be edited and/or verified for validity and integrity of its content. Right-click on the file node in the Workspace browser to invoke the context menu (if the file was loaded on the folder level, only this file on the folder level will have the context menu active), then chose Manage Edit CSV File The will open in a Spreadsheet File Editor similar to the one shown in Figure 80. Figure 80: A CSV file opened in a spreadsheet editor. From the spreadsheet editor, the user can make changes to the CSV resource file, apply changes, save the resource file as a spreadsheet file to the hard drive of the computer, etc. In addition, one can verify validity of the content of the CSV file by pressing Validate. For detail, see section 6.9.3.4. 6.9.3.4 Validating CSV File The content of a DTM CSV file can be validated and checked for errors. The verification process involves checking if the file satisfied the requirements outlined in section 6.9.3.1. With a CSV file loaded into the workspace (as described in section 6.9.3.2), the file validation feature can be accessed in two ways: Right-click on the file choosing Manage Edit CSV File and pressing Validate button. Right- click on the file choosing Manage Verify CSV File Whichever way is chosen, a window similar to the one shown in Figure 81 opens. It has four sections which list likely errors discovered related to the point paths, time stamps, names of the simulation hosts (being channels, sessions, sectors), and values. Figure 81: CSV file verification result window 6.9.3.5 Applying CSV File States Once the CSV file is created, its context menu (shown in Figure 82) can be used to perform the following actions: Apply State - displays a dialog that lists the state names in the CSV File. Select a desired state, then press OK to apply that state. Apply Next State - applies the next state in the file. Start/Stop Playback starts or stops CSV file playback using the current playback parameters. If needed, playback parameter can be edited by choosing Manage Edit CSV Playback prior to Starting Playback. Figure 82: CSV file context menu 6.9.3.6 CSV State Collections A system will typically contain many different devices and each device may behave differently for a system level event. For instance, during a fault in a single bay of a substation the devices in the bay with the fault will clearly behave differently than the other bays even though the actual devices may be the same model. CSV State Collections feature in DTM is a particular case of Applying CSV file states. It allows the user to group states from multiple different CSV files and apply them simultaneously. To create or edit CSV state collections, use the CSV State Collections view (shown in Figure 83) which can be enabled from the main DTM menu Views. Figure 83: CSV State Collections view. To create a new collection, click the New Collection button. This will display the State Collection Editor shown in Figure 84. Enter a name and description for the collection and check the state for each device that represents this state. Note that there should only be one state selected for each device in a given collection. Figure 84: State Collections editor. Once a collection is created, it will appear in the CSV State Collections view. The user can create as many collections as desired following the rules described above. The collections then can be applied one at a time by simply selecting the desired collection and clicking Apply Collection to apply it to all the devices. 6.9.4 Manipulating Data Using InSight InSight provides a variety of mechanisms to interact with simulated devices database. For more details, see the InSight section below and the InSight Users Guide. DTM Test Managers DTM includes two test managers that can be used to create, manage, and execute a variety of test plans. The first test manager uses a set of pre-defined actions to create and execute tests, the second uses a user defined script (JavaScript) to create and execute tests. Note that the pre-defined actions used by the action-based test manager are also developed with JavaScript but the goal is that TMW will provide actions that can be used directly or tweaked to satisfy most users needs, limiting, or eliminating altogether, any coding required by the user. The remainder of this section will first discuss the features common to both test managers. Then it will discuss the details specific to the action-based or scripted test manager. Specifically, how to create tests using each of the approaches. 6.10.1 Creating a new Test Manager To create a new Test Manager right-click on any folder in the Workspace View and select Manage Add Test Manager and select Gateway Tests (Actions) or Scripted from the context menu. Graphical user interface, application Description automatically generated Figure 85 Test Manager Menu After providing a name for the new Test Manager, the Test Manager node displays in the Workspace Browser as shown in Figure 86 . Graphical user interface, application Description automatically generated Figure 86 Test Action node Right-click on a Test Manager node to display the appropriate Test Manager View. For previously configured nodes, this view displays a list of tests supported by this node. If tests had been run previously, the results of the most recent run including each test result, the description and the log will be shown. If the tests of the Test Manager node have not been configured yet, the view will contain no tests. 6.10.2 Test Manager Views Figure 87 below shows the DTM Action-Based Test Manager View while Figure 88 shows the DTM Scripted Test Manager View. As can be seen the two views are similar, with the exception of one or two buttons that are used to create the tests to be managed by this test manager. First, we will discuss the common features. Then we will discuss the features specific to the action-based vs scripted test manager. Graphical user interface, text, application, email Description automatically generated Figure 87 DTM Action-Based Test Manager Graphical user interface, text, application Description automatically generated Figure 88 DTM Scripted Test Manager The main component of the Test Manager view is the test grid. This grid displays the current tests defined for this test manager and the tests most recent status. The columns displayed include: Name The name for this test. This is provided by the test script or when managing actions. Result The result of the most recent attempt to execute this test. The possible values are: o Empty Test has not been run yet. o Passed The most recent run passed. o Skipped This test has been marked to skip. o Failed The most recent run failed. o Blocked This test was blocked by the script. Time The time of the most recent attempt to execute this test. Message Any message returned from the test script/action. User Comments Comments entered by the user. In addition to the above columns there may be one or more custom columns as defined by the test script or actions selected. Below the test grid is a Description of the currently selected test. This description is provided by the selected action or script when the test is created. Below the description is a log of the most recent execution of the selected test. This will include any information reported during the execution of the test. This information is stored with the test and can be exported to a file as described below. A bar along the bottom of the view gives an overview of the cumulative test status. It displays the total number of tests run divided by color. Green indicates tests passed, red indicates tests failed, and blue indicates tests skipped. A tooltip is available to get the current counts and statistics. To the right of the Test Manager view are the buttons used to control the creation and execution of the tests. The Manage Tests, Edit Test Script, and Reload Tests are used to create the tests and are specific to which type of Test Manager is in use. These buttons will be discussed in the Action Based or Scripted test manager sections below. The following buttons are common for both test managers: Reset Tests/Reset Test Results Resets the results of all the tests. This will clear the Status, Time, Message, and User Comments columns. Skip Selected Test(s) Mark all selected tests as skipped. Skipped tests will not be run and will not be included in the overall test status. Run Selected Test(s) Run the currently selected tests. Run All Tests Select and run all the tests that have not been skipped. Stop on Error Stops test run when a test fails for any reason. Save State Saves the current state of all tests so they can be continued later. Restore State Restore a previously saved state. Export Results Export the current test state and results to XML, HTML, CSV, or plain Test file. 6.10.3 Running Tests There are many ways to run tests in a DTM Test Manager. Figure 89 below shows the Test Manager buttons associated with executing the tests. Graphical user interface, application Description automatically generated Figure 89 Test Execution Buttons As described above, the Reset Test Results button will reset any test results from previous attempts. Note that this is not required before attempting to re-run a test. The previous results for that test will automatically be reset. To run a single test either double click the desired test or select the test in the list and click the Run Selected Test(s) button. The run multiple tests select the desired tests and click the Run Selected Test(s) button. Multi-selection is supported via standard Shift/Ctrl key modifiers. To run all of the tests click the Run All Tests button. The tests will run sequentially. Multi test execution can be terminated by clicking the Stop button. Note that the current test may need to run to completion before the test execution can be terminated. Finally checking the Stop On Error checkbox will terminate a multi-test execution whenever a test fails for any reason. This is frequently useful to leave the system in the state when the error occurred. 6.10.4 Action Based Test Manager As described above the DTM Action Based Test Manager uses a set of predefined actions to specify the steps performed during a test. DTM includes a set of actions that should satisfy a lot of requirements and hence require no coding by the user. However, all actions are written in JavaScript so they can be tweaked if necessary or completely new actions developed by the user. A common use of the DTM Test Managers is to test gateways, data concentrators, protocol converters, etc. The actions provided with DTM mainly target this application since the requirements are relatively consistent and hence easier to develop a set of actions that are not specific to a particular implementation. Note that actions can be developed to support a variety of testing (not just gateways) but since these are frequently more implementation specific they will generally require custom actions developed by the user. A typical action provided by DTM will require a server (outstation, controlled station) and a client (master, controlling station) instantiated in the DTM workspace. When executed, the test action will set a user defined value, and optional quality, in the server and expect a specific value in the client to change as a result. The test actions may also test various aspects of the protocol messages used in the transfer of data between the server and the client. If the data is flowing through a gateway or protocol converter, the performance of the configuration of the gateway or converter will be verified. To specify and configure a set of tests to be run in a Action Based Test Manager click the Manage Tests button in the Test Manager View as shown in Figure 90. This will display the Manage Tests dialog as shown in Figure 91. Graphical user interface, text, application, email Description automatically generated Figure 90 Test Action Manager view Graphical user interface, text, application, email Description automatically generated Figure 91 Manage Tests 6.10.4.1 Default Test Actions When a new Gateway Testing (Action) node is created, DTM defaults to having only two actions immediately available to the user. These are Monitored Data Point Test and Controlled Data Point Test actions. If they are sufficient for the testing needs, the user can start adding tests by clicking on the plus sign (+) next to Click here to add new item in the Manage Tests dialog. See Figure 91. Each row in the grid corresponds to a separate test. The first column in the grid is the actions column labeled Test Action. The corresponding cell is a drop-down which allows the user to choose a test action. The other columns correspond to the input parameters for the currently available actions. Since the newly added test Gateway Testing (Action) node has only Monitored Data Point Test and Controlled Data Point Test actions, only the columns corresponding to the parameters for these actions will be visible. The Monitored Data Point Test performs the relatively simple function of setting a value and (optionally) a quality in a specified location in a DTM server and then monitoring a specified location in a client to see if the value in the client is updated to match the server. The test does not care about the details of the protocols used in the client and server. It only looks at the data received in the clients database within DTM. The Controlled Data Point Test is similar in that it simply looks at the value set in the servers database as a result of executing a control in the client. 6.10.4.2 Populating Test Action data Input parameters such as ServerPath1, ServerPath2, and ServerPath3 are used in test actions to uniquely identify a data point in IEC 61850 servers, DNP3 outstations, IEC 60870 controlled stations, Modbus slaves, and ICCP servers. Input parameters such as ClientPath1, ClientPath2, and ClientPath3 are used in test actions to uniquely identify a data point in IEC 61850 clients, DNP3 masters, IEC 60870 controlling stations, Modbus masters, and ICCP clients. Users can easily populate the path parameter in two ways: 1. Drag-and-drop a data point from the database or point list view to any of the path cells. DTM will automatically determine the Path1, Path2, and Path3 components. See Figure 92. 2. Copy the entire data point path from the database or point list view, then paste it to either ServerPath1 or ClientPath1 depending on the data point. See Figure 93. Graphical user interface, text, application, email Description automatically generated Figure 92 Populating path parameters in Manage Tests dialog via drag-drop Graphical user interface, text, application, email Description automatically generated Figure 93 Populating path parameters in Manage Tests dialog via Copy Path in the point list For reference: ServerPath1 - Server device name or DTM path ServerPath2 - Server IEC 61850/TASE2 attribute path or Object/Type id for point- based devices ServerPath3 - Server point index for point-based devices. Not used for IEC 61850 or TASE2 devices ClientPath1 - Client device name or DTM path ClientPath2 - Client IEC 61850/TASE2 attribute path or Object/Type id for point- based devices ClientPath3 - Client point index for point-based devices. Not used for IEC 61850 or TASE2 devices For example, if DTM has an IEC 61850 server with a device path of /BayController/AP, then to reference the status value attribute of a Pos Data Object in a CSWI, the three path parameters would be: ServerPath1: /BayController/AP ServerPath2: BayControllerQ/QA1CSWI1.Pos.stVal ServerPath3: (empty - not used) If DTM also has a DNP3 Master with a device path of sDNP/sDNP/sDNP, then to reference the 5th binary input in the outstation, the three path parameters would be: ClientPath1: /sDNP/sDNP/sDNP ClientPath2: 1 ClientPath3: 5 A Monitored Data Point Test could be constructed using the paths from the two examples above to test if a gateway had correctly mapped the data attribute in the IEC 61850 server to the DNP3 master. The path values could be generated by dragging the points into a row in the Manage Tests Dialog box or by including the path elements in a row of a CSV file. Other cells can be populated based on the parameters of the test. Note that hovering the mouse over a grid cell will generate a tooltip, which might help in populating the cells value. Blank cells may indicate that a default value is to be used by the test or may instruct the test to skip a certain part of the test. The tool tip for a cell in the Manage Tests Dialog box usually indicates the behavior for a blank cell. Special note needs to be given to cells with Quality and/or Flags information. Those are typically bitstrings. The format to enter a bitstring in the cell for such parameter as SetQuality is to have opening and closing square brackets ([ and ]) encapsulating the string of binaries, for example [00100110]. The meaning and position of each bit is protocol dependent. A convenient way to initialize a quality in the Manage Tests dialog is to display a model/point list from a device using the same protocol, copy and quality as a bitstring, and paste it into the Manage Tests grid. The test action parameters whose intent is to verify the received bitstring with the expected value (such as ExpQuality, ExpFlags, ExpIIN, or such), the format to enter a bitstring in the cell is similar to the format to enter SetQuality except that the use of a character x is allowed instead of any 0 or 1 in the bitstring, for example [xx10x110]. If a bit in the expected bitstring is marked with an x, this bit will not be tested/verified with respect to the bit at this position in the received bitstring. To set up multiple tests, DTM offers a convenient way to export the Manage Tests grid to a spreadsheet file (Export CSV File). Using a spreadsheet editor, test coverage can be changed by copying, inserting, or deleting entire rows. Values in each cell can be edited as well. This approach is helpful because the exported file will contain column headers for all the parameters across all the Test Actions in the Test Manager. The power of a spreadsheet editor can then be used to create many additional tests. A modified file can then be imported (Import CSV File) to quickly populate the test grid. 6.10.4.3 Access To Protocol Specific Test Actions If Monitored Data Point Test and Controlled Data Point Test actions may not be sufficient to perform some specific testing, the user may need access to lower-level test actions. For this, DTM offers protocol specific actions which allow for inspection of protocol specific items. For a list of available actions refer to Appendix section 9.3. If lower-level protocol specific actions are desired, click on Select Actions button in the Manage Tests dialog. The Select Actions dialog is mostly a tree with all available actions (refer to Figure 94). Checking an action's checkbox will add that action to the drop-down list in the Test Action column in the Manage Tests dialog box. Note that test actions for ICCP and IEC 60870 protocols have not yet been added in the DTM 1.6.0 release. Figure 94 Select Actions Graphical user interface, application Description automatically generated DTM offers an Advanced Select Actions mode in which the user can map his/her own parameters, add actions to the list with different parameter mappings, quickly export/import custom actions, etc. To enter the Advance mode, press the Advanced button. The Manage Actions dialog will open. Refer to Figure 95. Graphical user interface, application, table Description automatically generated Figure 95 Test Actions - Manage Actions dialog (the advanced Select Actions mode) To remove actions from the list, right-click on the action in the list, then click on Remove selected action(s) from the context menu. To add actions to the list, click the Add Action(s) button located in the lower left corner of the dialog. Managing the number of test actions in a given Test Action Manager may be useful to limit the number of parameters available. This will result in less cluttered dialog boxes and will simplify comma separated value files used as inputs to the Test Action Manager. Additionally, actions can be copied and customized by giving a selected action an individual name and then, later, custom parameter mappings. To manage custom inputs, one can add/remove them individually via a context menu, as shown in Figure 96, or in bulk (all at once). To populate all custom inputs at once, click the Import Input Names button. This will prompt the user to choose a CSV file. The headers found in this CSV file will be parsed as the custom (user) input names. Note, the CSV file can be (and usually is) the same as the one described in section 6.10.4.2 to Import CSV data to populate test action node tests grid. Table Description automatically generated Parameter mapping for the custom user inputs is performed by dragging-and-dropping of the desired input name to the needed Associated Parameter in the grid on the right-hand side of the Manage Tests dialog as shown in Figure 97. The current mapping can be overridden via another drag-and-drop over the same parameter. The mapping, which is no longer desired, can be completely removed by pressing the red backwards arrow next to the Input Name. Figure 96 Adding inputs manually DTM will remember the mapping created for a given user input and will try to propagate this mapping to all associated parameters in other test actions currently in the Custom Actions list. If actions are added/removed from the list of available actions, or parameter mapping has changed, it is important to go back to the Manage Tests dialog and verify the test setup data. Possibly, the data in individual cells needs to be entered/re-entered or the CSV file might need to be reloaded (Import CSV File). Graphical user interface, table Description automatically generated Figure 97 Mapping custom inputs to the DTM test action parameters via drag and drop 6.10.5 Scripted Test Manager The DTM Script-Based Test Manager uses a single script written in JavaScript to define and execute the tests for this instance of the test manager. 6.10.5.1 Implementing Tests The following Scripted Test Manager specific buttons are used to edit the associated test script, reload tests, and potentially save updates to the test matrix. Edit Test Script This button displays the JavaScript editor and allows the user to edit the script that allows the user to edit the script that specifies which tests are available and runs the test. For more information see section 6.10.5.1 Implementing Tests above Reload Tests The Reload Tests button runs the test script to initialize the tests. This is required after editing the test script, or to reset all the tests. Save Tests This button is only displayed if the associated test script defines a saveTests function. If defined this function should save the current state of the tests array. This information may then be used to create updated tests the next time the getTests function is called. The custom test scripts are maintained in a single JavaScript file. This file can be created and edited using the Edit Test Script button of the Scripted Test Manager. Each test script must implement certain functions that are called by the test manager to define what tests are available and to execute each test. The script must contain two functions, getTests, and runTest and two optional functions saveTests, and getCategories. 6.10.5.1.1 getCategories The getCategories function is used to specify additional columns to display in the Scripted Test Manager list. This function is optional, if the test script does not include a getCategories function only the default columns will be displayed. The getCategories function should return a JavaScript array of objects. Each object must include the following two properties: name the name to display in the column header. property the name of the property used to get the value of this column for each test. The array of tests returned by the getTests function described below should include this property for each test. type an options type for this property. The valid values are number or string. The default is string. editable an optional flag that determines whether this property should be editable in the test grid. Editing properties in the grid updates the corresponding value in the test array which can then be used when running tests. These updates can be saved using the Save Tests functionality. An example of the getCategories function is shown below. // function: getCategories // purpose: Return a set of categories which are displayed // as custom columns in the test grid and can be edited // and/or used to sort, group, or filter the tests. This // function is optional and can be undefined or return // null or an empty array if no custom columns are needed. // returns: An array of objects with the following properties // name - defines the test for the column header // property - defines the name of the property for this column. // type - optional, type for this property, valid options are // string or number, defaults to string. // editable - optional, true to make this column editable. // function getCategories() { categories = []; categories[0] = {}; categories[0].name = "Category 1"; categories[0].property = "category1"; categories[1] = {}; categories[1].name = "Category 2"; categories[1].property = "category2"; categories[1].type = "number"; categories[1].editable = true; return categories; } 6.10.5.1.2 getTests The getTest function is used to specify the tests supported by this instance of the Scripted Test Manager. The getTest function should return a JavaScript array of objects. Each object must include the following properties: id An id used to identify this test. This can be any value but should be unique for each test. This value will be used to identify a test to run via the runTest function described below. name The name of this test. This value will be displayed in the Name column of the Scripted Test Managers list. description A description for the test. This value will be displayed in the Description box of the Test Manager view. It should provide a more detailed description of the test and/or any instructions required to run the test. Any properties specified by the getCategory function described above. Any additional properties that are required by the runTest function. The Test Manager will ignore any additional properties. An example of the getTest function is shown below. // function: getTests // purpose: Returns an array of supported tests. // returns: An array of ojbects that contain an id, name, // and description for each test. Each object may also // contain values for the custom categories returned from // the getCategories function above. // function getTests() { tests = []; for(var i = 0; i < 50; i++) { tests[i] = {}; tests[i].id = i.toString(); tests[i].name = "Test " + i; tests[i].description = "This is test " + i; tests[i].category1 = i % 2 ? "Apple" : "Orange"; tests[i].category2 = i; } return tests; } 6.10.5.1.3 saveTests The saveTests function can be used to save updates to the test array either by the user or test script execution. If this feature is not required, the saveTests function can be left undefined in which case the Save Tests button in the Scripted Test Manager user interface will not be displayed. As mentioned, this function can be used to save the current state of the tests array. The result of this operation should then be used by the getTests function to load the updated values for future instances of this set of tests. Note that updated values to the test array are only persisted in DTM memory so they will be lost when the test manager view or workspace is closed and re-opened unless this function has been called. An example of the saveTests function is shown below. // function: saveTests // purpose: Optional function used to save the current state // of the test matrix. If this function is not defined the // Save Tests button in the Test Manager Pane will not be // visible. If this function is defined it will be called // whenever the user clicks the Save Tests button. // returns: true if successful, else false // function saveTests() { // Insert code to save the current tests array. This array // is updated when the user edits custom attributes (see // getCategories above). The edited values can be used // when running tests (see runTest below). These edited // values are only persisted in DTM memory and are lost // when the test manager view or workspace is closed. Use // this function if the edited values should be persisted. // Presumably the getTests function will read the desired // values from the output of this save. return true; } 6.10.5.1.4 runTest The runTest function is used to run an individual test. The test to run is identified by the id argument which is the value returned in the id property of the test array returned by the getTests function described above. The runTest function should execute the specified test and return a JavaScript object which includes the following properties: status an integer that specifies the results of the test. The valid values are: o 0 the test passed o 1 the test was skipped or blocked o < 0 the test failed message a text message that provides addition information about this run. This could include details such as latency when the test passes or the reason the test failed. An example of the runTest function is shown below. // function: runTest(id) // purpose: Run the specified test // returns: An object that contains the test status and an // optional message. // function runTest(id) { Print("Running test " + id); Print("Category 2 = " + tests[id].category2); Print("Delay 1 second"); ProcessScriptEvents(1000); var response = {}; if(id % 3 == 0) { response.status = 0; response.message = "Latency 1.0 seconds." Print("Test passed, " + response.message); } else if(id % 3 == 1) { response.status = 1; response.message = "Test skipped." Print("Test skipped, " + response.message); } else if(id % 3 == 2 && id % 5 != 0) { response.status = -1; response.message = "Test failed." PrintError("Test failed, " + response.message); } else { response.status = 2; response.message = "Test blocked." Print("Test blocked"); } return response; } 6.10.5.2 Add User Feedback to a Test If a test needs user feedback, for example to throw a switch or verify that an indicator changed on the HMI the dtmPrompt function can be used to display a dialog and wait for the user to respond. The dtmPrompt function accepts the following arguments: header The dialogs header. message The message to display in the dialog. buttons An array of strings that specify the buttons to display, each string will be displayed on a button. prompts An optional array of strings that specify prompts allowing the user to enter text. Each string will create a separate prompt/response. The dtmPrompt function returns an array of strings, the first entry contains the text of the button that was pushed, additional entries contain the user specified response to any prompts. The following example uses the dtmPrompt to ask the user to enter the results of the test: function runTest(id) { Print("Starting test " + tests[id].name); oldValue = GetTagValue(tests[id].i850Path); if(oldValue == "Off" || oldValue == false) _expectedValue = true; else _expectedValue = false; Print("Setting " + tests[id].i850Path + " = " + _expectedValue); SetTagValue(tests[id].i850Path, _expectedValue); var response = {}; var result = dtmPrompt("Result", "Verify " + tests[id].name + " is " + _expectedValue + ".", ["Pass", "Fail", "Skip"], ["Message"]); if(result[0] == "Fail") { response.status = -1; if(result.length > 1) response.message = result[1]; PrintError("Test Failed" + response.message != null ? ", " + response.message : "."); } else if(result[0] == "Pass") { response.status = 0; if(result.length > 1) response.message = result[1]; Print("Test Passed" + response.message != null ? ", " + response.message : "."); } else { response.status = 1; if(result.length > 1) response.message = result[1]; Print("Test skipped" + response.message != null ? ", " + response.message : "."); } UnsubscribeAllTagValues(); return response; } 6.10.6 Gateway Testing Considerations While the Test Actions described in this section can certainly be used for other activities, they were primarily designed for use in testing gateways and protocol translators. In order to successfully use the Test Actions, the tester must understand how the gateway is configured or at least how it is intended to be configured. While using the basic Monitored Data Point Test and Controlled Data Point Test actions do not require a lot of protocol knowledge, it is still important to understand what type of data exchange is set up both from DTM to the gateway and from the gateway back to DTM. This is particularly important when setting up the delays and timeouts required by the test actions. For example, if the gateway under test is configured to poll the server every 5 seconds, then the read delay parameter of a test action should be set to at least 5100 milliseconds to allow the data time to be polled and to propagate through the gateway. Setting the delay any shorter will result in a test failure for some portion of the tests. Even event driven reporting will have delays associated with the configuration. For example, a DNP outstation configured to send unsolicited messages to the gateway may have a configurable delay before it sends the message to collect multiple events. Since the test will only cause a single event, the delay timeout will need to expire before the message is sent. If a test is failing, a powerful tool for investigating the failure is the Protocol Analyzer built into DTM. By opening the analyzer and executing a single test, the tester can see the messages and the timing of those messages coming and going from the client and server. Subscribing to Reports (IEC 61850 and ICCP) IEC 61850 Servers specify data sets which in turn are referenced by Report Control Blocks (RCBs). The datasets determine which attributes from the servers model are to be reported and the RCBs determine how and when they are reported. ICCP Servers specify data sets which in turn are referenced by Data Set Transfer Sets (DSTSs). The data sets determine which attributes from the servers model are to be reported and the DSTSs determine how and when they are reported. IEC 61850 Clients enable/disable report control blocks on a server to determine which reports the client wants to receive. In DTM reports can be enabled using commands on the appropriate IEC 61850 Client channel or by locating the desired report control block in the clients model and selecting Enable Report Control Block from its context menu. ICCP Clients enable/disable Data Set Transfer Sets on a server to determine which reports the client wants to receive. In DTM, a client first needs to get the data set transfer set available from the server. Right-click on the DS Transfer Sets node in model tree and choose Get Next Available (see Figure 98). Figure 98: ICCP DS Transfer Sets - Get Next Available command. After that, a DS Transfer Set Configuration editor opens (shown in Figure 99) from which the user has a variety of options including whether to enable this DS Transfer Set (report) or not. Figure 99: ICCP DS Transfer Set configuration editor. Configuring and Subscribing to GOOSE (IEC 61850) This section is only intended to describe how to configure and use GOOSE messaging in DTM. Since GOOSE messaging is implemented directly at the Ethernet layer, any device that is to send or receive GOOSE messages needs to be told which Ethernet adapter to use. For IEC 61850 Servers one can specify the value for the adapter on the main dialog. For IEC 61850 Clients the user will need to click on the Advanced tab to specify the value for the adapter. See sections 6.6.7.1 and 6.6.7.2 which describe relevant configuration editors. In addition, if both GOOSE sender and subscribers will be on the same computer you must enable GOOSE loopback. Generally, it is best to enable this only on the GOOSE subscriber. 6.12.1 Publishing GOOSE In DTM, any IEC 61850 Server can be configured to publish GOOSE messages. This is accomplished by including the appropriate information in the SCL file used to configure the server and specifying an Ethernet adapter to use for GOOSE messaging when configuring the server as described above. Two sections of the IEC 61850 SCL file must be completed to configure a server to send GOOSE messages. These include the Communications and GSEControl elements. For example, the following XML fragments could be used to configure an IED to send GOOSE messages. Note that these are only the pertinent parts of a valid SCL file. <Communication> <SubNetwork > <ConnectedAP apName="P1" iedName="PQ1"> <Address> <P type="IP">127.0.0.1</P> <P type="OSI-AP-Title">1,3,9999,23</P> <P type="IP-SUBNET">255.255.255.0</P> <P type="IP-GATEWAY">10.10.0.1</P> <P type="OSI-AE-Qualifier">23</P> <P type="OSI-PSEL">00000001</P> <P type="OSI-SSEL">0001</P> <P type="OSI-TSEL">0001</P> <P type="IP-PORT">112</P> </Address> <GSE cbName="Control_DataSet4" ldInst="PROT"> <Private source="42" type="HELINKS_TAG"/> <Address> <P type="MAC-Address">01-0C-CD-01-00-01</P> <P type="APPID">9999</P> <P type="VLAN-ID">000</P> <P type="VLAN-PRIORITY">4</P> </Address> <MinTime multiplier="m" unit="s">15</MinTime> <MaxTime multiplier="m" unit="s">10000</MaxTime> </GSE> <PhysConn type="Connection"> <P type="Type"> 100BaseT</P> </PhysConn> </ConnectedAP> </SubNetwork> </Communication> <IED name="PQ1" > <AccessPoint name="P1"> <Server > <LDevice desc="Protection" inst="PROT"> <LN0 desc="General" lnType="S7SJ64/PROT/LLN0" inst="" lnClass="LLN0"> <DataSet name="DataSet4"> <FCDA fc="ST" lnClass="XCBR" lnInst="1" prefix="" ldInst="PROT" doName="Pos"/> </DataSet> <GSEControl name="Control_DataSet4" datSet="DataSet4" confRev="1" appID="0" type="GOOSE"/> </LN0> </LDevice> </Server> </AccessPoint> </IED> 6.12.2 Subscribing to GOOSE GOOSE messages can be received by any IEC 61850 Client or Server but are configured differently. Configuring an IEC 61850 Server to receive GOOSE messages is similar to configuring a server to send them. The user must specify an Ethernet adapter to use for GOOSE messaging and include the appropriate information in the SCL file used to configure the server. The following XML fragments could be used to configure an IEC 61850 Server to receive GOOSE messages. <Communication> <SubNetwork name="Subnet1" type="8-MMS"> <ConnectedAP apName="AccessPoint" iedName="TMW_BCU_J01"> <Address> <P type="IP">127.0.0.1</P> <P type="IP-PORT">105</P> <P type="OSI-AP-Title">1,3,9999,23</P> <P type="IP-SUBNET">255.255.255.0</P> <P type="IP-GATEWAY">10.101.3.1</P> <P type="OSI-AE-Qualifier">23</P> <P type="OSI-PSEL">00000001</P> <P type="OSI-SSEL">0001</P> <P type="OSI-TSEL">0001</P> </Address> <GSE cbName="gcb1" ldInst="1"> <Private source="42" type="HELINKS_TAG" /> <Address> <P type="MAC-Address">01-0C-CD-01-00-00</P> <P type="APPID">0000</P> <P type="VLAN-PRIORITY">4</P> <P type="VLAN-ID">000</P> </Address> <MinTime multiplier="m" unit="s">5</MinTime> <MaxTime multiplier="m" unit="s">10000</MaxTime> </GSE> <PhysConn type="Connection"> <P type="Type">100BaseT</P> </PhysConn> </ConnectedAP> </SubNetwork> </Communication> <IED name="TMW_BCU_J01" configVersion="1.0" manufacturer="TMW" type="TMW_BCU"> <AccessPoint name="AccessPoint"> <Server> <Authentication /> <LDevice inst="1"> <LN0 lnType="LLN0_TYPE_0" inst="" lnClass="LLN0"> <Inputs> <ExtRef daName="general" doName="Op" lnClass="PTRC" lnInst="1" prefix="" iedName="TMW_BPU_J01" ldInst="1" serviceType="GOOSE" srcCBName="gcb1" srcLDInst="1" srcLNClass="LLN0" /> <ExtRef daName="q" doName="Op" lnClass="PTRC" lnInst="1" prefix="" iedName="TMW_BPU_J01" ldInst="1" serviceType="GOOSE" srcCBName="gcb1" srcLDInst="1" srcLNClass="LLN0" /> </Inputs> <GSEControl desc="BreakerFailure" name="gcb1" datSet="ds_gcb1" confRev="10000" appID="TMW_BCU_J011/LLN0gcb1"> <Private source="42" type="HELINKS_TAG" /> </GSEControl> </LN0> </LDevice> </Server> </AccessPoint> </IED> IEC 61850 Servers do not have any place in their model to store received GOOSE messages, so DTM creates nodes under the ExtRef in the model that defines the input. For example, the above scenario would generate the following elements in the subscribing server (Figure 100). Figure 100: Server Elements To configure an IEC 61850 Client to receive GOOSE messages the user must specify an Ethernet adapter to use for GOOSE messaging and subscribe to the desired GOOSE messages using the Subscribe to GOOSE Messages command on the Client. 6.12.3 Simulated GOOSE Streams IEC 61850 GOOSE supports sending GOOSE messages with a simulation flag. This supports validating functionality in a live system by specifying which stream a subscriber will respond to and controlling that subscribers behavior using the IEC 61850 Mode/Behavior feature. To publish a simulated GOOSE stream for a simulated IEC 61850 server, show the servers model, navigate to the GOOSE Control Block for the desired stream, and set the Simulate attribute to true. This will set the simulate flags in the ethernet header (SimulateReserved) as well as the GOOSE message itself (SimulatePDU). Setting Simulate back to false will clear the simulate flags and return to publishing a non-simulated stream. In DTM IEC 61850 clients will receive both simulated and non-simulated GOOSE streams. The GOOSE log for a client will show both streams but only the non-simulated stream will update the clients data model (note that in DTM the clients data model is simply the model of the server the client is connected to). IEC 61850 servers will subscribe to the simulated or non-simulated stream based on the current value of the LPHD.Sim attribute. If LPHD.Sim is false, the server will ignore any simulated stream and subscribe to/respond to the non-simulated stream. If LPHD.Sim is true, the server will reply to the non-simulated stream if no simulated stream is available. When a simulated stream becomes available the server will subscribe to the simulated stream and ignore the non-simulated stream. Note that the IEC 61850 LGOS logical node and functionality can be used to monitor GOOSE subscriber status, including the availability of simulated or non-simulated GOOSE streams. Subscribing to Sampled Values (IEC 61850) As with GOOSE, Sampled Values also uses the Ethernet as the physical layer but is designed to allow the transmission of real time sampled values at rates sufficient to support critical functions in a power substation. Specifically, voltages and currents can be sampled and transferred fast enough to support distributed processing. As with GOOSE, the configuration of Sampled Values is done by the SCD file that is used to create the IEC 61850 Servers. Servers can be configured to publish and/or subscribe to Sampled Values. In addition, in DTM IEC 61850 Clients can subscribe to Sampled Values from their current server. The only DTM specific configuration required is to specify the desired Ethernet adapter to use for Sampled Values when configuring the IEC 61850 Server and/or Client. Assuming Sampled Value is properly configured, an IEC 61850 Command can be used to subscribe/unsubscribe from Sampled Values from the server (see Figure 101). When subscribing, the user can specify a buffer size. The client will fill this buffer from the sampled values stream until it is full. This buffer is transferred to the DTMAdmin when the client unsubscribes. DTM does not current stream sampled values data to the DTMAdmin in real time. Figure 101: Sampled Values Subscriptions. Commands DTM Commands are used to invoke specific requests. Commands can be added to a specific channel, session, or sector (depending on the protocol) or to a group of channels, sessions, or sectors contained in a folder. 6.14.1 Adding Commands To add a command to a specific channel, session, or sector right-click over the desired node and select the Commands command menu option. To add a command to any or all appropriate children of a folder, right-click on the folder and select the Commands protocol command menu option. This command will then be applied to all selected channels, sessions, or sectors contained in that folder. A typical command editor is shown in Figure 102. The command editor allows the user to specify a unique name for the command, provide a description of the command, configure command specific options, and finally schedule the command execution. Figure 102: DTM Command Editor. As an example, a DNP3 command editor shown. Command configuration is done by utilizing the following features in the editor window: Name. Enter a unique name for this command. Select Device(s). For folder-based commands use this pull-down menu to select which devices to link this command to. Note that only compatible devices will be listed. If the command is not folder-based, this option will be deactivated. Command Options. Select the command specific options. Scheduler. Select the command scheduling if desired. Refer to section 6.14.2 for more information. Execute button. Pressing Execute will execute the command once as configured. Apply button. Pressing Apply will create/update the command as configured in the workspace. OK button. Pressing OK will create/update the command as configured in the workspace and then close the dialog. Close button. Close will close the dialog without creating/updating the command. 6.14.2 Command Execution and Scheduling Workspace commands can be executed interactively by using the commands context menu. In addition, commands can be configured to execute using the following options: On Connect Configures the command to run automatically whenever the channel connects or reconnects. Periodically Run the command periodically at the specified interval. Note that periodic commands can be started/stopped from the commands context menu in the workspace. 6.14.3 Common Protocol Specific Commands The following communication protocols are supported in DTM: DNP3, Modbus, IEC60870-5- 101, IEC 60870-5-104, IEC61850, and ICCP (Tase.2). Subsections 6.14.3.1 through 6.14.3.5 list commands currently supported in DTM. Refer to specific protocol documentation for more protocol specific command information. 6.14.3.1 DNP Commands Integrity Data Poll - Class 0123 The Integrity Poll retrieves all event (class 123) and static (Class 0) data from the device. It is typically sent after device restart, loss of communication, or on a periodic basis to ensure all data is accurate. RBE Data Poll - Event Class 123 The Report by Exception (RBE) Data Poll retrieves all change event data (class 123) from the device. It is typically sent every few seconds to collect any changes in the device database. A null response indicates no changes have occurred in the database since the last RBE Data Poll. Data Set Command To be added. Analog Output Block The remote device may support analog output block control operations to multiple data points in the same message, but all points are not required to change at the same time. Read Specific Data Type As an alternative to DNP3 Class 0,1,2,3 data polling, the DNP3 read command can be used to poll each static or event data type at a different rate. For example, to save network bandwidth, a read request for binary input change events could be done every 2 seconds while less important analog change events are requested in a separate command every 10 seconds. Freeze Counters Freeze the specified DNP3 running counters. Enable Unsolicited Messages The remote device must be configured to allow unsolicited responses for the Enable unsolicited response command to work. In addition, the remote device may wait a time period after the first event and/or for a number of change events before sending an unsolicited response. Clear Restart IIN This command will be sent automatically anytime the Restart IIN (Internal INdication) bit is set in a response message if the session is opened with the Clear Restart bit when restart received option. Control Relay Output Block The remote device may support binary output control operations to multiple data points in the same message, but all points are not required to change at the same time. Import User Certificate Import a user certificate from an external source. Initiate User Status Change Send an Initiate User Status Change Requests to the DNP3 Authority for this master. Time Synchronization Set the time in the remote device. File Write To be added. File Read To be added. File Delete To be added. Update Key Change Send an Update Key Change Request for the specified user. User Status Change Send a User Status Change Request for the specified user. 6.14.3.2 Modbus Commands Read Read Coils, Discrete Inputs, Holding Registers, or Input Registers. Write Write Coils or Holding Registers. 6.14.3.3 IEC60870 Commands Bitstring (C_BO_NA) The Bitstring command writes a new data value to the specified point. Double Point (C_DC_NA) The Double Point command writes a new data value to the specified point. Floating Measurand (C_SE_NC) The Floating Measurand command writes a new data value to the specified point. Normalized Measurand (C_SE_NA) The Normalized Measurand command writes a new data value to the specified point. Scaled Measurand (C_SE_NB) The Scaled Measurand command writes a new data value to the specified point. Regulating Step (C_RC_NA) The Regulating Step command writes a new data value to the specified point. Single Point (C_SC_NA) The Single Point command writes a new data value to the specified point. Clock Synchronization The Clock Synchronization command synchronizes the clocks between the Master and Slave(s) by writing the time from the Master to each of the slaves (individually or via a broadcast message. Counter Interrogation The Counter Interrogation command reads, freezes, and/or resets a group of counters. Four modes of acquisition of integrated totals or incremental information are defined: Mode A: Local freeze with spontaneous transmission Mode B: Local freeze with counter interrogation Mode C: Counter interrogation commands from the controlling station initiate the freeze, freeze with reset, or reset commands. Counters are read with subsequent counter interrogation commands. Mode D: Counter interrogation commands from the controlling station initiate the freeze operation, and the frozen values are reported spontaneously. General Interrogation The General Interrogation command retrieves all or a specified group of static data. It is typically sent after device restart, loss of communication, or on a periodic basis to ensure no changes have been missed in the spontaneous data reporting. 6.14.3.4 IEC61850 Commands Enable Report Control Blocks Enable or disable the selected report control blocks. Hint: Use the grid's filtering capabilities to display the desired control blocks and then select or deselect using the column header. Subscribe to GOOSE messages Subscribe to GOOSE messages. This enables a GOOSE Control on the server and enables the client to start receiving GOOSE messages. Subscribe to Sampled Values Subscribe to Sampled Values. This Subscribe to a Sampled Value Control on the server and enables the client to start receiving Sampled Value messages. Read Read element(s) from IEC 61850 server. Write Write element(s) to IEC 61850 server. 6.14.3.5 ICCP (Tase.2) Commands Connect Connect client to ICCP/Tase.2 server. Disconnect Disconnect client from ICCP/Tase.2 server. Select Read SBO value. Operate Attempt to write to the node, then read the node. Select and Operate Read SBO value, followed by an attempt to write to the node, then read the node. Enable DS Transfer Set To be added. Get Next Available DS Transfer Set To be added. Controlling DTM from the Windows Command Line 6.15.1 DTMAdmin Command Line Arguments DTMAdmin.exe supports the following command line arguments that can be used to control how the application starts up. -workspace <full path to DTM workspace> - If the specified workspace is already loaded do nothing. If not, close the current workspace and load the specified workspace. -scl <path to SCL file on disk> If a workspace is loaded the specified SCL file is updated in the existing workspace. If a workspace is not loaded the Import Substation Wizard is displayed allowing the user to import the specified SCL file. -layout <layout name> - Load the specified layout from the current workspace. -fullscreen Start DTMAdmin in full screen mode (can be toggled using the F11 function key). 6.15.2 DTExec The DTExec.exe application included in the Distributed Test Manager installation folder can be used to manage DTM from the Windows command line. The DTExec application can be found in the Bin folder inside the DTM installation folder. The default DTM installation folder is C:\Program Files\Triangle MicroWorks\Distributed Test Manager. For DTExec to be able to execute its commands, DTM Services must already be running. Refer to section 4.3.1.2 for more detail. At the end of its run, DTExec application returns 0 if processing its command line arguments was successful, and it returns -1 if a failure was detected. DTExec application can be used in other command line interfaces such as batch files. 6.15.2.1 DTExec command line arguments The DTExec.exe application accepts the following command line arguments: -help Provide other possible options to use when running the dtexec application from the Windows command line. -host Provide IP address and TCP listening port of master DTM computer. This option arguments are [IPAddress:Port]. The defaults are 127.0.0.1 and 42145 respectively. The user can skip the -host option if DTExec is run on the machine where DTManager is running (master DTM computer). -reset Reset (unload) currently loaded workspace. -load Load a workspace. This option has an argument [WorkspacePath/WorkspaceName]. -run Run a script. This option has arguments [ScriptPath] [Arguments]. Here [ScriptPath] is the InSight script path that is part of the currently loaded workspace. [Arguments] are additional (and optional) space-separated arguments, the first being the name of a Javascript function in the script to execute and all the following arguments being parameters of this method. -log Enable output logging to a file. This option has an argument [LogFileFolder] which is the system path to a folder on your machine. If the log file folder option is not specified, then invoking the -log command line argument will start logging into the default directory c:\temp\DTMLogFiles. -debuglevel Set debug level. This command line argument sets the debug message sensitivity to a desired level. Allowed values are 0, 1, 2. The default is 0. The syntaxis is: -debuglevel=1 6.15.2.2 Examples of using DTExec commands with options Below are some examples of running the DTExec application with options: >dtexec -reset Will reset the workspace. DTExec is run on master DTM computer. >dtexec -host 192.168.1.10:42145 -reset Will reset the workspace. DTExec is run on a slave machine connected to the master DTM computer on address 192.168.1.10, port 42145. >dtexec -load /Workspaces/My Favorite Folder/My Favorite Workspace Will load a workspace My Favorite Workspace found under the workspace folder /Workspaces/My Favorite Folder. See section which discusses Workspaces for details on how DTM handles workspaces using DTMAdmin. >dtexec -run /Test.js Will execute a script (must be a part of current workspace) with name Test.js >dtexec -run /Test.js PrintMe Called from Windows Will execute method PrintMe in script Test.js. The PrintMe method will have a passed parameter Called from Windows resulting in printed Called from Windows to the Output view. 6.15.2.3 Examples of .bat files 6.15.2.3.1 Batch file example 1 An example of .bat file which starts DTControl (as a prerequisite to running DTExec if DTControl is not running yet) then loads a workspace is given below: echo OFF cd "c:\Program Files\Triangle MicroWorks\Distributed Test Manager\Bin" echo Launch DTControl service start DTControl.exe echo Wait 30 seconds to have DTM ready PING localhost -n 30 > NUL echo Load Workspace DTExec -load "/Workspaces/My Favorite Folder/My Favorite Workspace" 6.15.2.3.2 Batch file example 2 An example of .bat file which launches DTExec.exe with command options invoking running script myScript.js located at the top level of a workspace (as indicated by the path /myScript.js): @echo off REM Change the path to the directory where DTExec executable is cd "c:\Program Files\Triangle MicroWorks\Distributed Test Manager\Bin" DTExec -run "/myScript.js" > c:\temp\myScriptBatFileResults.txt if %errorlevel%==-1 (echo "The script run failed") If myScript.js is written to return a value (result), the redirect > to the file myScriptBatFileResults.txt will document this result. In addition, if DTExec execution fails for any reason, its return value is -1 and thus the errorlevel is flagged as -1. InSight This section discusses functionality available in the Triangle MicroWorks InSight Toolkit. For details on the InSight Toolkit and how to use it see the InSight Users Guide available from the DTM Help menu. This section discusses aspects of this system that are specific to the DTM implementation. Paths The InSight tools reference data points in the DTM system using a path, or tag (sometimes called a TagPath). This path consists of two parts. The first part is a DTM specific Path which consists of the nodes in the DTM tree from the workspace down to the DTM container that owns the data point separated by /, for example /Outstation/sDNP/sDNP/sDNP. This is followed by a delimiter &&&. The second part is the path to the specific point inside the container and looks something like sDNP.sDNP.T1.P0 were the first two fields are the channel and session names, the third field is the type specifier (object id for DNP3) and the last is the point index. DTM supports absolute paths (as described above) and relative paths that select data points relative to the location of the InSight file in the DTM workspace. Relative paths begin with the delimiter $$$. For example, if an InSight script is in the Outstation folder in the above example and path of $$$/sDNP/sDNP/sDNP would be equivalent to /Outstation/sDNP/sDNP/sDNP. The user will seldom need to create one of these paths themselves because they are usually created by dragging the desired points from a point list and dropping it in the JavaScript editor or display at the appropriate location. When you drop a point in the JavaScript editor it creates an InSight tag. The variable (the left-hand part of the tag) can then be used as a variable in the JavaScript code similar to any other variable. When you drop a point on an InSight Display it will automatically associate it with any compatible object on which it was dropped, or it will create a new object if none was found at the drop site. If, the user does need to specify point paths from within a script, here is a quick reference to the format for the data points expected: IEC 60870, DNP, Modbus "/MyFolder/sDNP/sDNP/sDNP&&&sDNP.sDNP.T30.P0" where MyFolder is the name of a folder in the DTM workspace, sDNP is the name of a device in the DTM workspace, another sDNP is the name of the channel, and another sDNP is the name of the session (in case of IEC60870 protocols, there will also a sector part of the path). Finally, sDNP.sDNP.T30.P0"is the path to a specific data attribute. IEC 61850 "/MyFolder/c61850/c61850&&&BayControllerQ/T1MMXU1.A.phsA.cVal.mag.f" where MyFolder is the name of a folder in the DTM workspace, c61850 is the name of a device in the DTM workspace, and another c61850 is the name of the channel. BayControllerQ is the name of the IED, and T1MMXU1.A.phsA.cVal.mag.f is the path to a specific data attribute. ICCP (Tase.2) "/MyFolder/Device1/ICCPchannel/ICCPserver&&&ICC1/r" where MyFolder is the name of a folder in the DTM workspace, Device1 is the name of a device in the DTM workspace, ICCPchannel is the name of the channel, and ICCPserver is the name of the ICCP session. ICC1 is the name of the Logical Device, and r is the path to a specific data attribute. Scripting The InSight system includes a script capability via an embedded JavaScript engine. General information on this functionality can be found in the InSight Users Guide and/or numerous JavaScript references online. DTM exposes functionality to the InSight Scripts via a set of methods. These methods support: 1. The creation/management/deletion of simulated devices. 2. Instructing simulated clients/masters to issue commands. 3. Updating/monitoring data in simulated devices. DTM includes a set of sample scripts that illustrate all the supported functionality. The following scripts exist for each supported protocol: 1. Protocol Master and Outstation.js Demonstrates how to create and manage devices for the specified protocol. 2. Protocol Commands.js Demonstrates the available commands and how to execute them. 3. Protocol Data Access.js Demonstrates how to set and get values from simulated devices. 4. Protocol CSV file.js Demonstrates how to load CSV files and apply columns from them. Data Type Formats Accepted from Scripts Device points in DTM can only accept very specific data types. However, the formats of data type representation which can be accepted from scripts can sometimes vary. Below, one can find information on some acceptable formats for various data types. 7.3.1 DNP Formats Binary Input on, off 0, 1 false, true Double Bit Input To set the value of a Double Bit Input data point either of these two formats can be used: Intermediate, Off, On, Indeterminate 0, 1, 2, 3 Note, when the value of a Double Bit Input data point is requested, the script method(s) return a value in this format: Intermediate, Off, On, Indeterminate Binary Output Status on, off 0, 1 false, true Running Counter Any 32-bit unsigned integer number is accepted. Frozen Counter Any 32-bit unsigned integer number is accepted. Analog Input Any 32-bit single-precision floating point number is accepted. Analog Output Status Any 32-bit single-precision floating point number is accepted. String Data Any text data is accepted. Virtual Terminal Outputs Any text data is accepted. 7.3.2 Modbus Formats Coil on, off 0, 1 false, true Discrete Input Register on, off 0, 1 false, true Holding Register Any 32-bit single-precision floating point number is accepted. However, the number will be rounded up to the nearest 32-bit integer. If the entered number if negative, the value of the Input Register will be set to 0. Input Register Any 32-bit single-precision floating point number is accepted. However, the number will be rounded up to the nearest 32-bit integer. If the entered number if negative, the value of the Input Register will be set to 0. Device Identification Object Any text data is accepted. 7.3.3 IEC60870-5-101 and IEC60870-5-104 Formats Single Point Information on, off 0, 1 false, true Double Point Information To set the value of a Double Point Information data point either of these two formats can be used: Intermediate, Off, On, Indeterminate 0, 1, 2, 3 Note, when the value of a Double Point Information data point is requested, the script method(s) return a value in this format: Intermediate, Off, On, Indeterminate Step Position Information Any 32-bit unsigned integer number is accepted. Bitstring of 32 Bits A bit string of 32 bits such as, for example, [00000000000000000000000000000000] is accepted. Measurand Value, Normalized Any 32-bit single-precision floating point number is accepted. However, the number will be rounded up to the nearest 32-bit integer. Measurand Value, Scaled Any 32-bit single-precision floating point number is accepted. However, the number will be rounded up to the nearest 32-bit integer. Measurand Value, Short Floating Point Any 32-bit single-precision floating point number is accepted. Integrated Totals Any 32-bit unsigned integer number is accepted. Measurand Value, Normalized without Quality Descriptor Any 32-bit single-precision floating point number is accepted. However, the number will be rounded up to the nearest 32-bit integer. 7.3.4 IEC61850 Formats BOOLEAN To set the value of a BOOLEAN data point either of these two formats can be used: False, True 0, 1 Note, when the value of a BOOLEAN data point is requested, the script method(s) return a value in this format: False, True INT8 Any signed 8-bit integer number is accepted (-128 to 127). INT8U Any unsigned 8-bit integer number is accepted (0 to 255). INT16 Any signed 16-bit integer number is accepted (-32,768 to 32,767). INT16U Any unsigned 16-bit integer number is accepted (0 to 65,535). INT24 Any signed 24-bit integer number is accepted (-16,777,216 to 16,777,215). INT24U Any unsigned 24-bit integer number is accepted (0 to 33,554,430). INT32 Any unsigned 32-bit integer number is accepted (-2,147,483,648 to 2,147,483,647). INT32U Any unsigned 32-bit integer number is accepted (0 to 4,294,967,295). INT64 Any signed 64-bit integer number is accepted. FLOAT32 Any 32-bit single-precision floating point number is accepted. FLOAT64 Any 64-bit double-precision floating point number is accepted. Enum The accepted values are SCL file dependent. To set the value of an Enum data point either of these two formats can be used: EnumName0, EnumName1, EnumName2, EnumName3, 0, 1, 2, 3, Note, when the value of an Enum data point is requested, the script method(s) return a value in this format: 0, 1, 2, 3, Dbpos To set the value of a Dbpos data point either of these tree formats can be used: Intermediate, Off, On, Bad 0, 1, 2, 3 [00], [01], [10], [11] Note, when the value of a Dbpos data point is requested, the script method(s) return a value in this format: Intermediate, Off, On, Bad Tcmd To set the value of a Tcmd data point either of these tree formats can be used: Stop, Lower, Higher, Reserved 0, 1, 2, 3 [00], [01], [10], [11] Note, when the value of a Tcmd data point is requested, the script method(s) return a value in this format: Stop, Lower, Higher, Reserved Quality A bit string of 13 bits such as, for example, [000000000010] is accepted. Timestamp The user is not supposed to set the Timestamp value. When the value of a Timestamp data point is requested, the script method(s) return a value in the date-time format. VisString32 Maps to a string. VisString64 Maps to a string. VisString65 Maps to a string. VisString129 Maps to a string. VisString255 Maps to a string. Octet64 A string of unsigned chars (unsigned 8-bit integers with values ranging from 0 to 255). Unicode255 Maps to a string of UTF-8 characters. 7.3.5 ICCP Formats SupportedFeatures A bit string of 12 bits such as, for example, [000000000000] is accepted. Discrete Any unsigned 32-bit integer number is accepted (-2,147,483,648 to 2,147,483,647). Real Any 32-bit single-precision floating point number is accepted. State A bit string of 8 bits such as, for example, [00000000] is accepted. Flags A bit string of 8 bits such as, for example, [00000000] is accepted. Note, the first two bits in the Flags data point are ignored. Quality A bit string of 13 bits such as, for example, [000000000000] is accepted. TimeStamp The user is not supposed to set the Timestamp value. When the value of a Timestamp data point is requested, the script method(s) return a value in the date-time format. Counter The user is not supposed to change with value with a script. When the value of a Counter data point is requested, the script method(s) return a value in the format of an unsigned 32-bit integer. ControlCommand Any signed 16-bit integer number is accepted (-32,768 to 32,767). DiscreteSetPoint Any signed 16-bit integer number is accepted (-32,768 to 32,767). RealSetPoint Any 32-bit single-precision floating point number is accepted. VisString32 Maps to a string. INT16 Any signed 16-bit integer number is accepted (-32,768 to 32,767). INT32 Any unsigned 32-bit integer number is accepted (-2,147,483,648 to 2,147,483,647). Flow Charting The InSight system includes an interactive visual programming environment utilizing flow charts. General information on this functionality can be found in the InSight Users Guide. Custom Displays The InSight system includes an interactive display toolset that can be used to create displays to manage/monitor DTM. General information on this functionality can be found in the InSight Users Guide. InSight Files DTM supports storing InSight files externally in the file system or as part of the DTM workspace. When stored externally they will not be saved and restored with the workspace and have to be reload manually each time DTM is run. When stored as part of the workspace they will get saved and restored with the workspace and stored as part of the Workspaces layout. 7.6.1 External Files To work with external files, launch the appropriate InSight tools from the DTM InSight menu or toolbar and proceed as described in the InSight Users Guide. 7.6.2 Workspace Files To work with workspace files, click the right mouse button over the desired folder and select the InSight Add Script/Display/Flowchart menu option. This displays the appropriate resource selector and allows the user to select an existing InSight resource or import one, or simply type in a name to create a new one. IEC 61131 Application (Structured Text) DTM 1.5 supports a limited set of the IEC 61131 functionality. Specifically, the IEC 61131 Application environment and Structured Text. Function Block Diagrams are planned for a future release. To add an IEC 61131 Application to the DTMs workspace, right-click on a root, or any other, folder in the workspace and choose InSight Add IEC 61131 Application. After you give the new IEC 61131 Application a name and, optionally a description, an IEC 61131 node will appear in the workspace. See Figure 103. In order to build the IEC 61131 Application in DTM, right-click on the 61131 Application node and choose Manage Edit IEC 61131 Application as shown in Figure 104. Figure 103: IEC 61131 Application node in the DTM workspace A screenshot of a cell phone Description automatically generated Figure 104: Edit IEC 61131 Application The IEC 61131 Application View, as shown in Figure 105 below, contains a menu bar and several sub panes which are used to view and monitor the IEC 61131 application. A screenshot of a social media post Description automatically generated Figure 105: DTM with IEC 61131 Application window The sub panes include: Application Displays the application structure and contents (currently limited to structured text files). Variables Displays all the variables used by the current application. This window allows the user to update the values of the variables but is not updated by the application for performance reasons. Watch The Watch pane provides a live view of specific variables from the current application. This pane allows the user to observe the value of the desired variables in real time. To add variables to the Watch pane, drag the desired variables from the Variables pane and drop them into the Watch pane. Variables can be deleted from the Watch pane by selecting them and hitting the delete key. Output The messages generated by the application engine. Context menus are available in the Application pane by right clicking on the desired node. The Application context menu is shown in Figure 106 below. A screenshot of a cell phone Description automatically generated Figure 106: Edit IEC 61131 Application menu options The Edit Application Settings option allows the user to edit overall application settings for this application. If the Edit External References menu option is selected, an IEC 61131 Reference Editor window opens (Figure 107). The user can then drag attributes from a DTM point list or model view and drop them into the reference editor dialog to create variables in the application linked to data points in DTM. The items dragged into the editor will be given default names, however the user can change these names by the corresponding cell in the Name column. The items in the editor list will be used as external variables for the application and will show in the Variables pane. A screenshot of a cell phone Description automatically generated Figure 107: IEC 61131 Reference Editor window To create a structured text file, select the Add Structured Text option from the Application context menu. To edit the content of a structured file, right-click on the file in the tree and choose Edit Structured Text. See Figure 108. A screenshot of a cell phone Description automatically generated Figure 108: Editing Structured Text file Troubleshooting This section provides some hints or guidelines to troubleshoot issues with DTM. DTM Connection Issues The Distributed Test Manager is a distributed system made up of a variety of clients and servers that can be run on a single computer or distributed across many computers. These applications must communicate with each other in order to function properly. To communicate the DTM services use standard TCP sockets over Ethernet, typically on a local area network. The DTM Network View (an example is shown in Figure 14) is used to monitor these processes and connections. If an anomaly is detected in the DTM Network, the network icon in the upper right corner of the DTM Admin main window will be bordered in red and the service(s) that are not properly connected will be highlighted as shown below. Figure 109: Network Errors. Figure 109 shows that the DTHost1 service is not connected (note, DTM Admin needs to be in the user-managed Host Management Mode to see individual hosts). If a whole computer cannot be contacted, it will be shown with a red X mark as well. The following sections attempt to itemize most of the anomalies that can occur with the DTM Network and provide suggestions as to how to resolve them. Note that some of the situations described below may occur simply because a workspace was loaded that contained computers and/or DTHosts that were present when the workspace was saved but are not currently available. In that case the situation can be resolved by simply starting the appropriate services or moving the simulated devices to existing services and deleting the services that no longer exist. 8.1.1 Firewall configuration When DTM services are first run on a given computer, firewall configuration is done by allowing network access for the DTM services. If this does not work, the easiest option is to simply disable the firewall on the network domain in use. For instance, if all the computers are on the private LAN, then the user should disable the firewall for the Private domain. The Windows Firewall can be configured from the Windows Control Panel Windows Firewall application. Third party firewalls will have their own specific configuration tools. If disabling the firewall entirely on the applicable network domain is not an option Inbound and Outbound rules can be defined that allow connections to the DTM processes. Rules will need to be created for the DTMAdmin.exe, DTControl.exe, DTMgr.exe, and DTHost.exe applications. These can all be found in the DTM installation folder. To define rules for the Windows Firewall configuration screen, select Advanced Settings. In the Windows Firewall with Advanced Security screen select Inbound Rules. For each of the applications mentioned above create a new Rule that allows connections to that application. Again, third party firewalls will have their own configuration tools. 8.1.2 Adapter Configuration The adapter configuration is usually done during DTM installation process. An adapter, or more specifically the address, must be the address that other machines on the network will use to communicate with the computer being configured. If the entire DTM Network is run on one computer (one DTM Master and no DTM Slave computers), the Software Loopback Address (127.0.0.1) will work fine as the adapter. If DTM Networks is comprised of multiple computers (one DTM Master and one or more DTM Slaves), verify that each computers Default Adapter should be configured to an adapter/address that all the other computers in the network can access. Frequently, it is desirable, but not required, that the computers in the DTM Network have multiple network adapters, one of which is dedicated to the DTM communication (and other non-SCADA traffic) and the other used for the protocol traffic. For instance, if a computer has a wireless adapter and a hardwire adapter, the wireless adapter can be used as the DTM adapter and the hardwire adapter used for all the devices. This isolates the network traffic of the SCADA system and provides a better estimate of load and response times. Please, verify that addresses for all adapters (hardwire, wireless) are set correctly. 8.1.3 Using DNS When configuring DTM it is generally acceptable to use either host computer name or address when specifying a device. When using the host computer name, it is important to verify that the names will resolve to the same address on all computers that are part of the DTM network. If unsure use the nslookup command to verify or use IP addresses to configure DTM. 8.1.4 Disable Computer Power Save All computers that are part of the DTM Network should have their power save disabled so the computer will not automatically put itself to sleep. Simply running the DTM services on the computer is not enough to keep the computer from shutting itself down. 8.1.5 Resetting DTM The DTM Network was implemented to recover from spontaneous disconnects or outages and recover when the connection is restored. Occasionally it will be desired to reset all the services. The following process will accomplish this. 1) Shut down all copies of the DTMAdmin. 2) On each of the DTM Slave computers. a. Using the DTM Configuration Tool Services tab Stop the DTM Services on that computer. b. Optionally use the Windows Task Manager to verify that no instances of DTHost and DTControl are still running. 3) On the DTM Master computer. a. Using the DTM Configuration Tool Services tab Stop the DTM Services on that computer. b. Optionally use the Windows Task Manager to verify that no instances of DTHost and DTControl are still running. c. Using the DTM Configuration Tool Services tab Start the DTM Services. 4) On each of the DTM Slave Computers use the DTM Configuration Tool Services tab to start the DTM Services. 5) Start DTMAdmin and use the Network View to verify that all computers are running and connected. 8.1.6 Missing DTM Slave Computer The DTM Master computer (daveg-pc in the above network) should always be available and connected because the DTMAdmin requires that before allowing any user actions. DTM Slave computers however must be configured properly and running in order to connect and become part of the DTM Network. If the DTM Services have been installed and configured on a DTM Slave computer and the services are running (either as a Windows Service or a console application) but the computer does not show up at all in the DTMAdmins Network View the most likely cause is that DTMs DTControl service on the DTM Slave is unable to connect and register with the DTControl service on the DTM Master computer. The following steps should be taken to attempt to diagnose and fix the problem. 1) First determine the IP Address and port that the DTM Masters DTControl process is listening on. This can be done by hovering over the DTM Masters computer in the Network View. 2) On the DTM Slave use ping (or some other network tool) to verify that the slave can ping the DTM Master computer. 3) On the DTM Slave use the DTM Configuration Tool to verify that the slave is configured as a DTM Slave, the masters IP address and port number match those found in step 1, and that the DTM Slaves adapter/address is configured to an address that is accessible from the DTM Master. 4) On the DTM Master use ping (or some other network tool) to verify the master can ping the desired DTM Slave computer. 5) Using the Services tab of the DTM Configuration Tool on the DTM Slave click the Stop DTM Services button to stop the services (if they are currently running) and then use the Start DTM Services button to restart them. 6) On the DTM Master look at the Network View to see if the computer has shown up. 7) If not, either disable all firewalls on both computers or add a rule allowing TCP connections to the DTM Services (DTControl.exe, DTMgr.exe, and DTHost.exe) found in the DTM Installations bin folder. 8) Restart the DTM Services on the DTM Slave as described in step 5 above. 9) On the DTM Master look at the Network View to see if the computer has shown up. 8.1.7 Disconnected DTM Slave Computer If a DTM Slave computer is displayed in the Network View but not connected as shown in Figure 109: Network Errors above, it implies that the DTM Slave is able to register with the master but the DTMAdmin is not able to connect to the DTControl process running on the DTM Slave computer. Note that this condition may occur simply because a workspace was loaded that contained this computer, but it is not currently running. If this is the case, simply start the DTM Services on the computer or move any simulated devices to an existing computer and delete this computer if it is no longer needed. If this is not a workspace issue and the DTM Slave computer is currently running the following steps should be taken to attempt to diagnose and fix the problem. 1) Hover over the disconnected DTM Slave computer in the Network View to determine the IP Address and port being used to attempt to connect to the DTControl Service on the DTM Slave. 2) On the DTM Master use ping (or some other network tool) to verify the master can ping the desired DTM Slave computer. 3) On the DTM Slave computer run the DTM Configuration Tool and verify the computer is configured as a DTM Slave and DTM Services adapter is configured to a valid IP address (should be the same one found in step 1 above). 4) Using the DTM Configuration Tool on the DTM Slave stop and restart the DTM services using the Services tab. 5) On the DTM Master look at the Network View to see if the computer is now connected. 8.1.8 Disconnected DTHost If the DTM Master and DTM Slave(s) computers are connected but a DTHost is not connected it means the DTHost is able to register itself with the DTM Network but the DTMAdmin is not able to connect to this DTHost. Note that this condition may occur simply because a workspace was loaded that used to contain this DTHost but the DTHost is not currently running. If this is the case, simply start the DTHost by right clicking on it in the Network View and select Manage Start Host or move any simulated devices to another existing DTHost and delete disconnected DTHost if it is no longer needed. If this is not a workspace issue and the DTHost is currently running the following steps should be taken to attempt to diagnose and fix the problem. 1) Hover over the disconnected DTHost in the Network View to determine the IP Address and port being used to attempt to connect to the DTHost service on the DTM Slave. 2) On the DTM Master use ping (or some other network tool) to verify the master can ping the desired DTM Slave computer. 3) On the DTM Slave computer run the DTM Configuration Tool and verify the computer is configured as a DTM Slave and the DTM Services adapter is configured to a valid IP Address (should be the same one found in step 1 above). 4) Using the DTM Configuration Tool on the DTM Slave stop and restart the DTM Services using the Services tab. 5) On the DTM Master look at the Network View to see if the DTHost is now connected. IEC 61850 Messaging Issues 8.2.1 Troubleshooting GOOSE Messaging Configuring GOOSE messaging can frequently be challenging. If GOOSE messaging does not appear to be working the following procedure may help diagnose the problem. First it is useful to verify whether GOOSE is working correctly with a known workspace. The IEC 61850 GOOSE sample workspace shipped with DTM includes a single GOOSE publisher, an IEC 61850 Server, and two GOOSE subscribers, an IEC 61850 Server and Client. Load this workspace and follow the instructions in the workspace description (displayed in the load workspace dialog or by hovering on the workspace name in the Workspace browser) to test GOOSE. If the GOOSE sample workspace can send and receive GOOSE messages than the problem is most likely a configuration error with the target workspace. Review the GOOSE Configuration section to attempt to isolate the errors. If the error cannot be found, contact Triangle MicroWorks, Inc. for assistance. If the GOOSE sample workspace does not appear to work, verify that a valid GOOSE adapter is specified for all channels. If a default GOOSE adapter was specified in the DTM Configuration Tool (run when DTM was installed or later from the Windows Start menu) the workspace should automatically use this adapter. If no default GOOSE adapter was specified, the user needs to specify one after importing any workspace that uses GOOSE messaging. This is done by editing the IEC 61850 Client and Server channels and specifying the GOOSE Adapter setting (note this setting is found in the Advanced tab for IEC 61850 Clients). If the IEC 61850 GOOSE sample workspace does not work after making sure the correct GOOSE adapter is configured verify that only one application on the computer is configured to receive GOOSE messages. Adapter/implementation limitations restrict GOOSE subscribers to one per computer. Once this subscriber consumes the GOOSE message it is not available to other subscribers, hence they will not receive the message. For example, if another TMW IEC 61850 product such as the TMW Test Suites Hammer or Anvil applications are currently running on the computer they may be subscribed to GOOSE messages which would preclude DTM from receiving them. Similarly, two or more DTHost processes on the same computer would only allow GOOSE messages to be received by one of them. Make sure the DTHost that is subscribed to GOOSE messages is the only process on that computer that is currently subscribed to them. If the GOOSE sample does not work after verifying the GOOSE adapters are correctly specified, the next thing to check would be temporarily disable any firewalls on the computer(s) being used. Frequently firewalls will default to filter all unrecognized or non-standard network packets. The process for disabling firewalls depends on the specific firewall being used. If none of the above works, it is possible that the adapter is not compatible with GOOSE messaging. If possible, try a different adapter or contact Triangle MicroWorks, Inc. for assistance. Appendices Grid View Sorting Whenever sorting is required for a specific column in a grid, one needs to click on the column header. Repeated clicking on the column header alternates between ascending, descending, and no-sort modes. Sometime, multi-sorting is required. In order to sort by more than one column, first, one has to sort by the first desired column as usually by clicking on the column header. Then, to sort by the second desired column, one must click on that column header while simultaneously pressing the SHIFT key button on the keyboard. The sorting priority will be shown by a number in the column header. Clearing multi-sorting is done by clicking on any column header. Multi-Cell Editing Several DTM dialogs which offer editing of multiple DTM nodes at the same time (like Import Substation in section 6.6.7.3, Multi-device Editor in section 6.6.2.3, and Configure Network Adapters in section 6.5.1) offer entering the same value to multiple cells with just a few clicks. For this, multi-select the cells of same kind, then enter a desired value in the Edit Selected Cells box, and finally press the Apply Value to Selected Cells button. A screenshot of a cell phone Description automatically generated Test Manager Actions DTM offers a range of protocol specific actions to use in its Test Action Manager nodes. Refer to section Error! Reference source not found. for more information on the test action node functionality. Gateway Monitored Data Point Test - Controlled Data Point Test - DNP3 Binary Input and Double Bit Binary Input ReadBIIndex Tests mappings to Binary Inputs and Double Bit Binary Inputs when the DNP master is reading indexes. ReadBIClass - Tests mappings to Binary Inputs and Double Bit Binary Inputs when the DNP master is reading events by class. UnsolBI - Tests mappings to Binary Inputs and Double Bit Binary Inputs when the DNP master is expecting unsolicited messages. Binary Output CROB Tests Binary Outputs. If the Binary Output is mapped to a DNP Outstation, then parameters of the command (ontime, offtime, pulse, etc.) can be verified at the destination. Otherwise, the command value is verified for other protocols. Analog Input and Counters ReadAIIndex - Tests mappings to Analog Inputs and Counters when the DNP master is reading indexes. ReadAIClass - Tests mappings to Analog Inputs and Counters when the DNP master is reading events by class. UnsolAI - Tests mappings to Analog Inputs and Counters when the DNP master is expecting unsolicited messages. Analog Output WriteAO Tests Analog Outputs writes. Modbus Read ReadCoils - ReadDiscs - ReadHRegs - ReadIRegs - Write WriteCoils - WriteHRegs - IEC 61850 Controls ControlSelect - ControlOperate - ControlCancel - IEC 61870 Not yet implemented ICCP Not yet implemented 
 